# Unit Testing {#unit}

## Questions {#unit-questions}

```{r questions, child="questions/unit.md"}
```

## Objectives {#unit-objectives}

```{r objectives, child="objectives/unit.md"}
```

## Introduction {#unit-intro}

[Term frequency-inverse document frequency][tf-idf] (TF-IDF)
is a way to determine how relevant a document is in a search.
A term's frequency is how often it occurs in a document *d* divided by the number of words in that document;
inverse document frequency is the ratio of the total number of documents
to the number of documents in which the word occurs:

$$
\tau_w = \frac{n_w(d) / n_*(d)}{D / D_w}
$$

<!-- == noindent -->
If a word is very common in one document, but absent in others, TF-IDF is high for that (word, document) pair.
If a word is common in all documents, or if a word is rare in a document, TF-IDF is lower.
This simple measure can therefore be used to rank documents' relevance with respect to that term.

Calculating TF-IDF depends on counting words in a document...
while ignoring leading/trailing punctuation...
except for words like "Ph.D."
And then there's the question of hyphenation,
which can either signal a multi-part word or a line break,
unless of course it indicates that a multi-part happened to land at the end of a line.
In short,
this relatively simple description can be implemented in many subtly different ways,
many of which are wrong.
In order to tell which one we've actually built and whether we've built it correctly,
we need to write some tests.
This lesson therefore introduces some key ideas and tools.

This material is drawn in part from
[Testing and Continuous Integration with Python][huff-testing].

## What are realistic goals for testing? {#unit-goals}

Any discussion of software and correctness has to start with two ideas.
The first is that no amount of testing can ever prove that a piece of code is correct.
A function that takes three arguments,
each of which can have a hundred different values,
theoretically needs a million tests.
Even if we were to write them,
how would we check that the values we're testing against are correct?
And how would we tell that we had typed in those comparison values correctly,
or that we were using the right equality checks in our code, or---the list goes on and on.

The second big idea is that the real situation is far less dire.
Suppose we want to test a function that returns the sign of a number.
If it works for the number 3,
it will almost certainly work for 4,
and for 5,
and so on.
In fact, there are only a handful of cases that we actually need to test:

| Value    | Expected | Reason                              |
| -------- | -------- | ----------------------------------- |
| `-Inf`   | -1       | Only value of its kind              |
| `Inf`    | 1        | Only value of its kind              |
| `NaN`    | NaN      | Only value of its kind              |
| `-5`     | -1       | Or some other negative number6      |
| `0`      | 0        | The only value that has a sign of 0 |
| `127489` | 1        | Or some other positive number       |

As this table shows,
we can divide test inputs into equivalence classes and check one member of each.
Yes, there's a chance that we'll miss something---that the code we're testing will behave differently
for values we have put in the same class---but
this approach reduces the number of tests we have to write
*and* makes it easier for the next person reading our code to understand what it does.

## What does a systematic software testing framework look like? {#unit-systematic}

A framework for software testing has to:

-   make it easy for people to write tests (because otherwise they won't do it);
-   run a set of tests;
-   report which ones have failed;
-   give some idea of where or why they failed (to help debugging); and
-   give some idea of whether any results have changed since the last run.

Any single test can have one of three results:

-   [success](glossary.html#test-success), meaning that the test passed correctly;
-   [failure](glossary.html#test-failure), meaning that the software being tested didn't do what it was supposed to; or
-   [error](glossary.html#test-error), meaning that the test itself failed (in which case, we don't know anything about the software being tested).

A [unit test](glossary.html#unit-test) is a function that runs some code and produces one of these three results.
The input data to the unit test is called the [fixture](glossary.html#fixture);
we tell if the test passed or not by comparing the [actual output](glossary.html#actual-output) to the [expected output](glossary.html#expected-output).
For example, here's a very badly written version of `numSign` and an equally badly written pair of tests for it:

```r
numSign <- function(x) {
  if (x > 0) {
    1
  } else {
    -1
  }
}

stopifnot(numSign(1) == 1)
stopifnot(numSign(-Inf) == -1)
stopifnot(numSign(NULL) == 0)
```

Here, the fixtures are 1, `NULL`, and `-Inf`,
and the corresponding expected outputs are 1, 0, and -1.
These tests are badly written for two reasons:

1.  Execution halts at the first failed test,
    which means we get less information than we could about the state of the system we're testing.
2.  Each test prints its output to the screen:
    there is no overall summary and no easy way to tell which test produced which result.
    This isn't a problem when there are only three tests,
    but experience shows that if the output isn't comprehensible at a glance,
    developers will stop paying attention to it.

One other mistake that's often made in testing is making tests depend on each other.
Good tests are independent:
they should produce the same results no matter what other tests are run in what order,
so that a failure in an early tests doesn't cause a later test to fail when it should pass
or pass when it should fail.
This means that every test should start from a freshly-generated fixture
rather than using the output of previous tests.

## How can I manage tests systematically? {#unit-pytest}

A [test framework](glossary.html#test-framework) is a library that provides us with functions that help us write tests,
and includes a [test runner](glossary.html#test-runner) that will find tests,
execute them,
and report both individual results that require attention and a summary of overall results.

There are many test frameworks for Python.
One of the most popular is [pytest][pytest],
which structures tests according to three simple rules:

1.  All tests are put in files whose names begin with `test_`.
2.  Each test is a function whose name also begins with `test_`.
3.  Test functions use `assert` to check that results are as expected.

<!-- == noindent -->
For example,
we could test the `count_words` function by putting the following code in `test_count.py`:

<!-- used="unit/tf_idf.py" -->

<!-- src="unit/test_count.py" -->
```python
from tf_idf import count_word

def test_single_word():
    assert count_words('word') == {'word' : 1}

def test_single_repeated_word():
   assert count_words('word word') == {'word' : 2}

def test_two_words():
   assert count_words('another word') == {'another' : 1, 'word' : 1}

def test_trailing_punctuation():
   assert count_words("anothers' word") == {'anothers' : 1, 'word' : 1}
```

The fixture in the last test is the string `"anothers' word"`,
and the expected result is the dictionary `{'anothers' : 1, 'word' : 1}`.
Note that the `assert` statement doesn't include an error message;
pytest will include the name of test function in its output if the test fails,
and that name should be all the documentation we need.
(If the test is so complicated that more is needed,
we should probably write a simpler test.)

We can run our tests from the shell with a single command:

```shell
$ pytest
```

<!-- == noindent -->
As it runs tests,
pytest prints `.` for each one that passes and `F` for each one that fails.
After all of the tests have been run,
it prints a summary of the failures:

```text
============================= test session starts ==============================
platform darwin -- Python 3.6.5, pytest-3.5.1, py-1.5.3, pluggy-0.6.0
rootdir: /Users/pterry/still-magic/src/unit, inifile:
plugins: remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2
collected 4 items

test_count.py ...F                                                       [100%]

=================================== FAILURES ===================================
__________________________ test_trailing_punctuation ___________________________

    def test_trailing_punctuation():
>      assert count_words("anothers' word") == {'anothers' : 1, 'word' : 1}
E      assert {"anothers'": 1, 'word': 1} == {'anothers': 1, 'word': 1}
E        Omitting 1 identical items, use -vv to show
E        Left contains more items:
E        {"anothers'": 1}
E        Right contains more items:
E        {'anothers': 1}
E        Use -v to get the full diff

test_count.py:13: AssertionError
====================== 1 failed, 3 passed in 0.05 seconds ======================
```

pytest searches for all files named `test_*.py` or `*_test.py` in the current directory and its sub-directories.
We can use command-line options to narrow the search:
for example, `pytest test_count.py` runs only the tests in `test_count.py`.
It automatically records and reports pass/fail/error counts and gives us a nicely-formatted report,
but most importantly,
it works the same way for everyone,
so we can test without having think about *how* (only about *what*).
That said,
fitting tests into this framework sometimes requires a few tricks,
which we will explore in the sections that follow.

## How can I tell if my software failed as it was supposed to? {#unit-exception}

Many errors in production systems happen because people don't test their error handling code.
@Yuan2014 found that almost all (92%) of catastrophic system failures
were the result of incorrect handling of non-fatal errors explicitly signalled in software,
and that in 58% of the catastrophic failures,
the underlying faults could easily have been detected through simple testing of error handling code.
Our tests should therefore check that the software fails as it's supposed to
and when it's supposed to;
if it doesn't,
we run the risk of a [silent error](glossary.html#silent-error).

We can check for exceptions manually using this pattern:

<!-- src="unit/test_exception_manual.py" -->
```python
# Expect count_words to raise ValueError for empty input.
def test_text_not_empty():
    try:
        count_words('')
        assert False, 'Should not get this far'
    except ValueError:
        pass
```

<!-- == noindent -->
This code runs the test (i.e., calls `count_words`)
and then fails on purpose if execution reaches the next line.
If the right kind of exception is raised,
on the other hand,
it does nothing.
(We'll take a look in the exercises at how to improve this
to catch the case where the wrong kind of exception is raised.)

This pattern works,
but it violates our first rule of testing:
if writing tests is clumsy,
developers won't do it.
To make life easier,
`pytest` provides a [context manager](glossary.html#context-manager) called `pytest.raises` to handle tests for exceptions.
A context manager creates an object that lives exactly as long as a block of code,
and which can do setup and cleanup actions at the start and end of that block.
We can use `pytest.raises` with Python's `with` keyword
to say that we expect a particular exception to be raised in that code block,
and should report an error if one isn't raised:

<!-- src="unit/test_exception_with.py" -->
```python
import pytest

def test_text_not_empty():
    with pytest.raises(ValueError):
        count_words('')
```
```text
============================= test session starts ==============================
platform darwin -- Python 3.6.5, pytest-3.5.1, py-1.5.3, pluggy-0.6.0
rootdir: /Users/gvwilson/merely-useful/still-magic/src/unit, inifile:
plugins: remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2
collected 1 item

test_exception.py F                                                      [100%]

=================================== FAILURES ===================================
_____________________________ test_text_not_empty ______________________________

    def test_text_not_empty():
        with pytest.raises(ValueError):
>           count_words('')
E           Failed: DID NOT RAISE <class 'ValueError'>

test_exception.py:6: Failed
=========================== 1 failed in 0.04 seconds ===========================
```

<!-- == noindent -->
The output tells us that `count_words` doesn't raise an exception when given an empty string,
so we should either decide that the count in this case is zero,
or go back and fix our function.

## How can I test software that includes randomness? {#unit-random}

Data scientists use a lot of random numbers;
testing code that relies on them makes use of the fact that they aren't actually random.
A [pseudorandom number generator](glossary.html#prng) (PRNG) uses a complex algorithm
to create a stream of values that have all the properties of a truly random sequence.
A key feature of PRNGs is that they can be initialized with a [seed](glossary.html#prng-seed),
and that given the same seed,
the PRNG will always produce the same sequence of values.
If you want your work to be reproducible,
you should always seed your PRNG,
and always record the seed somewhere so that you can re-run exactly the same calculations.

Pseudorandom numbers aren't the only unpredictable things in programs.
If you rely on the current date and time,
that counts as "randomness" as well.
For example,
suppose you want to test that a function correctly counts
the number of dates between the start of the year and the current date.
As time goes by,
the correct answer will change,
so how can you write a reusable function?

The answer is to write a wrapper for the function in question
that either calls the actual function or does what you need for testing,
and then to use that wrapper, and *only* that wrapper,
everywhere in your program.
The example below uses this approach:
if the value `TESTING_DATE` has been set,
`weeks_since_01` returns the number of weeks from `start` to that date,
and if not,
it returns the number of weeks from `start to the current (unpredictable) date:

<!-- src="unit/wrappers.py" -->
```python
import datetime

DAYS_PER_WEEK = 7

TESTING_DATE = None
def weeks_since_01(start):
    current = TESTING_DATE
    if current is None:
        current = datetime.date.today()
    return round((current - start).days / DAYS_PER_WEEK)
```

<!-- == noindent -->
If this code is in a file called `wrappers.py`,
we would use it like this:

<!-- src="unit/demo_test_weeks.py" -->
```python
print('current', wrappers.weeks_since_01(datetime.date(2018, 8, 1)))
wrappers.TESTING_DATE = datetime.date(2018, 8, 30)
print('fixed', wrappers.weeks_since_01(datetime.date(2018, 8, 1)))
```
```text
current 25
fixed 4
```

A cleaner approach is to make the test control an [attribute](glossary.html#function-attribute) of the function.
(Remember,
functions are objects in memory like anything else,
so we can attach other data to them.)
Using this approach allows us to import the function on its own as a self-contained unit,
and avoids making those functions depend on externally-defined (global) variables.
The method works because the body of a function isn't executed as the function is defined,
so it's OK to refer to values that are added afterward.

Here's what defining a wrapper function looks like:

<!-- src="unit/wrappers.py" -->
```python
def weeks_since_02(start):
    current = weeks_since_02.testing_date
    if current is None:
        current = datetime.date.today()
    return round((current - start).days / DAYS_PER_WEEK)
weeks_since_02.testing_date = None
```

<!-- == noindent -->
and here's how we would use it:

<!-- src="unit/demo_test_weeks.py" -->
```python
# demo_test_weeks.py
print('first', weeks_since_02(datetime.date(2018, 8, 1)))
weeks_since_02.testing_date = datetime.date(2018, 8, 30)
print('second', weeks_since_02(datetime.date(2018, 8, 1)))
```
```text
current 25
fixed 4
```

## How can I test software that does I/O? {#unit-io}

A lot of early books on unit testing said that tests shouldn't rely on external files,
both because file I/O was slow and because those files could easily get lost.
Neither stricture applies today:
file I/O is much faster than it was in the 1990s,
and if files of test data are stored in version control,
they're no more likely to be lost than the source code.

That said,
it's often easier to read unit tests if the "files" used as fixtures
are included right next to the tests,
and it's easier to test *output* if the "files" that are created only ever live in memory:
temporary output files can always be cleaned up after tests complete,
but it's one extra burden on test authors.

A good way to avoid all of these problems is to treat strings in memory as if they were files,
which is what Python's `StringIO` module does.
A `StringIO` object has the same methods as a file object,
but reads and writes text in memory instead of bytes on disk:

<!-- src="unit/stringio_example.py" -->
```python
from io import StringIO

writer = StringIO()
for word in 'first second third'.split():
    writer.write('{}\n'.format(word))
print(writer.getvalue())
```
```text
first
second
third
```

`StringIO` objects can also be initialized with some data that a program can read.
(Notice that the lengths reported below include the newline character at the end of each line.)

<!-- src="unit/stringio_example.py" -->
```python
DATA = '''first
second
third'''

for line in StringIO(DATA):
    print(len(line))
```
```text
6
7
5
```

In order to use `StringIO` in tests,
we may need to refactor our code a bit (Chapter \@ref(refactor)).
It's common to have a function open a file,
read its contents,
and return the result like this:

```python
def main(infile, outfile):
    with open(infile, 'r') as reader:
        with open(outfile, 'w') as writer:
            # ...read from reader and write to writer...
```

<!-- == noindent -->
However,
this `main` function is hard to test,
since there's no easy way to substitute a `StringIO` for the file inside that function.
What we can do is reorganize the software so that file opening is done separately from reading and writing.
This is good practice anyway for handling `stdin` and `stdout` in command-line tools,
which don't need to be opened:

```python
def main(infile, outfile):
    if infile == '-':
        reader = stdin
    else:
        reader = open(infile, 'r')
    if outfile == '-':
        writer = stdout
    else:
        writer = open(outfile, 'r')

    process(reader, writer)

    if infile == '-':
        reader.close()
    if outfile == '-':
        writer.close()

def process(reader, writer):
    # ...read from reader and write to writer...
```

<!-- == noindent -->
After moving the reading and writing into `process`,
we can easily pass in a couple of `StringIO` objects for testing.

## How can I tell which parts of my software have (not) been tested? {#unit-coverage}

Take a moment to study the code shown below.
Can you tell which lines are and aren't being executed?

<!-- src="unit/demo_coverage.py" -->
```python
def first(left, right):
    if left < right:
        left, right = right, left
    while left > right:
        value = second(left, right)
        left, right = right, int(right/2)
    return value

def second(check, balance):
    if check > 0:
        return balance
    else:
        return 0

def main():
    final = first(3, 5)
    print(3, 5, final)

if __name__ == '__main__':
    main()
```

<!-- == noindent -->
The answer is probably "no",
but the second half of the answer should be "that's what computers are for".
[Coverage](glossary.html#coverage) measures which parts of program are and are not executed.
In principle,
a coverage tool keep a list of Booleans, one per line, all of which are initialized to `False`.
Each time a line is executed,
the coverage tool sets the corresponding flag to `True`.
After the program finishes,
the tool reports which lines have and have not been executed,
along with summary statistics like the percentage of code executed.

It's easy and wrong to obsess about meeting specific targets for [test coverage](glossary.html#test-coverage).
However,
anything that *isn't* tested should be assumed to be wrong,
and drops in coverage often indicate new [technical debt](glossary.html#technical-debt).

Use `pip install coverage` to install the standard Python coverage tool.
Once you have done that,
use `coverage run filename.py` instead of `python filename.py` to run your program.
This creates a file in the current directory called `.coverage`;
once your program completes,
you can run `coverage report` to get a summary of the most recent report:

```shell
$ coverage run demo_coverage.py
```
```text
Name               Stmts   Miss  Cover
--------------------------------------
demo_coverage.py      16      1    94%
```

If you want the details,
you can use `coverage html` to generate an HTML listing:

FIXME: include unit/coverage.html

A more advanced tool called a [profiler](glossary.html#profiler) can give you even more information;
we will discuss profilers briefly in Section \@ref(tools-profiler).

## Summary {#unit-summary}

One practice we haven't described in this section is [test-driven development](glossary.html#tdd) (TDD).
Rather than writing code and then writing tests,
many developers believe we should write tests first to help us figure out what the code is supposed to do,
and then write just enough code to make those tests pass.
Once the code works,
we should clean it up and commit it,
then move on to the next task.

TDD's advocates claim that writing tests first
focuses people's minds on what code is supposed to
so that they're not subject to confirmation bias when viewing test results.
They also claim that TDD ensures that code actually *is* testable,
and that tests are actually written.
However,
the evidence backing these claims is contradictory:
empirical studies have not found a strong effect @Fucc2016,
and at least one study suggests that it may not be the order of testing and coding that matters,
but whether developers work in short, interleaved bursts @Fucc2017.
Many productive programmers still believe in TDD,
so it's possible that we are measuring the wrong things.

FIXME: create concept map for unit testing

## Exercises {#unit-exercises}

### Handle the wrong kind of exception

-   FIXME: modify explicit exception testing code to handle the wrong kind of exception.

## Key Points {#unit-keypoints}

```{r keypoints, child="keypoints/unit.md"}
```

```{r links, child="etc/links.md"}
```
