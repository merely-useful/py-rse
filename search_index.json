[["index.html", "Research Software Engineering with Python Building software that makes research possible Welcome 0.1 The Big Picture 0.2 Intended Audience 0.3 What You Will Learn 0.4 Using this Book 0.5 Contributing and Re-Use 0.6 Acknowledgments 0.7 Dedications", " Research Software Engineering with Python Building software that makes research possible Damien Irving, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte Wickham, and Greg Wilson 2021-01-12 Welcome It’s still magic even if you know how it’s done. — Terry Pratchett Software is now as essential to research as telescopes, test tubes, and reference libraries. This means that researchers need to know how to build, check, use, and share programs. However, most introductions to programming focus on developing commercial applications, not on exploring problems whose answers aren’t yet known. Our goal is to show you how to do that, both on your own and as part of a team. We believe every researcher should know how to write short programs that clean and analyze data in a reproducible way and how to use version control to keep track of what they have done. But just as some astronomers spend their careers designing telescopes, some researchers focus on building the software that makes research possible. People who do this are called research software engineers; the aim of this book is to get you ready for this role by helping you go from writing code for yourself to creating tools that help your entire field advance. 0.1 The Big Picture Our approach to research software engineering is based on three related concepts: Open science: Making data, methods, and results freely available to all by publishing them under open licenses. Reproducible research: Ensuring that anyone with access to the data and software can feasibly reproduce results, both to check them and to build on them. Sustainable software: The ease with which to maintain and extend it rather than to replace it. Sustainability isn’t just a property of the software: it also depends on the skills and culture of its users. People often conflate these three ideas, but they are distinct. For example, if you share your data and the programs that analyze it, but don’t document what steps to take in what order, your work is open but not reproducible. Conversely, if you completely automate your analysis, but your data is only available to people in your lab, your work is reproducible but not open. Finally, if a software package is being maintained by a couple of post-docs who are being paid a fraction of what they could earn in industry and have no realistic hope of promotion because their field doesn’t value tool building, then sooner or later it will become abandonware, at which point openness and reproducibility become less relevant. Nobody argues that research should be irreproducible or unsustainable, but “not against it” and actively supporting it are very different things. Academia doesn’t yet know how to reward people for writing useful software, so while you may be thanked, the effort you put in may not translate into academic job security or decent pay. Some people worry that if they make their data and code publicly available, someone else will use it and publish a result they could have come up with themselves. This is almost unheard of in practice, but that doesn’t stop it being used as a scare tactic. Other people are afraid of looking foolish or incompetent by sharing code that might contain bugs. This isn’t just impostor syndrome: members of marginalized groups are frequently judged more harshly than others, so being wrong in public is much riskier for them. With this course, we hope to give researchers the tools and knowledge to be better research software developers, to be more efficient in their work, make less mistakes, and work more openly and reproducibly. We hope that by having more researchers with these skills and knowledge, research culture can improve to address the issues raised above. 0.2 Intended Audience This book is written for researchers who are already using Python for their data analysis, but who want to take their coding and software development to the next level. You don’t have to be highly proficient with Python, but you should already be comfortable doing things like reading data from files and writing loops, conditionals, and functions. The following personas are examples of the types of people that are our target audience. Amira Khan completed a master’s in library science five years ago and has since worked for a small aid organization. She did some statistics during her degree, and has learned some R and Python by doing data science courses online, but has no formal training in programming. Amira would like to tidy up the scripts, data sets, and reports she has created in order to share them with her colleagues. These lessons will show her how to do this. Jun Hsu completed an Insight Data Science fellowship last year after doing a PhD in Geology and now works for a company that does forensic audits. He uses a variety of machine learning and visualization packages, and would now like to turn some of his own work into an open source project. This book will show him how such a project should be organized and how to encourage people to contribute to it. Sami Virtanen became a competent programmer during a bachelor’s degree in applied math and was then hired by the university’s research computing center. The kinds of applications they are being asked to support have shifted from fluid dynamics to data analysis; this guide will teach them how to build and run data pipelines so that they can pass those skills on to their users. 0.3 What You Will Learn Rather than simply providing reference material about good coding practices, the book follows Amira and Sami as they work together to write an actual software package to address a real research question. The data analysis task that we focus on relates to a fascinating result in the field of quantitative linguistics. Zipf’s Law states that the second most common word in a body of text appears half as often as the most common, the third most common appears a third as often, and so on. To test whether Zipf’s Law holds for a collection of classic novels that are freely available from Project Gutenberg, we write a software package that counts and analyzes the word frequency distribution in any arbitrary body of text. In the process of writing and publishing a Python package to verify Zipf’s Law, we will show you how to: Organize small and medium-sized data science projects. Use the Unix shell to efficiently manage your data and code. Write Python programs that can be used on the command line. Use Git and GitHub to track and share your work. Work productively in a small team where everyone is welcome. Use Make to automate complex workflows. Enable users to configure your software without modifying it directly. Test your software and know which parts have not yet been tested. Find, handle, and fix errors in your code. Publish your code and research in open and reproducible ways. Create Python packages that can be installed in standard ways. 0.4 Using this Book This book was written to be used as the material for a (potentially) semester long course at the university level, although it can also be used for independent self-study. Participatory live-coding is the anticipated style for teaching the material, rather than lectures simply talking about the code presented (Brown and Wilson 2018; Wilson 2019a). The chapters and their content are generally designed to be used in the order given. Chapters are structured with the introduction at the start, content in the middle, and exercises at the end. Callout boxes are interspersed throughout the content to be used as a supplement to the main text, but not a requirement for the course overall. Early chapters have many small exercises; later chapters have fewer but larger exercises. In order to break up long periods of live-coding while teaching, it may be preferable to stop and complete some of the exercises at key points throughout the chapter, rather than waiting until the end. Possible exercise solutions are provided (Appendix A), in addition to learning objectives (Appendix B) and key points (Appendix C) for each chapter. 0.5 Contributing and Re-Use The source for the book can be found at the py-rse GitHub repository and any corrections, additions, or contributions are very welcome. Everyone whose work is included will be credited in the acknowledgements. Check out our contributing guidelines as well as our Code of Conduct for more information on how to contribute. The content and code of this book can be freely re-used as it is licensed under a Creative Commons Attribution 4.0 International License (CC-BY 4.0) and a MIT License, so the material can be used, re-used, and modified, as long as there is attribution to this source. 0.6 Acknowledgments This book owes its existence to everyone we met through The Carpentries. We are also grateful to Insight Data Science for sponsoring the early stages of this work, to the authors of Noble (2009), Haddock and Dunn (2010), Wilson et al. (2014), Scopatz and Huff (2015), Taschuk and Wilson (2017), Wilson et al. (2017), Brown and Wilson (2018), Devenyi et al. (2018), Sholler et al. (2019), Wilson (2019b) and to everyone who has contributed, including Madeleine Bonsma-Fisher, Jonathan Dursi, Christina Koch, Sara Mahallati, Brandeis Marshall, and Elizabeth Wickes. Many of the explanations and exercises in Chapters 2–4 have been adapted from Software Carpentry’s lesson The Unix Shell. Many of explanations and exercises in Chapters 6 and 7 have been adapted from Software Carpentry’s lesson Version Control with Git and an adaptation/extension of that lesson maintained by the University of Wisconsin-Madison Data Science Hub. Chapter 9 is based on Software Carpentry’s lesson Automation and Make and on Jonathan Dursi’s Introduction to Pattern Rules. Chapter 14 is based in part on Python 102 by Ashwin Srinath. 0.7 Dedications To David Flanders who taught me so much about growing and sustaining coding communities. — Damien To the UofT Coders Group who taught us much more than we taught them. — Luke and Joel To my parents Judy and John who taught me to love books and everything I can learn from them. — Kate To Joshua. — Charlotte To Brent Gorda without whom none of this would have happened. — Greg All royalties from this book are being donated to The Carpentries, an organization that teaches foundational coding and data science skills to researchers worldwide. "],["getting-started.html", "Chapter 1 Getting Started 1.1 Project Structure 1.2 Downloading the Data 1.3 Installing the Software 1.4 Summary 1.5 Exercises 1.6 Key Points", " Chapter 1 Getting Started Everything starts somewhere, though many physicists disagree. — Terry Pratchett As with many research projects, the first step in our Zipf’s Law analysis is to download the research data and install the required software. Before doing that, it’s worth taking a moment to think about how we are going to organize everything. We will soon have a number of books from Project Gutenberg in the form of a series of text files, plots we’ve produced showing the word frequency distribution in each book, as well as the code we’ve written to produce those plots and to document and release our software package. If we aren’t organized from the start, things could get messy later on. 1.1 Project Structure Project organization is like a diet: everyone has one, it’s just a question of whether it’s healthy or not. In the case of a project, “healthy” means that people can find what they need and do what they want without becoming frustrated. This depends on how well organized the project is and how familiar people are with that style of organization. As with good coding style, small pieces in predictable places with readable names are easier to find and use than large chunks that vary from project to project and have names like “stuff”. We can be messy while we are working and then tidy up later, but experience teaches that we will be more productive if we make tidiness a habit. In building the Zipf’s Law project we’ll follow a widely-used template for organizing small and medium-sized data analysis projects (Noble 2009). The project will live in a directory called zipf, which will also be a Git repository stored on GitHub (Chapter 6). The following is an abbreviated version of the project directory tree as it appears towards the end of the book: zipf/ ├── .gitignore ├── CITATION.md ├── CONDUCT.md ├── CONTRIBUTING.md ├── LICENSE.md ├── README.md ├── Makefile ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ └── ... ├── data │ ├── README.md │ ├── dracula.txt │ ├── frankenstein.txt │ └── ... ├── docs │ └── ... ├── results │ ├── collated.csv │ ├── dracula.csv │ ├── dracula.png │ └── ... └── ... The full, final directory tree is documented in Appendix D. 1.1.1 Standard information Our project will contain a few standard files that should be present in every research software project, open source or otherwise: README includes basic information on our project. We’ll create it in Chapter 7, and extend it in Chapter 14. LICENSE is the project’s license. We’ll add it in Section 8.4. CONTRIBUTING explains how to contribute to the project. We’ll add it in Section 8.11. CONDUCT is the project’s Code of Conduct. We’ll add it in Section 8.3. CITATION explains how to cite the software. We’ll add it in Section 14.7. Some projects also include a CONTRIBUTORS or AUTHORS file that lists everyone who has contributed to the project, while others include that information in the README (we do this in Chapter 7) or make it a section in CITATION. These files are often called boilerplate, meaning they are copied without change from one use to the next. 1.1.2 Organizing project content Following Noble (2009), the directories in the repository’s root are organized according to purpose: Runnable programs go in bin/ (an old Unix abbreviation for “binary”, meaning “not text”). This will include both shell scripts, e.g. book_summary.sh developed in Chapter 4, and Python programs, e.g. countwords.py, developed in Chapter 5. Raw data goes in data/ and is never modified after being stored. You’ll set up this directory, and its contents in Section 1.2. Results are put in results/. This includes cleaned-up data, figures, and everything else created using what’s in bin and data. In this project, we’ll describe exactly how bin and data are used with Makefile created in Chapter 9. Finally, documentation and manuscripts go in docs/. In this project docs will contain automatically generated documentation for the Python package, created in Section 14.6.2. This structure works well for many computational research projects and we encourage its use beyond just this book. We will add some more folders and files not directly addressed by Noble (2009) when we talk about testing (Chapter 11), provenance (Chapter 13), and packaging (Chapter 14). 1.2 Downloading the Data The data files used in the book are archived at an online repository called Figshare (which we discuss in detail in Section 13.1.2) and can be accessed at: https://doi.org/10.6084/m9.figshare.13040516 We can download a zip file containing the data files by clicking “download all” at this URL and then unzipping the contents into a new zipf/data directory (also called a folder) that follows the project structure described above. Here’s how things look once we’re done: zipf/ └── data ├── README.md ├── dracula.txt ├── frankenstein.txt ├── jane_eyre.txt ├── moby_dick.txt ├── sense_and_sensibility.txt ├── sherlock_holmes.txt └── time_machine.txt 1.3 Installing the Software In order to conduct our analysis we need to install the following software: A Bash shell Git version control A text editor Python 3 (via the Anaconda distribution) GNU Make Comprehensive software installation instructions for Windows, Mac, and Linux operating systems (with video tutorials) are maintained by The Carpentries as part of their workshop website template at: https://carpentries.github.io/workshop-template/#setup We can follow those instructions to install the Bash shell, Git, a text editor and Anaconda. We recommend Anaconda as the method for installing Python, as it includes Conda as well as many of the packages we’ll use in this book. You can check if Make is already on your computer by typing make -v into the Bash shell. If it is not, you can install it as follows: Linux (Debian/Ubuntu): Install it from the Bash shell using sudo apt-get install make. Mac: Install Xcode (via the App Store). Windows: Follow the installation instructions maintained by the Master of Data Science program at the University of British Columbia. conda in the Shell on Windows If you are using Windows and the conda command isn’t available at the Bash shell, you’ll need to open the Anaconda Prompt program (via the Windows start menu) and run the command conda init bash (this only needs to be done once). After that, your shell will be configured to use conda going forward. Software Versions Throughout the book, we’ll be showing you examples of the output you can expect to see. This output is derived from running a Mac with: Git version 2.29.2, Python version 3.7.6, GNU bash version 3.2.57(1)-release (x86_64-apple-darwin19), GNU Make 3.81, and conda 4.9.2. In some cases, what you see printed to the screen may differ slightly based on software version. We’ll help you understand how to interpret the output so you can keep working and troubleshoot regardless of software version. 1.4 Summary Now that our project structure is set up, our data is downloaded, and our software is installed, we are ready to start our analysis. 1.5 Exercises 1.5.1 Getting ready Make sure you’ve downloaded the required data files (following Section 1.2) and installed the required software (following Section 1.3) before progressing to the next chapter. 1.6 Key Points Make tidiness a habit, rather than cleaning up your project files later. Include a few standard files in all your projects, such as README, LICENSE, CONTRIBUTING, CONDUCT and CITATION. Put runnable code in a bin/ directory. Put raw/original data in a data/ directory and never modify it. Put results in a results/ directory. This includes cleaned-up data and figures (i.e. everything created using what’s in bin and data). Put documentation and manuscripts in a docs/ directory. Refer to The Carpentries software installation guide if you’re having trouble. "],["bash-basics.html", "Chapter 2 The Basics of the Unix Shell 2.1 Exploring Files and Directories 2.2 Moving Around 2.3 Creating New Files and Directories 2.4 Moving Files and Directories 2.5 Copying Files and Directories 2.6 Deleting Files and Directories 2.7 Wildcards 2.8 Reading the Manual 2.9 Summary 2.10 Exercises 2.11 Key Points", " Chapter 2 The Basics of the Unix Shell Ninety percent of most magic merely consists of knowing one extra fact. — Terry Pratchett Computers do four basic things: store data, run programs, talk with each other, and interact with people. They do the interacting in many different ways, of which graphical user interfaces (GUI) are the most widely used. The computer displays icons to show our files and programs, and we tell it to copy or run those by clicking with a mouse. GUIs are easy to learn but hard to automate, and don’t create a record of what we did. In contrast, when we use a command-line interface (CLI) we communicate with the computer by typing commands, and the computer responds by displaying text. CLIs existed long before GUIs; they have survived because they are efficient, easy to automate, and automatically record what we have done. The heart of every CLI is a read-evaluate-print loop (REPL). When we type a command and press Return (also called Enter) the CLI reads the command, evaluates it (i.e., executes it), prints the command’s output, and loops around to wait for another command. If you have used an interactive console for R or Python, you have already used a simple CLI. This lesson introduces another CLI that lets us interact with our computer’s operating system. It is called a “command shell”, or just shell for short, and in essence is a program that runs other programs on our behalf (Figure 2.1). Those “other programs” can do things as simple as telling us the time or as complex as modeling global climate change; as long as they obey a few simple rules, the shell can run them without having to know what language they are written in or how they do what they do. Figure 2.1: The Bash shell. What’s in a Name? Programmers have written many different shells over the last forty years, just as they have created many different text editors and plotting packages. The most popular shell today is called Bash (an acronym of Bourne Again SHell, and a weak pun on the name of its predecessor, the Bourne shell). Other shells may differ from Bash in minor ways, but the core commands and ideas remain the same. In particular, the most recent versions of MacOS use a shell called the Z Shell or zsh; we will point out a few differences as we go along. Please see Section 1.3 for instructions on how to install and launch the shell on your computer. 2.1 Exploring Files and Directories Our first shell commands will let us explore our folders and files, and will also introduce us to several conventions that most Unix tools follow. To start, when Bash runs it presents us with a prompt to indicate that it is waiting for us to type something. This prompt is a simple dollar sign by default: $ However, different shells may use a different symbol: in particular, the zsh shell that is the default on newer version of MacOS uses %. As we’ll see in Section 4.6, we can customize the prompt to give us more information. Don’t Type the Dollar Sign We show the $ prompt so that it’s clear what you are supposed to type, particularly when several commands appear in a row, but you should not type it yourself. Let’s run a command to find out who the shell thinks we are: $ whoami amira Learn by Doing Amira is one of the learners described in Section 0.2. For the rest of the book, we’ll present code and examples from her perspective. You should follow along on your own computer, though what you see might deviate in small ways because of differences in operating system (and because your name probably isn’t Amira). Now that we know who we are, we can explore where we are and what we have. The part of the operating system that manages files and directories (also called folders) is called the filesystem. Some of the most commonly-used commands in the shell create, inspect, rename, and delete files and directories. Let’s start exploring them by running the command pwd, which stands for print working directory. The “print” part of its name is straightforward; the “working directory” part refers to the fact that the shell keeps track of our current working directory at all times. Most commands read and write files in the current working directory unless we tell them to do something else, so knowing where we are before running a command is important. $ pwd /Users/amira Here, the computer’s response is /Users/amira, which tells us that we are in a directory called amira that is contained in a top-level directory called Users. This directory is Amira’s home directory; to understand what that means, we must first understand how the filesystem is organized. On Amira’s computer it looks like Figure 2.2. Figure 2.2: A sample filesystem. At the top is the root directory that holds everything else, which we can refer to using a slash character / on its own. Inside that directory are several other directories, including bin (where some built-in programs are stored), data (for miscellaneous data files), tmp (for temporary files that don’t need to be stored long-term), and Users (where users’ personal directories are located). We know that /Users is stored inside the root directory / because its name begins with /, and that our current working directory /Users/amira is stored inside /Users because /Users is the first part of its name. A name like this is called a path because it tells us how to get from one place in the filesystem (e.g., the root directory) to another (e.g., Amira’s home directory). Slashes The / character means two different things in a path. At the front of a path or on its own, it refers to the root directory. When it appears inside a name, it is a separator. Windows uses backslashes (\\\\) instead of forward slashes as separators. Underneath /Users, we find one directory for each user with an account on this machine. Jun’s files are stored in /Users/jun, Sami’s in /Users/sami, and Amira’s in /Users/amira. This is where the name “home directory” comes from: when we first log in, the shell puts us in the directory that holds our files. Home Directory Variations Our home directory will be in different places on different operating systems. On Linux it may be /home/amira, and on Windows it may be C:\\Documents and Settings\\amira or C:\\Users\\amira (depending on the version of Windows). Our examples show what we would see on MacOS. Now that we know where we are, let’s see what we have using the command ls (short for “listing”), which prints the names of the files and directories in the current directory: $ ls Applications Downloads Music todo.txt Desktop Library Pictures zipf Documents Movies Public Again, our results may be different depending on our operating system and what files or directories we have. We can make the output of ls more informative using the -F option (also sometimes called a switch or a flag). Options are exactly like arguments to a function in R or Python; in this case, -F tells ls to decorate its output to show what things are. A trailing / indicates a directory, while a trailing * tells us something is a runnable program. Depending on our setup, the shell might also use colors to indicate whether each entry is a file or directory. $ ls -F Applications/ Downloads/ Music/ todo.txt Desktop/ Library/ Pictures/ zipf/ Documents/ Movies/ Public/ Here, we can see that almost everything in our home directory is a subdirectory; the only thing that isn’t is a file called todo.txt. Spaces Matter 1+2 and 1 + 2 mean the same thing in mathematics, but ls -F and ls-F are very different things in the shell. The shell splits whatever we type into pieces based on spaces, so if we forget to separate ls and -F with at least one space, the shell will try to find a program called ls-F and (quite sensibly) give an error message like ls-F: command not found. Some options tell a command how to behave, but others tell it what to act on. For example, if we want to see what’s in the /Users directory, we can type: $ ls /Users amira jun sami We often call the file and directory names that we give to commands arguments to distinguish them from the built-in options. We can combine options and arguments: $ ls -F /Users amira/ jun/ sami/ but we must put the options (like -F) before the names of any files or directories we want to work on, because once the command encounters something that isn’t an option it assumes there aren’t any more: $ ls /Users -F ls: -F: No such file or directory amira jun sami Command Line Differences Code can sometimes behave in unexpected ways on different computers, and this applies to the command line as well. For example, the following code actually does work on some Linux operating systems: $ ls /Users -F Some people think this is convenient; others (including us) believe it is confusing, so it’s best to avoid doing this. 2.2 Moving Around Let’s run ls again. Without any arguments, it shows us what’s in our current working directory: $ ls -F Applications/ Downloads/ Music/ todo.txt Desktop/ Library/ Pictures/ zipf/ Documents/ Movies/ Public/ If we want to see what’s in the zipf directory we can ask ls to list its contents: $ ls -F zipf data/ Notice that zipf doesn’t have a leading slash before its name. This absence tells the shell that it is a relative path, i.e., that it identifies something starting from our current working directory. In contrast, a path like /Users/amira is an absolute path: it is always interpreted from the root directory down, so it always refers to the same thing. Using a relative path is like telling someone to go two kilometers north and then half a kilometer east; using an absolute path is like giving them the latitude and longitude of their destination. We can use whichever kind of path is easiest to type, but if we are going to do a lot of work with the data in the zipf directory, the easiest thing would be to change our current working directory so that we don’t have to type zipf over and over again. The command to do this is cd, which stands for change directory. This name is a bit misleading because the command doesn’t change the directory; instead, it changes the shell’s idea of what directory we are in. Let’s try it out: $ cd zipf cd doesn’t print anything. This is normal: many shell commands run silently unless something goes wrong, on the theory that they should only ask for our attention when they need it. To confirm that cd has done what we asked, we can use pwd: $ pwd /Users/amira/zipf $ ls -F data/ Missing Directories and Unknown Options If we give a command an option that it doesn’t understand, it will usually print an error message and (if we’re lucky) tersely remind us of what we should have done: $ cd -j -bash: cd: -j: invalid option cd: usage: cd [-L|-P] [dir] On the other hand, if we get the syntax right but make a mistake in the name of a file or directory, it will tell us that: $ cd whoops -bash: cd: whoops: No such file or directory We now know how to go down the directory tree, but how do we go up? This doesn’t work: $ cd amira cd: amira: No such file or directory because amira on its own is a relative path meaning “a file or directory called amira below our current working directory”. To get back home, we can either use an absolute path: $ cd /Users/amira or a special relative path called .. (two periods in a row with no spaces), which always means “the directory that contains the current one”. The directory that contains the one we are in is called the parent directory, and sure enough, .. gets us there: $ cd .. $ pwd /Users/amira ls usually doesn’t show us this special directory—since it’s always there, displaying it every time would be a distraction. We can ask ls to include it using the -a option, which stands for “all”. Remembering that we are now in /Users/amira: $ ls -F -a ./ Documents/ Music/ zipf/ ../ Downloads/ Pictures/ Applications/ Library/ Public/ Desktop/ Movies/ todo.txt The output also shows another special directory called . (a single period), which refers to the current working directory. It may seem redundant to have a name for it, but we’ll see some uses for it soon. Combining Options You’ll occasionally need to use multiple options in the same command. In most command line tools, multiple options can be combined with a single - and no spaces between the options: $ ls -Fa This command is synonymous with the previous example. While you may see commands written like this, we don’t recommend you use this approach in your own work. This is because some commands take long options with multi-letter names, and it’s very easy to mistake --no (meaning “answer ‘no’ to all questions”) with -no (meaning -n -o). The special names . and .. don’t belong to cd: they mean the same thing to every program. For example, if we are in /Users/amira/zipf, then ls .. will display a listing of /Users/amira. When the meanings of the parts are the same no matter how they’re combined, programmers say they are orthogonal. Orthogonal systems tend to be easier for people to learn because there are fewer special cases to remember. Other Hidden Files In addition to the hidden directories .. and ., we may also comes across files with names like .jupyter or .Rhistory. These usually contain settings or other data for particular programs; the prefix . is used to prevent ls from cluttering up the output when we run ls. We can always use the -a option to display them. cd is a simple command, but it allows us to explore several new ideas. First, several .. can be joined by the path separator to move higher than the parent directory in a single step. For example, cd ../.. will move us up two directories (e.g., from /Users/amira/zipf to /Users), while cd ../Movies will move us up from zipf and back down into Movies. What happens if we type cd on its own without giving a directory? $ pwd /Users/amira/Movies $ cd $ pwd /Users/amira No matter where we are, cd on its own always returns us to our home directory. We can achieve the same thing using the special directory name ~, which is a shortcut for our home directory: $ ls ~ Applications Downloads Music todo.txt Desktop Library Pictures zipf Documents Movies Public (ls doesn’t show any trailing slashes here because we haven’t used -F.) We can use ~ in paths, so that (for example) ~/Downloads always refers to our download directory. Finally, cd interprets the shortcut - (a single dash) to mean the last directory we were in. Using this is usually faster and more reliable than trying to remember and type the path, but unlike ~, it only works with cd: ls - tries to print a listing of a directory called - rather than showing us the contents of our previous directory. 2.3 Creating New Files and Directories We now know how to explore files and directories, but how do we create them? To find out, let’s go back to our zipf directory: $ cd ~/zipf $ ls -F data/ To create a new directory, we use the command mkdir (short for make directory): $ mkdir docs Since docs is a relative path (i.e., does not have a leading slash) the new directory is created below the current working directory: $ ls -F data/ docs/ Using the shell to create a directory is no different than using a graphical tool. If we look at the current directory with our computer’s file browser we will see the docs directory there too. The shell and the file explorer are two different ways of interacting with the files; the files and directories themselves are the same. Naming Files and Directories Complicated names of files and directories can make our life painful. Following a few simple rules can save a lot of headaches: Don’t use spaces. Spaces can make a name easier to read, but since they are used to separate arguments on the command line, most shell commands interpret a name like My Thesis as two names My and Thesis. Use - or _ instead, e.g, My-Thesis or My_Thesis. Don’t begin the name with - (dash) to avoid confusion with command options like -F. Stick with letters, digits, . (period or ‘full stop’), - (dash) and _ (underscore). Many other characters mean special things in the shell. We will learn about some of these during this lesson, but these are always safe. If we need to refer to files or directories that have spaces or other special characters in their names, we can surround the name in quotes (\"\"). For example, ls \"My Thesis\" will work where ls My Thesis does not. Since we just created the docs directory, ls doesn’t display anything when we ask for a listing of its contents: $ ls -F docs Let’s change our working directory to docs using cd, then use a very simple text editor called Nano to create a file called draft.txt (Figure 2.3): $ cd docs $ nano draft.txt Figure 2.3: The Nano editor. When we say “Nano is a text editor” we really do mean “text”: it can only work with plain character data, not spreadsheets, images, Microsoft Word files, or anything else invented after 1970. We use it in this lesson because it runs everywhere, and because it is as simple as something can be and still be called an editor. However, that last trait means that we shouldn’t use it for larger tasks like writing a program or a paper. Recycling Pixels Unlike most modern editors, Nano runs inside the shell window instead of opening a new window of its own. This is a holdover from an era when graphical terminals were a rarity and different applications had to share a single screen. Once Nano is open we can type in a few lines of text, then press Ctrl+O (the Control key and the letter ‘O’ at the same time) to save our work. Nano will ask us what file we want to save it to; press Return to accept the suggested default of draft.txt. Once our file is saved, we can use Ctrl+X to exit the editor and return to the shell. Control, Ctrl, or ^ Key The Control key, also called the “Ctrl” key, can be described in a bewildering variety of ways. For example, Control plus X may be written as: Control-X Control+X Ctrl-X Ctrl+X C-x ^X When Nano runs it displays some help in the bottom two lines of the screen using the last of these notations: for example, ^G Get Help means “use Ctrl+G to get help” and ^O WriteOut means “use Ctrl+O to write out the current file”. Nano doesn’t leave any output on the screen after it exits, but ls will show that we have indeed created a new file draft.txt: $ ls draft.txt Dot Something All of Amira’s files are named “something dot something”. This is just a convention: we can call a file mythesis or almost anything else. However, both people and programs use two-part names to help them tell different kinds of files apart. The part of the filename after the dot is called the filename extension and indicates what type of data the file holds: .txt for plain text, .pdf for a PDF document, .png for a PNG image, and so on. This is just a convention: saving a PNG image of a whale as whale.mp3 doesn’t somehow magically turn it into a recording of whalesong, though it might cause the operating system to try to open it with a music player when someone double-clicks it. 2.4 Moving Files and Directories Let’s go back to our zipf directory: cd ~/zipf The docs directory contains a file called draft.txt. That isn’t a particularly informative name, so let’s change it using mv (short for move): $ mv docs/draft.txt docs/prior-work.txt The first argument tells mv what we are “moving”, while the second is where it’s to go. “Moving” docs/draft.txt to docs/prior-work.txt has the same effect as renaming the file: $ ls docs prior-work.txt We must be careful when specifying the destination because mv will overwrite existing files without warning. An option -i (for “interactive”) makes mv ask us for confirmation before overwriting. mv also works on directories, so mv analysis first-paper would rename the directory without changing its contents. Now suppose we want to move prior-work.txt into the current working directory. If we don’t want to change the file’s name, just its location, we can provide mv with a directory as a destination and it will move the file there. In this case, the directory we want is the special name . that we mentioned earlier: $ mv docs/prior-work.txt . ls now shows us that docs is empty: $ ls docs and that our current directory now contains our file: $ ls data/ docs/ prior-work.txt If we only want to check that the file exists, we can give its name to ls just like we can give the name of a directory: $ ls prior-work.txt prior-work.txt 2.5 Copying Files and Directories The cp command copies files. It works like mv except it creates a file instead of moving an existing one: $ cp prior-work.txt docs/section-1.txt We can check that cp did the right thing by giving ls two arguments to ask it to list two things at once: $ ls prior-work.txt docs/section-1.txt docs/section-1.txt prior-work.txt Notice that ls shows the output in alphabetical order. If we leave off the second filename and ask it to show us a file and a directory (or multiple directories) it lists them one by one: $ ls prior-work.txt docs prior-work.txt docs: section-1.txt Copying a directory and everything it contains is a little more complicated. If we use cp on its own, we get an error message: $ cp docs backup cp: analysis is a directory (not copied). If we really want to copy everything, we must give cp the -r option (meaning recursive): $ cp -r docs backup Once again we can check the result with ls: $ ls docs backup docs/: section-1.txt backup/: section-1.txt Copying Files to and from Remote Computers For many researchers, a motivation for learning how to use the shell is that it’s often the only way to connect to a remote computer (e.g. located at a supercomputing facility or in a university department). Similar to the cp command, there exists a secure copy (scp) command for copying files between computers. See Appendix E for details, including how to set up a secure connection to a remote computer via the shell. 2.6 Deleting Files and Directories Let’s tidy up by removing the prior-work.txt file we created in our zipf directory. The command to do this is rm (for remove): $ rm prior-work.txt We can confirm the file is gone using ls: $ ls prior-work.txt ls: prior-work.txt: No such file or directory Deleting is forever: unlike most GUIs, the Unix shell doesn’t have a trash bin that we can recover deleted files from. Tools for finding and recovering deleted files do exist, but there is no guarantee they will work, since the computer may recycle the file’s disk space at any time. In most cases, when we delete a file it really is gone. In a half-hearted attempt to stop us from erasing things accidentally, rm refuses to delete directories: $ rm docs rm: docs: is a directory We can tell rm we really want to do this by giving it the recursive option -r: $ rm -r docs rm -r should be used with great caution: in most cases, it’s safest to add the -i option (for interactive) to get rm to ask us to confirm each deletion. As a halfway measure, we can use -v (for verbose) to get rm to print a message for each file it deletes. This option works the same way with mv and cp. 2.7 Wildcards zipf/data contains the text files for several ebooks from Project Gutenberg: $ ls data README.md moby_dick.txt dracula.txt sense_and_sensibility.txt frankenstein.txt sherlock_holmes.txt jane_eyre.txt time_machine.txt The wc command (short for word count) tells us how many lines, words, and letters there are in one file: $ wc data/moby_dick.txt 22331 215832 1276222 data/moby_dick.txt What’s in a Word? wc only considers spaces to be word breaks: if two words are connected by a long dash—like “dash” and “like” in this sentence—then wc will count them as one word. We could run wc more times to find out how many lines there are in the other files, but that would be a lot of typing and we could easily make a mistake. We can’t just give wc the name of the directory as we do with ls: $ wc data wc: data: read: Is a directory Instead, we can use wildcards to specify a set of files at once. The most commonly-used wildcard is * (a single asterisk). It matches zero or more characters, so data/*.txt matches all of the text files in the data directory: $ ls data/*.txt data/dracula.txt data/sense_and_sensibility.txt data/frankenstein.txt data/sherlock_holmes.txt data/jane_eyre.txt data/time_machine.txt data/moby_dick.txt while data/s*.txt only matches the two whose names begin with an ‘s’: $ ls data/s*.txt data/sense_and_sensibility.txt data/sherlock_holmes.txt Wildcards are expanded to match filenames before commands are run, so they work exactly the same way for every command. This means that we can use them with wc to (for example) count the number of words in the books with names that contains an underscore: $ wc data/*_*.txt 21054 188460 1049294 data/jane_eyre.txt 22331 215832 1253891 data/moby_dick.txt 13028 121593 693116 data/sense_and_sensibility.txt 13053 107536 581903 data/sherlock_holmes.txt 3582 35527 200928 data/time_machine.txt 73048 668948 3779132 total or the number of words in Frankenstein: $ wc data/frank*.txt 7832 78100 442967 data/frankenstein.txt The exercises will introduce and explore other wildcards. For now, we only need to know that it’s possible for a wildcard expression to not match anything. In this case, the command will usually print an error message: $ wc data/*.csv wc: data/*.csv: open: No such file or directory 2.8 Reading the Manual wc displays lines, words, and characters by default, but we can ask it to display only the number of lines: $ wc -l data/s*.txt 13028 sense_and_sensibility.txt 13053 sherlock_holmes.txt 26081 total wc has other options as well. We can use the man command (short for manual) to find out what they are: $ man wc Paging Through the Manual If our screen is too small to display an entire manual page at once, the shell will use a paging program called less to show it piece by piece. We can use ↑ and ↓ to move line-by-line or Ctrl+Spacebar and Spacebar to skip up and down one page at a time. (B and F also work.) To search for a character or word, use / followed by the character or word to search for. If the search produces multiple hits, we can move between them using N (for “next”). To quit, press Q. Manual pages contain a lot of information—often more than we really want. Figure 2.3 includes excerpts from the manual on your screen, and highlights a few of features useful for beginners. Figure 2.4: Key features of Unix manual pages. Some commands have a --help option that provides a succinct summary of possibilities, but the best place to go for help these days is probably the TLDR website. The acronym stands for “too long, didn’t read”, and its help for wc displays this: wc Count words, bytes, or lines. Count lines in file: wc -l {{file}} Count words in file: wc -w {{file}} Count characters (bytes) in file: wc -c {{file}} Count characters in file (taking multi-byte character sets into account): wc -m {{file}} edit this page on github As the last line suggests, all of its examples are in a public GitHub repository so that users like you can add the examples you wish it had. For more information, we can search on Stack Overflow or browse the GNU manuals (particularly those for the core GNU utilities, which include many of the commands introduced in this lesson). In all cases, though, we need to have some idea of what we’re looking for in the first place: someone who wants to know how many lines there are in a data file is unlikely to think to look for wc. 2.9 Summary The original Unix shell is celebrating its fiftieth anniversary. Its commands may be cryptic, but few programs have remained in daily use for so long. The next chapter will explore how we can combine and repeat commands in order to create powerful, efficient workflows. 2.10 Exercises The exercises below involve creating and moving new files, as well as considering hypothetical files. Please note that if you create or move any files or directories in your Zipf’s Law project, you may want to reorganize your files following the outline at the beginning of the next chapter. If you accidentally delete necessary files, you can start with a fresh copy of the data files by following the instructions in Section 1.2. 2.10.1 Exploring more ls flags What does the command ls do when used with the -l option? What happens if you use two options at the same time, such as ls -l -h? 2.10.2 Listing recursively and by time The command ls -R lists the contents of directories recursively, which means the subdirectories, sub-subdirectories, and so on at each level are listed. The command ls -t lists things by time of last change, with most recently changed files or directories first. In what order does ls -R -t display things? Hint: ls -l uses a long listing format to view timestamps. 2.10.3 Absolute and relative paths Starting from /Users/amira/data, which of the following commands could Amira use to navigate to her home directory, which is /Users/amira? cd . cd / cd /home/amira cd ../.. cd ~ cd home cd ~/data/.. cd cd .. cd ../. 2.10.4 Relative path resolution Using the filesystem shown in Figure 2.5, if pwd displays /Users/sami, what will ls -F ../backup display? ../backup: No such file or directory final original revised final/ original/ revised/ data/ analysis/ doc/ Figure 2.5: Filesystem for exercises. 2.10.5 ls reading comprehension Using the filesystem shown in Figure 2.5, if pwd displays /Users/backup, and -r tells ls to display things in reverse order, what command(s) will result in the following output: doc/ data/ analysis/ ls pwd ls -r -F ls -r -F /Users/backup 2.10.6 Creating files a different way What happens when you execute touch my_file.txt? (Hint: use ls -l to find information about the file) When might you want to create a file this way? 2.10.7 Using rm safely What would happen if you executed rm -i my_file.txt on the file created in the previous exercise? Why would we want this protection when using rm? 2.10.8 Moving to the current folder After running the following commands, Amira realizes that she put the (hypothetical) files chapter1.txt and chapter2.txt into the wrong folder: $ ls -F data/ docs/ $ ls -F data README.md frankenstein.txt sherlock_holmes.txt chapter1.txt jane_eyre.txt time_machine.txt chapter2.txt moby_dick.txt dracula.txt sense_and_sensibility.txt $ cd docs Fill in the blanks to move these files to the current folder (i.e., the one she is currently in): $ mv ___/chapter1.txt ___/chapter2.txt ___ 2.10.9 Renaming files Suppose that you created a plain-text file in your current directory to contain a list of the statistical tests you will need to do to analyze your data, and named it: statstics.txt After creating and saving this file you realize you misspelled the filename! You want to correct the mistake, which of the following commands could you use to do so? cp statstics.txt statistics.txt mv statstics.txt statistics.txt mv statstics.txt . cp statstics.txt . 2.10.10 Moving and copying Assuming the following hypothetical files, what is the output of the closing ls command in the sequence shown below? $ pwd /Users/amira/data $ ls books.dat $ mkdir doc $ mv books.dat doc/ $ cp doc/books.dat ../books-saved.dat $ ls books-saved.dat doc doc books.dat doc books-saved.dat 2.10.11 Copy with multiple filenames This exercise explores how cp responds when attempting to copy multiple things. What does cp do when given several filenames followed by a directory name? $ mkdir backup $ cp dracula.txt frankenstein.txt backup/ What does cp do when given three or more file names? $ cp dracula.txt frankenstein.txt jane_eyre.txt 2.10.12 List filenames matching a pattern When run in the data directory of your project directory, which ls command(s) will produce this output? jane_eyre.txt sense_and_sensibility.txt ls ??n*.txt ls *e_*.txt ls *n*.txt ls *n?e*.txt 2.10.13 Organizing directories and files Amira is working on a project and she sees that her files aren’t very well organized: $ ls -F books.txt data/ results/ titles.txt The books.txt and titles.txt files contain output from her data analysis. What command(s) does she need to run to produce the output shown? $ ls -F data/ results/ $ ls results books.txt titles.txt 2.10.14 Reproduce a directory structure You’re starting a new analysis, and would like to duplicate the directory structure from your previous experiment so you can add new data. Assume that the previous experiment is in a folder called 2016-05-18, which contains a data folder that in turn contains folders named raw and processed that contain data files. The goal is to copy the folder structure of 2016-05-18/data into a folder called 2016-05-20 so that your final directory structure looks like this: 2016-05-20/ └── data ├── processed └── raw Which of the following set of commands would achieve this objective? What would the other commands do? # Set 1 $ mkdir 2016-05-20 $ mkdir 2016-05-20/data $ mkdir 2016-05-20/data/processed $ mkdir 2016-05-20/data/raw # Set 2 $ mkdir 2016-05-20 $ cd 2016-05-20 $ mkdir data $ cd data $ mkdir raw processed # Set 3 $ mkdir 2016-05-20/data/raw $ mkdir 2016-05-20/data/processed # Set 4 $ mkdir 2016-05-20 $ cd 2016-05-20 $ mkdir data $ mkdir raw processed 2.10.15 Wildcard expressions Wildcard expressions can be very complex, but you can sometimes write them in ways that only use simple syntax, at the expense of being a bit more verbose. In your data/ directory, the wildcard expression [st]*.txt matches all files beginning with s or t and ending with .txt. Imagine you forgot about this. Can you match the same set of files with basic wildcard expressions that do not use the [] syntax? Hint: You may need more than one expression. Under what circumstances would your new expression produce an error message where the original one would not? 2.10.16 Removing unneeded files Suppose you want to delete your processed data files, and only keep your raw files and processing script to save storage. The raw files end in .txt and the processed files end in .csv. Which of the following would remove all the processed data files, and only the processed data files? rm ?.csv rm *.csv rm * .csv rm *.* 2.10.17 Other wildcards The shell provides several wildcards beyond the widely-used *. To explore them, explain in plain language what (hypothetical) files the expression novel-????-[ab]*.{txt,pdf} matches and why. 2.11 Key Points A shell is a program that reads commands and runs other programs. The filesystem manages information stored on disk. Information is stored in files, which are located in directories (folders). Directories can also store other directories, which forms a directory tree. pwd prints the user’s current working directory. / on its own is the root directory of the whole filesystem. ls prints a list of files and directories. An absolute path specifies a location from the root of the filesystem. A relative path specifies a location in the filesystem starting from the current directory. cd changes the current working directory. .. means the parent directory. . on its own means the current directory. mkdir creates a new directory. cp copies a file. rm removes (deletes) a file. mv moves (renames) a file or directory. * matches zero or more characters in a filename. ? matches any single character in a filename. wc counts lines, words, and characters in its inputs. man displays the manual page for a given command; some commands also have a --help option. "],["bash-tools.html", "Chapter 3 Building Tools with the Unix Shell 3.1 Combining Commands 3.2 How Pipes Work 3.3 Repeating Commands on Many Files 3.4 Variable Names 3.5 Redoing Things 3.6 Creating New Filenames Automatically 3.7 Summary 3.8 Exercises 3.9 Key Points", " Chapter 3 Building Tools with the Unix Shell Wisdom comes from experience. Experience is often a result of lack of wisdom. — Terry Pratchett The shell’s greatest strength is that it lets us combine programs to create pipelines that can handle large volumes of data. This lesson shows how to do that, and how to repeat commands to process as many files as we want automatically. We’ll be continuing to work in the zipf project, which after the previous chapter should contain the following files: zipf/ └── data ├── README.md ├── dracula.txt ├── frankenstein.txt ├── jane_eyre.txt ├── moby_dick.txt ├── sense_and_sensibility.txt ├── sherlock_holmes.txt └── time_machine.txt 3.1 Combining Commands To see how the shell lets us combine commands, let’s go into the zipf/data directory and count the number of lines in each file once again: $ cd ~/zipf/data $ wc -l *.txt 15975 dracula.txt 7832 frankenstein.txt 21054 jane_eyre.txt 22331 moby_dick.txt 13028 sense_and_sensibility.txt 13053 sherlock_holmes.txt 3582 time_machine.txt 96855 total Which of these books is shortest? We can check by eye when there are only 16 files, but what if there were eight thousand? Our first step toward a solution is to run this command: $ wc -l *.txt &gt; lengths.txt The greater-than symbol &gt; tells the shell to redirect the command’s output to a file instead of printing it. Nothing appears on the screen; instead, everything that would have appeared has gone into the file lengths.txt. The shell creates this file if it doesn’t exist, or overwrites it if it already exists. We can print the contents of lengths.txt using cat, which is short for concatenate (because if we give it the names of several files it will print them all in order): $ cat lengths.txt 15975 dracula.txt 7832 frankenstein.txt 21054 jane_eyre.txt 22331 moby_dick.txt 13028 sense_and_sensibility.txt 13053 sherlock_holmes.txt 3582 time_machine.txt 96855 total We can now use sort to sort the lines in this file: $ sort lengths.txt -n 3582 time_machine.txt 7832 frankenstein.txt 13028 sense_and_sensibility.txt 13053 sherlock_holmes.txt 15975 dracula.txt 21054 jane_eyre.txt 22331 moby_dick.txt 96855 total Just to be safe, we use sort’s -n option to specify that we want to sort numerically. Without it, sort would order things alphabetically so that 10 would come before 2. sort does not change lengths.txt. Instead, it sends its output to the screen just as wc did. We can therefore put the sorted list of lines in another temporary file called sorted-lengths.txt using &gt; once again: $ sort lengths.txt &gt; sorted-lengths.txt Redirecting to the Same File It’s tempting to send the output of sort back to the file it reads: $ sort -n lengths.txt &gt; lengths.txt However, all this does is wipe out the contents of lengths.txt. The reason is that when the shell sees the redirection, it opens the file on the right of the &gt; for writing, which erases anything that file contained. It then runs sort, which finds itself reading from a newly-empty file. Creating intermediate files with names like lengths.txt and sorted-lengths.txt works, but keeping track of those files and cleaning them up when they’re no longer needed is a burden. Let’s delete the two files we just created: rm lengths.txt sorted-lengths.txt We can produce the same result more safely and with less typing using a pipe: $ wc -l *.txt | sort -n 3582 time_machine.txt 7832 frankenstein.txt 13028 sense_and_sensibility.txt 13053 sherlock_holmes.txt 15975 dracula.txt 21054 jane_eyre.txt 22331 moby_dick.txt 96855 total The vertical bar | between the wc and sort commands tells the shell that we want to use the output of the command on the left as the input to the command on the right. Running a command with a file as input has a clear flow of information: the command performs a task on that file and prints the output to the screen (Figure 3.1a). When using pipes, however, the information flows differently after the first (upstream) command. The downstream command doesn’t read from a file. Instead, it reads the output of the upstream command (Figure 3.1b). Figure 3.1: Piping commands. We can use | to build pipes of any length. For example, we can use the command head to get just the first three lines of sorted data, which shows us the three shortest books: $ wc -l *.txt | sort -n | head -n 3 3582 time_machine.txt 7832 frankenstein.txt 13028 sense_and_sensibility.txt Options Can Have Values When we write head -n 3, the value 3 is not input to head. Instead, it is associated with the option -n. Many options take values like this, such as the names of input files or the background color to use in a plot. Some versions of head may allow you to use head -3 as a shortcut, though this can be confusing if other options are included. We could always redirect the output to a file by adding &gt; shortest.txt to the end of the pipeline, thereby retaining our answer for later reference. In practice, most Unix users would create this pipeline step by step, just as we have: by starting with a single command and adding others one by one, checking the output after each change. The shell makes this easy by letting us move up and down in our command history with the ↑ and ↓ keys. We can also edit old commands to create new ones, so a very common sequence is: Run a command and check its output. Use ↑ to bring it up again. Add the pipe symbol | and another command to the end of the line. Run the pipe and check its output. Use ↑ to bring it up again. And so on. 3.2 How Pipes Work In order to use pipes and redirection effectively, we need to know a little about how they work. When a computer runs a program—any program—it creates a process in memory to hold the program’s instructions and data. Every process in Unix has an input channel called standard input and an output channel called standard output. (By now you may be surprised that their names are so memorable, but don’t worry: most Unix programmers call them “stdin” and “stdout”, which are pronounced “stuh-Din” and “stuh-Dout”). The shell is a program like any other, and like any other, it runs inside a process. Under normal circumstances its standard input is connected to our keyboard and its standard output to our screen, so it reads what we type and displays its output for us to see (Figure 3.2a). When we tell the shell to run a program it creates a new process and temporarily reconnects the keyboard and stream to that process’s standard input and output (Figure 3.2b). Figure 3.2: Standard I/O. If we provide one or more files for the command to read, as with sort lengths.txt, the program reads data from those files. If we don’t provide any filenames, though, the Unix convention is for the program to read from standard input. We can test this by running sort on its own, typing in a few lines of text, and then pressing Ctrl+D to signal the end of input . sort will then sort and print whatever we typed: $ sort one two three four ^D four one three two Redirection with &gt; tells the shell to connect the program’s standard output to a file instead of the screen (Figure 3.2c). When we create a pipe like wc *.txt | sort, the shell creates one process for each command so that wc and sort will run simultaneously, and then connects the standard output of wc directly to the standard input of sort (Figure 3.2d). wc doesn’t know whether its output is going to the screen, another program, or to a file via &gt;. Equally, sort doesn’t know if its input is coming from the keyboard or another process; it just knows that it has to read, sort, and print. Why Isn’t It Doing Anything? What happens if a command is supposed to process a file but we don’t give it a filename? For example, what if we type: $ wc -l but don’t type *.txt (or anything else) after the command? Since wc doesn’t have any filenames, it assumes it is supposed to read from the keyboard, so it waits for us to type in some data. It doesn’t tell us this: it just sits and waits. This mistake can be hard to spot, particularly if we put the filename at the end of the pipeline: $ wc -l | sort moby_dick.txt In this case, sort ignores standard input and reads the data in the file, but wc still just sits there waiting for input. If we make this mistake, we can end the program by typing Ctrl+C. We can also use this to interrupt programs that are taking a long time to run or are trying to connect to a website that isn’t responding. Just as we can redirect standard output with &gt;, we can connect standard input to a file using &lt;. In the case of a single file, this has the same effect as providing the file’s name to the command: $ wc &lt; moby_dick.txt 22331 215832 1276222 If we try to use redirection with a wildcard, though, the shell doesn’t concatenate all of the matching files: $ wc &lt; *.txt -bash: *.txt: ambiguous redirect It also doesn’t print the error message to standard output, which we can prove by redirecting: $ wc &lt; *.txt &gt; all.txt -bash: *.txt: ambiguous redirect $ cat all.txt cat: all.txt: No such file or directory Instead, every process has a second output channel called standard error (or stderr). Programs use it for error messages so that their attempts to tell us something has gone wrong don’t vanish silently into an output file. There are ways to redirect standard error, but doing so is almost always a bad idea. 3.3 Repeating Commands on Many Files A loop is a way to repeat a set of commands for each item in a list. We can use them to build complex workflows out of simple pieces, and (like wildcards) they reduce the typing we have to do and the number of mistakes we might make. Let’s suppose that we want to take a section out of each book whose name starts with the letter “s” in the data directory. More specifically, suppose we want to get the first 8 lines of each book after the 9 lines of license information that appear at the start of the file. If we only cared about one file, we could write a pipeline to take the first 17 lines and then take the last 8 of those: $ head -n 17 sense_and_sensibility.txt | tail -n 8 Title: Sense and Sensibility Author: Jane Austen Editor: Release Date: May 25, 2008 [EBook #161] Posting Date: Last updated: February 11, 2015 Language: English If we try to use a wildcard to select files, we only get 8 lines of output, not the 16 we expect: $ head -n 17 s*.txt | tail -n 8 Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Editor: Release Date: April 18, 2011 [EBook #1661] Posting Date: November 29, 2002 Latest Update: Language: English The problem is that head is producing a single stream of output containing 17 lines for each file (along with a header telling us the file’s name): $ head -n 17 s*.txt ==&gt; sense_and_sensibility.txt &lt;== The Project Gutenberg EBook of Sense and Sensibility, by Jane Austen This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: Sense and Sensibility Author: Jane Austen Editor: Release Date: May 25, 2008 [EBook #161] Posting Date: Last updated: February 11, 2015 Language: English ==&gt; sherlock_holmes.txt &lt;== Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Editor: Release Date: April 18, 2011 [EBook #1661] Posting Date: November 29, 2002 Latest Update: Language: English Let’s try this instead: $ for filename in sense_and_sensibility.txt sherlock_holmes.txt &gt; do &gt; head -n 17 $filename | tail -n 8 &gt; done Title: Sense and Sensibility Author: Jane Austen Editor: Release Date: May 25, 2008 [EBook #161] Posting Date: Last updated: February 11, 2015 Language: English Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Editor: Release Date: April 18, 2011 [EBook #1661] Posting Date: November 29, 2002 Latest Update: Language: English As the output shows, the loop runs our pipeline once for each file. There is a lot going on here, so we will break it down into pieces: The keywords for, in, do, and done create the loop, and must always appear in that order. filename is a variable just like a variable in R or Python. At any moment it contains a value, but that value can change over time. The loop runs once for each item in the list. Each time it runs, it assigns the next item to the variable. In this case filename will be sense_and_sensibility.txt the first time around the loop and sherlock_holmes.txt the second time. The commands that the loop executes are called the body of the loop and appear between the keywords do and done. Those commands use the current value of the variable filename, but to get it, we must put a dollar sign $ in front of the variable’s name. If we forget and use filename instead of $filename, the shell will think that we are referring to a file that is actually called filename. The shell prompt changes from $ to a continuation prompt &gt; as we type in our loop to remind us that we haven’t finished typing a complete command yet. We don’t type the &gt;, just as we don’t type the $. The continuation prompt &gt; has nothing to do with redirection; it’s used because there are only so many punctuation symbols available. Continuation Prompts May Differ Too As mentioned in Chapter 2, there is variation in how different shells look and operate. If you noticed the second, third, and fourth code lines in your for loop were prefaced with for, it’s not because you did something wrong! That difference is one of the ways in which zsh differs from bash. It is very common to use a wildcard to select a set of files and then loop over that set to run commands: $ for filename in s*.txt &gt; do &gt; head -n 17 $filename | tail -n 8 &gt; done Title: Sense and Sensibility Author: Jane Austen Editor: Release Date: May 25, 2008 [EBook #161] Posting Date: Last updated: February 11, 2015 Language: English Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Editor: Release Date: April 18, 2011 [EBook #1661] Posting Date: November 29, 2002 Latest Update: Language: English 3.4 Variable Names We should always choose meaningful names for variables, but we should remember that those names don’t mean anything to the computer. For example, we have called our loop variable filename to make its purpose clear to human readers, but we could equally well write our loop as: $ for x in s*.txt &gt; do &gt; head -n 17 $x | tail -n 8 &gt; done or as: $ for username in s*.txt &gt; do &gt; head -n 17 $username | tail -n 8 &gt; done Don’t do this. Programs are only useful if people can understand them, so meaningless names like x and misleading names like username increase the odds of misunderstanding. 3.5 Redoing Things Loops are useful if we know in advance what we want to repeat, but we can also repeat commands that we have run recently. One way is to use ↑ and ↓ to go up and down in our command history as described earlier. Another is to use history to get a list of the last few hundred commands we have run: $ history 551 wc -l *.txt | sort -n 552 wc -l *.txt | sort -n | head -n 3 553 wc -l *.txt | sort -n | head -n 1 &gt; shortest.txt We can use an exclamation mark ! followed by a number to repeat a recent command: $ !552 wc -l *.txt | sort -n | head -n 3 3582 time_machine.txt 7832 frankenstein.txt 13028 sense_and_sensibility.txt The shell prints the command it is going to re-run to standard error before executing it, so that (for example) !572 &gt; results.txt puts the command’s output in a file without also writing the command to the file. Having an accurate record of the things we have done and a simple way to repeat them are two of the main reasons people use the Unix shell. In fact, being able to repeat history is such a powerful idea that the shell gives us several ways to do it: !head re-runs the most recent command starting with head, while !wc re-runs the most recent starting with wc. If we type Ctrl+R (for reverse search) the shell searches backward through its history for whatever we type next. If we don’t like the first thing it finds, we can type Ctrl+R again to go further back. If we use history, ↑, or Ctrl+R we will quickly notice that loops don’t have to be broken across lines. Instead, their parts can be separated with semi-colons: $ for filename in s*.txt ; do head -n 17 $filename | tail -n 8; done This is fairly readable, though it becomes more challenging if our for loop includes multiple commands. For example, we may choose to include the echo command, which prints its arguments to the screen, so we can keep track of progress or for debugging. Compare this: $ for filename in s*.txt &gt; do &gt; echo $filename &gt; head -n 17 $filename | tail -n 8 &gt; done with this: $ for filename in s*.txt; do echo $filename; head -n 17 $filename | tail -n 8; done Even experienced users have a tendency to (incorrectly) put the semi-colon after do instead of before it. If our loop contains multiple commands, though, the multi-line format is much easier to read and troubleshoot. Note that (depending on the size of your shell window) the format separated by semi-colons may be printed onto more than one line, as shown in the previous code example. You can tell whether code entered into your shell is intended to be run as a single line based on the prompt: both the original command prompt ($) and the continuation prompt (&gt;) indicate the code is on separate lines; the absence of either in shell commands indicates it is a single line of code. 3.6 Creating New Filenames Automatically Suppose we want to create a backup copy of each book whose name ends in “e”. If we don’t want to change the files’ names, we can do this with cp: $ cd ~/zipf $ mkdir backup $ cp data/*e.txt backup $ ls backup jane_eyre.txt time_machine.txt Warnings If you attempt to re-execute the code chunk above, you’ll end up with an error after the second line: mkdir: backup: File exists This warning isn’t necessarily a cause for alarm. It lets you know that the command couldn’t be completed, but will not prevent you from proceeding. But what if we want to append the extension .bak to the files’ names? cp can do this for a single file: $ cp data/time_machine.txt backup/time_machine.txt.bak but not for all the files at once: $ cp data/*e.txt backup/*e.txt.bak cp: target &#39;backup/*e.txt.bak&#39; is not a directory backup/*e.txt.bak doesn’t match anything—those files don’t yet exist—so after the shell expands the * wildcards, what we are actually asking cp to do is: $ cp data/jane_eyre.txt data/time_machine.txt backup/*e.bak This doesn’t work because cp only understands how to do two things: copy a single file to create another file, or copy a bunch of files into a directory. If we give it more than two names as arguments, it expects the last one to be a directory. Since backup/*e.bak is not, cp reports an error. Instead, let’s use a loop to copy files to the backup directory and append the .bak suffix: $ cd data $ for filename in *e.txt &gt; do &gt; cp $filename ../backup/$filename.bak &gt; done $ ls ../backup jane_eyre.txt.bak time_machine.txt.bak 3.7 Summary The shell’s greatest strength is the way it combines a few powerful ideas with pipes and loops. The next chapter will show how we can make our work more reproducible by saving commands in files that we can run over and over again. 3.8 Exercises The exercises below involve creating and moving new files, as well as considering hypothetical files. Please note that if you create or move any files or directories in your Zipf’s Law project, you may want to reorganize your files following the outline at the beginning of the next chapter. If you accidentally delete necessary files, you can start with a fresh copy of the data files by following the instructions in Section 1.2. 3.8.1 What does &gt;&gt; mean? We have seen the use of &gt;, but there is a similar operator &gt;&gt; which works slightly differently. We’ll learn about the differences between these two operators by printing some strings. We can use the echo command to print strings e.g. $ echo The echo command prints text The echo command prints text Now test the commands below to reveal the difference between the two operators: $ echo hello &gt; testfile01.txt and: $ echo hello &gt;&gt; testfile02.txt Hint: Try executing each command twice in a row and then examining the output files. 3.8.2 Appending data Given the following commands, what will be included in the file extracted.txt: $ head -n 3 dracula.txt &gt; extracted.txt $ tail -n 2 dracula.txt &gt;&gt; extracted.txt The first three lines of dracula.txt The last two lines of dracula.txt The first three lines and the last two lines of dracula.txt The second and third lines of dracula.txt 3.8.3 Piping commands In our current directory, we want to find the 3 files which have the least number of lines. Which command listed below would work? wc -l * &gt; sort -n &gt; head -n 3 wc -l * | sort -n | head -n 1-3 wc -l * | head -n 3 | sort -n wc -l * | sort -n | head -n 3 3.8.4 Why does uniq only remove adjacent duplicates? The command uniq removes adjacent duplicated lines from its input. Consider a hypothetical file genres.txt containing the following data: science fiction fantasy science fiction fantasy science fiction science fiction Running the command uniq genres.txt produces: science fiction fantasy science fiction fantasy science fiction Why do you think uniq only removes adjacent duplicated lines? (Hint: think about very large datasets.) What other command could you combine with it in a pipe to remove all duplicated lines? 3.8.5 Pipe reading comprehension A file called titles.txt contains a list of book titles and publication years: Dracula,1897 Frankenstein,1818 Jane Eyre,1847 Moby Dick,1851 Sense and Sensibility,1811 The Adventures of Sherlock Holmes,1892 The Invisible Man,1897 The Time Machine,1895 Wuthering Heights,1847 What text passes through each of the pipes and the final redirect in the pipeline below? $ cat titles.txt | head -n 5 | tail -n 3 | sort -r &gt; final.txt Hint: build the pipeline up one command at a time to test your understanding 3.8.6 Pipe construction For the file titles.txt from the previous exercise, consider the following command: $ cut -d , -f 2 titles.txt What does the cut command (and its options) accomplish? 3.8.7 Which pipe? Consider the same titles.txt from the previous exercises. The uniq command has a -c option which gives a count of the number of times a line occurs in its input. If titles.txt was in your working directory, what command would you use to produce a table that shows the total count of each publication year in the file? sort titles.txt | uniq -c sort -t, -k2,2 titles.txt | uniq -c cut -d, -f 2 titles.txt | uniq -c cut -d, -f 2 titles.txt | sort | uniq -c cut -d, -f 2 titles.txt | sort | uniq -c | wc -l 3.8.8 Doing a dry run A loop is a way to do many things at once—or to make many mistakes at once if it does the wrong thing. One way to check what a loop would do is to echo the commands it would run instead of actually running them. Suppose we want to preview the commands the following loop will execute without actually running those commands (analyze is a hypothetical command): $ for file in *.txt &gt; do &gt; analyze $file &gt; analyzed-$file &gt; done What is the difference between the two loops below, and which one would we want to run? (Note: The lines starting with # are not code, and are included for labelling purposes only; we’ll discuss this more in the next chapter) # Version 1 $ for file in *.txt &gt; do &gt; echo analyze $file &gt; analyzed-$file &gt; done # Version 2 $ for file in *.txt &gt; do &gt; echo &quot;analyze $file &gt; analyzed-$file&quot; &gt; done 3.8.9 Variables in loops Given the files in data/, what is the output of the following code? # Version 1 $ for datafile in *.txt &gt; do &gt; ls *.txt &gt; done Now, what is the output of the following code? # Version 2 $ for datafile in *.txt &gt; do &gt; ls $datafile &gt; done Why do these two loops give different outputs? 3.8.10 Limiting sets of files What would be the output of running the following loop in your data/ directory? # Version 1 $ for filename in d* &gt; do &gt; ls $filename &gt; done How would the output differ from using this command instead? # Version 2 $ for filename in *d* &gt; do &gt; ls $filename &gt; done 3.8.11 Saving to a file in a loop Consider running the following loop in the data/ directory: # Version 1 for book in *.txt &gt; do &gt; echo $book &gt; head -n 16 $book &gt; headers.txt &gt; done Why would the following loop be preferable? # Version 2 for book in *.txt &gt; do &gt; head -n 16 $book &gt;&gt; headers.txt &gt; done 3.8.12 Why does history record commands before running them? If you run the command: $ history | tail -n 5 &gt; recent.sh the last command in the file is the history command itself, i.e., the shell has added history to the command log before actually running it. In fact, the shell always adds commands to the log before running them. Why do you think it does this? 3.9 Key Points cat displays the contents of its inputs. head displays the first few lines of its input. tail displays the last few lines of its input. sort sorts its inputs. Use the up-arrow key to scroll up through previous commands to edit and repeat them. Use history to display recent commands and !number to repeat a command by number. Every process in Unix has an input channel called standard input and an output channel called standard output. &gt; redirects a command’s output to a file, overwriting any existing content. &gt;&gt; appends a command’s output to a file. &lt; operator redirects input to a command. A pipe | sends the output of the command on the left to the input of the command on the right. A for loop repeats commands once for every thing in a list. Every for loop must have a variable to refer to the thing it is currently operating on and a body containing commands to execute. Use $name or ${name} to get the value of a variable. "],["bash-advanced.html", "Chapter 4 Going Further with the Unix Shell 4.1 Creating New Commands 4.2 Making Scripts More Versatile 4.3 Turning Interactive Work into a Script 4.4 Finding Things in Files 4.5 Finding Files 4.6 Configuring the Shell 4.7 Summary 4.8 Exercises 4.9 Key Points", " Chapter 4 Going Further with the Unix Shell There isn’t a way things should be. There’s just what happens, and what we do. — Terry Pratchett The previous chapters explained how we can use the command line to do all of the things we can do with a GUI, and how to combine commands in new ways using pipes and redirection. This chapter extends those ideas to show how we can create new tools by saving commands in files and how to use a more powerful version of wildcards to extract data from files. We’ll be continuing to work in the zipf project, which after the previous chapter should contain the following files: zipf/ └── data ├── README.md ├── dracula.txt ├── frankenstein.txt ├── jane_eyre.txt ├── moby_dick.txt ├── sense_and_sensibility.txt ├── sherlock_holmes.txt └── time_machine.txt Deleting Extra Files You may have additional files if you worked through all of the exercises in the previous chapter. Feel free to delete them or move them to a separate directory. If you have accidentally deleted files you need, you can download them again by following the instructions in Section 1.2. 4.1 Creating New Commands Loops let us run the same commands many times, but we can go further and save commands in files so that we can repeat complex operations with a few keystrokes. For historical reasons a file full of shell commands is usually called a shell script, but it is really just another kind of program. Let’s start by creating a new directory for our runnable programs called bin/, consistent with the project structure described in Section 1.1.2. $ cd ~/zipf $ mkdir bin $ cd bin Edit a new file called book_summary.sh to hold our shell script: $ nano book_summary.sh and insert this line: head -n 17 ../data/moby_dick.txt | tail -n 8 Note that we do not put the $ prompt at the front of the line. We have been showing that to highlight interactive commands, but in this case we are putting the command in a file rather than running it immediately. Empty Line at the End of a File? You’ll often see scripts from many languages that end in an empty line. What you are seeing, though, is the last line of code ending in a newline character. This indicates to the computer that the code has ended. While this newline character is not required for shell scripts to work, and sometimes isn’t shown by coding tools, it does make it easier to view and modify scripts. When you are copying code from this book, remember to add an empty line at the end! Once we have added this line, we can save the file with Ctrl+O and exit with Ctrl+X. ls shows that our file now exists: $ ls book_summary.sh We can check the contents of the file using cat book_summary.sh. More importantly, we can now ask the shell to run this file: $ bash book_summary.sh Title: Moby Dick or The Whale Author: Herman Melville Editor: Release Date: December 25, 2008 [EBook #2701] Posting Date: Last Updated: December 3, 2017 Language: English Sure enough, our script’s output is exactly the same text we would get if we ran the command directly. If we want, we can pipe the output of our shell script to another command to count how many lines it contains: $ bash book_summary.sh | wc -l 8 What if we want our script to print the name of the book’s author? The command grep finds and prints lines that match a pattern. We’ll learn more about grep in Section 4.4, but for now we can edit the script: $ nano book_summary.sh and add a search for the word “Author”: head -n 17 ../data/moby_dick.txt | tail -n 8 | grep Author Sure enough, when we run our modified script: $ bash book_summary.sh we get the line we want: Author: Herman Melville And once again we can pipe the output of our script into other commands just as we would pipe the output from any other program. Here, we count the number of words in the author line: $ bash book_summary.sh | wc -w 3 4.2 Making Scripts More Versatile Getting the name of the author for only one of the books isn’t particularly useful. What we really want is a way to get the name of the author from any of our files. Let’s edit book_summary.sh again and replace ../data/moby_dick.txt with a special variable $1. Once our change is made, book_summary.sh should contain: head -n 17 $1 | tail -n 8 | grep Author Inside a shell script, $1 means “the first argument on the command line”. If we now run our script like this: $ bash book_summary.sh ../data/moby_dick.txt then $1 is assigned ../data/moby_dick.txt and we get exactly the same output as before. If we give the script a different filename: $ bash book_summary.sh ../data/frankenstein.txt we get the name of the author of that book instead: Author: Mary Wollstonecraft (Godwin) Shelley Our small script is now doing something useful, but it may take the next person who reads it a moment to figure out exactly what. We can improve our script by adding comments at the top: # Get author information from a Project Gutenberg eBook. # Usage: bash book_summary.sh /path/to/file.txt head -n 17 $1 | tail -n 8 | grep Author As in R and Python, a comment starts with a # character and runs to the end of the line. The computer ignores comments, but they help people (including our future self) understand and use what we’ve created. Let’s make one more change to our script. Instead of always extracting the author name, let’s have it select whatever information the user specified: # Get desired information from a Project Gutenberg eBook. # Usage: bash book_summary.sh /path/to/file.txt what_to_look_for head -n 17 $1 | tail -n 8 | grep $2 The change is very small: we have replaced the fixed string ‘Author’ with a reference to the special variable $2, which is assigned the value of the second command-line argument we give the script when we run it. Update Your Comments As you update the code in your script, don’t forget to update the comments that describe the code. A description that sends readers in the wrong direction is worse than none at all, so do your best to avoid this common oversight. Let’s check that it works by asking for Frankenstein’s release date: $ bash book_summary.sh ../data/frankenstein.txt Release Release Date: June 17, 2008 [EBook #84] 4.3 Turning Interactive Work into a Script Suppose we have just run a series of commands that did something useful, such as summarizing all books in a given directory: $ for x in ../data/*.txt &gt; do &gt; echo $x &gt; bash book_summary.sh $x Author &gt; done &gt; authors.txt $ for x in ../data/*.txt &gt; do &gt; echo $x &gt; bash book_summary.sh $x Release &gt; done &gt; releases.txt $ ls authors.txt book_summary.sh releases.txt $ mkdir ../results $ mv authors.txt releases.txt ../results Instead of typing those commands into a file in an editor (and potentially getting them wrong) we can use history and redirection to save recent commands to a file. For example, we can save the last six commands to summarize_all_books.sh: $ history 6 &gt; summarize_all_books.sh $ cat summarize_all_books.sh 297 for x in ../data/*.txt; do echo $x; bash book_summary.sh $x Author; done &gt; authors.txt 298 for x in ../data/*.txt; do echo $x; bash book_summary.sh $x Release; done &gt; releases.txt 299 ls 300 mkdir ../results 301 mv authors.txt releases.txt ../results 302 history 6 &gt; summarize_all_books.sh We can now open the file in an editor, remove the serial numbers at the start of each line, and delete the lines we don’t want to create a script that captures exactly what we actually did. This is how we usually develop shell scripts: run commands interactively a few times to make sure they are doing the right thing, then save our recent history to a file and turn that into a reusable script. 4.4 Finding Things in Files We can use head and tail to select lines from a file by position, but we also often want to select lines that contain certain values. This is called filtering, and we usually do it in the shell with the command grep that we briefly met in Section 4.1. Its name is an acronym of “global regular expression print”, which was a common sequence of operations in early Unix text editors. To show how grep works, we will use our sleuthing skills to explore data/sherlock_holmes.txt. First, let’s find lines that contain the word “Sherlock”. Since there are likely to be hundreds of matches, we will pipe grep’s output to head to show only the first few: $ cd ~/zipf $ grep Sherlock data/sherlock_holmes.txt | head -n 5 Here, Sherlock is our (very simple) pattern. grep searches the file line by line and shows those lines that contain matches, so the output is: Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyle Title: The Adventures of Sherlock Holmes To Sherlock Holmes she is always THE woman. I have seldom heard as I had pictured it from Sherlock Holmes&#39; succinct description, &quot;Good-night, Mister Sherlock Holmes.&quot; If we run grep sherlock instead we get no output, since grep patterns are case-sensitive. If we wanted to make the search case-insensitive, we can add the option -i: $ grep -i sherlock data/sherlock_holmes.txt | head -n 5 Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyle Title: The Adventures of Sherlock Holmes *** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES *** THE ADVENTURES OF SHERLOCK HOLMES To Sherlock Holmes she is always THE woman. I have seldom heard This output is different from our previous output because of the lines containing “SHERLOCK” near the top of the file. Next, let’s search for the pattern on: $ grep on data/sherlock_holmes.txt | head -n 5 Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or with this eBook or online at www.gutenberg.net Author: Arthur Conan Doyle In each of these lines, our pattern (“on”) is part of a larger word such as “Conan”. To restrict matching to lines containing on by itself, we can give grep the -w option (for “match words”): $ grep -w on data/sherlock_holmes.txt | head -n 5 One night--it was on the twentieth of March, 1888--I was put on seven and a half pounds since I saw you.&quot; that I had a country walk on Thursday and came home in a dreadful &quot;It is simplicity itself,&quot; said he; &quot;my eyes tell me that on the on the right side of his top-hat to show where he has secreted What if we want to search for a phrase rather than a single word? $ grep on the data/sherlock_holmes.txt | head -n 5 grep: the: No such file or directory data/sherlock_holmes.txt:Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyle data/sherlock_holmes.txt:This eBook is for the use of anyone anywhere at no cost and with data/sherlock_holmes.txt:almost no restrictions whatsoever. You may copy it, give it away or data/sherlock_holmes.txt:with this eBook or online at www.gutenberg.net data/sherlock_holmes.txt:Author: Arthur Conan Doyle In this case, grep uses on as the pattern and tries to find it in files called the and data/sherlock_holmes.txt. It then tells us that the file the cannot be found, but prints data/sherlock_holmes.txt as a prefix to each other line of output to tell us which file those lines came from. If we want to give grep both words as a single argument, we must wrap them in quotation marks as before: $ grep &quot;on the&quot; data/sherlock_holmes.txt | head -n 5 One night--it was on the twentieth of March, 1888--I was drug-created dreams and was hot upon the scent of some new &quot;It is simplicity itself,&quot; said he; &quot;my eyes tell me that on the on the right side of his top-hat to show where he has secreted pink-tinted note-paper which had been lying open upon the table. Quoting Quotation marks aren’t specific to grep: the shell interprets them before running commands, just as it expands wildcards to create filenames no matter what command those filenames are being passed to. This allows us to do things like head -n 5 \"My Thesis.txt\" to get lines from a file that has a space in its name. It is also why many programmers write \"$variable\" instead of just $variable when creating loops or shell scripts: if there’s any chance at all that the variable’s value will contain spaces, it’s safest to put it in quotes. One of the most useful options for grep is -n, which numbers the lines that match the search: $ grep -n &quot;on the&quot; data/sherlock_holmes.txt | head -n 5 105:One night--it was on the twentieth of March, 1888--I was 118:drug-created dreams and was hot upon the scent of some new 155:&quot;It is simplicity itself,&quot; said he; &quot;my eyes tell me that on the 165:on the right side of his top-hat to show where he has secreted 198:pink-tinted note-paper which had been lying open upon the table. grep has many options—so many, in fact, that almost every letter of the alphabet means something to it: $ man grep GREP(1) BSD General Commands Manual GREP(1) NAME grep, egrep, fgrep, zgrep, zegrep, zfgrep -- file pattern searcher SYNOPSIS grep [-abcdDEFGHhIiJLlmnOopqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color[=when]] [--colour[=when]] [--context[=num]] [--label] [--line-buffered] [--null] [pattern] [file ...] ...more... We can combine options to grep as we do with other Unix commands. For example, we can combine two options we’ve covered previously with -v to invert the match—i.e., to print lines that don’t match the pattern: $ grep -i -n -v the data/sherlock_holmes.txt | head -n 5 2: 4:almost no restrictions whatsoever. You may copy it, give it away or 6:with this eBook or online at www.gutenberg.net 7: 8: As we learned in Section 2.2, we can write this command as grep -inv, but probably shouldn’t for the sake of readability. If we want to search several files at once, all we have to do is give grep all of their names. The easiest way to do this is usually to use wildcards. For example, this command counts how many lines contain “pain” in all of our books: $ grep -w pain data/*.txt | wc -l 122 Alternatively, the -r option (for “recursive”) tells grep to search all of the files in or below a directory: $ grep -w -r pain data | wc -l 122 grep becomes even more powerful when we start using regular expressions, which are sets of letters, numbers, and symbols that define complex patterns. For example, this command finds lines that start with the letter ‘T’: $ grep -E &quot;^T&quot; data/sherlock_holmes.txt | head -n 5 This eBook is for the use of anyone anywhere at no cost and with Title: The Adventures of Sherlock Holmes THE ADVENTURES OF SHERLOCK HOLMES To Sherlock Holmes she is always THE woman. I have seldom heard The distinction is clear. For example, you have frequently seen The -E option tells grep to interpret the pattern as a regular expression, rather than searching for an actual circumflex followed by an upper-case ‘T’. The quotation marks prevent the shell from treating special characters in the pattern as wildcards, and the ^ means that a line only matches if it begins with the search term—in this case, T. Many tools support regular expressions: we can use them in programming languages, database queries, online search engines, and most text editors (though not Nano—its creators wanted to keep it as small as possible). A detailed guide of regular expressions is outside the scope of this book, but a wide range of tutorials are available online, and Goyvaerts and Levithan (2012) is a useful companion if you need to go further. 4.5 Finding Files While grep finds things in files, the find command finds files themselves. It also has a lot of options, but unlike most Unix commands they are written as full words rather than single-letter abbreviations. To show how it works, we will use the entire contents of our zipf directory, including files we created earlier in this chapter: zipf/ ├── bin │ ├── book_summary.sh │ ├── summarize_all_books.sh ├── data │ ├── README.md │ ├── dracula.txt │ ├── frankenstein.txt │ ├── jane_eyre.txt │ ├── moby_dick.txt │ ├── sense_and_sensibility.txt │ ├── sherlock_holmes.txt │ └── time_machine.txt └── results ├── authors.txt └── releases.txt For our first command, let’s run find . to find and list everything in this directory. As always, . on its own means the current working directory, which is where we want our search to start. $ cd ~/zipf $ find . . ./bin ./bin/summarize_all_books.sh ./bin/book_summary.sh ./results ./results/releases.txt ./results/authors.txt ./data ./data/moby_dick.txt ./data/sense_and_sensibility.txt ./data/sherlock_holmes.txt ./data/time_machine.txt ./data/frankenstein.txt ./data/README.md ./data/dracula.txt ./data/jane_eyre.txt If we only want to find directories, we can tell find to show us things of type d: $ find . -type d . ./bin ./results ./data If we change -type d to -type f we get a listing of all the files instead: $ find . -type f ./bin/summarize_all_books.sh ./bin/book_summary.sh ./results/releases.txt ./results/authors.txt ./data/moby_dick.txt ./data/sense_and_sensibility.txt ./data/sherlock_holmes.txt ./data/time_machine.txt ./data/frankenstein.txt ./data/README.md ./data/dracula.txt ./data/jane_eyre.txt Now let’s try matching by name: $ find . -name &quot;*.txt&quot; ./results/releases.txt ./results/authors.txt ./data/moby_dick.txt ./data/sense_and_sensibility.txt ./data/sherlock_holmes.txt ./data/time_machine.txt ./data/frankenstein.txt ./data/dracula.txt ./data/jane_eyre.txt Notice the quotes around \"*.txt\". If we omit them and type: $ find . -name *.txt then the shell tries to expand the * wildcard in *.txt before running find. Since there aren’t any text files in the current directory, the expanded list is empty, so the shell tries to run the equivalent of $ find . -name and gives us the error message: find: -name: requires additional arguments We have seen before how to combine commands using pipes. Let’s use another technique to see how large our books are: $ wc -l $(find . -name &quot;*.txt&quot;) 14 ./results/releases.txt 14 ./results/authors.txt 22331 ./data/moby_dick.txt 13028 ./data/sense_and_sensibility.txt 13053 ./data/sherlock_holmes.txt 3582 ./data/time_machine.txt 7832 ./data/frankenstein.txt 15975 ./data/dracula.txt 21054 ./data/jane_eyre.txt 96883 total When the shell executes our command, it runs whatever is inside $(...) and then replaces $(...) with that command’s output. Since the output of find is the paths to our text files, the shell constructs the command: $ wc -l ./results/releases.txt ... ./data/jane_eyre.txt (We are using ... in place of seven files’ names in order to fit things neatly on the printed page.) This results in the output as seen above. It is exactly like expanding the wildcard in *.txt, but more flexible. We will often use find and grep together. The first command finds files whose names match a pattern, while the second looks for lines inside those files that match another pattern. For example, we can look for Authors in all our text files: $ grep &quot;Author:&quot; $(find . -name &quot;*.txt&quot;) ./results/authors.txt:Author: Bram Stoker ./results/authors.txt:Author: Mary Wollstonecraft (Godwin) Shelley ./results/authors.txt:Author: Charlotte Bronte ./results/authors.txt:Author: Herman Melville ./results/authors.txt:Author: Jane Austen ./results/authors.txt:Author: Arthur Conan Doyle ./results/authors.txt:Author: H. G. Wells ./data/moby_dick.txt:Author: Herman Melville ./data/sense_and_sensibility.txt:Author: Jane Austen ./data/sherlock_holmes.txt:Author: Arthur Conan Doyle ./data/time_machine.txt:Author: H. G. Wells ./data/frankenstein.txt:Author: Mary Wollstonecraft (Godwin) Shelley ./data/dracula.txt:Author: Bram Stoker ./data/jane_eyre.txt:Author: Charlotte Bronte We can also use $(...) expansion to create a list of filenames to use in a loop: $ for file in $(find . -name &quot;*.txt&quot;) &gt; do &gt; cp $file $file.bak &gt; done $ find . -name &quot;*.bak&quot; ./results/releases.txt.bak ./results/authors.txt.bak ./data/frankenstein.txt.bak ./data/sense_and_sensibility.txt.bak ./data/dracula.txt.bak ./data/time_machine.txt.bak ./data/moby_dick.txt.bak ./data/jane_eyre.txt.bak ./data/sherlock_holmes.txt.bak 4.6 Configuring the Shell As Section 3.3 explained, the shell is a program, and like any other program it has variables. Some of those variables control the shell’s operations; by changing their values we can change how the shell and other programs behave. Let’s run the command set and look at some of the variables the shell defines: $ set COMPUTERNAME=TURING HOME=/Users/amira HOMEDRIVE=C: HOSTNAME=TURING HOSTTYPE=i686 NUMBER_OF_PROCESSORS=4 OS=Windows_NT PATH=/Users/amira/anaconda3/bin:/usr/bin: /bin:/usr/sbin:/sbin:/usr/local/bin PWD=/Users/amira UID=1000 USERNAME=amira ... There are many more than are shown here—roughly a hundred in our current shell session. And yes, using set to show things might seem a little strange, even for Unix, but if we don’t give it any arguments, the command might as well show us things we could set. By convention, shell variables that are always present have upper-case names. All shell variables’ values are strings, even those (such as UID) that look like numbers. It’s up to programs to convert these strings to other types when necessary. For example, if a program wanted to find out how many processors the computer had, it would convert the value of NUMBER_OF_PROCESSORS from a string to an integer. Similarly, some variables (like PATH) store lists of values. In this case, the convention is to use a colon ‘:’ as a separator. If a program wants the individual elements of such a list, it must split the variable’s value into pieces. Let’s have a closer look at PATH. Its value defines the shell’s search path, which is the list of directories that the shell looks in for programs when we type in a command name without specifying exactly where it is. For example, when we type a command like analyze, the shell needs to decide whether to run ./analyze (in our current directory) or /bin/analyze (in a system directory). To do this, the shell checks each directory in the PATH variable in turn. As soon as it finds a program with the right name, it stops searching and runs what it has found. To show how this works, here are the components of PATH listed one per line: /Users/amira/anaconda3/bin /usr/bin /bin /usr/sbin /sbin /usr/local/bin Suppose that our computer has three programs called analyze: /bin/analyze, /usr/local/bin/analyze, and /Users/amira/analyze. Since the shell searches the directories in the order they’re listed in PATH, it finds /bin/analyze first and runs that. Since /Users/amira is not in our path, Bash will never find the program /Users/amira/analyze unless we type the path in explicitly (for example, as ./analyze if we are in /Users/amira). If we want to see a variable’s value, we can print it using the echo command introduced at the end of Section 3.5. Let’s look at the value of the variable HOME, which keeps track of our home directory: $ echo HOME HOME Whoops: this just prints “HOME”, which isn’t what we wanted. Instead, we need to run this: $ echo $HOME /Users/amira As with loop variables (Section 3.3), the dollar sign before the variable names tells the shell that we want the variable’s value. This works just like wildcard expansion—the shell replaces the variable’s name with its value before running the command we’ve asked for. Thus, echo $HOME becomes echo /Users/amira, which displays the right thing. Creating a variable is easy: we assign a value to a name using “=”, putting quotes around the value if it contains spaces or special characters: $ DEPARTMENT=&quot;Library Science&quot; $ echo $DEPARTMENT Library Science To change the value, we simply assign a new one: $ DEPARTMENT=&quot;Information Science&quot; $ echo $DEPARTMENT Information Science If we want to set some variables automatically every time we run a shell, we can put commands to do this in a file called .bashrc in our home directory. For example, here are two lines in /Users/amira/.bashrc: export DEPARTMENT=&quot;Library Science&quot; export TEMP_DIR=/tmp export BACKUP_DIR=$TEMP_DIR/backup These three lines create the variables DEPARTMENT, TEMP_DIR, and BACKUP_DIR, and export them so that any programs the shell runs can see them as well. Notice that BACKUP_DIR’s definition relies on the value of TEMP_DIR, so that if we change where we put temporary files, our backups will be relocated automatically. However, this will only happen once we restart the shell, because .bashrc is only executed when the shell starts up. What’s in a Name? The ‘.’ character at the front of the name .bashrc prevents ls from listing this file unless we specifically ask it to using -a. The “rc” at the end is an abbreviation for “run commands”, which meant something really important decades ago, and is now just a convention everyone follows without understanding why. While we’re here, it’s also common to use the alias command to create shortcuts for things we frequently type. For example, we can define the alias backup to run /bin/zback with a specific set of arguments: alias backup=/bin/zback -v --nostir -R 20000 $HOME $BACKUP_DIR Aliases can save us a lot of typing, and hence a lot of typing mistakes. The name of an alias can be the same as an existing command, so we can use them to change the behavior of a familiar command: # Long list format including hidden files alias ls=&#39;ls -la&#39; # Print the file paths that were copied/moved alias mv=&#39;mv -v&#39; alias cp=&#39;cp -v&#39; # Request confirmation to remove a file and # print the file path that is removed alias rm=&#39;rm -iv&#39; We can find interesting suggestions for other aliases by searching online for “sample bashrc”. While searching for additional aliases, you’re likely to encounter references to other common shell features to customize, such as the color of your shell’s background and text. As mentioned in Chapter 2, another important feature to consider customizing is your shell prompt. In addition to a standard symbol (like $), your computer may include other information as well, such as the working directory, username, and/or date/time. If your shell does not include that information and you would like to see it, or if your current prompt is too long and you’d like to shorten it, you can include a line in your .bashrc file that defines $PS1: PS1=&quot;\\u \\w $ &quot; This changes the prompt to include your username and current working directory: amira ~/Desktop $ 4.7 Summary As powerful as the Unix shell is, it does have its shortcomings: dollar signs, quotes, and other punctuation can make a complex shell script look as though it was created by a cat dancing on a keyboard. However, it is the glue that holds data science together: shell scripts are used to create pipelines from miscellaneous sets of programs, while shell variables are used to do everything from specifying package installation directories to managing database login credentials. And while grep and find may take some getting used to, they and their cousins can handle enormous datasets very efficiently. If you would like to go further, Ray and Ray (2014) is an excellent general introduction, while Janssens (2014) looks specifically at how to process data on the command line. 4.8 Exercises As with the previous chapter, extra files and directories created during these exercises may need to be removed when you are done. 4.8.1 Cleaning up As we have gone through this chapter, we have created several files that we won’t need again. We can clean them up with the following commands; briefly explain what each line does. $ cd ~/zipf $ for file in $(find . -name &quot;*.bak&quot;) &gt; do &gt; rm $file &gt; done $ rm bin/summarize_all_books.sh $ rm -r results 4.8.2 Variables in shell scripts Imagine you have a shell script called script.sh that contains: head -n $2 $1 tail -n $3 $1 With this script in your data directory, you type the following command: $ bash script.sh &#39;*.txt&#39; 1 1 Which of the following outputs would you expect to see? All of the lines between the first and the last lines of each file ending in .txt in the data directory The first and the last line of each file ending in .txt in the data directory The first and the last line of each file in the data directory An error because of the quotes around *.txt 4.8.3 Find the longest file with a given extension Write a shell script called longest.sh that takes the name of a directory and a filename extension as its arguments, and prints out the name of the file with the most lines in that directory with that extension. For example: $ bash longest.sh data/ txt would print the name of the .txt file in data that has the most lines. 4.8.4 Script reading comprehension For this question, consider your data directory once again. Explain what each of the following three scripts would do when run as bash script1.sh *.txt, bash script2.sh *.txt, and bash script3.sh *.txt respectively. # script1.sh echo *.* # script2.sh for filename in $1 $2 $3 do cat $filename done # script3.sh echo $@.txt (You may need to search online to find the meaning of $@.) 4.8.5 Using grep Assume the following text from The Adventures of Sherlock Holmes is contained in a file called excerpt.txt: To Sherlock Holmes she is always THE woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. Which of the following commands would provide the following output: and predominates the whole of her sex. It was not that he felt grep \"he\" excerpt.txt grep -E \"he\" excerpt.txt grep -w \"he\" excerpt.txt grep -i \"he\" excerpt.txt 4.8.6 Tracking publication years In Exercise 3.8.6 you examined code that extracted the publication year from a list of book titles. Write a shell script called year.sh that takes any number of filenames as command-line arguments, and uses a variation of the code you used earlier to print a list of the unique publication years appearing in each of those files separately. 4.8.7 Counting names You and your friend have just finished reading Sense and Sensibility and are now having an argument. Your friend thinks that the elder of the two Dashwood sisters, Elinor, was mentioned more frequently in the book, but you are certain it was the younger sister, Marianne. Luckily, sense_and_sensibility.txt contains the full text of the novel. Using a for loop, how would you tabulate the number of times each of the sisters is mentioned? Hint: one solution might employ the commands grep and wc and a |, while another might utilize grep options. There is often more than one way to solve a problem with the shell; people choose solutions based on readability, speed, and what commands they are most familiar with. 4.8.8 Matching and subtracting Assume you are in the root directory of the zipf project. Which of the following commands will find all files in data whose names end in e.txt, but do not contain the word machine? find data -name '*e.txt' | grep -v machine find data -name *e.txt | grep -v machine grep -v \"machine\" $(find data -name '*e.txt') None of the above. 4.8.9 find pipeline reading comprehension Write a short explanatory comment for the following shell script: wc -l $(find . -name &#39;*.dat&#39;) | sort -n 4.8.10 Finding files with different properties The find command can be given criteria called “tests” to locate files with specific attributes, such as creation time, size, or ownership. Use man find to explore these, then write a single command using -type, -mtime, and -user to find all files in or below your Desktop directory that are owned by you and were modified in the last 24 hours. Explain why the value for -mtime needs to be negative. 4.9 Key Points Save commands in files (usually called shell scripts) for re-use. bash filename runs the commands saved in a file. $@ refers to all of a shell script’s command-line arguments. $1, $2, etc., refer to the first command-line argument, the second command-line argument, etc. Place variables in quotes if the values might have spaces or other special characters in them. find lists files with specific properties or whose names match patterns. $(command) inserts a command’s output in place. grep selects lines in files that match patterns. Use the .bashrc file in your home directory to set shell variables each time the shell runs. Use alias to create shortcuts for things you type frequently. "],["scripting.html", "Chapter 5 Building Command-Line Tools with Python 5.1 Programs and Modules 5.2 Handling Command-Line Options 5.3 Documentation 5.4 Counting Words 5.5 Pipelining 5.6 Positional and Optional Arguments 5.7 Collating Results 5.8 Writing Our Own Modules 5.9 Plotting 5.10 Summary 5.11 Exercises 5.12 Key Points", " Chapter 5 Building Command-Line Tools with Python Multiple exclamation marks are a sure sign of a diseased mind. — Terry Pratchett The Jupyter Notebook, PyCharm, and other graphical interfaces are great for prototyping code and exploring data, but eventually we may need to apply our code to thousands of data files, run it with many different parameters, or combine it with other programs as part of a data analysis pipeline. The easiest way to do this is often to turn our code into a standalone program that can be run in the Unix shell just like other command-line tools (Taschuk and Wilson 2017). In this chapter we will develop some command-line Python programs that handle input and output in the same way as other shell commands, can be controlled by several option flags, and provide useful information when things go wrong. The result will have more scaffolding than useful application code, but that scaffolding stays more or less the same as programs get larger. After the previous chapters, our Zipf’s Law project should have the following files and directories: zipf/ ├── bin │ └── book_summary.sh └── data ├── README.md ├── dracula.txt ├── frankenstein.txt ├── jane_eyre.txt ├── moby_dick.txt ├── sense_and_sensibility.txt ├── sherlock_holmes.txt └── time_machine.txt Python Style When writing Python code there are many style choices to make. How many spaces should I put between functions? Should I use capital letters in variable names? How should I order all the different elements of a Python script? Fortunately, there are well established conventions and guidelines for good Python style. We follow those guidelines throughout this book and discuss them in detail in Appendix F. 5.1 Programs and Modules To create a Python program that can run from the command line, the first thing we do is to add the following to the bottom of the file: if __name__ == &#39;__main__&#39;: This strange-looking check tells us whether the file is running as a standalone program or whether it is being imported as a module by some other program. When we import a Python file as a module in another program, the __name__ variable is automatically set to the name of the file. When we run a Python file as a standalone program, on the other hand, __name__ is always set to the special string \"__main__\". To illustrate this, let’s consider a script named print_name.py that prints the value of the __name__ variable: print(__name__) When we run this file directly, it will print __main__: $ python print_name.py __main__ But if we import print_name.py from another file or from the Python interpreter, it will print the name of the file, i.e. print_name. $ python Python 3.7.6 (default, Jan 8 2020, 13:42:34) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import print_name print_name Checking the value of the variable __name__ therefore tells us whether our file is the top-level program or not. If it is, we can handle command-line options, print help, or whatever else is appropriate; if it isn’t, we should assume that some other code is doing this. We could put the main program code directly under the if statement like this: if __name__ == &quot;__main__&quot;: # code goes here but that is considered poor practice, since it makes testing harder (Chapter 11). Instead, we put the high-level logic in a function, then call that function if our file is being run directly: def main(): # code goes here if __name__ == &quot;__main__&quot;: main() This top-level function is usually called main, but we can use whatever name we want. 5.2 Handling Command-Line Options The main function in a program usually starts by parsing any options the user gave on the command line. The most commonly used library for doing this in Python is argparse, which can handle options with or without arguments, convert arguments from strings to numbers or other types, display help, and many other things. The simplest way to explain how argparse works is by example. Let’s create a short Python program called script_template.py: import argparse def main(args): print(&#39;Input file:&#39;, args.infile) print(&#39;Output file:&#39;, args.outfile) if __name__ == &#39;__main__&#39;: USAGE = &#39;One-line description of what the script does.&#39; parser = argparse.ArgumentParser(description=USAGE) parser.add_argument(&#39;infile&#39;, type=str, help=&#39;Input file name&#39;) parser.add_argument(&#39;outfile&#39;, type=str, help=&#39;Output file name&#39;) args = parser.parse_args() main(args) Empty Lines, Again As we discussed in the last chapter for shell scripts, remember to end your Python scripts in a newline character (which we view as an empty line). If script_template.py is run as a standalone program at the command line then __name__ == '__main__' is true, so the program uses argparse to create an argument parser. It then specifies that it expects two command-line arguments: an input filename (infile) and output filename (outfile). The program uses parser.parse_args() to parse the actual command-line arguments given by the user and stores the result in a variable called args, which it passes to main. That function can then get the values using the names specified in the parser.add_argument calls. Specifying Types We have passed type=str to add_argument to tell argparse that we want infile and outfile to be treated as strings. str is not quoted because it is not a string itself: instead, it is the built-in Python function that converts things to strings. As we will see below, we can pass in other functions like int if we want arguments converted to numbers. If we run script_template.py at the command line the output shows us that argparse has successfully handled the arguments: $ cd ~/zipf $ python script_template.py in.csv out.png Input file: in.csv Output file: out.png It also displays an error message if we give the program invalid arguments: $ python script_template.py in.csv usage: script_template.py [-h] infile outfile script_template.py: error: the following arguments are required: outfile Finally, it automatically generates help information (which we can get using the -h option): $ python script_template.py -h usage: script_template.py [-h] infile outfile One-line description of what the script does. positional arguments: infile Input file name outfile Output file name optional arguments: -h, --help show this help message and exit 5.3 Documentation Our program template is a good starting point, but we improve it right away by adding a bit of documentation. To demonstrate, let’s write a function that doubles a number: def double(num): &#39;Double the input.&#39; return 2 * num The first line of this function is a string that isn’t assigned to a variable. Such a string is called a documentation string, or docstring for short. If we call our function it does what we expect: double(3) 6 However, we can also ask for the function’s documentation, which is stored in double.__doc__: double.__doc__ &#39;Double the input.&#39; Python creates the variable __doc__ automatically for every function, just as it creates the variable __name__ for every file. If we don’t write a docstring for a function, __doc__’s value is an empty string. We can put whatever text we want into a function’s docstring, but it is usually used to provide online documentation. We can also put a docstring at the start of a file, in which case it is assigned to a variable called __doc__ that is visible inside the file. If we add documentation to our template, it becomes: &quot;&quot;&quot;One-line description of what the script does.&quot;&quot;&quot; import argparse def main(args): &quot;&quot;&quot;Run the program.&quot;&quot;&quot; print(&#39;Input file:&#39;, args.infile) print(&#39;Output file:&#39;, args.outfile) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=str, help=&#39;Input file name&#39;) parser.add_argument(&#39;outfile&#39;, type=str, help=&#39;Output file name&#39;) args = parser.parse_args() main(args) Note that docstrings are usually written using triple-quoted strings, since these can span multiple lines. Note also how we pass description=__doc__ to argparse.ArgumentParser. This saves us from typing the same information twice, but more importantly ensures that the help message provided in response to the -h option will be the same as the interactive help. Let’s try this out in an interactive Python session. (Remember, do not type the &gt;&gt;&gt; prompt: Python provides this for us.) $ python Python 3.7.6 (default, Jan 8 2020, 13:42:34) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import script_template &gt;&gt;&gt; script_template.__doc__ &#39;One-line description of what the script does.&#39; &gt;&gt;&gt; help(script_template) Help on module script_template: NAME script_template - One-line description of what the script does. FUNCTIONS main(args) Run the program. FILE /Users/amira/script_template.py As this example shows, if we ask for help on the module, Python formats and displays all of the docstrings for everything in the file. We talk more about what to put in a docstring in Appendix G. 5.4 Counting Words Now that we have a template for command-line Python programs, we can use it to check Zipf’s Law for our collection of classic novels. We start by moving the template into the directory where we store our runnable programs (Section 1.1.2): $ mv script_template.py bin Next, let’s write a function that counts how often words appear in a file. Our function splits the text on whitespace characters (which is the default behavior of the string object’s split method), then strips leading and trailing punctuation. This isn’t completely correct—if two words are joined by a long dash like “correct” and “if” in this sentence, for example, they will be treated as one word—but given that long dashes are used relatively infrequently it’s close enough to correct for our purposes. (We will submit a bug report about the long dash issue in Section 8.6). We also use the Counter class from the collections library to count how many times each word occurs. If we give Counter a list of words, the result is an object that contains the number of times each one appears in the list: import string from collections import Counter def count_words(reader): &quot;&quot;&quot;Count the occurrence of each word in a string.&quot;&quot;&quot; text = reader.read() chunks = text.split() stripped = [word.strip(string.punctuation) for word in chunks] word_list = [word.lower() for word in stripped if word] word_counts = Counter(word_list) return word_counts Let’s try our function on Dracula: with open(&#39;data/dracula.txt&#39;, &#39;r&#39;) as reader: word_counts = count_words(reader) print(word_counts) Counter({&#39;the&#39;: 8036, &#39;and&#39;: 5896, &#39;i&#39;: 4712, &#39;to&#39;: 4540, &#39;of&#39;: 3738, &#39;a&#39;: 2961, &#39;in&#39;: 2558, &#39;he&#39;: 2543, &#39;that&#39;: 2455, &#39;it&#39;: 2141, &#39;was&#39;: 1877, &#39;as&#39;: 1581, &#39;we&#39;: 1535, &#39;for&#39;: 1534, ...}) If we want the word counts in a format like CSV for easier processing, we can write another small function that takes our Counter object, orders its contents from most to least frequent, and then writes it to standard output as CSV: import sys import csv def collection_to_csv(collection): &quot;&quot;&quot;Write out collection of items and counts in csv format.&quot;&quot;&quot; collection = collection.most_common() writer = csv.writer(sys.stdout) writer.writerows(collection) Running this would print all the distinct words in the book along with their counts. This list could well be several thousand lines long, so to make the output a little easier to view on our screen, we can add an option to limit the output to the most frequent words. We set its default value to None so that we can easily tell if the caller hasn’t specified a cutoff, in which case we display the whole collection: def collection_to_csv(collection, num=None): &quot;&quot;&quot;Write out collection of items and counts in csv format.&quot;&quot;&quot; collection = collection.most_common() if num is None: num = len(collection) writer = csv.writer(sys.stdout) writer.writerows(collection[0:num]) collection_to_csv(word_counts, num=10) the,8036 and,5896 i,4712 to,4540 of,3738 a,2961 in,2558 he,2543 that,2455 it,2141 To make our count_words and collection_to_csv functions available at the command line, we need to insert them into our script template and call them from within the main function. Let’s call our program countwords.py and put it in the bin subdirectory of the zipf project: &quot;&quot;&quot; Count the occurrences of all words in a text and output them in CSV format. &quot;&quot;&quot; import sys import argparse import string import csv from collections import Counter def collection_to_csv(collection, num=None): &quot;&quot;&quot;Write out collection of items and counts in csv format.&quot;&quot;&quot; collection = collection.most_common() if num is None: num = len(collection) writer = csv.writer(sys.stdout) writer.writerows(collection[0:num]) def count_words(reader): &quot;&quot;&quot;Count the occurrence of each word in a string.&quot;&quot;&quot; text = reader.read() chunks = text.split() stripped = [word.strip(string.punctuation) for word in chunks] word_list = [word.lower() for word in stripped if word] word_counts = Counter(word_list) return word_counts def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; with open(args.infile, &#39;r&#39;) as reader: word_counts = count_words(reader) collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=str, help=&#39;Input file name&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) Note that we have replaced the 'outfile' argument from our template script with an optional -n (or --num) flag to control how much output is printed and modified collection_to_csv so that it always prints to standard output. If we want that output in a file, we can redirect with &gt;. Let’s take our program for a test drive: $ python bin/countwords.py data/dracula.txt -n 10 the,8036 and,5896 i,4712 to,4540 of,3738 a,2961 in,2558 he,2543 that,2455 it,2141 5.5 Pipelining As discussed in Section 3.2, most Unix commands follow a useful convention: if the user doesn’t specify the names of any input files, they read from standard input. Similarly, if no output file is specified, the command sends its results to standard output. This makes it easy to use the command in a pipeline. Our program always sends its output to standard output; as noted above, we can always redirect it to a file with &gt;. If we want countwords.py to read from standard input, we only need to change the handling of infile in the argument parser and simplify main to match: def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; word_counts = count_words(args.infile) collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Input file name&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) There are two changes to how add_argument handles infile: Setting type=argparse.FileType('r') tells argparse to treat the argument as a filename and open that file for reading. This is why we no longer need to call open ourselves, and why main can pass args.infile directly to count_words. The number of expected arguments (nargs) is set to ?. This means that if an argument is given it will be used, but if none is provided, a default of '-' will be used instead. argparse.FileType('r') understands '-' to mean “read from standard input”; this is another Unix convention that many programs follow. After these changes, we can create a pipeline like this to count the words in the first 500 lines of a book: $ head -n 500 data/dracula.txt | python bin/countwords.py --n 10 the,227 and,121 of,116 i,98 to,80 in,58 a,49 it,45 was,42 that,41 5.6 Positional and Optional Arguments We have met two kinds of command-line arguments while writing countwords.py. Optional arguments are defined using a leading - or -- (or both), which means that all three of the following definitions are valid: parser.add_argument(&#39;-n&#39;, type=int, help=&#39;Limit output&#39;) parser.add_argument(&#39;--num&#39;, type=int, help=&#39;Limit output&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, help=&#39;Limit output&#39;) The convention is for - to precede a short (single letter) option and -- a long (multi-letter) option. The user can provide optional arguments at the command line in any order they like. Positional arguments have no leading dashes and are not optional: the user must provide them at the command line in the order in which they are specified to add_argument (unless nargs='?' is provided to say that the value is optional). 5.7 Collating Results Ultimately, we want to save the word counts to a CSV file for further analysis and plotting. Let’s create a subdirectory to hold our results (following the structure described in Section 1.1): $ mkdir results and then save the counts for various files: $ python bin/countwords.py data/dracula.txt &gt; results/dracula.csv $ python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv $ python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv As in the previous chapter, we’ve split long lines of to code onto separate lines for formatting purposes; each of the three code chunks above should be run as a single line of code. Now that we can get word counts for individual books we can collate the counts for several books. This can be done using a loop that adds up the counts of a word from the each of the CSV files created by countwords.py. Using the same template as before, we can write a program called collate.py: &quot;&quot;&quot; Combine multiple word count CSV-files into a single cumulative count. &quot;&quot;&quot; import sys import csv import argparse from collections import Counter def collection_to_csv(collection, num=None): &quot;&quot;&quot;Write out collection of items and counts in csv format.&quot;&quot;&quot; collection = collection.most_common() if num is None: num = len(collection) writer = csv.writer(sys.stdout) writer.writerows(collection[0:num]) def update_counts(reader, word_counts): &quot;&quot;&quot;Update word counts with data from another reader/file.&quot;&quot;&quot; for word, count in csv.reader(reader): word_counts[word] += int(count) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; word_counts = Counter() for fname in args.infiles: with open(fname, &#39;r&#39;) as reader: update_counts(reader, word_counts) collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infiles&#39;, type=str, nargs=&#39;*&#39;, help=&#39;Input file names&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) The loop in the main function iterates over each filename in infiles, opens the CSV file, and calls update_counts with the input stream as one parameter and the counter as the other. update_counts then iterates through all the words the CSV-files and increments the counts using the += operator. Note that we have not used type=argparse.FileType('r') here. Instead, we have called the option infiles (plural) and specified nargs='*' to tell argparse that we will accept zero or more filenames. We must then open the files ourselves. Let’s give collate.py a try (using -n 10 to limit the number of lines of output): $ python bin/collate.py results/dracula.csv results/moby_dick.csv results/jane_eyre.csv -n 10 the,30505 and,18916 of,14908 to,14369 i,13572 a,12059 in,9547 that,6984 it,6821 he,6142 5.8 Writing Our Own Modules countwords.py and collate.py both now contain the function collection_to_csv. Having the same function in two or more places is a bad idea: if we want to improve it or fix a bug, we have to find and change every single script that contains a copy. The solution is to put the shared functions in a separate file and load that file as a module. Let’s create a file called utilities.py in the bin directory that looks like this: &quot;&quot;&quot;Collection of commonly used functions.&quot;&quot;&quot; import sys import csv def collection_to_csv(collection, num=None): &quot;&quot;&quot; Write out collection of items and counts in csv format. Parameters ---------- collection : collections.Counter Collection of items and counts num : int Limit output to N most frequent items &quot;&quot;&quot; collection = collection.most_common() if num is None: num = len(collection) writer = csv.writer(sys.stdout) writer.writerows(collection[0:num]) Note that we have written a much more detailed docstring for collection_to_csv: as a rule, the more widely used code is, the more it’s worth explaining exactly what it does. We can now import our utilities into our programs just as we would import any other Python module using either import utilities (to get the whole thing) or something like from utilities import collection_to_csv (to get a single function). After making this change, countwords.py looks like this: &quot;&quot;&quot; Count the occurrences of all words in a text and write them to a CSV-file. &quot;&quot;&quot; import argparse import string from collections import Counter import utilities as util def count_words(reader): &quot;&quot;&quot;Count the occurrence of each word in a string.&quot;&quot;&quot; text = reader.read() chunks = text.split() stripped = [word.strip(string.punctuation) for word in chunks] word_list = [word.lower() for word in stripped if word] word_counts = Counter(word_list) return word_counts def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; word_counts = count_words(args.infile) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Input file name&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) collate.py is now: &quot;&quot;&quot; Combine multiple word count CSV-files into a single cumulative count. &quot;&quot;&quot; import csv import argparse from collections import Counter import utilities as util def update_counts(reader, word_counts): &quot;&quot;&quot;Update word counts with data from another reader/file.&quot;&quot;&quot; for word, count in csv.reader(reader): word_counts[word] += int(count) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; word_counts = Counter() for fname in args.infiles: with open(fname, &#39;r&#39;) as reader: update_counts(reader, word_counts) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infiles&#39;, type=str, nargs=&#39;*&#39;, help=&#39;Input file names&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) Any Python source file can be imported by any other. This is why Python files should be named using snake case (e.g. some_thing) instead of kebab case (e.g. some-thing): an expression like import some-thing isn’t allowed because some-thing isn’t a legal variable name. When a file is imported, the statements in it are executed as it loads. Variables, functions, and items defined in the file are then available as module.thing, where module is the filename (without the .py extension) and thing is the name of the item. The __pycache__ Directory When we import a file, Python translates the source code into instructions called byte codes that it can execute efficiently. Since the byte codes only change when the source changes, Python saves the byte code in a separate file, and reloads that file instead of re-translating the source code the next time it’s asked to import the file (unless the file has changed, in which case Python starts from the beginning). Python creates a subdirectory called __pycache__ that holds the byte code for the files imported from that directory. We typically don’t want to put the files in __pycache__ in version control, so we normally tell Git to ignore it as discussed in Section 6.9. 5.9 Plotting The last thing for us to do is to plot the word count distribution. Recall that Zipf’s Law states the second most common word in a body of text appears half as often as the most common, the third most common appears a third as often, and so on. Mathematically, this might be written as “word frequency is proportional to 1/rank.” The following code plots the word frequency against the inverse rank using the pandas library: import pandas as pd input_csv = &#39;results/jane_eyre.csv&#39; df = pd.read_csv(input_csv, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] scatplot = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;inverse_rank&#39;, figsize=[12, 6], grid=True) fig = scatplot.get_figure() fig.savefig(&#39;results/jane_eyre.png&#39;) Figure 5.1: Word frequency distribution for Jane Eyre. You’ll build on this code to create a plotting script for your project in Exercise 5.11.4. 5.10 Summary Why is building a simple command-line tool so complex? One answer is that the conventions for command-line programs have evolved over several decades, so libraries like argparse must now support several different generations of option handling. Another is that the things we want to do genuinely are complex: read from either standard input or a list of files, display help when asked to, respect parameters that might not be there, and so on. As with many other things in programming (and life), everyone wishes it was simpler, but no one can agree on what to throw away. The good news is that this complexity is a fixed cost: our template for command-line tools can be re-used for programs that are much larger than the examples shown in this chapter. Making tools that behave in ways people expect greatly increases the chances that others will find them useful. 5.11 Exercises 5.11.1 Running Python statements from the command line We don’t need to open the interactive interpreter to run Python code. Instead, we can invoke Python with the command flag -c and the statement we want to run: $ python -c &quot;print(2+3)&quot; 5 When and why is this useful? 5.11.2 Listing files A Python library called glob can be used to create a list of files matching a pattern, much like the ls shell command. $ python Python 3.7.6 (default, Jan 8 2020, 13:42:34) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import glob &gt;&gt;&gt; glob.glob(&#39;data/*.txt&#39;) [&#39;data/moby_dick.txt&#39;, &#39;data/sense_and_sensibility.txt&#39;, &#39;data/sherlock_holmes.txt&#39;, &#39;data/time_machine.txt&#39;, &#39;data/frankenstein.txt&#39;, &#39;data/dracula.txt&#39;, &#39;data/jane_eyre.txt&#39;] Using script_template.py as a guide, write a new script called my_ls.py that takes as input a directory and suffix (e.g. py, txt, md, sh) and outputs a list of the files (sorted alphabetically) in that directory ending in that suffix. The help information for the new script should read as follows: $ python bin/my_ls.py -h usage: my_ls.py [-h] dir suffix List the files in a given directory with a given suffix. positional arguments: dir Directory suffix File suffix (e.g. py, sh) optional arguments: -h, --help show this help message and exit and an example of the output would be: $ python bin/my_ls.py data/ txt data/dracula.txt data/frankenstein.txt data/jane_eyre.txt data/moby_dick.txt data/sense_and_sensibility.txt data/sherlock_holmes.txt data/time_machine.txt Note: we will not be including this script in subsequent chapters. 5.11.3 Sentence ending punctuation Our countwords.py script strips the punctuation from a text, which means it provides no information on sentence endings. Using script_template.py and countwords.py as a guide, write a new script called sentence_endings.py that counts the occurrence of full stops, question marks and exclamation points and prints that information to the screen. Hint: String objects have a count method, e.g: $ python Python 3.7.6 (default, Jan 8 2020, 13:42:34) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; &quot;Hello! Are you ok?&quot;.count(&#39;!&#39;) 1 When you’re done, the script should be able to accept an input file: $ python bin/sentence_endings.py data/dracula.txt Number of . is 8505 Number of ? is 492 Number of ! is 752 or standard input: $ head -n 500 data/dracula.txt | python bin/sentence_endings.py Number of . is 148 Number of ? is 8 Number of ! is 8 Note: we will not be including this script in subsequent chapters. 5.11.4 A better plotting program Using script_template.py as a guide, take the plotting code from Section 5.9 and write a new Python program called plotcounts.py. The script should: Use the type=argparse.FileType('r'), nargs='?' and default='-' options for the input file argument (i.e. similar to the countwords.py script) so that plotcounts.py uses standard input if no csv file is given. Include an optional --outfile argument for the name of the output image file. The default value should be plotcounts.png. Include an optional --xlim argument so that the user can change the x-axis bounds. When you are done, generate a plot for Jane Eyre by passing the word counts to plotcounts.py via a CSV file: $ python bin/plotcounts.py results/jane_eyre.csv --outfile results/jane_eyre.png and by standard input: $ python bin/countwords.py data/jane_eyre.txt | python bin/plotcounts.py --outfile results/jane_eyre.png Note: the solution to this exercise is used in following chapters. 5.12 Key Points Write command-line Python programs that can be run in the Unix shell like other command-line tools. If the user does not specify any input files, read from standard input. If the user does not specify any output files, write to standard output. Place all import statements at the start of a module. Use the value of __name__ to determine if a file is being run directly or being loaded as a module. Use argparse to handle command-line arguments in standard ways. Use short options for common controls and long options for less common or more complicated ones. Use docstrings to document functions and scripts. Place functions that are used across multiple scripts in a separate file that those scripts can import. "],["git-cmdline.html", "Chapter 6 Using Git at the Command Line 6.1 Setting Up 6.2 Creating a New Repository 6.3 Adding Existing Work 6.4 Describing Commits 6.5 Saving and Tracking Changes 6.6 Synchronizing with Other Repositories 6.7 Exploring History 6.8 Restoring Old Versions of Files 6.9 Ignoring Files 6.10 Summary 6.11 Exercises 6.12 Key Points", " Chapter 6 Using Git at the Command Line +++ Divide By Cucumber Error. Please Reinstall Universe And Reboot +++ — Terry Pratchett A version control system tracks changes to files and helps people share those changes with each other. These things can be done by emailing files to colleagues or by using Microsoft Word and Google Docs, but version control does both more accurately and efficiently. Originally developed to support software development, over the past fifteen years it has become the cornerstone of reproducible research. Version control works by storing a master copy of your code in a repository, which you can’t edit directly. Instead, you check out a working copy of the code, edit that code, then commit changes back to the repository. In this way, version control records a complete revision history (i.e. of every commit), so that you can retrieve and compare previous versions at any time. This is useful from an individual viewpoint, because you don’t need to store multiple (but slightly different) copies of the same script (Figure 6.1). It’s also useful from a collaboration viewpoint, because the system keeps a record of who made what changes and when. Figure 6.1: Managing different versions of a file without version control. There are many different version control systems, such as CVS, Subversion, and Mercurial, but the most widely used version control system today is Git. Many people first encounter it through a GUI like GitKraken or the RStudio IDE. However, these tools are actually wrappers around Git’s original command-line interface, which gives us access to all of Git’s features. This lesson describes how to perform fundamental operations using that interface; Chapter 7 then introduces more advanced operations that can be used to implement a smoother research workflow. To show how Git works, we will apply it to the Zipf’s Law project. Our project directory should currently include: zipf/ ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── script_template.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ ├── frankenstein.txt │ └── ... └── results ├── dracula.csv ├── jane_eyre.csv ├── jane_eyre.png └── moby_dick.csv bin/plotcounts.py is the solution to Exercise 5.11.4; over the course of this chapter we will edit it to produce more informative plots. Initially, it looks like this: &quot;&quot;&quot;Plot word counts.&quot;&quot;&quot; import argparse import pandas as pd def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; df = pd.read_csv(args.infile, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;inverse_rank&#39;, figsize=[12, 6], grid=True, xlim=args.xlim) ax.figure.savefig(args.outfile) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Word count csv file name&#39;) parser.add_argument(&#39;--outfile&#39;, type=str, default=&#39;plotcounts.png&#39;, help=&#39;Output image file name&#39;) parser.add_argument(&#39;--xlim&#39;, type=float, nargs=2, metavar=(&#39;XMIN&#39;, &#39;XMAX&#39;), default=None, help=&#39;X-axis limits&#39;) args = parser.parse_args() main(args) 6.1 Setting Up We write Git commands as git verb options, where the subcommand verb tells Git what we want to do and options provide whatever additional information that subcommand needs. Using this syntax, the first thing we need to do is configure Git. $ git config --global user.name &quot;Amira Khan&quot; $ git config --global user.email &quot;amira@zipf.org&quot; (Please use your own name and email address instead of the one shown.) Here, config is the verb and the rest of the command are options. We put the name in quotation marks because it contains a space; we don’t actually need to quote the email address, but do so for consistency. Since we are going to be using GitHub, the email address should be the same as you have or intend to use when setting up your GitHub account. The --global option tells Git to use the settings for all of our projects on this computer, so these two commands only need to be run once. However, we can re-run them any time if we want to change our details. We can also check our settings using the --list option: $ git config --list user.name=Amira Khan user.email=amira@zipf.org core.autocrlf=input core.editor=nano core.repositoryformatversion=0 core.filemode=true core.bare=false core.ignorecase=true ... Depending on your operating system and version of Git, your configuration list may look a bit different. Most of these differences shouldn’t matter right now, as long as your user name and email are accurate. Git Help and Manual If we forget a Git command, we can list which ones are available using --help: $ git --help This option also gives us more information about specific commands: $ git config --help 6.2 Creating a New Repository Once Git is configured, we can use it to track work on our Zipf’s Law project. Let’s make sure we are in the top-level directory of our project: $ cd ~/zipf $ ls bin data results We want to make this directory a repository, i.e., a place where Git can store versions of our files. We do this using the init command with . to mean “the current directory”: $ git init . Initialized empty Git repository in /Users/amira/zipf/.git/ ls seems to show that nothing has changed: $ ls bin data results but if we add the -a flag to show everything, we can see that Git has created a hidden directory within zipf called .git: $ ls -a . .. .git bin data results Git stores information about the project in this special subdirectory. If we ever delete it, we will lose that history. We can check that everything is set up correctly by asking Git to tell us the status of our project: $ git status On branch master No commits yet Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) bin/ data/ results/ nothing added to commit but untracked files present (use &quot;git add&quot; to track) “No commits yet” means that Git hasn’t recorded any history yet, while “Untracked files” means Git has noticed that there are things in bin/, data/ and results/ that it is not yet keeping track of. 6.3 Adding Existing Work Now that our project is a repository, we can tell Git to start recording its history. To do this, we add things to the list of things Git is tracking using git add. We can do this for single files: $ git add bin/countwords.py or entire directories: $ git add bin The easiest thing to do with an existing project is to tell Git to add everything in the current directory using .: $ git add . We can then check the repository’s status to see what files have been added: $ git status On branch master No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: bin/book_summary.sh new file: bin/collate.py new file: bin/countwords.py new file: bin/plotcounts.py new file: bin/script_template.py new file: bin/utilities.py new file: data/README.md new file: data/dracula.txt new file: data/frankenstein.txt new file: data/jane_eyre.txt new file: data/moby_dick.txt new file: data/sense_and_sensibility.txt new file: data/sherlock_holmes.txt new file: data/time_machine.txt new file: results/dracula.csv new file: results/jane_eyre.csv new file: results/jane_eyre.png new file: results/moby_dick.csv Adding all of our existing files this way is easy, but we can accidentally add things that should never be in version control, such as files containing passwords or other sensitive information. The output of git status tells us that we can remove such files from the list of things to be saved using git rm --cached; we will practice this in Exercise 6.11.2. What to Save We always want to save programs, manuscripts, and everything else we have created by hand in version control. In this project, we have also chosen to save our data files and the results we have generated (including our plots). This is a project-specific decision: if these files are very large, for example, we may decide to save them elsewhere, while if they are easy to re-create, we may not save them at all. We will explore this issue further in Chapter 13. We no longer have any untracked files, but the tracked files haven’t been committed (i.e., saved permanently in our project’s history). We can do this using git commit: $ git commit -m &quot;Add scripts, novels, word counts, and plots&quot; [master (root-commit) 173222b] Add scripts, novels, word counts, and plots 18 files changed, 145296 insertions(+) create mode 100644 bin/book_summary.sh create mode 100644 bin/collate.py create mode 100644 bin/countwords.py create mode 100644 bin/plotcounts.py create mode 100644 bin/script_template.py create mode 100644 bin/utilities.py create mode 100644 data/README.md create mode 100644 data/dracula.txt create mode 100644 data/frankenstein.txt create mode 100644 data/jane_eyre.txt create mode 100644 data/moby_dick.txt create mode 100644 data/sense_and_sensibility.txt create mode 100644 data/sherlock_holmes.txt create mode 100644 data/time_machine.txt create mode 100644 results/dracula.csv create mode 100644 results/jane_eyre.csv create mode 100644 results/jane_eyre.png create mode 100644 results/moby_dick.csv git commit takes everything we have told Git to save using git add and stores a copy permanently inside the repository’s .git directory. This permanent copy is called a commit or a revision. Git gives is a unique identifier, and the first line of output from git commit displays its short identifier 2dc78f0, which is the first few characters of that unique label. We use the -m option (short for message) to record a short comment with the commit to remind us later what we did and why. (Once again, we put it in double quotes because it contains spaces.) If we run git status now: $ git status the output tells us that all of our existing work is tracked and up to date: On branch master nothing to commit, working tree clean This first commit becomes the starting point of our project’s history: we won’t be able to see changes made before this point. This implies that we should make our project a Git repository as soon as we create it rather than after we have done some work. 6.4 Describing Commits If we run git commit without the -m option, Git opens a text editor so that we can write a longer commit message. In this message, the first line is referred to as the “subject” and the rest as the “body”, just as in an email. When we use -m, we are only writing the subject line; this makes things easier in the short run, but if our project’s history fills up with one-liners like “Fixed problem” or “Updated”, our future self will wish that we had taken a few extra seconds to explain things in a little more detail. Following these guidelines will help: Separate the subject from the body with a blank line so that it is easy to spot. Limit subject lines to 50 characters so that they are easy to scan. Write the subject line in Title Case (like a section heading). Do not end the subject line with a period. Write as if giving a command (e.g., “Make each plot half the width of the page”). Wrap the body (i.e., insert line breaks to format text as paragraphs rather than relying on editors to wrap lines automatically). Use the body to explain what and why rather than how. Which Editor? The default editor in the Unix shell is called Vim. It has many useful features, but no one has ever claimed that its interface is intuitive. (“How do I exit the Vim editor?” is one of the most frequently read questions on Stack Overflow.) To configure Git to use the nano editor introduced in Chapter 2 instead, execute the following command: $ git config --global core.editor &quot;nano -w&quot; 6.5 Saving and Tracking Changes Our initial commit gave us a starting point. The process to build on top of it is similar: first add the file, then commit changes. Let’s check that we’re in the right directory: $ pwd /Users/amira/zipf Let’s use plotcounts.py to plot the word counts in results/dracula.csv: $ python bin/plotcounts.py results/dracula.csv --outfile results/dracula.png If we check the status of our repository again, Git tells us that we have a new file: $ git status On branch master Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) results/dracula.png nothing added to commit but untracked files present (use &quot;git add&quot; to track) Git isn’t tracking this file yet because we haven’t told it to. Let’s do that with git add and then commit our change: $ git add results/dracula.png $ git commit -m &quot;Add plot of word counts for &#39;Dracula&#39;&quot; [master 851d590] Add plot of word counts for &#39;Dracula&#39; 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 results/dracula.png If we want to know what we’ve done recently, we can display the project’s history using git log: $ git log commit 851d590a214c7859eafa0998c6c951f8e0eb359b (HEAD -&gt; master) Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:32:41 2020 -0800 Add plot of word counts for &#39;Dracula&#39; commit 173222bf90216b408c8997f4e143572b99637750 Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:30:23 2020 -0800 Add scripts, novels, word counts, and plots git log lists all commits made to a repository in reverse chronological order. The listing for each commit includes the commit’s full identifier (which starts with the same characters as the short identifier printed by git commit), the commit’s author, when it was created, and the commit message that we wrote. Scrolling Through Logs Our log this time isn’t very long, so you were likely able to see it printed to your screen without needing to scroll. When you begin working with longer logs (like later in this chapter), you’ll notice that the commits are shown in a pager program, as you saw in Section 2.8 with manual pages. You can apply the same keystrokes to scroll through the log and exit the paging program. The plot we have made is shown in Figure 6.2. It could be better: most of the visual space is devoted to a few very common words, which makes it hard to see what is happening with the other ten thousand or so words. Figure 6.2: Inverse rank versus word frequency for Dracula. An alternative way to visually evaluate Zipf’s Law is to plot the word frequency against rank on log-log axes. Let’s change the section: ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;inverse_rank&#39;, figsize=[12, 6], grid=True, xlim=args.xlim) to put 'rank' on the y-axis and add loglog=True: ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) When we check our status again, it prints: $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: bin/plotcounts.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) The last line tells us that a file Git already knows about has been modified. Hints from Git After executing Git commands, you may see output that differs slightly from what is shown here. For example, you may see a suggestion for git checkout in place of git restore after executing the code above, which means you’re running an different version of Git. As with most tasks in coding, there are often multiple commands to accomplish the same action with Git. This chapter will show output from Git version 2.29. If you see something different in your Git output, you can try the commands we present here, or follow the suggestions included in the output you see. When in doubt, check the documentation (e.g., git checkout --help) if you get stuck. To save those changes in the repository’s history, we must git add and then git commit. Before we do, though, let’s review the changes using git diff. This command shows us the differences between the current state of our repository and the most recently saved version: $ git diff diff --git a/bin/plotcounts.py b/bin/plotcounts.py index f274473..c4c5b5a 100644 --- a/bin/plotcounts.py +++ b/bin/plotcounts.py @@ -13,7 +13,7 @@ def main(args): method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, - y=&#39;inverse_rank&#39;, + y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) The output is cryptic, even by the standards of the Unix command line, because it is actually a series of commands telling editors and other tools how to turn the file we had into the file we have. If we break it down into pieces: The first line tells us that Git is producing output in the format of the Unix diff command. The second line tells exactly which versions of the file Git is comparing: f274473 and c4c5b5a are the short identifiers for those versions. The third and fourth lines once again show the name of the file being changed; the name appears twice in case we are renaming a file as well as modifying it. The remaining lines show us the changes and the lines on which they occur. A minus sign - in the first column indicates a line that is being removed, while a plus sign + shows a line that is being added. Lines without either plus or minus signs have not been changed, but are provided around the lines that have been changed to add context. Git’s default is to compare line by line, but it can be instructive to instead compare word by word using the --word-diff or --color-words options. These are particularly useful when running git diff on prose rather than code. After reviewing our change we can commit it just as we did before: $ git commit -m &quot;Plot frequency against rank on log-log axes&quot; On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: bin/plotcounts.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Whoops: we forgot to add the file to the set of things we want to commit. Let’s do that and then try the commit again: $ git add bin/plotcounts.py $ git status On branch master Changes to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: bin/plotcounts.py $ git commit -m &quot;Plot frequency against rank on log-log axes&quot; [master 582f7f6] Plot frequency against rank on log-log axes 1 file changed, 1 insertion(+), 1 deletion(-) The Staging Area Git insists that we add files to the set we want to commit before actually committing anything. This allows us to commit our changes in stages and capture changes in logical portions rather than only large batches. For example, suppose we add a few citations to the introduction of our thesis, which is in the file introduction.tex. We might want to commit those additions but not commit the changes to conclusion.tex (which we haven’t finished writing yet). To allow for this, Git has a special staging area where it keeps track of things that have been added to the current changeset but not yet committed (Figure 6.3). Figure 6.3: The staging area in Git. Let’s take a look at our new plot (Figure 6.4): $ python bin/plotcounts.py results/dracula.csv --outfile results/dracula.png Figure 6.4: Rank versus word frequency on log-log axes for ‘Dracula’. Interpreting Our Plot If Zipf’s Law holds, we should still see a linear relationship, although now it will be negative, rather than positive (since we’re plotting the rank instead of the reverse rank). The low-frequency words (below about 120 instances) seem to follow a straight line very closely, but we currently have to make this evaluation by eye. In the next chapter, we’ll write code to fit and add a line to our plot. Running git status again shows that our plot has been modified: On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: results/dracula.png no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Since results/dracula.png is a binary file rather than text, git diff can’t show what has changed. It therefore simply tells us that the new file is different from the old one: diff --git a/results/dracula.png b/results/dracula.png index c1f62fd..57a7b70 100644 Binary files a/results/dracula.png and b/results/dracula.png differ This is one of the biggest weaknesses of Git (and other version control systems): they are built to handle text. They can track changes to images, PDFs, and other formats, but they cannot do as much to show or merge differences. In a better world than ours, programmers fixed this years ago. If we are sure we want to save all of our changes, we can add and commit in a single command by giving git commit the -a option: $ git commit -a -m &quot;Update dracula plot&quot; [master ee8684c] Update dracula plot 1 file changed, 0 insertions(+), 0 deletions(-) rewrite results/dracula.png (99%) The Git commands we’ve covered so far (git add, git commit, git diff) represent the tasks you perform in a basic Git workflow in a local repository (Figure 6.5a). Figure 6.5: Commands included in basic Git workflows. 6.6 Synchronizing with Other Repositories Sooner or later our computer will experience a hardware failure, be stolen, or be thrown in the lake by someone who thinks that we shouldn’t spend the entire vacation working on our thesis. Even before that happens we will probably want to collaborate with others, which we can do by linking our local repository to one stored on a hosting service such as GitHub. Where’s My Repository? So far we’ve worked with repositories located on your own computer, which we’ll also refer to as local or desktop repositories. The alternative is hosting repositories on GitHub or another server, which we’ll refer to as a remote or GitHub repository. The first steps are to create an account on GitHub, and then select the option there to create a new remote repository to synchronize with our local repository. Select the option on GitHub to create a new repository, then add the requested information for your Zipf’s Law project. The remote repository doesn’t have to have the same name as the local one, but we will probably get confused if they are different, so the repository we create on GitHub will also be called zipf. The other default options are likely appropriate for your remote repository. Because we are synchronizing with an existing repository, do not add a README, .gitignore, or license; we’ll discuss these additions in other chapters. Next, we need to connect our desktop repository with the one on GitHub. We do this by making the GitHub repository a remote of the local repository. The home page of our new repository on GitHub includes the string we need to identify it (Figure 6.6). Figure 6.6: Find the link for a GitHub repository. We can click on “HTTPS” to change the URL from SSH to HTTPS and then copy that URL. HTTPS vs. SSH We use HTTPS here because it does not require additional configuration. If we want to set up SSH access so that we do not have to type in our password as often, the tutorials from GitHub, BitBucket, or GitLab explain the steps required. Next, let’s go into the local zipf repository and run this command: $ cd ~/zipf $ git remote add origin https://github.com/amira-khan/zipf.git Make sure to use the URL for your repository instead of the one shown: the only difference should be that it includes your username instead of amira-khan. A Git remote is like a bookmark: it gives a short name to a URL. In this case the remote’s name is origin; we could use anything we want, but origin is Git’s default, so we will stick with it. We can check that the command has worked by running git remote -v (where the -v option is short for verbose): $ git remote -v origin https://github.com/amira-khan/zipf.git (fetch) origin https://github.com/amira-khan/zipf.git (push) Git displays two lines because it’s actually possible to set up a remote to download from one URL but upload to another. Sensible people don’t do this, so we won’t explore this possibility any further. Now that we have configured a remote, we can push the work we have done so far to the repository on GitHub: $ git push origin master This may prompt us to enter our username and password; once we do that, Git prints a few lines of administrative information: Enumerating objects: 35, done. Counting objects: 100% (35/35), done. Delta compression using up to 4 threads Compressing objects: 100% (35/35), done. Writing objects: 100% (35/35), 2.17 MiB | 602.00 KiB/s, done. Total 35 (delta 7), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (7/7), done. To https://github.com/amira-khan/zipf.git * [new branch] master -&gt; master If we view our GitHub repository in the browser, it now includes all of our project files, along with all of the commits we have made so far (Figure 6.7). Figure 6.7: Viewing the repository’s history on GitHub. We can also pull changes from the remote repository to the local one: $ git pull origin master From https://github.com/amira-khan/zipf * branch master -&gt; FETCH_HEAD Already up-to-date. Pulling has no effect in this case because the two repositories are already synchronized. Fetching The second line in the remote configuration we viewed earlier is labeled push, which makes sense given the command we used (git push) to upload changes from our local to remote repositories. Why is the other line labeled fetch instead of pull? Fetching and pulling both download new data from a remote repository, but only pulling integrates those changes into your local repository’s version history. Because git fetch doesn’t alter your local files, it’s used to view changes between local and remote versions. The Git commands we’ve covered in this section (git pull, git push) are the main tasks associated with incorporating remote repositories into your workflow (Figure 6.5b). Amira’s Repository Amira’s repository referenced in this section does exist in on GitHub at amira-khan/zipf; you may find it a useful reference point when proceeding through the rest of the book. 6.7 Exploring History Git lets us look at previous versions of files and restore specific files to earlier states if we want to. In order to do these things, we need to identify the versions we want. The two ways to do this are analogous to absolute and relative paths. The “absolute” version is the unique identifier that Git gives to each commit. These identifiers are 40 characters long, but in most situations Git will let us use just the first half dozen characters or so. For example, if we run git log right now, it shows us something like this: commit ee8684ca123e1e829fc995d672e3d7e4b00f2610 (HEAD -&gt; master, origin/master) Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:52:04 2020 -0800 Update dracula plot commit 582f7f6f536d520b1328c04c9d41e24b54170656 Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:37:25 2020 -0800 Plot frequency against rank on log-log axes commit 851d590a214c7859eafa0998c6c951f8e0eb359b Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:32:41 2020 -0800 Add plot of word counts for &#39;Dracula&#39; commit 173222bf90216b408c8997f4e143572b99637750 Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:30:23 2020 -0800 Add scripts, novels, word counts, and plots The commit in which we changed plotcounts.py has the absolute identifier 582f7f6f536d520b1328c04c9d41e24b54170656, but we can use 582f7f6 to reference it in almost all situations. While git log includes the commit message, it doesn’t tell us exactly what changes were made in each commit. If we add the -p option (short for patch), we get the same kind of details git diff provides to describe the changes in each commit: $ git log -p The first part of the output is shown below; we have truncated the rest, since it is very long: commit ee8684ca123e1e829fc995d672e3d7e4b00f2610 (HEAD -&gt; master, origin/master) Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:52:04 2020 -0800 Update dracula plot diff --git a/results/dracula.png b/results/dracula.png index c1f62fd..57a7b70 100644 Binary files a/results/dracula.png and b/results/dracula.png differ ... Alternatively, we can use git diff directly to examine the differences between files at any stage in the repository’s history. Let’s explore this with the plotcounts.py file. We no longer need the line of code in plotcounts.py that calculates the inverse rank: df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] If we delete that line from bin/plotcounts.py, git diff on its own will show the difference between the file as it is now and the most recent version: diff --git a/bin/plotcounts.py b/bin/plotcounts.py index c4c5b5a..c511da1 100644 --- a/bin/plotcounts.py +++ b/bin/plotcounts.py @@ -11,7 +11,6 @@ def main(args): names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) - df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], git diff 582f7f6, on the other hand, shows the difference between the current state and the commit referenced by the short identifier: diff --git a/bin/plotcounts.py b/bin/plotcounts.py index c4c5b5a..c511da1 100644 --- a/bin/plotcounts.py +++ b/bin/plotcounts.py @@ -11,7 +11,6 @@ def main(args): names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) - df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], diff --git a/results/dracula.png b/results/dracula.png index c1f62fd..57a7b70 100644 Binary files a/results/dracula.png and b/results/dracula.png differ Note that you will need to reference your git log to replace 582f7f6 in the code above, since Git assigned your commit a different unique identifier. Note also that we have not committed the last change to plotcounts.py; we will look at ways of undoing it in the next section. The “relative” version of history relies on a special identifier called HEAD, which always refers to the most recent version in the repository. git diff HEAD therefore shows the same thing as git diff, but instead of typing in a version identifier to back up one commit, we can use HEAD~1 (where ~ is the tilde symbol). This shorthand is read “HEAD minus one”, and gives us the difference to the previous saved version. git diff HEAD~2 goes back two revisions and so on. We can also look at the differences between two saved versions by separating their identifiers with two dots .. like this: $ git diff HEAD~1..HEAD~2 diff --git a/bin/plotcounts.py b/bin/plotcounts.py index c4c5b5a..f274473 100644 --- a/bin/plotcounts.py +++ b/bin/plotcounts.py @@ -13,7 +13,7 @@ def main(args): method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, - y=&#39;rank&#39;, loglog=True, + y=&#39;inverse_rank&#39;, figsize=[12, 6], grid=True, xlim=args.xlim) If we want to see the changes made in a particular commit, we can use git show with an identifier and a file name: $ git show HEAD~1 bin/plotcounts.py commit 582f7f6f536d520b1328c04c9d41e24b54170656 Author: Amira Khan &lt;amira@zipf.org&gt; Date: Sat Dec 19 09:37:25 2020 -0800 Plot frequency against rank on log-log axes diff --git a/bin/plotcounts.py b/bin/plotcounts.py index f274473..c4c5b5a 100644 --- a/bin/plotcounts.py +++ b/bin/plotcounts.py @@ -13,7 +13,7 @@ def main(args): method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, - y=&#39;inverse_rank&#39;, + y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) If we wanted to view the contents of a file at a given point in the version history, we could use the same command, but separating the identifier and file with a colon: $ git show HEAD~1:bin/plotcounts.py This allows us to look through the file using a paging program. 6.8 Restoring Old Versions of Files We can see what we changed, but how can we restore it? Suppose we change our mind about the last update to bin/plotcounts.py (removing df['inverse_rank'] = 1 / df['rank']) before we add or commit it. git status tells us that the file has been changed, but those changes haven’t been staged: $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: bin/plotcounts.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) We can put things back the way they were in the last saved revision using git restore, as the screen output suggests: $ git restore bin/plotcounts.py $ git status On branch master nothing to commit, working tree clean As its name suggests, git restore restores an earlier version of a file. In this case, we used it to recover the version of the file in the most recent commit. Checking Out with Git If you’re running a different version of Git, you may see a suggestion for git checkout instead of git restore. As of Git version 2.29, git restore is still an experimental command, and operates as a specialized form of git checkout. git checkout HEAD bin/plotcounts.py is equivalent to the last command run. We can confirm the file has been restored by printing the relevant lines of the file: $ head -n 19 bin/plotcounts.py | tail -n 8 df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) Because git restore is designed to restore working files, we’ll need to use git checkout to revert to earlier versions of files. We can use a specific commit identifier rather than HEAD to go back as far as we want: $ git checkout 851d590 bin/plotcounts.py Updated 1 path from c8d6a33 Doing this does not change the history: git log still shows our four commits. Instead, it replaces the content of the file with the old content: $ git status On branch master Changes to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: bin/plotcounts.py $ head -n 19 bin/plotcounts.py | tail -n 8 df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;inverse_rank&#39;, figsize=[12, 6], grid=True, xlim=args.xlim) If we change our mind again, we can use the suggestion in the output to restore the earlier version. Because checking out the changes added them to the staging area, we need to first remove them from the staging area: $ git restore --staged bin/plotcounts.py However, the changes have been unstaged but still exist in the file. We can return the file to the state of the most recent commit: $ git restore bin/plotcounts.py $ git status On branch master nothing to commit, working tree clean $ head -n 19 bin/plotcounts.py | tail -n 8 df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) We have restored the most recent commit. Since we didn’t commit the change that removed the line that calculates the inverse rank, that work is now lost: Git can only go back and forth between committed versions of files. This section has demonstrated a few different ways to view differences among versions, and to work with those changes (Figure 6.5c). These commands can operate on either individual files or entire commits, and the behavior of them can sometimes differ based on your version of Git. Remember to reference documentation, and use git status and git log frequently to understand your workflow. 6.9 Ignoring Files We don’t always want Git to track every file’s history. For example, we might want to track text files with names ending in .txt but not data files with names ending in .dat. To stop Git from telling us about these files every time we call git status, we can create a file in the root directory of our project called .gitignore. This file can contain filenames like thesis.pdf or wildcard patterns like *.dat. Each must be on a line of its own, and Git will ignore anything that matches any of these lines. For now we only need one entry in our .gitignore file: __pycache__ which tells Git to ignore any __pycache__ directory created by Python (Section 5.8). Remember to Ignore Don’t forget to commit .gitignore to your repository so that Git knows to use it. 6.10 Summary The biggest benefit of version control for individual research is that we can always go back to the precise set of files that we used to produce a particular result. While Git is complex (Perez De Rosso and Jackson 2013), being able to back up our changes on sites like GitHub with just a few keystrokes can save us a lot of pain, and some of Git’s advanced features make it even more powerful. We will explore these in the next chapter. 6.11 Exercises 6.11.1 Places to create Git repositories Along with information about the Zipf’s Law project, Amira would also like to keep some notes on Heaps’ Law. Despite her colleagues’ concerns, Amira creates a heaps-law project inside her zipf project as follows: $ cd ~/zipf # go into zipf directory, which is a Git repo $ mkdir heaps-law # make a subdirectory zipf/heaps-law $ cd heaps-law # go into heaps-law subdirectory $ git init heaps-law # make heaps-law a Git repository Is the git init command that she runs inside the heaps-law subdirectory required for tracking files stored there? 6.11.2 Removing before saving If you’re working on an older version of Git, you may see an output from git status suggesting you can take files out of the staging area using git rm --cached. Try this out: Create a new file in an initialized Git repository called example.txt. Use git add example.txt to add this file. Use git status to check that Git has noticed it. Use git rm --cached example.txt to remove it from the list of things to be saved. What does git status now show? What (if anything) has happened to the file? 6.11.3 Viewing changes Make a few changes to a file in a Git repository, then view those differences using both git diff and git diff --word-diff. Which output do you find easiest to understand? Note: If you performed this exercise in your Zipf’s Law project, we recommend discarding (not committing) your changes made to a file. 6.11.4 Committing changes Which command(s) below would save changes to myfile.txt to a local Git repository? # Option 1 $ git commit -m &quot;Add recent changes&quot; # Option 2 $ git init myfile.txt $ git commit -m &quot;Add recent changes&quot; # Option 3 $ git add myfile.txt $ git commit -m &quot;Add recent changes&quot; # Option 4 $ git commit -m myfile.txt &quot;Add recent changes&quot; 6.11.5 Write your biography Create a new Git repository on your computer called bio. Make sure the directory containing this repository is outside your zipf project directory! Write a three-line biography for yourself in a file called me.txt and commit your changes. Modify one line and add a fourth line. Display the differences between the file’s original state and its updated state. 6.11.6 Committing multiple files The staging area can hold changes from any number of files that you want to commit as a single snapshot. From your new bio directory, following after the previous exercise (in which you have uncommitted changes to me.txt): Create another new file employment.txt and add the details of your most recent job. Add the new changes to both me.txt and employment.txt to the staging area and commit those changes. 6.11.7 GitHub timestamps Create a remote repository on GitHub for your new bio repository. Push the contents of your local repository to the remote. Make another change to your local repository (e.g. by editing me.txt or employment.txt) and push these changes as well. Go to the repo you just created on GitHub and check the timestamps of the files. How does GitHub record times, and why? 6.11.8 Workflow and history Assume you made the following changes in your bio repository. What is the output of the last command in the sequence below? $ echo &quot;Sharing information about myself.&quot; &gt; motivation.txt $ git add motivation.txt $ echo &quot;Documenting major milestones.&quot; &gt; motivation.txt $ git commit -m &quot;Motivate project&quot; $ git restore motivation.txt $ cat motivation.txt Sharing information about myself. Documenting major milestones. Sharing information about myself. Documenting major milestones. An error message because we have changed motivation.txt without committing first. 6.11.9 Ignoring nested files Suppose our project has a directory results with two subdirectories called data and plots. How would we ignore all of the files in results/plots but not ignore files in results/data? 6.11.10 Including specific files How would you ignore all .dat files in your root directory except for final.dat? (Hint: find out what the exclamation mark ! means in a .gitignore file.) 6.11.11 Exploring the GitHub interface Browse to your zipf repository on GitHub. Under the Code tab, find and click on the text that says “NN commits” (where “NN” is some number). Hover over and click on the three buttons to the right of each commit. What information can you gather/explore from these buttons? How would you get that same information in the shell? 6.11.12 Push versus commit Explain in one or two sentences how git push is different from git commit. 6.11.13 License and README files When we initialized our remote zipf GitHub repo, we didn’t add a README.md or license file. If we had, what would have happened when we tried to link our local and remote repositories? 6.11.14 Recovering older versions of a file Amira made changes this morning to a shell script called data_cruncher.sh that she has been working on for weeks. Her changes broke the script, and she has now spent an hour trying to get it back in working order. Luckily, she has been keeping track of her project’s versions using Git. Which of the commands below can she use to recover the last committed version of her script? $ git checkout HEAD $ git checkout HEAD data_cruncher.sh $ git checkout HEAD~1 data_cruncher.sh $ git checkout &lt;unique ID of last commit&gt; data_cruncher.sh $ git restore data_cruncher.sh $ git restore HEAD 6.11.15 Understanding git diff Using your zipf project directory: What would the command git diff HEAD~9 bin/plotcounts.py do if we run it? What does it actually do? What does git diff HEAD bin/plotcounts.py do? 6.11.16 Getting rid of staged changes git checkout can be used to restore a previous commit when unstaged changes have been made, but will it also work for changes that have been staged but not committed? To find out, use your zipf project directory to: Change bin/plotcounts.py. Use git add on those changes to bin/plotcounts.py. Use git checkout to see if you can remove your change. Does it work? 6.11.17 Figuring out who did what We never committed the last edit that removes the calculation of inverse rank. Remove this line from plotcounts.py, then commit the change: df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] Run the command git blame bin/plotcounts.py. What does each line of the output show? 6.12 Key Points Use git config with the --global option to configure your user name, email address, and other preferences once per machine. git init initializes a repository. Git stores all repository management data in the .git subdirectory of the repository’s root directory. git status shows the status of a repository. git add puts files in the repository’s staging area. git commit saves the staged content as a new commit in the local repository. git log lists previous commits. git diff shows the difference between two versions of the repository. Synchronize your local repository with a remote repository on a forge such as GitHub. git remote manages bookmarks pointing at remote repositories. git push copies changes from a local repository to a remote repository. git pull copies changes from a remote repository to a local repository. git restore and git checkout recover old versions of files. The .gitignore file tells Git what files to ignore. "],["git-advanced.html", "Chapter 7 Going Further with Git 7.1 What’s a Branch? 7.2 Creating a Branch 7.3 What Curve Should We Fit? 7.4 Verifying Zipf’s Law 7.5 Merging 7.6 Handling Conflicts 7.7 A Branch-Based Workflow 7.8 Using Other People’s Work 7.9 Pull Requests 7.10 Handling Conflicts in Pull Requests 7.11 Summary 7.12 Exercises 7.13 Key Points", " Chapter 7 Going Further with Git It’s got three keyboards and a hundred extra knobs, including twelve with ‘?’ on them. — Terry Pratchett Two of Git’s advanced features let us to do much more than just track our work. Branches let us work on multiple things simultaneously in a single repository; pull requests (PRs) let us submit our work for review, get feedback, and make updates. Used together, they allow us to go through the write-review-revise cycle familiar to anyone who has ever written a journal paper in hours rather than weeks. Your zipf project directory should now include: zipf/ ├── .gitignore ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── script_template.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ ├── frankenstein.txt │ └── ... └── results ├── dracula.csv ├── dracula.png ├── jane_eyre.csv ├── jane_eyre.png └── moby_dick.csv All of these files should also be tracked in your version history. We’ll use them and some additional analyses to explore Zipf’s Law using Git’s advanced features. 7.1 What’s a Branch? So far we have only used a sequential timeline with Git: each change builds on the one before, and only on the one before. However, there are times when we want to try things out without disrupting our main work. To do this, we can use branches to work on separate tasks in parallel. Each branch is a parallel timeline; changes made on the branch only affect that branch unless and until we explicitly combine them with work done in another branch. We can see what branches exist in a repository using this command: $ git branch * master When we initialize a repository, Git automatically creates a branch called master. It is often considered the “official” version of the repository. The asterisk * indicates that it is currently active, i.e., that all changes we make will take place in this branch by default. (The active branch is like the current working directory in the shell.) Default Branches In mid-2020, GitHub changed the name of the default branch (the first branch created when a repository is initialized) from “master” to “main.” Owners of repositories may also change the name of the default branch. This means that the name of the default branch may be different among repositories based on when and where it was created, as well as who manages it. In the previous chapter, we foreshadowed some experimental changes that we could try and make to plotcounts.py. Making sure our project directory is our working directory, we can inspect our current plotcounts.py: $ cd ~/zipf $ cat bin/plotcounts.py &quot;&quot;&quot;Plot word counts.&quot;&quot;&quot; import argparse import pandas as pd def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; df = pd.read_csv(args.infile, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) ax.figure.savefig(args.outfile) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Word count csv file name&#39;) parser.add_argument(&#39;--outfile&#39;, type=str, default=&#39;plotcounts.png&#39;, help=&#39;Output image file name&#39;) parser.add_argument(&#39;--xlim&#39;, type=float, nargs=2, metavar=(&#39;XMIN&#39;, &#39;XMAX&#39;), default=None, help=&#39;X-axis limits&#39;) args = parser.parse_args() main(args) We used this version of plotcounts.py to display the word counts for Dracula on a log-log plot (Figure 6.4). The relationship between word count and rank looked linear, but since the eye is easily fooled, we should fit a curve to the data. Doing this will require more than just a trivial change to the script, so to ensure that this version of plotcounts.py keeps working while we try to build a new one, we will do our work in a separate branch. Once we have successfully added curve fitting to plotcounts.py, we can decide if we want to merge our changes back into the master branch. 7.2 Creating a Branch To create a new branch called fit, we run: $ git branch fit We can check that the branch exists by running git branch again: $ git branch * master fit Our branch is there, but the asterisk * shows that we are still in the master branch. (By analogy, creating a new directory doesn’t automatically move us into that directory.) As a further check, let’s see what our repository’s status is: $ git status On branch master nothing to commit, working directory clean To switch to our new branch we can use the checkout command that we first saw in Chapter 6: $ git checkout fit $ git branch master * fit In this case, we’re using git checkout to check out a whole repository, i.e., switch it from one saved state to another. We should choose the name to signal the purpose of the branch, just as we choose the names of files and variables to indicate what they are for. We haven’t made any changes since switching to the fit branch, so at this point master and fit are at the same point in the repository’s history. Commands like ls and git log therefore show that the files and history haven’t changed. Where Are Branches Saved? Git saves every version of every file in the .git directory that it creates in the project’s root directory. When we switch from one branch to another, it replaces the files we see with their counterparts from the branch we’re switching to. It also rearranges directories as needed so that those files are in the right places. 7.3 What Curve Should We Fit? Before we make any changes to our new branch, we need to figure out how to fit a line to the word count data. Zipf’s Law says that: The second most common word in a body of text appears half as often as the most common, the third most common appears a third as often, and so on. In other words, the frequency of a word (\\(f\\)) is proportional to its inverse rank (\\(r\\)), \\[ f \\propto \\frac{1}{r^\\alpha} \\] with a value of \\(\\alpha\\) close to one. The reason \\(\\alpha\\) must be close to one for Zipf’s Law to hold becomes clear if we include it in a modified version of the earlier definition: The most frequent word will occur approximately \\(2^\\alpha\\) times as often as the second most frequent word, \\(3^\\alpha\\) times as often as the third most frequent word, and so on. This mathematical expression for Zipf’s Law is an example of a power law. In general, when two variables \\(x\\) and \\(y\\) are related through a power law, so that \\[ y = ax^b \\] taking logarithms of both sides yields a linear relationship: \\[ \\log(y) = \\log(a) + b\\log(x) \\] Hence, plotting the variables on a log-log scale reveals this linear relationship. If Zipf’s Law holds, we should have \\[ r = cf^{\\frac{-1}{\\alpha}} \\] where \\(c\\) is a constant of proportionality. The linear relationship between the log word frequency and log rank is then \\[ \\log(r) = \\log(c) - \\frac{1}{\\alpha}\\log(f) \\] This suggests that the points on our log-log plot should fall on a straight line with a slope of \\(- \\tfrac{1}{\\alpha}\\) and intercept \\(\\log(c)\\). To fit a line to our word count data we therefore need to estimate the value of \\(\\alpha\\); we’ll see later that \\(c\\) is completely defined. In order to determine the best method for estimating \\(\\alpha\\) we turn to Moreno-Sánchez, Font-Clos, and Corral (2016), which suggests using a method called maximum likelihood estimation. The likelihood function is the probability of our observed data as a function of the parameters in the statistical model that we assume generated it. We estimate the parameters in the model by choosing them to maximize this likelihood; computationally, it is often easier to minimize the negative log likelihood function. Moreno-Sánchez, Font-Clos, and Corral (2016) define the likelihood using a parameter \\(\\beta\\), which is related to the \\(\\alpha\\) parameter in our definition of Zipf’s Law through \\(\\alpha = \\tfrac{1}{\\beta-1}\\). Under their model, the value of \\(c\\) is the total number of unique words, or equivalently the largest value of the rank. Expressed as a Python function, the negative log likelihood function is: import numpy as np def nlog_likelihood(beta, counts): &quot;&quot;&quot;Log-likelihood function.&quot;&quot;&quot; likelihood = - np.sum(np.log((1/counts)**(beta - 1) - (1/(counts + 1))**(beta - 1))) return likelihood Obtaining an estimate of \\(\\beta\\) (and thus \\(\\alpha\\)) then becomes a numerical optimization problem, for which we can use the scipy.optimize library. Again following Moreno-Sánchez, Font-Clos, and Corral (2016), we use Brent’s Method with \\(1 &lt; \\beta \\leq 4\\). from scipy.optimize import minimize_scalar def get_power_law_params(word_counts): &quot;&quot;&quot;Get the power law parameters.&quot;&quot;&quot; mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4), args=word_counts, method=&#39;brent&#39;) beta = mle.x alpha = 1 / (beta - 1) return alpha We can then plot the fitted curve on the plot axes (ax) defined in the plotcounts.py script: def plot_fit(curve_xmin, curve_xmax, max_rank, alpha, ax): &quot;&quot;&quot; Plot the power law curve that was fitted to the data. Parameters ---------- curve_xmin : float Minimum x-bound for fitted curve curve_xmax : float Maximum x-bound for fitted curve max_rank : int Maximum word frequency rank. alpha : float Estimated alpha parameter for the power law. ax : matplotlib axes Scatter plot to which the power curve will be added. &quot;&quot;&quot; xvals = np.arange(curve_xmin, curve_xmax) yvals = max_rank * (xvals**(-1 / alpha)) ax.loglog(xvals, yvals, color=&#39;grey&#39;) where the maximum word frequency rank corresponds to \\(c\\), and \\(-1 / \\alpha\\) the exponent in the power law. We have followed the numpydoc format for the detailed docstring in plot_fit - see Appendix G for more information about docstring formats. 7.4 Verifying Zipf’s Law Now that we can fit a curve to our word count plots, we can update plotcounts.py so the entire script reads as follows: &quot;&quot;&quot;Plot word counts.&quot;&quot;&quot; import argparse import numpy as np import pandas as pd from scipy.optimize import minimize_scalar def nlog_likelihood(beta, counts): &quot;&quot;&quot;Log-likelihood function.&quot;&quot;&quot; likelihood = - np.sum(np.log((1/counts)**(beta - 1) - (1/(counts + 1))**(beta - 1))) return likelihood def get_power_law_params(word_counts): &quot;&quot;&quot;Get the power law parameters.&quot;&quot;&quot; mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4), args=word_counts, method=&#39;brent&#39;) beta = mle.x alpha = 1 / (beta - 1) return alpha def plot_fit(curve_xmin, curve_xmax, max_rank, alpha, ax): &quot;&quot;&quot; Plot the power law curve that was fitted to the data. Parameters ---------- curve_xmin : float Minimum x-bound for fitted curve curve_xmax : float Maximum x-bound for fitted curve max_rank : int Maximum word frequency rank. alpha : float Estimated alpha parameter for the power law. ax : matplotlib axes Scatter plot to which the power curve will be added. &quot;&quot;&quot; xvals = np.arange(curve_xmin, curve_xmax) yvals = max_rank * (xvals**(-1 / alpha)) ax.loglog(xvals, yvals, color=&#39;grey&#39;) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; df = pd.read_csv(args.infile, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) word_counts = df[&#39;word_frequency&#39;].to_numpy() alpha = get_power_law_params(word_counts) print(&#39;alpha:&#39;, alpha) # Since the ranks are already sorted, we can take the last # one instead of computing which row has the highest rank max_rank = df[&#39;rank&#39;].to_numpy()[-1] # Use the range of the data as the boundaries # when drawing the power law curve curve_xmin = df[&#39;word_frequency&#39;].min() curve_xmax = df[&#39;word_frequency&#39;].max() plot_fit(curve_xmin, curve_xmax, max_rank, alpha, ax) ax.figure.savefig(args.outfile) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Word count csv file name&#39;) parser.add_argument(&#39;--outfile&#39;, type=str, default=&#39;plotcounts.png&#39;, help=&#39;Output image file name&#39;) parser.add_argument(&#39;--xlim&#39;, type=float, nargs=2, metavar=(&#39;XMIN&#39;, &#39;XMAX&#39;), default=None, help=&#39;X-axis limits&#39;) args = parser.parse_args() main(args) We can then run the script to obtain the \\(\\alpha\\) value for Dracula and a new plot with a line fitted. $ python bin/plotcounts.py results/dracula.csv --outfile results/dracula.png alpha: 1.0866646252515038 So according to our fit, the most frequent word will occur approximately \\(2^{1.1}=2.1\\) times as often as the second most frequent word, \\(3^{1.1}=3.3\\) times as often as the third most frequent word, and so on. Figure 7.1 shows the plot. Figure 7.1: Word frequency distribution for Dracula. The script appears to be working as we’d like, so we can go ahead and commit our changes to the fit development branch: $ git add bin/plotcounts.py results/dracula.png $ git commit -m &quot;Added fit to word count data&quot; [fit 38c209b] Added fit to word count data 2 files changed, 57 insertions(+) rewrite results/dracula.png (99%) If we look at the last couple of commits using git log, we see our most recent change: $ git log --oneline -n 2 38c209b (HEAD -&gt; fit) Added fit to word count data ddb00fb (origin/master, master) removing inverse rank calculation (We use --oneline and -n 2 to shorten the log display.) But if we switch back to the master branch: $ git checkout master $ git branch fit * master and look at the log, our change is not there: $ git log --oneline -n 2 ddb00fb (HEAD -&gt; master, origin/master) removing inverse rank calculation 7de9877 ignoring __pycache__ We have not lost our work: it just isn’t included in this branch. We can prove this by switching back to the fit branch and checking the log again: $ git checkout fit $ git log --oneline -n 2 38c209b (HEAD -&gt; fit) Added fit to word count data ddb00fb (origin/master, master) removing inverse rank calculation We can also look inside plotcounts.py and see our changes. If we make another change and commit it, that change will also go into the fit branch. For instance, we could add some additional information to one of our docstrings to make it clear what equations were used in estimating \\(\\alpha\\). def get_power_law_params(word_counts): &quot;&quot;&quot; Get the power law parameters. References ---------- Moreno-Sanchez et al (2016) define alpha (Eq. 1), beta (Eq. 2) and the maximum likelihood estimation (mle) of beta (Eq. 6). Moreno-Sanchez I, Font-Clos F, Corral A (2016) Large-Scale Analysis of Zipf&#39;s Law in English Texts. PLoS ONE 11(1): e0147073. https://doi.org/10.1371/journal.pone.0147073 &quot;&quot;&quot; mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4), args=word_counts, method=&#39;brent&#39;) beta = mle.x alpha = 1 / (beta - 1) return alpha $ git add bin/plotcounts.py $ git commit -m &quot;Adding Moreno-Sanchez et al (2016) reference&quot; [fit 1577404] Adding Moreno-Sanchez et al (2016) reference 1 file changed, 14 insertions(+), 1 deletion(-) Finally, if we want to see the differences between two branches, we can use git diff with the same double-dot .. syntax used to view differences between two revisions: $ git diff master..fit diff --git a/bin/plotcounts.py b/bin/plotcounts.py index c511da1..6905b6e 100644 --- a/bin/plotcounts.py +++ b/bin/plotcounts.py @@ -2,7 +2,62 @@ import argparse +import numpy as np import pandas as pd +from scipy.optimize import minimize_scalar + + +def nlog_likelihood(beta, counts): + &quot;&quot;&quot;Log-likelihood function.&quot;&quot;&quot; + likelihood = - np.sum(np.log((1/counts)**(beta - 1) + - (1/(counts + 1))**(beta - 1))) + return likelihood + + +def get_power_law_params(word_counts): + &quot;&quot;&quot; + Get the power law parameters. + + References + ---------- + Moreno-Sanchez et al (2016) define alpha (Eq. 1), + beta (Eq. 2) and the maximum likelihood estimation (mle) + of beta (Eq. 6). + + Moreno-Sanchez I, Font-Clos F, Corral A (2016) + Large-Scale Analysis of Zipf&#39;s Law in English Texts. + PLoS ONE 11(1): e0147073. + https://doi.org/10.1371/journal.pone.0147073 + &quot;&quot;&quot; + mle = minimize_scalar(nlog_likelihood, + bracket=(1 + 1e-10, 4), + args=word_counts, + method=&#39;brent&#39;) + beta = mle.x + alpha = 1 / (beta - 1) + return alpha + + +def plot_fit(curve_xmin, curve_xmax, max_rank, alpha, ax): + &quot;&quot;&quot; + Plot the power law curve that was fitted to the data. + + Parameters + ---------- + curve_xmin : float + Minimum x-bound for fitted curve + curve_xmax : float + Maximum x-bound for fitted curve + max_rank : int + Maximum word frequency rank. + alpha : float + Estimated alpha parameter for the power law. + ax : matplotlib axes + Scatter plot to which the power curve will be added. + &quot;&quot;&quot; + xvals = np.arange(curve_xmin, curve_xmax) + yvals = max_rank * (xvals**(-1 / alpha)) + ax.loglog(xvals, yvals, color=&#39;grey&#39;) def main(args): @@ -16,6 +71,21 @@ def main(args): figsize=[12, 6], grid=True, xlim=args.xlim) + + word_counts = df[&#39;word_frequency&#39;].to_numpy() + alpha = get_power_law_params(word_counts) + print(&#39;alpha:&#39;, alpha) + + # Since the ranks are already sorted, we can take the last + # one instead of computing which row has the highest rank + max_rank = df[&#39;rank&#39;].to_numpy()[-1] + + # Use the range of the data as the boundaries + # when drawing the power law curve + curve_xmin = df[&#39;word_frequency&#39;].min() + curve_xmax = df[&#39;word_frequency&#39;].max() + + plot_fit(curve_xmin, curve_xmax, max_rank, alpha, ax) ax.figure.savefig(args.outfile) diff --git a/results/dracula.png b/results/dracula.png index 57a7b70..5f10271 100644 Binary files a/results/dracula.png and b/results/dracula.png differ Why Branch? Why go to all this trouble? Imagine we are in the middle of debugging a change like this when we are asked to make final revisions to a paper that was created using the old code. If we revert plotcount.py to its previous state we might lose our changes. If instead we have been doing the work on a branch, we can switch branches, create the plot, and switch back in complete safety. 7.5 Merging We could proceed in three ways at this point: Add our changes to plotcounts.py once again in the master branch. Stop working in master and start using the fit branch for future development. Merge the fit and master branches. The first option is tedious and error-prone; the second will lead to a bewildering proliferation of branches, but the third option is simple, fast, and reliable. To start, let’s make sure we’re in the master branch: $ git checkout master $ git branch fit * master We can now merge the changes in the fit branch into our current branch with a single command: $ git merge fit Updating ddb00fb..1577404 Fast-forward bin/plotcounts.py | 70 ++++++++++++++++++++++++++++++++++++++ +++++++++++++++++++++++++++++++ results/dracula.png | Bin 23291 -&gt; 38757 bytes 2 files changed, 70 insertions(+) Merging doesn’t change the source branch fit, but once the merge is done, all of the changes made in fit are also in the history of master: $ git log --oneline -n 4 1577404 (HEAD -&gt; master, fit) Adding Moreno-Sanchez et al (2016) reference 38c209b Added fit to word count data ddb00fb (origin/master) removing inverse rank calculation 7de9877 ignoring __pycache__ Note that Git automatically creates a new commit (in this case, 1577404) to represent the merge. If we now run git diff master..fit, Git doesn’t print anything because there aren’t any differences to show. Now that we have merged all of the changes from fit into master there is no need to keep the fit branch, so we can delete it: $ git branch -d fit Deleted branch fit (was 1577404). Not Just the Command Line We have been creating, merging, and deleting branches on the command line, but we can do all of these things using GitKraken, the RStudio IDE, and other GUIs. The operations stay the same; all that changes is how we tell the computer what we want to do. 7.6 Handling Conflicts A conflict occurs when a line has been changed in different ways in two separate branches or when a file has been deleted in one branch but edited in the other. Merging fit into master went smoothly because there were no conflicts between the two branches, but if we are going to use branches, we must learn how to merge conflicts. To start, use nano to add the project’s title to a new file called README.md in the master branch, which we can the view: $ cat README.md # Zipf&#39;s Law $ git add README.md $ git commit -m &quot;Initial commit of README file&quot; [master 232b564] Initial commit of README file 1 file changed, 1 insertion(+) create mode 100644 README.md Now let’s create a new development branch called docs to work on improving the documentation for our code. We will use git checkout -b to create a new branch and switch to it in a single step: $ git checkout -b docs Switched to a new branch &#39;docs&#39; $ git branch * docs master On this new branch, let’s add some information to the README file: # Zipf&#39;s Law These Zipf&#39;s Law scripts tally the occurrences of words in text files and plot each word&#39;s rank versus its frequency. $ git add README.md $ git commit -m &quot;Added repository overview&quot; [docs a0b88e5] Added repository overview 1 file changed, 3 insertions(+) In order to create a conflict, let’s switch back to the master branch. The changes we made in the docs branch are not present: $ git checkout master Switched to branch &#39;master&#39; $ cat README.md # Zipf&#39;s Law Let’s add some information about the contributors to our work: # Zipf&#39;s Law ## Contributors - Amira Khan &lt;amira@zipf.org&gt; $ git add README.md $ git commit -m &quot;Added contributor list&quot; [master 45a576b] Added contributor list 1 file changed, 4 insertions(+) We now have two branches, master and docs, in which we have changed README.md in different ways: $ git diff docs..master diff --git a/README.md b/README.md index f40e895..71f67db 100644 --- a/README.md +++ b/README.md @@ -1,4 +1,5 @@ # Zipf&#39;s Law -These Zipf&#39;s Law scripts tally the occurrences of words in text -files and plot each word&#39;s rank versus its frequency. +## Contributors + +- Amira Khan &lt;amira@zipf.org&gt; When we try to merge docs into master, Git doesn’t know which of these changes to keep: $ git merge docs master Auto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result. If we look in README.md, we see that Git has kept both sets of changes, but has marked which came from where: $ cat README.md # Zipf&#39;s Law &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ## Contributors - Amira Khan &lt;amira@zipf.org&gt; ======= These Zipf&#39;s Law scripts tally the occurrences of words in text files and plot each word&#39;s rank versus its frequency. &gt;&gt;&gt;&gt;&gt;&gt;&gt; docs The lines from &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD to ======= are what was in master, while the lines from there to &gt;&gt;&gt;&gt;&gt;&gt;&gt; docs show what was in docs. If there were several conflicting regions in the same file, Git would mark each one this way. We have to decide what to do next: keep the master changes, keep those from docs, edit this part of the file to combine them, or write something new. Whatever we do, we must remove the &gt;&gt;&gt;, ===, and &lt;&lt;&lt; markers. Let’s combine the two sets of changes so the resulting file reads: # Zipf&#39;s Law These Zipf&#39;s Law scripts tally the occurrences of words in text files and plot each word&#39;s rank versus its frequency. ## Contributors - Amira Khan &lt;amira@zipf.org&gt; We can now add the file and commit the change, just as we would after any other edit: $ git add README.md $ git commit -m &quot;Merging README additions&quot; [master 55c63d0] Merging README additions Our branch’s history now shows a single sequence of commits, with the master changes on top of the earlier docs changes: $ git log --oneline -n 4 55c63d0 (HEAD -&gt; master) Merging README additions 45a576b Added contributor list a0b88e5 (docs) Added repository overview 232b564 Initial commit of README file If we want to see what really happened, we can add the --graph option to git log: $ git log --oneline --graph -n 4 * 55c63d0 (HEAD -&gt; master) Merging README additions |\\ | * a0b88e5 (docs) Added repository overview * | 45a576b Added contributor list |/ * 232b564 Initial commit of README file At this point we can delete the docs branch: $ git branch -d docs Deleted branch docs (was a0b88e5). Alternatively, we can keep using docs for documentation updates. Each time we switch to it, we merge changes from master into docs, do our editing (while switching back to master or other branches as needed to work on the code), and then merge from docs to master once the documentation is updated. Remember to Push If you are using a remote repository, don’t forget to use git push to keep your version on GitHub up to date with your local version. 7.7 A Branch-Based Workflow What is the best way to incorporate branching into our regular coding practice? If we are working on our own computer, this workflow will help us keep track of what we are doing: git checkout master to make sure we are in the master branch. git checkout -b name-of-feature to create a new branch. We always create a branch when making changes, since we never know what else might come up. The branch name should be as descriptive as a variable name or filename would be. Make our changes. If something occurs to us along the way—for example, if we are writing a new function and realize that the documentation for some other function should be updated—we do not do that work in this branch just because we happen to be there. Instead, we commit our changes, switch back to master, and create a new branch for the other work. When the new feature is complete, we git merge master name-of-feature to get any changes we merged into master after creating name-of-feature and resolve any conflicts. This is an important step: we want to do the merge and test that everything still works in our feature branch, not in master. Finally, we switch back to master and git merge name-of-feature master to merge our changes into master. We should not have any conflicts, and all of our tests should pass. Most experienced developers use this branch-per-feature workflow, but what exactly is a “feature”? These rules make sense for small projects: Anything cosmetic that is only one or two lines long can be done in master and committed right away. Here, “cosmetic” means changes to comments or documentation: nothing that affects how code runs, not even a simple variable renaming. A pure addition that doesn’t change anything else is a feature and goes into a branch. For example, if we run a new analysis and save the results, that should be done on its own branch because it might take several tries to get the analysis to run, and we might interrupt ourselves to fix things that we discover aren’t working. Every change to code that someone might want to undo later in one step is a feature. For example, if a new parameter is added to a function, then every call to the function has to be updated. Since neither alteration makes sense without the other, those changes are considered a single feature and should be done in one branch. The hardest thing about using a branch-per-feature workflow is sticking to it for small changes. As the first point in the list above suggests, most people are pragmatic about this on small projects; on large ones, where dozens of people might be committing, even the smallest and most innocuous change needs to be in its own branch so that it can be reviewed (which we discuss below). 7.8 Using Other People’s Work So far we have used Git to manage individual work, but it really comes into its own when we are working with other people. We can do this in two ways: Everyone has read and write access to a single shared repository. Everyone can read from the project’s main repository, but only a few people can commit changes to it. The project’s other contributors fork the main repository to create one that they own, do their work in that, and then submit their changes to the main repository. The first approach works well for teams of up to half a dozen people who are all comfortable using Git, but if the project is larger, or if contributors are worried that they might make a mess in the master branch, the second approach is safer. Git itself doesn’t have any notion of a “main repository”, but forges like GitHub, GitLab, and BitBucket all encourage people to use Git in ways that effectively create one. Suppose, for example, that Sami wants to contribute to the Zipf’s Law code that Amira is hosting on GitHub at https://github.com/amira-khan/zipf. Sami can go to that URL and click on the “Fork” button in the upper right corner (Figure 7.2). GitHub immediately creates a copy of Amira’s repository within Sami’s account on GitHub’s own servers. Figure 7.2: Forking a repository. When the command completes, the setup on GitHub now looks like Figure 7.3. Nothing has happened yet on Sami’s own machine: the new repository exists only on GitHub. When Sami explores its history, they see that it contains all of the changes Amira made. Figure 7.3: Repositories on GitHub after forking. A copy of a repository is called a clone. In order to start working on the project, Sami needs a clone of their repository (not Amira’s) on their own computer. We will modify Sami’s prompt to include their desktop user ID (sami) and working directory (initially ~) to make it easier to follow what’s happening: sami:~ $ git clone https://github.com/sami-virtanen/zipf.git Cloning into &#39;zipf&#39;... remote: Enumerating objects: 64, done. remote: Counting objects: 100% (64/64), done. remote: Compressing objects: 100% (43/43), done. remote: Total 64 (delta 20), reused 63 (delta 19), pack-reused 0 Receiving objects: 100% (64/64), 2.20 MiB | 2.66 MiB/s, done. Resolving deltas: 100% (20/20), done. This command creates a new directory with the same name as the project, i.e., zipf. When Sami goes into this directory and runs ls and git log, they see that all of the project’s files and history are there: sami:~ $ cd zipf sami:~/zipf $ ls README.md bin data results sami:~/zipf $ git log --oneline -n 4 55c63d0 (HEAD -&gt; master, origin/master, origin/HEAD) Merging README additions 45a576b Added contributor list a0b88e5 Added repository overview 232b564 Initial commit of README file Sami also sees that Git has automatically created a remote for their repository that points back at their repository on GitHub: sami:~/zipf $ git remote -v origin https://github.com/sami-virtanen/zipf.git (fetch) origin https://github.com/sami-virtanen/zipf.git (push) Sami can pull changes from their fork and push work back there, but needs to do one more thing before getting the changes from Amira’s repository: sami:~/zipf $ git remote add upstream https://github.com/amira-khan/zipf.git sami:~/zipf $ git remote -v origin https://github.com/sami-virtanen/zipf.git (fetch) origin https://github.com/sami-virtanen/zipf.git (push) upstream https://github.com/amira-khan/zipf.git (fetch) upstream https://github.com/amira-khan/zipf.git (push) Sami has called their new remote upstream because it points at the repository theirs is derived from. They could use any name, but upstream is a nearly universal convention. With this remote in place, Sami is finally set up. Suppose, for example, that Amira has modified the project’s README.md file to add Sami as a contributor. (Again, we show Amira’s user ID and working directory in her prompt to make it clear who’s doing what): # Zipf&#39;s Law These Zipf&#39;s Law scripts tally the occurrences of words in text files and plot each word&#39;s rank versus its frequency. ## Contributors - Amira Khan &lt;amira@zipf.org&gt; - Sami Virtanen Amira commits her changes and pushes them to her repository on GitHub: amira:~/zipf $ git commit -a -m &quot;Adding Sami as a contributor&quot; [master 35fca86] Adding Sami as a contributor 1 file changed, 1 insertion(+) amira:~/zipf $ git push origin master Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 4 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 315 bytes | 315.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To https://github.com/amira-khan/zipf.git 55c63d0..35fca86 master -&gt; master Amira’s changes are now on her desktop and in her GitHub repository but not in either of Sami’s repositories (local or remote). Since Sami has created a remote that points at Amira’s GitHub repository, though, they can easily pull those changes to their desktop: sami:~/zipf $ git pull upstream master From https://github.com/amira-khan/zipf * branch master -&gt; FETCH_HEAD * [new branch] master -&gt; upstream/master Updating 55c63d0..35fca86 Fast-forward README.md | 1 + 1 file changed, 1 insertion(+) Pulling from a repository owned by someone else is no different than pulling from a repository we own. In either case, Git merges the changes and asks us to resolve any conflicts that arise. The only significant difference is that, as with git push and git pull, we have to specify both a remote and a branch: in this case, upstream and master. 7.9 Pull Requests Sami can now get Amira’s work, but how can Amira get Sami’s? She could create a remote that pointed at Sami’s repository on GitHub and periodically pull in Sami’s changes, but that would lead to chaos, since we could never be sure that everyone’s work was in any one place at the same time. Instead, almost everyone uses pull requests. They aren’t part of Git itself, but are supported by all major online forges. A pull request is essentially a note saying, “Someone would like to merge branch A of repository B into branch X of repository Y”. The pull request does not contain the changes, but instead points at two particular branches. That way, the difference displayed is always up to date if either branch changes. But a pull request can store more than just the source and destination branches: it can also store comments people have made about the proposed merge. Users can comment on the pull request as a whole, or on particular lines, and mark comments as out of date if the author of the pull request updates the code that the comment is attached to. Complex changes can go through several rounds of review and revision before being merged, which makes pull requests the review system we all wish journals actually had. To see this in action, suppose Sami wants to add their email address to README.md. They create a new branch and switch to it: sami:~/zipf $ git checkout -b adding-email Switched to a new branch &#39;adding-email&#39; then make a change and commit it: sami:~/zipf $ git commit -a -m &quot;Adding my email address&quot; [adding-email 3e73dc0] Adding my email address 1 file changed, 1 insertion(+), 1 deletion(-) sami:~/zipf $ git diff HEAD~1 diff --git a/README.md b/README.md index e8281ee..e1bf630 100644 --- a/README.md +++ b/README.md @@ -6,4 +6,4 @@ and plot each word&#39;s rank versus its frequency. ## Contributors - Amira Khan &lt;amira@zipf.org&gt; -- Sami Virtanen +- Sami Virtanen &lt;sami@zipf.org&gt; Sami’s changes are only in their local repository. They cannot create a pull request until those changes are on GitHub, so they push their new branch to their repository on GitHub: sami:~/zipf $ git push origin adding-email Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 4 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 315 bytes | 315.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. remote: remote: Create a pull request for &#39;adding-email&#39; on GitHub by visiting: remote: https://github.com/sami-virtanen/zipf/pull/new/adding-email remote: To https://github.com/sami-virtanen/zipf.git * [new branch] adding-email -&gt; adding-email When Sami goes to their GitHub repository in the browser, GitHub notices that they have just pushed a new branch and asks them if they want to create a pull request (Figure 7.4). Figure 7.4: Repository state after Sami pushes. When Sami clicks on the button, GitHub displays a page showing the default source and destination of the pull request and a pair of editable boxes for the pull request’s title and a longer comment (Figure 7.5). Figure 7.5: Starting a pull request. If they scroll down, Sami can see a summary of the changes that will be in the pull request (Figure 7.6). Figure 7.6: Summary of a pull request. The top (title) box is autofilled with the previous commit message, so Sami adds an extended explanation to provide additional context before clicking on “Create Pull Request” (Figure 7.7). When they do, GitHub displays a page showing the new pull request, which has a unique serial number (Figure 7.8). Note that this pull request is displayed in Amira’s repository rather than Sami’s since it is Amira’s repository that will be affected if the pull request is merged. Figure 7.7: Filling in a pull request. Figure 7.8: Creating a new pull request. Amira’s repository now shows a new pull request (Figure 7.9). Clicking on the “Pull requests” tab brings up a list of PRs (Figure 7.10) and clicking on the pull request link itself displays its details (Figure 7.11). Sami and Amira can both see and interact with these pages, though only Amira has permission to merge. Figure 7.9: Viewing a pull request. Figure 7.10: Listing pull requests. Figure 7.11: Details of pull requests. Since there are no conflicts, GitHub will let Amira merge the PR immediately using the “Merge pull request” button. She could also discard or reject it without merging using the “Close pull request” button. Instead, she clicks on the “Files changed” tab to see what Sami has changed (Figure 7.12). Figure 7.12: Viewing changes to files. If she moves her mouse over particular lines, a white-on-blue cross appears near the numbers to indicate that she can add comments (Figure 7.13). She clicks on the marker beside her own name and writes a comment: She only wants to make one comment rather than write a lengthier multi-comment review, so she chooses “Add single comment” (Figure 7.14). GitHub redisplays the page with her remarks inserted (Figure 7.15). Figure 7.13: A GitHub comment marker. Figure 7.14: Writing a comment on a pull request. Figure 7.15: Viewing a comment on a pull request. While Amira is working, GitHub has been emailing notifications to both Sami and Amira. When Sami clicks on the link in their email notification, it takes them to the PR and shows Amira’s comment. Sami changes README.md, commits, and pushes, but does not create a new pull request or do anything to the existing one. As explained above, a PR is a note asking that two branches be merged, so if either end of the merge changes, the PR updates automatically. Sure enough, when Amira looks at the PR again a few moments later she sees Sami’s changes (Figure 7.16). Satisfied, she goes back to the “Conversation” tab and clicks on “Merge”. The icon at the top of the PR’s page changes text and color to show that the merge was successful (Figure 7.17). Figure 7.16: A pull request with a fix. Figure 7.17: After a successful merge. To get those changes from GitHub to her desktop repository, Amira uses git pull: amira:~/zipf $ git pull origin master From https://github.com/amira-khan/zipf * branch master -&gt; FETCH_HEAD Updating 35fca86..a04e3b9 Fast-forward README.md | 4 ++-- 1 file changed, 2 insertions(+), 2 deletions(-) To get the change they just made from their adding-email branch into their master branch, Sami could use git merge on the command line. It’s a little clearer, though, if they also use git pull from their upstream repository (i.e., Amira’s repository) so that they’re sure to get any other changes that Amira may have merged: sami:~/zipf $ git checkout master Switched to branch &#39;master&#39; Your branch is up to date with &#39;origin/master&#39;. sami:~/zipf $ git pull upstream master From https://github.com/amira-khan/zipf * branch master -&gt; FETCH_HEAD Updating 35fca86..a04e3b9 Fast-forward README.md | 4 ++-- 1 file changed, 2 insertions(+), 2 deletions(-) Finally, Sami can push their changes back to the master branch in their own remote repository: sami:~/zipf $ git push origin master Total 0 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/sami-virtanen/zipf.git 35fca86..a04e3b9 master -&gt; master All four repositories are now synchronized. 7.10 Handling Conflicts in Pull Requests Finally, suppose that Amira and Sami have decided to collaborate more extensively on this project. Amira has added Sami as a collaborator to the GitHub repository. Now Sami can make contributions directly to the repository, rather than via a pull request from a forked repository. Sami makes a change to README.md in the master branch on GitHub. Meanwhile, Amira is making a conflicting change to the same file in a different branch. When Amira creates her pull request, GitHub will detect the conflict and report that the PR cannot be merged automatically (Figure 7.18). Figure 7.18: Showing a conflict in a pull request. Amira can solve this problem with the tools she already has. If she has made her changes in a branch called editing-readme, the steps are: Pull Sami’s on the master branch of the GitHub repository into the master branch of her desktop repository. Merge from the master branch of her desktop repository to the editing-readme branch in the same repository. Push her updated editing-readme branch to her repository on GitHub. The pull request from there back to the master branch of the main repository will update automatically. GitHub and other forges do allow people to merge conflicts through their browser-based interfaces, but doing it on our desktop means we can use our favorite editor to resolve the conflict. It also means that if the change affects the project’s code, we can run everything to make sure it still works. But what if Sami or someone else merges another change while Amira is resolving this one, so that by the time she pushes to her repository there is another, different, conflict? In theory this cycle could go on forever; in practice, it reveals a communication problem that Amira (or someone) needs to address. If two or more people are constantly making incompatible changes to the same files, they should discuss who’s supposed to be doing what, or rearrange the project’s contents so that they aren’t stepping on each other’s toes. 7.11 Summary Branches and pull requests seem complicated at first, but they quickly become second nature. Everyone involved in the project can work at their own pace on what they want to, picking up others’ changes and submitting their own whenever they want. More importantly, this workflow gives everyone has a chance to review each other’s work. As we discuss in Section F.5, doing reviews doesn’t just prevent errors from creeping in: it is also an effective way to spread understanding and skills. 7.12 Exercises 7.12.1 Explaining options What do the --oneline and -n options for git log do? What other options does git log have that you would find useful? 7.12.2 Modifying prompt Modify your shell prompt so that it shows the branch you are on when you are in a repository. 7.12.3 Ignoring files GitHub maintains a collection of .gitignore files for projects of various kinds. Look at the sample .gitignore file for Python: how many of the ignored files do you recognize? Where could you look for more information about them? 7.12.4 Creating the same file twice Create a branch called same. In it, create a file called same.txt that contains your name and the date. Switch back to master. Check that same.txt does not exist, then create the same file with exactly the same contents. What will git diff master..same show? (Try to answer the question before running the command.) What will git merge same master do? (Try to answer the question before running the command.) 7.12.5 Deleting a branch without merging Create a branch called experiment. In it, create a file called experiment.txt that contains your name and the date, then switch back to master. What happens when you try to delete the experiment branch using git branch -d experiment? Why? What option can you give Git to delete the experiment branch? Why should you be very careful using it? What do you think will happen if you try to delete the branch you are currently on using this flag? 7.12.6 Tracing changes Chartreuse and Fuchsia are collaborating on a project. Describe what is in each of the four repositories involved after each of the steps below. Chartreuse creates a repository containing a README.md file on GitHub and clones it to their desktop. Fuchsia forks that repository on GitHub and clones their copy to their desktop. Fuchsia adds a file fuchsia.txt to the master branch of their desktop repository and pushes that change to their repository on GitHub. Fuchsia creates a pull request from the master branch of their repository on GitHub to the master branch of Chartreuse’s repository on GitHub. Chartreuse does not merge Fuchsia’s PR. Instead, they add a file chartreuse.txt to the master branch of their desktop repository and push that change to their repository on GitHub. Fuchsia adds a remote to their desktop repository called upstream that points at Chartreuse’s repository on GitHub and runs git pull upstream master, then merges any changes or conflicts. Fuchsia pushes from the master branch of their desktop repository to the master branch of their GitHub repository. Chartreuse merges Fuchsia’s pull request. Chartreuse runs git pull origin master on the desktop. 7.13 Key Points Use a branch-per-feature workflow to develop new features while leaving the master branch in working order. git branch creates a new branch. git checkout switches between branches. git merge merges changes from another branch into the current branch. Conflicts occur when files or parts of files are changed in different ways on different branches. Version control systems do not allow people to overwrite changes silently; instead, they highlight conflicts that need to be resolved. Forking a repository makes a copy of it on a server. Cloning a repository with git clone creates a local copy of a remote repository. Create a remote called upstream to point to the repository a fork was derived from. Create pull requests to submit changes from your fork to the upstream repository. "],["teams.html", "Chapter 8 Working in Teams 8.1 What is a Project? 8.2 Include Everyone 8.3 Establish a Code of Conduct 8.4 Include a License 8.5 Planning 8.6 Bug Reports 8.7 Labeling Issues 8.8 Prioritizing 8.9 Meetings 8.10 Making Decisions 8.11 Make All This Obvious to Newcomers 8.12 Handling Conflict 8.13 Summary 8.14 Exercises 8.15 Key Points", " Chapter 8 Working in Teams Evil begins when you begin to treat people as things. — Terry Pratchett Projects can run for years with poorly-written code, but none will survive for long if people are confused, pulling in different directions, or hostile to each other. This chapter therefore looks at how to create a culture of collaboration that will help people who want to contribute to your project, and introduce a few ways to manage projects and teams as they develop. Our recommendations draw on Fogel (2005), which describes how good open source software projects are run, and on Bollier (2014), which explains what a commons is and when it’s the right model to use. At this point, the Zipf’s Law project should include: zipf/ ├── .gitignore ├── README.md ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── script_template.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ └── ... └── results ├── dracula.csv ├── dracula.png └── ... 8.1 What is a Project? The first decision we have to make is what exactly constitutes a “project” (Wilson et al. 2017). Some examples are: A dataset that is being used by several research projects. The project includes the raw data, the programs used to tidy that data, the tidied data, the extra files needed to make the dataset a package, and a few text files describing the data’s authors, license, and provenance. A set of annual reports written for an NGO. The project includes several Jupyter notebooks, some supporting Python libraries used by those notebooks, copies of the HTML and PDF versions of the reports, a text file containing links to the datasets used in the report (which can’t be stored on GitHub since they contain personal identifying information), and a text file explaining details of the analysis that the authors didn’t include in the reports themselves. A software library that provides an interactive glossary of data science terms in both Python and R. The project contains the files needed to create a package in both languages, a Markdown file full of terms and definitions, and a Makefile with targets to check cross-references, compile packages, and so on. Some common criteria for creating projects are one per publication, one per deliverable piece of software, or one per team. The first tends to be too small: a good dataset will result in several reports, and the goal of some projects is to produce a steady stream of reports (such as monthly forecasts). The second is a good fit for software engineering projects whose primary aim is to produce tools rather than results, but can be an awkward fit for data analysis work. The third tends to be too large: a team of half a dozen people may work on many different things at once, and a repository that holds them all quickly looks like someone’s basement. One way to decide what makes up a project is to ask what people have meetings about. If the same group needs to get together on a regular basis to talk about something, that “something” probably deserves its own repository. And if the list of people changes slowly over time but the meetings continue, that’s an even stronger sign. 8.2 Include Everyone Most research software projects begin as the work of one person, who may continue to do the bulk of the coding and data analysis throughout its existence (Majumder et al. 2019). As projects become larger, though, they eventually need more contributors to sustain them. Involving more people also improves the functionality and robustness of the code, since newcomers bring their own expertise or see old problems in new ways. In order to leverage a group’s expertise, a project must do more than allow people to contribute: its leaders must communicate that the project wants contributions, and that newcomers are welcome and valued (Sholler et al. 2019). Saying “the door is open” is not enough: many potential contributors have painful personal experience of being less welcome than others. In order to create a truly welcoming environment for everyone, the project must explicitly acknowledge that some people are treated unfairly and actively take steps to remedy this. Doing this increases diversity within the team, which makes it more productive (Zhang 2020). More importantly, it is the right thing to do. Terminology Privilege is an unearned advantage given to some people but not all, while oppression is systemic inequality that benefits the privileged and harms those without privilege (Aurora and Gardiner 2018). In Europe, the Americas, Australia, and New Zealand, a straight, white, affluent, physically able male is less likely to be interrupted when speaking, more likely to be called on in class, and more likely to get a job interview based on an identical CV than someone who is outside these categories. People who are privileged are often not aware of it, as they’ve lived in a system that provides unearned advantages their entire lives. In John Scalzi’s memorable phrase, they’ve been playing on the lowest difficulty setting there is their whole lives, and as a result don’t realize how much harder things are for others (Scalzi 2012). The targets of oppression are often called “members of a marginalized group”, but targets don’t choose to be marginalized: people with privilege marginalize them. Finally, an ally is a member of a privileged group who is working to understand their own privilege and end oppression. Encouraging inclusivity is a shared responsibility. If we are privileged, we should educate ourselves and call out peers who are marginalizing others, even if (or especially if) they aren’t conscious of doing it. As project leaders, part of our job is to teach contributors how to be allies and to ensure an inclusive culture (Lee 1962). 8.3 Establish a Code of Conduct The starting point for making a project more inclusive is to establish a Code of Conduct. This does four things: It promotes fairness within a group. It reassures members of marginalized groups who have experienced harassment or unwelcoming behavior before that this project takes inclusion seriously. It ensures that everyone knows what the rules are. (This is particularly important when people come from different cultural backgrounds.) It prevents anyone who misbehaves from pretending that they didn’t know what they did was unacceptable. More generally, a Code of Conduct makes it easier for people to contribute by reducing uncertainty about what behaviors are acceptable. Some people may push back claiming that it’s unnecessary, or that it infringes freedom of speech, but what they usually mean is that thinking about how they might have benefited from past inequity makes them feel uncomfortable. If having a Code of Conduct leads to them going elsewhere, that will probably make the project run more smoothly. By convention, we add a Code of Conduct to our project by creating a file called CONDUCT.md in the project’s root directory. Writing a Code of Conduct that is both comprehensive and readable is hard. We therefore recommend using one that other groups have drafted, refined, and tested. The Contributor Covenant is relevant for projects being developed online, such as those based on GitHub: # Contributor Covenant Code of Conduct ## Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. ## Our Standards Examples of behavior that contributes to a positive environment for our community include: * Demonstrating empathy and kindness toward other people * Being respectful of differing opinions, viewpoints, and experiences * Giving and gracefully accepting constructive feedback * Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience * Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: * The use of sexualized language or imagery, and sexual attention or advances of any kind * Trolling, insulting or derogatory comments, and personal or political attacks * Public or private harassment * Publishing others&#39; private information, such as a physical or email address, without their explicit permission * Other conduct which could reasonably be considered inappropriate in a professional setting ## Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. ## Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. ## Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [INSERT CONTACT METHOD]. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. ## Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: ### 1. Correction **Community Impact**: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. **Consequence**: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. ### 2. Warning **Community Impact**: A violation through a single incident or series of actions. **Consequence**: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. ### 3. Temporary Ban **Community Impact**: A serious violation of community standards, including sustained inappropriate behavior. **Consequence**: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. ### 4. Permanent Ban **Community Impact**: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. **Consequence**: A permanent ban from any sort of public interaction within the community. ## Attribution This Code of Conduct is adapted from the [Contributor Covenant][covenant], version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by [Mozilla&#39;s code of conduct enforcement ladder](https://github.com/mozilla/diversity). [homepage]: https://www.contributor-covenant.org For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations. As you can see, the Contributor Covenant defines expectations for behavior, the consequences of non-compliance, and the mechanics of reporting and handling violations. The third part is as important as the first two, since rules are meaningless without a method to enforce them; Aurora and Gardiner (2018) is a short, practical guide that every project lead should read. In-Person Events The Contributor Covenant works well for interactions that are largely online, which is the case for many research software projects. The best option for in-person events is the model Code of Conduct from the Geek Feminism Wiki, which is used by many open source organizations and conferences. If your project is sited at a university or within a company, it may already have Code of Conduct: the Human Resources department is usually the most helpful place to ask. 8.4 Include a License While a Code of Conduct describes how contributors should interact with each other, a license dictates how project materials can be used and redistributed. If the license or a publication agreement makes it difficult for people to contribute, the project is less likely to attract new members, so the choice of license is crucial to the project’s long-term sustainability. Open Except… Projects that are only developing software may not have any problem making everything open. Teams working with sensitive data, on the other hand, must be careful to ensure that what should be private isn’t inadvertently shared. In particular, people who are new to Git (and even people who aren’t) occasionally add raw data files containing personal identifying information to repositories. It’s possible to rewrite the project’s history to remove things when this happens, but that doesn’t automatically erase copies people may have in forked repositories. Every creative work has some sort of license; the only question is whether authors and users know what it is and choose to enforce it. Choosing a license for a project can be complex, not least because the law hasn’t kept up with everyday practice. Morin, Urban, and Sliz (2012) and VanderPlas (2014) are good starting points to understand licensing and intellectual property from a researcher’s point of view, while Lindberg (2008) is a deeper dive for those who want details. Depending on country, institution, and job role, most creative works are automatically eligible for intellectual property protection. However, members of the team may have different levels of copyright protection. For example, students and faculty may have a copyright on the research work they produce, but university staff members may not, since their employment agreement may state that what they create on the job belongs to their employer. To avoid legal messiness, every project should include an explicit license. This license should be chosen early, since changing a license can be complicated. For example, each collaborator may hold copyright on their work and therefore need to be asked for approval when a license is changed. Similarly, changing a license does not change it retroactively, so different users may wind up operating under different licensing structures. Leave It To The Professionals Don’t write your own license. Legalese is a highly technical language, and words don’t mean what you think they do. To make license selection for code as easy as possible, GitHub allows us to select one of several common software licenses when creating a repository. The Open Source Initiative maintains a list of open licenses, and choosealicense.com will help us find a license that suits our needs. Some of the things we need to think about are: Do we want to license the work at all? Is the content we are licensing source code? Do we require people distributing derivative works to also distribute their code? Do we want to address patent rights? Is our license compatible with the licenses of the software we depend on? Do our institutions have any policies that may overrule our choices? Are there any copyright experts within our institution who can assist us? Unfortunately, GitHub’s list does not include common licenses for data or written works like papers and reports. Those can be added in manually, but it’s often hard to understand the interactions between multiple licenses on different kinds of material (Almeida et al. 2017). Just as the project’s Code of Conduct is usually placed in a root-level file called CONDUCT.md, its license is usually put in a file called LICENSE.md that is also in the project’s root directory. 8.4.1 Software In order to choose the right license for our software, we need to understand the difference between two kinds of license. The MIT License (and its close sibling the BSD License) say that people can do whatever they want to with the software as long as they cite the original source, and that the authors accept no responsibility if things go wrong. The GNU Public License (GPL) gives people similar rights, but requires them to share their own work on the same terms: You may copy, distribute and modify the software as long as you track changes/dates in source files. Any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL along with build and install instructions. — tl;dr In other words, if someone modifies GPL-licensed software or incorporates it into their own project, and then distributes what they have created, they have to distribute the source code for their own work as well. The GPL was created to prevent companies from taking advantage of open software without contributing anything back. The last thirty years have shown that this restriction isn’t necessary: many projects have survived and thrived without this safeguard. We therefore recommend that projects choose the MIT license, as it places the fewest restrictions on future action. MIT License Copyright (c) 2020 Amira Khan Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &quot;Software&quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. First, Do No Harm The Hippocratic License is a newer license that is quickly becoming popular. Where the GPL requires people to share their work, the Hippocratic License requires them to do no harm. More precisely, it forbids people from using the software in ways that violate the Universal Declaration of Human Rights. We have learned the hard way that software and science can be mis-used; adopting the Hippocratic License is a small step toward preventing this. 8.4.2 Data and reports The MIT license, the GPL, and the Hippocratic License are intended for use with software. When it comes to data and reports, the most widely used family of licenses are those produced by Creative Commons. These have been written and checked by lawyers and are well understood by the community. The most liberal option is referred to as CC-0, where the “0” stands for “zero restrictions”. This puts work in the public domain, i.e., allows anyone who wants to use it to do so however they want with no restrictions. CC-0 is usually the best choice for data, since it simplifies aggregate analysis involving datasets from different sources. It does not negate the scholarly tradition and requirement of citing sources; it just doesn’t make it a legal requirement. The next step up from CC-0 is the Creative Commons–Attribution license, usually referred to as CC-BY. This allows people to do whatever they want to with the work as long as they cite the original source. This is the best license to use for manuscripts: we want people to share them widely but also want to get credit for our work. Other Creative Commons licenses incorporate various restrictions, and are usually referred two using the two-letter abbreviations listed below: ND (no derivative works) prevents people from creating modified versions of our work. Unfortunately, this also inhibits translation and reformatting. SA (share-alike) requires people to share work that incorporates ours on the same terms that we used. Again, it is fine in principle but in practice makes aggregation and recombination difficult. NC (no commercial use) does not mean that people cannot charge money for something that includes our work, though some publishers still try to imply that in order to scare people away from open licensing. Instead, the NC clause means that people cannot charge for something that uses our work without our explicit permission, which we can give under whatever terms we want. To apply these concepts to our Zipf’s Law project, we need to consider both our data (which other people created) and our results (which we create). We can view the license for the novels by looking in data/README.md, which tells us that the Gutenberg Project books are in the public domain (i.e., CC-0). This is a good choice for our results as well, but after reflection, we decide to choose CC-BY for our papers so that everyone can read them (and cite them). 8.5 Planning Codes of conduct and licenses are a project’s constitution, but how do contributors know what they should actually be doing on any given day? Whether we are working by ourselves or with a group of people, the best way to manage this is to use an issue tracking system to keep track of tasks we need to complete or problems we need to fix. Issues are sometimes called tickets, so issue tracking systems are sometimes called ticketing systems. They are also often called bug trackers, but they can be used to manage any kind of work, and are often a convenient way to manage discussions as well. Like other forges, GitHub allows participants to create issues for a project, comment on existing issues, and search all available issues. Every issue can hold: A unique ID, such as #123, which is also part of its URL. This makes issues easy to find and refer to: GitHub automatically turns the expression #123 in a commit message into a link to that issue. A one-line title to aid browsing and search. The issue’s current status. In simple systems (like GitHub’s) each issue is either open or closed, and by default, only open issues are displayed. Closed items are generally removed from default interfaces, so issues should only be closed when they no longer require any attention. The user ID of the issue’s creator. Just as #123 refers to a particular issue, @name is automatically translated into a link to that person. The IDs of people who have commented on it or modified it are embedded in the issue’s history, which helps figure out who to talk to about what. The user ID of the person assigned to review the issue, if someone is assigned. A full description that may include screenshots, error messages, and anything else that can be put in a web page. Replies, counter-replies, and so on from people who are interested in the issue. Broadly speaking, people create three kinds of issues: Bug reports to describe problems they have encountered. Feature requests describing what could be done next, such as “add this function to this package” or “add a menu to the website”. Questions about how to use the software, how parts of the project work, or its future directions. These can eventually turn into bug reports or feature requests, and can often be recycled as documentation. Helping Users Find Information Many projects encourage people to ask questions on a mailing list or in a chat channel. However, answers given there can be hard to find later, which leads to the same questions coming up over and over again. If people can be persuaded to ask questions by filing issues, and to respond to issues of this kind, then the project’s old issues become a customized Stack Overflow for the project. Some projects go so far as to create a page of links to old questions and answers that are particularly helpful. 8.6 Bug Reports One steady supply of work in any active project is bug reports. Unsurprisingly, a well-written bug report is more likely to get a fast response that actually addresses the problem (Bettenburg et al. 2008). To write a good bug report: Make sure the problem actually is a bug. It’s always possible that we have called a function the wrong way or done an analysis using the wrong configuration file. If we take a minute to double-check, or ask someone else on our team to check our logic, we could well fix the problem ourselves. Try to come up with a reproducible example or “reprex” that includes only the steps needed to make the problem happen, and that (if possible) uses simplified data rather than a complete dataset. Again, we can often we solve the problem ourselves as we trim down the steps to create one. Write a one-line title for the issue and a longer (but still brief) description that includes relevant details. Attach any screenshots that show the problem, resulting errors, or (slimmed-down) input files needed to re-create it. Describe the version of the software we were using, the operating system we were running on, which version of the programming language we ran it with, and anything else that might affect behavior. If the software in question uses a logging framework (Section 12.4), turn debugging output on and include it with the issue. Describe each problem separately so that each one can be tackled on its own. This parallels the rule about creating a branch in version control for each bug fix or feature discussed in Section 7. An example of a well-written bug report with all of the components mentioned above is shown in Figure 8.1. Figure 8.1: Example of a bug report. It takes time and energy to write a good error report. If the report is being filed by a member of the development team, the incentive to document errors well is that resolving the issue later is easier. You can encourage users from outside the project to write thorough error reports by including an issue template for your project. An issue template is a file included in your GitHub repository that proliferates each new issue with text that describes expectations for content that should be submitted. You can’t force new issues to be as complete as you might like, but you can use an issue template to make it easier for contributors to remember and complete documentation about bug reports. Sometimes the person creating the issue may not know or have the right answer for some of these things, and will be doing their best with limited information about the error. Responding with kindness and encouragement is important to maintain a healthy community, and should be enforced by the project’s Code of Conduct (Section 8.3). 8.7 Labeling Issues The bigger or older a project gets, the harder it is to find things—unless, that is, the project’s members put in a bit of work to make things findable (Lin, Ali, and Wilson, n.d.). Issue trackers let project members add labels to issues to make things easier to search and organize. Labels are also often called tags; whatever term is used, each one is just a descriptive word or two. GitHub allows project owners to define any labels they want. A small project should always use some variation on these three: Bug: something should work but doesn’t. Enhancement: something that someone wants added to the software. Task: something needs to be done, but won’t show up in code (e.g., organizing the next team meeting). Projects also often use: Question: where is something or how is something supposed to work? As noted above, issues with this label can often be recycled as documentation. Discussion or Proposal: something the team needs to make a decision about or a concrete proposal to resolve such a discussion. All issues can have discussion: this category is for issues that start that way. (Issues that are initially labeled Question are often relabeled Discussion or Proposal after some back and forth.) Suitable for Newcomer or Beginner-Friendly: to identify an easy starting point for someone who has just joined the project. If we help potential new contributors find places to start, they are more likely to do so (Steinmacher et al. 2014). The labels listed above identify the kind of work an issue describes. A separate set of labels can be used to indicate the state of an issue: Urgent: work needs to be done right away. (This label is typically reserved for security fixes). Current: this issue is included in the current round of work. Next: this issue is (probably) going to be included in the next round. Eventually: someone has looked at the issue and believes it needs to be tackled, but there’s no immediate plan to do it. Won’t Fix: someone has decided that the issue isn’t going to be addressed, either because it’s out of scope or because it’s not actually a bug. Once an issue has been marked this way, it is usually then closed. When this happens, send the issue’s creator a note explaining why the issue won’t be addressed and encourage them to continue working with the project. Duplicate: this issue is a duplicate of one that’s already in the system. Issues marked this way are usually also then closed; this is another opportunity to encourage people to stay involved. Some of the labels that GitHub creates for repositories by default are shown in Figure 8.2. These labels can be modified or otherwise customized for each repository. Figure 8.2: Labels for GitHub issues. Some projects use labels corresponding to upcoming software releases, journal issues, or conferences instead of Current, Next, and Eventually. This approach works well in the short-term, but becomes unwieldy as labels with names like sprint-2020-08-01 and spring-2020-08-16 pile up. Instead, a project team will usually create a milestone, which is a set of issues and pull requests in a single project repository. GitHub milestones can have a due date and display aggregate progress toward completion, so the team can easily see when work is due and how much is left to be done. Teams can also create projects, which can include issues and pull requests from several repositories as well as notes and reminders for miscellaneous tasks. 8.7.1 Standardizing workflows Adding labels to issues also helps us standardize a workflow for the project. Conventions about who can do what to issues with various labels, and who can change those labels, let us define a workflow like the one shown in Figure 8.3. Figure 8.3: Example of issue lifecycle. An Open issue becomes Assigned when someone is made responsible for it. They can then move the issue to Cancelled if they think it was filed mistakenly or doesn’t need to be fixed. (This is different from closing the issue after working on it.) An Assigned issue becomes Active when someone starts to work on it. When work is done it becomes Ready for Review. From there it is either Assigned again because it needs more work or moved to Fixed, which means the change is ready to be incorporated into the project. When the change is actually merged, the issue’s state is changed to reflect that. Small projects do not need this much formality, but when the team is distributed contributors need to be able to find out what’s going on without having to wait for someone to respond to email (or wondering who they should have emailed). 8.8 Prioritizing Between bug reports, feature requests, and general cleanup, there is always more work to do than time to do it, so every project needs some way to figure out what to focus on. Labeling issues helps with triage, which is the process of deciding what is a priority and what isn’t. This is never an easy job for software projects that need to balance fixing bugs with creating new features, and is even more challenging for research projects for which “done” is hard to define or whose team members are widely distributed or do not all work for the same institution. Many commercial and open source teams have adopted agile development as a solution to these problems. Instead of carefully formulating long-term plans that could be derailed by changing circumstances, agile development uses a sequence of short development sprints, each typically one or two weeks long. Each sprint starts with a planning session lasting one or two hours in which the successes and failures of the previous sprint are reviewed and issues to be resolved in the current sprint are selected. If team members believe an issue is likely to take longer than a single sprint to complete, it should be broken into smaller pieces that can be finished so that the team can track progress more accurately. (Something large can be “90% done” for weeks; with smaller items, it’s easier to see how much headway is being made.) To decide which issues to work on in the next sprint, a team can construct an impact/effort matrix (Figure 8.4). Impact measures how important the issue is to reaching the team’s goals, and is typically measured on a low–medium–high scale. (Some teams use ratings from 1 to 10, but this just leads to arguing over whether something is a 4 or a 5.) Effort measures how much work the issue requires. Since this can’t always be estimated accurately, it’s common to classify things as “an hour”, “a day”, or “multiple days”. Again, anything that’s likely to take longer than multiple days should be broken down so that planning and progress tracking can be more accurate. Figure 8.4: An impact/effort matrix. The impact/effort matrix makes the priorities for the coming sprint clear: anything that is of high importance and requires little effort should be included, while things of low importance that require a lot of effort should not. The team must still make hard decisions, though: Should a a single large high-priority item be done, or should several smaller low-priority items be tackled instead? What should be done about medium-priority items that keep being put off? Each team has to answer these questions for each sprint, but that begs the question of exactly who has the final say in answering them. In a large project, a product manager decides how important items are, while a project manager is responsible for estimating effort and tracking progress. In a typical research software project, the principal investigator either makes the decision or delegates that responsibility (and authority) to the lead developer. Regardless of who is ultimately responsible, it is essential to include project participants in the planning and decision making. This may be as simple as having them add up-votes and down-votes to issues to indicate their opinions on importance, or as complex as asking them to propose a multi-sprint breakdown of a particularly complex feature. Doing this shows people that their contributions are valued, which in turn increase their commitment to doing the work. It also produces better plans, since everyone knows something that someone else doesn’t. 8.9 Meetings Pull requests and GitHub issues are good tools for asynchronous work, but team meetings are often a more efficient way to make decisions, and help build a sense of community. Knowing how to run a meeting well is as important as knowing how to use version control; the rules doing so are simple but rarely followed: Decide if there actually needs to be a meeting. If the only purpose is to share information, have everyone send a brief email instead. Remember, people can read faster than they can speak: if someone has facts for the rest of the team to absorb, the most polite way to communicate them is to type them in. Write an agenda. If nobody cares enough about the meeting to prepare a point-form list of what’s to be discussed, the meeting itself probably doesn’t need to happen. Note that “the agenda is all the open issues in our GitHub repo” doesn’t count. Include timings in the agenda. Timings help prevent early items stealing time from later ones. The first estimates with any new group are inevitably optimistic, so we should revise them upward for subsequent meetings. However, we shouldn’t have a second or third meeting just because the first one ran over-time: instead, we should try to figure out why we’re running over and fix the underlying problem. Prioritize. Tackle issues that will have high impact but take little time first, and things that will take more time but have less impact later. That way, if the first things run over time, the meeting will still have accomplished something. Make one person responsible for keeping things moving. One person should be made moderator and be responsible for keeping items to time, chiding people who are having side conversations or checking email, and asking people who are talking too much to get to the point. The moderator should not do all the talking: in fact, whoever is in charge will talk less in a well-run meeting than most other participants. This should be a rotating duty among members. Require politeness. No one gets to be rude, no one gets to ramble, and if someone goes off topic, it’s the moderator’s job to say, “Let’s discuss that elsewhere.” No interruptions. Participants should raise a finger, hand, put up a sticky note, or make another well understood gesture to indicate when they want to speak. The moderator should keep track of who wants to speak and give them time in turn. No distractions. Side conversations make meetings less efficient because nobody can actually pay attention to two things at once. Similarly, if someone is checking their email or texting a friend during a meeting, it’s a clear signal that they don’t think the speaker or their work is important. This doesn’t mean a complete ban on technology—people may need accessibility aids, or may be waiting for a call from a dependent—but by default, phones should be face down and laptops should be closed during in-person meetings. Take minutes. Someone other than the moderator should take point-form notes about the most important information that was shared, and about every decision that was made or every task that was assigned to someone. This should be a rotating duty among members. End early. If the meeting is scheduled for 10:00–11:00, aim to end at 10:50 to give people a break before whatever they’re doing next. As soon as the meeting is over, circulate the minutes by emailing them to everyone or adding a text file to the project’s repository: People who weren’t at the meeting can follow what’s going on. We all have to juggle tasks from several projects or courses, which means that sometimes we can’t make it to meetings. Checking a written record is a more accurate and efficient way to catch up than asking a colleague, “So, what did I miss?” Everyone can check what was actually said or promised. More than once, one of us has looked over the minutes of a meeting and thought, “Did I say that?” or, “I didn’t promise to have it ready then!” Accidentally or not, people will often remember things differently; writing them down gives everyone a chance to correct mistakes, misinterpretations, or misrepresentations. People can be held accountable at subsequent meetings. There’s no point making lists of questions and action items if we don’t follow up on them later. If we are using an issue-tracking system, we should create a ticket for each new question or task right after the meeting and update those that are being carried forward. This helps a lot when the time comes to draw up the agenda for the next meeting. 8.9.1 Air time One of the problem in a synchronous meeting is the tendency of some people to speak far more than others. Other meeting members may be so accustomed to this that they don’t speak up even when they have valuable points to make. One way to combat this is to give everyone three sticky notes at the start of the meeting. Every time they speak, they have to give up one sticky note. When they’re out of stickies, they aren’t allowed to speak until everyone has used at least one, at which point everyone gets all of their sticky notes back. This ensures that nobody talks more than three times as often as the quietest person in the meeting, and completely changes group dynamics. People who have given up trying to be heard suddenly have space to contribute, and the overly-frequent speakers realize how unfair they have been. Another useful technique is called interruption bingo. Draw a grid and label the rows and columns with the participants’ names. Each time one person interrupts another, add a tally mark to the appropriate cell; halfway through the meeting, take a moment to look at the results. In most cases it will be clear that one or two people are doing all of the interrupting. After that, saying, “All right, I’m adding another tally to the bingo card,” is often enough to get them to throttle back. 8.9.2 Online meetings Online meetings provide special challenges, both in the context of regulating how often individuals speak, as well as running meetings in general. Troy (2018) discusses why online meetings are often frustrating and unproductive and points out that in most online meetings, the first person to speak during a pause gets the floor. As a result, “If you have something you want to say, you have to stop listening to the person currently speaking and instead focus on when they’re gonna pause or finish so you can leap into that nanosecond of silence and be the first to utter something. The format…encourages participants who want to contribute to say more and listen less.” The solution is to run a text chat beside the video conference where people can signal that they want to speak. The moderator can then select people from the waiting list. This practice can be reinforced by having everyone mute themselves, and only allowing the moderator to unmute people. Brookfield and Preskill (2016) has many other useful suggestions for managing meetings. 8.10 Making Decisions The purpose of a well-run meeting is to make decisions, so sooner or later, the members of a project must decide who has a say in what. The first step is to acknowledge that every team has a power structure: the question is whether it’s formal or informal—in other words, whether it’s accountable or unaccountable (Freeman 1972). The latter can work for groups of up to half a dozen people in which everyone knows everyone else. Beyond that, groups need to spell out who has the authority to make which decisions and how to achieve consensus. In short, they need explicit governance. Martha's Rules are a practical way to do this in groups of up to a few dozen members (Minahan 1986): Before each meeting, anyone who wishes may sponsor a proposal. Proposals must be filed at least 24 hours before a meeting in order to be considered at that meeting, and must include: a one-line summary the full text of the proposal any required background information pros and cons possible alternatives A quorum is established in a meeting if half or more of voting members are present. Once a person has sponsored a proposal, they are responsible for it. The group may not discuss or vote on the issue unless the sponsor or their delegate is present. The sponsor is also responsible for presenting the item to the group. After the sponsor presents the proposal, a sense vote is cast for the proposal prior to any discussion: Who likes the proposal? Who can live with the proposal? Who is uncomfortable with the proposal? If all of the group likes or can live with the proposal, it passes with no further discussion. If most of the group is uncomfortable with the proposal, it is sent back to its sponsor for further work. (The sponsor may decide to drop it if it’s clear that the majority isn’t going to support it.) If some members are uncomfortable with the proposal, a timer is set for a brief discussion moderated by the meeting moderator. After 10 minutes or when no one has anything further to add, the moderator calls for a straight yes-or-no vote on the question: “Should we implement this decision over the stated objections?” If a majority votes “yes” the proposal is implemented. Otherwise, it is returned to the sponsor for further work. Every group that uses Martha’s Rules must make two procedural decisions: How are proposals put forward? In a software development project, the easiest way is to file an issue in the project’s GitHub repository tagged Proposal, or to create a pull request containing a single file with the text of the proposal. Team members can then comment on the proposal, and the sponsor can revise it before bringing it to a vote. Who gets to vote? The usual answer is “whoever is working on the project,” but as it attracts more volunteer contributors, a more explicit rule is needed. One common method is for existing members to nominate new ones, who are then voted on using the process described above. 8.11 Make All This Obvious to Newcomers Rules that people don’t know about can’t help them. Once your team agrees on a project structure, a workflow, how to get items on a meeting agenda, or how to make decisions, you should therefore take the time to document this for newcomers. This information may be included as sections in the existing README file or put into files of their own: CONTRIBUTING explains how to contribute, i.e., what naming conventions to use for functions, what tags to put on issues (Section 8.5), or how to install and configure the software needed to start work on the project. These instructions can also be included as a section in README; wherever they go, remember that the easier it is for people to get set up and contribute, the more likely they are to do so (Steinmacher et al. 2014). GOVERNANCE explains how the project is run (Section 8.10). It is still uncommon for this to be in a file of its own—it is more often included in README or CONTRIBUTING—but open communities have learned the hard way that not being explicit about who has a voice in decisions and how contributors can tell what decisions have been made causes trouble sooner or later. Having these files helps new contributors orient themselves, and also signals that the project is well run. 8.12 Handling Conflict You just missed an important deadline, and people are unhappy. The sick feeling in the pit of your stomach has turned to anger: you did your part, but Sylvie didn’t finish her stuff until the very last minute, which meant that no one else had time to spot the two big mistakes she’d made. As for Cho, he didn’t deliver at all—again. If something doesn’t change, contributors are going to start looking for a new project. Conflicts like this come up all the time. Broadly speaking, there are four ways we can deal with them: Cross our fingers and hope that things will get better on their own, even though they didn’t the last three times. Do extra work to make up for others’ shortcomings. This saves us the mental anguish of confronting others in the short run, but the time for that “extra” has to come from somewhere. Sooner or later, our personal lives or other parts of the project will suffer. Lose our temper. People often wind up displacing anger into other parts of their life: they may yell at someone for taking an extra thirty seconds to make a change when what they really need to do is tell their boss that they won’t work through another holiday weekend to make up for management’s decision to short-staff the project. Take constructive steps to fix the underlying problem. Most of us find the first three options easiest, even though they don’t actually fix the problem. The fourth option is harder because we don’t like confrontation. If we manage it properly, though, it is a lot less bruising, which means that we don’t have to be as afraid of initiating it. Also, if people believe that we will take steps when they bully, lie, procrastinate, or do a half-assed job, they will usually avoid making it necessary. Make sure we are not guilty of the same sin. We won’t get very far complaining about someone else interrupting in meetings if we do it just as frequently. Check expectations. Are we sure the offender knows what standards they are supposed to be meeting? This is where things like job descriptions or up-front discussion of who’s responsible for what come in handy. Check the situation. Is someone dealing with an ailing parent or immigration woes? Have they been put to work on three other projects that we don’t know about? Use open questions like, “Can you help me understand this?” when checking in. This gives them the freedom to explain something you may not have expected, and avoids the pressure of being asked directly about something they don’t want to explain. Document the offense. Write down what the offender has actually done and why it’s not good enough. Doing this helps us clarify what we’re upset about and is absolutely necessary if we have to escalate. Check with other team members. Are we alone in feeling that the offender is letting the team down? If so, we aren’t necessarily wrong, but it’ll be a lot easier to fix things if we have the support of the rest of the team. Finding out who else on the team is unhappy can be the hardest part of the whole process, since we can’t even ask the question without letting on that we are upset and word will almost certainly get back to whoever we are asking about, who might then accuse us of stirring up trouble. Talk with the offender. This should be a team effort: put it on the agenda for a team meeting, present the complaint, and make sure that the offender understands it. This is often enough: if someone realizes that they’re going to be called on their hitchhiking or bad manners, they will usually change their ways. Escalate as soon as there’s a second offense. People who don’t have good intentions count on us giving them one last chance after another until the project is finished and they can go suck the life out of their next victim. Don’t fall into this trap. If someone stole a laptop, we would report it right away. If someone steals time, we are being foolish if we give them a chance to do it again and again. In academic research projects, “escalation” means “taking the issue to the project’s principal investigator”. Of course, the PI has probably had dozens of students complain to her over the years about teammates not doing their share, and it isn’t uncommon to have both halves of a pair tell the supervisor that they’re doing all the work. (This is yet another reason to use version control: it makes it easy to check who’s actually written what.) In order to get her to take us seriously and help us fix our problem, we should send her an email signed by several people that describes the problem and the steps we have already taken to resolve it. Make sure the offender gets a copy as well, and ask the supervisor to arrange a meeting to resolve the issue. Hitchhikers Hitchhikers who show up but never actually do anything are particularly difficult to manage, in part because they are usually very good at appearing reasonable. They will nod as we present our case, then say, “Well, yes, but…” and list a bunch of minor exceptions or cases where others on the team have also fallen short of expectations. Having collaborator guidelines (Section 8.3) and tracking progress (Section 8.7.1) are essential for handling them. If we can’t back up our complaint, our supervisor will likely be left with the impression that the whole team is dysfunctional. What can we do if conflict becomes more personal and heated, especially if it relates to violations of our Code of Conduct? A few simple guidelines will go a long way: Be short, simple, and firm. Don’t try to be funny. It almost always backfires, or will later be used against us. Play for the audience. We probably won’t change the person we are calling out, but we might change the minds or strengthen the resolve of people who are observing. Pick our battles. We can’t challenge everyone, every time, without exhausting ourselves and deafening our audience. An occasional sharp retort will be much more effective than constant criticism. Don’t shame or insult one group when trying to help another. For example, don’t call someone stupid when what we really mean is that they’re racist or homophobic. Captain Awkward has useful advice for discussions like these, and Charles’ Rules of Argument are very useful online. Finally, it’s important to recognize that good principles sometimes conflict. For example, consider this scenario: A manager consistently uses male pronouns to refer to software and people of unknown gender. When you tell them it makes you uncomfortable to treat maleness as the norm, they say that male is the default gender in their first language and you should be more considerate of people from other cultures. On the one hand, we want to respect other people’s cultures; on the other hand, we want to be inclusive of women. In this case, the manager’s discomfort about changing pronouns matters less than the career harm caused by them being exclusionary, but many cases are not this clear cut. 8.13 Summary This chapter was the hardest in this book to write, but is probably also the most important. A project can survive bad code or stumbles with Git, but not confusion and interpersonal conflict. Collaboration and management become easier with practice, and everything you learn from taking part in research software projects will help other things you do as well. 8.14 Exercises 8.14.1 Finding information Take a look at the GitHub repository for this book. Where is the information for licensing and contributing? 8.14.2 Add a Code of Conduct to your project Add a CONDUCT.md file to your Zipf’s Law project repository. Use the Contributor Covenant Code of Conduct template and modify the places that need updating (e.g. who to contact). Be sure to edit the contact information in both before committing the files. 8.14.3 Add a license to your project Add either an MIT or a GPL LICENSE.md file to your Zipf’s Law project repository. Modify the contents to include your full name and year. 8.14.4 Adding contribution guidelines Add a section to the README.md file in the Zipf’s Law project to tell people where to find out more about contributing. Add a CONTRIBUTING.md file in the Zipf’s Law project to describe how other people can contribute to it. Be sure to add it to the root directory of your Git repository, so that when someone opens a pull request or creates an issue on GitHub, they will be presented with a link to the CONTRIBUTING file (see the GitHub contributors guide for details). 8.14.5 File an issue Create a feature request issue in your Zipf’s Law project repository to ask that unit tests be written for countwords.py (we will do this in Chapter 11). 8.14.6 Label issues Create the labels current and discussion to help organize and prioritize your issues. Delete at least one of the labels that GitHub automatically created for you. Apply at least one label to each issue in your repository. 8.14.7 Balancing individual and team needs A new member of your team has a medically diagnosed attention disorder. In order to help themselves focus, they need to talk to themselves while coding. Several other members of your team have come to you privately to say that they find this distracting. What steps would you take? 8.14.8 Crediting invisible contributions Your team has a rule: if someone’s name appears in the Git history for a project, they are listed as a co-author on papers for that project. A new member of your team has complained that this is unfair: people who haven’t contributed for over two years are still being included as authors, while they aren’t included because the many hours they have spent doing code reviews don’t show up in the Git history. How would you address this issue? 8.14.9 Who are you? Which (if any) of the profiles below best describes you? How would you handle each of these people if they were on your team? Anna thinks she knows more about every subject than everyone else on the team put together. No matter what you say, she’ll correct you; no matter what you know, she knows better. If you keep track in team meetings of how often people interrupt one another, her score is usually higher than everyone else’s put together. Bao is a contrarian: no matter what anyone says, he’ll take the opposite side. This is healthy in small doses, but when Bao does it, there’s always another objection lurking behind the first half dozen. Frank believes that knowledge is power. He enjoys knowing things that other people don’t—or to be more accurate, he enjoys it when people know he knows things they don’t. Frank can actually make things work, but when asked how he did it, he’ll grin and say, “Oh, I’m sure you can figure it out.” Hediyeh is quiet. Very quiet. She never speaks up in meetings, even when she knows that what other people are saying is wrong. She might contribute to the mailing list, but she’s very sensitive to criticism, and will always back down rather than defending her point of view. Kenny is a hitchhiker. He has discovered that most people would rather shoulder some extra work than snitch, and he takes advantage of it at every turn. The frustrating thing is that he’s so damn plausible when someone finally does confront him. “There have been mistakes on all sides,” he says, or, “Well, I think you’re nit-picking.” Melissa would easily have made the varsity procrastination team if she’d bothered to show up to tryouts. She means well—she really does feel bad about letting people down—but somehow her tasks are never finished until the last possible moment. Of course, that means that everyone who is depending on her can’t do their work until after the last possible moment… Petra’s favorite phrase is “why don’t we”. Why don’t we write a GUI to help people edit the program’s configuration files? Hey, why don’t we invent our own little language for designing GUIs? Raj is rude. “It’s just the way I talk,” he says. “If you can’t hack it, maybe you should find another team.” His favorite phrase is, “That’s stupid,” and he uses obscenity in every second sentence. 8.15 Key Points Welcome and nurture community members proactively. Create an explicit Code of Conduct for your project modelled on the Contributor Covenant. Include a license in your project so that it’s clear who can do what with the material. Create issues for bugs, enhancement requests, and discussions. Label issues to identify their purpose. Triage issues regularly and group them into milestones to track progress. Include contribution guidelines in your project that specify its workflow and its expectations of participants. Make rules about governance explicit. Use common-sense rules to make project meetings fair and productive. Manage conflict between participants rather than hoping it will take care of itself. "],["automate.html", "Chapter 9 Automating Analyses with Make 9.1 Updating a Single File 9.2 Managing Multiple Files 9.3 Updating Files When Programs Change 9.4 Reducing Repetition in a Makefile 9.5 Automatic Variables 9.6 Generic Rules 9.7 Defining Sets of Files 9.8 Documenting a Makefile 9.9 Automating Entire Analyses 9.10 Summary 9.11 Exercises 9.12 Key Points", " Chapter 9 Automating Analyses with Make The three rules of the Librarians of Time and Space are: 1) Silence; 2) Books must be returned no later than the last date shown; and 3) Do not interfere with the nature of causality. — Terry Pratchett It’s easy to run one program to process a single data file, but what happens when our analysis depends on many files, or when we need to re-do the analysis every time new data arrives? What should we do if the analysis has several steps that we have to do in a particular order? If we try to keep track of this ourselves we will inevitably forget some crucial steps and it will be hard for other people to pick up our work. Instead, we should use a build manager to keep track of what depends on what and run our analysis programs automatically. These tools were invented to help programmers compile complex software, but can be used to automate any workflow. Our Zipf’s Law project currently includes these files: zipf/ ├── .gitignore ├── CONDUCT.md ├── CONTRIBUTING.md ├── LICENSE.md ├── README.md ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── script_template.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ ├── frankenstein.txt │ └── ... └── results ├── dracula.csv ├── dracula.png └── ... Now that the project’s main building blocks are in place, we’re ready to automate our analysis using a build manager. We will use a program called Make to do this so that every time we add a new book to our data, we can create a new plot of the word count distribution with a single command. Make works as follows: Every time the operating system creates, reads, or changes a file, it updates a timestamp on the file to show when the operation took place. Make can compare these timestamps to figure out whether files are newer or older than one another. A user can describe which files depend on each other by writing rules in a Makefile. For example, one rule could say that results/moby_dick.csv depends on data/moby_dick.txt, while another could say that the plot results/comparison.png depends on all of the CSV files in the results directory. Each rule also tells Make how to update an out-of-date file. For example, the rule for Moby Dick could tell Make to run bin/countwords.py if the result file is older than either the raw data file or the program. When the user runs Make, the program checks all of the rules in the Makefile and runs the commands needed to update any that are out of date. If there are transitive dependencies—i.e., if A depends on B and B depends on C—then Make will trace them through and run all of the commands it needs to in the right order. This chapter uses a version of Make called GNU Make. It comes with MacOS and Linux; please see Section 1.3 for Windows installation instructions. Keep Tracking With Version Control We learned about tracking our project’s version history using Git in Chapters 6 and 7. We encourage you to continue apply the Git workflow throughout the rest of our code development, though we won’t continue to remind you. 9.1 Updating a Single File To start automating our analysis, let’s create a file called Makefile in the root of our project and add the following: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv As in the shell and many other programming languages, # indicates that the first line is a comment. The second and third lines form a build rule: the target of the rule is results/moby_dick.csv, its single prerequisite is the file data/moby_dick.txt, and the two are separated by a single colon :. There is no limit on the length of statement lines in Makefiles, but to aid readability we have used a backslash (\\) character to split the lengthy third line in this example. The target and prerequisite tell Make what depends on what. The line below them describes the recipe that will update the target if it is out of date. The recipe consists of one or more shell commands, each of which must be prefixed by a single tab character. Spaces cannot be used instead of tabs here, which can be confusing as they are interchangeable in most other programming languages. In the rule above, the recipe is “run bin/countwords.py on the raw data file and put the output in a CSV file in the results directory”. To test our rule, run this command in the shell: $ make Make automatically looks for a file called Makefile, follows the rules it contains, and prints the commands that were executed. In this case it displays: python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv Indentation Errors If a Makefile indents a rule with spaces rather than tabs, Make produces an error message like this: Makefile:3: *** missing separator. Stop. When Make follows the rules in our Makefile, one of three things will happen: If results/moby_dick.csv doesn’t exist, Make runs the recipe to create it. If data/moby_dick.txt is newer than results/moby_dick.csv, Make runs the recipe to update the results. If results/moby_dick.csv is newer than its prerequisite, Make does nothing. In the first two cases, Make prints the commands it runs along with anything those command prints to the screen via standard output or standard error. There is no screen output in this case, so we only see the command. No matter what happened the first time we ran make, if we run it again right away it does nothing because our rule’s target is now up to date. It tells us this by displaying the message: make: `results/moby_dick.csv&#39; is up to date. We can check that it is telling the truth by listing the files with their timestamps, ordered by how recently they have been updated: $ ls -l -t data/moby_dick.txt results/moby_dick.csv -rw-r--r-- 1 amira staff 274967 Nov 29 12:58 results/moby_dick.csv -rw-r--r-- 1 amira staff 1253891 Nov 27 20:56 data/moby_dick.txt As a further test: Delete results/moby_dick.csv and run make again. This is case #1, so Make runs the recipe. Use touch data/moby_dick.txt to update the timestamp on the data file, then run make. This is case #2, so again, Make runs the recipe. Managing Makefiles We don’t have to call our file Makefile: if we prefer something like workflows.mk, we can tell Make to read recipes from that file using make -f workflows.mk. 9.2 Managing Multiple Files Our Makefile documents exactly how to reproduce one specific result. Let’s add another rule to reproduce another result: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv When we run make it tells us: make: `results/moby_dick.csv&#39; is up to date. By default Make only attempts to update the first target it finds in the Makefile, which is called the default target. In this case, the first target is results/moby_dick.csv, which is already up to date. To update something else, we need to tell Make specifically what we want: $ make results/jane_eyre.csv python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv If we have to run make once for each result we’re right back where we started. However, we can add a rule to our Makefile to update all of our results at once. We do this by creating a phony target that doesn’t correspond to an actual file. Let’s add this line to the top of our Makefile: # Regenerate all results. all : results/moby_dick.csv results/jane_eyre.csv There is no file called all, and this rule doesn’t have any recipes of its own, but when we run make all, Make finds everything that all depends on, then brings each of those prerequisites up to date (Figure 9.1). Figure 9.1: Dependency graph when making everything. The order in which rules appear in the Makefile does not necessarily determine the order in which recipes are run. Make is free to run commands in any order so long as nothing is updated before its prerequisites are up to date. We can use phony targets to automate and document other steps in our workflow. For example, let’s add another target to our Makefile to delete all of the result files we have generated so that we can start afresh. By convention this target is called clean, and we’ll place it below the two existing targets: # Remove all generated files. clean : rm -f results/*.csv The -f flag to rm means “force removal”: if it is present, rm won’t complain if the files we have told it to remove are already gone. If we now run: $ make clean Make will delete any results files we have. This is a lot safer than typing rm -f results/*.csv at the command-line each time, because if we mistakenly put a space before the * we would delete all of the CSV files in the project’s root directory. Phony targets are very useful, but there is a catch. Try doing this: $ mkdir clean $ make clean make: `clean&#39; is up to date. Since there is a directory called clean, Make thinks the target clean in the Makefile refers to this directory. Since the rule has no prerequisites, it can’t be out of date, so no recipes are executed. We can unconfuse Make by putting this line at the top of Makefile to explicitly state which targets are phony: .PHONY : all clean 9.3 Updating Files When Programs Change Our current Makefile says that each result file depends on the corresponding data file. That’s not entirely true: each result also depends on the program used to generate it. If we change our program, we should regenerate our results. To get Make to do that, we can change our prerequisites to include the program: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt bin/countwords.py python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt bin/countwords.py python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv To run both of these rules, we can type make all. Alternatively, since all is the first target in our Makefile, Make will use it if we just type make on its own: $ touch bin/countwords.py $ make python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv The exercises will explore how we can write a rule to tell us whether our results will be different after a change to a program without actually updating them. Rules like this can help us test our programs: if we don’t think an addition or modification ought to affect the results, but it would, we may have some debugging to do. 9.4 Reducing Repetition in a Makefile Our Makefile now mentions bin/countwords.py four times. If we ever change the name of the program or move it to a different location, we will have to find and replace each of those occurrences. More importantly, this redundancy makes our Makefile harder to understand, just as scattering magic numbers through programs makes them harder to understand. The solution is the same one we use in programs: define and use variables. Let’s modify the results regeneration code by creating targets for the word-counting script and the command used to run it. The entire file should now read: .PHONY : all clean COUNT=bin/countwords.py RUN_COUNT=python $(COUNT) # Regenerate all results. all : results/moby_dick.csv results/jane_eyre.csv # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt $(COUNT) $(RUN_COUNT) data/moby_dick.txt &gt; results/moby_dick.csv # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt $(COUNT) $(RUN_COUNT) data/jane_eyre.txt &gt; results/jane_eyre.csv # Remove all generated files. clean : rm -f results/*.csv Each definition takes the form NAME=value. Variables are written in upper case by convention so that they’ll stand out from filenames (which are usually in lower case), but Make doesn’t require this. What is required is using parentheses to refer to the variable, i.e., to use $(NAME) and not $NAME. Why the Parentheses? For historical reasons, Make interprets $NAME to be a “variable called N followed by the three characters ‘AME’”, If no variable called N exists, $NAME becomes AME, which is almost certainly not what we want. As in programs, variables don’t just cut down on typing. They also tell readers that several things are always and exactly the same, which reduces cognitive load. 9.5 Automatic Variables We could add a third rule to analyze a third novel and a fourth to analyze a fourth, but that won’t scale to hundreds or thousands of novels. Instead, we can write a generic rule that does what we want for every one of our data files. To do this, we need to understand Make’s automatic variables. The first step is to use the very cryptic expression $@ in the rule’s recipe to mean “the target of the rule”. It lets us turn this: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt $(COUNT) $(RUN_COUNT) data/moby_dick.txt &gt; results/moby_dick.csv into this: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt $(COUNT) $(RUN_COUNT) data/moby_dick.txt &gt; $@ Make defines a value of $@ separately for each rule, so it always refers to that rule’s target. And yes, $@ is an unfortunate name: something like $TARGET would have been easier to understand, but we’re stuck with it now. The next step is to replace the explicit list of prerequisites in the recipe with the automatic variable $^, which means “all the prerequisites in the rule”: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt $(COUNT) $(RUN_COUNT) $^ &gt; $@ However, this doesn’t work. The rule’s prerequisites are the novel and the word-counting program. When Make expands the recipe, the resulting command tries to process the program bin/countwords.py as if it was a data file: python bin/countwords.py \\ data/moby_dick.txt bin/countwords.py &gt; results/moby_dick.csv Make solves this problem with another automatic variable $&lt;, which means “only the first prerequisite”. Using it lets us rewrite our rule as: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt $(COUNT) $(RUN_COUNT) $&lt; &gt; $@ If you use this approach, the rule for Jane Eyre should be updated as well. The next section, however, includes instructions for generalizing rules. 9.6 Generic Rules $&lt; &gt; $@ is even harder to read than $@ on its own, but we can now replace all the rules for generating results files with one pattern rule using the wildcard %, which matches zero or more characters in a filename. Whatever matches % in the target also matches in the prerequisites, so the rule: results/%.csv : data/%.txt $(COUNT) $(RUN_COUNT) $&lt; &gt; $@ will handle Jane Eyre, Moby Dick, The Time Machine, and every other novel in the data/ directory. % cannot be used in rules’ recipes, which is why $&lt; and $@ are needed. With this rule in place, our entire Makefile is reduced to: .PHONY: all clean COUNT=bin/countwords.py RUN_COUNT=python $(COUNT) # Regenerate all results. all : results/moby_dick.csv results/jane_eyre.csv \\ results/time_machine.csv # Regenerate result for any book. results/%.csv : data/%.txt $(COUNT) $(RUN_COUNT) $&lt; &gt; $@ # Remove all generated files. clean : rm -f results/*.csv We now have fewer lines of text, but we’ve also included a third book. To test our shortened Makefile, let’s delete all of the results files: $ make clean rm -f results/*.csv and then recreate them: $ make # Same as `make all` as &quot;all&quot; is the first target python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv python bin/countwords.py \\ data/time_machine.txt &gt; results/time_machine.csv We can still rebuild individual files if we want, since Make will take the target filename we give on the command line and see if a pattern rule matches it: $ touch data/jane_eyre.txt $ make results/jane_eyre.csv python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv 9.7 Defining Sets of Files Our analysis is still not fully automated: if we add another book to data, we have to remember to add its name to the all target in the Makefile as well. Once again we will fix this in steps. To start, imagine that all the results files already exist and we just want to update them. We can define a variable called RESULTS to be a list of all the results files using the same wildcards we would use in the shell: RESULTS=results/*.csv We can then rewrite all to depend on that list: # Regenerate all results. all : $(RESULTS) However, this only works if the results files already exist. If one doesn’t, its name won’t be included in RESULTS and Make won’t realize that we want to generate it. What we really want is to generate the list of results files based on the list of books in the data/ directory. We can create that list using Make’s wildcard function: DATA=$(wildcard data/*.txt) This calls the function wildcard with the argument data/*.txt. The result is a list of all the text files in the data directory, just as we would get with data/*.txt in the shell. The syntax is odd because functions were added to Make long after it was first written, but at least they have readable names. To check that this line does the right thing, we can add another phony target called settings that uses the shell command echo to print the names and values of our variables: .PHONY: all clean settings # ...everything else... # Show variables&#39; values. settings : echo COUNT: $(COUNT) echo DATA: $(DATA) Let’s run this: $ make settings echo COUNT: bin/countwords.py COUNT: bin/countwords.py echo DATA: data/dracula.txt data/frankenstein.txt data/jane_eyre.txt data/moby_dick.txt data/sense_and_sensibility.txt data/sherlock_holmes.txt data/time_machine.txt DATA: data/dracula.txt data/frankenstein.txt data/jane_eyre.txt data/moby_dick.txt data/sense_and_sensibility.txt data/sherlock_holmes.txt data/time_machine.txt The output appears twice because Make shows us the command it’s going to run before running it. Putting @ before the command in the recipe prevents this, which makes the output easier to read: settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) $ make settings COUNT: bin/countwords.py DATA: data/dracula.txt data/frankenstein.txt data/jane_eyre.txt data/moby_dick.txt data/sense_and_sensibility.txt data/sherlock_holmes.txt data/time_machine.txt We now have the names of our input files. To create a list of corresponding output files, we use Make’s patsubst function (short for pattern substitution): RESULTS=$(patsubst data/%.txt,results/%.csv,$(DATA)) The first argument to patsubst is the pattern to look for, which in this case is a text file in the data directory. We use % to match the stem of the file’s name, which is the part we want to keep. The second argument is the replacement we want. As in a pattern rule, Make replaces % in this argument with whatever matched % in the pattern, which creates the name of the result file we want. Finally, the third argument is what to do the substitution in, which is our list of books’ names. Let’s check the RESULTS variable by adding another command to the settings target: settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) @echo RESULTS: $(RESULTS) $ make settings COUNT: bin/countwords.py DATA: data/dracula.txt data/frankenstein.txt data/jane_eyre.txt data/moby_dick.txt data/sense_and_sensibility.txt data/sherlock_holmes.txt data/time_machine.txt RESULTS: results/dracula.csv results/frankenstein.csv results/jane_eyre.csv results/moby_dick.csv results/sense_and_sensibility.csv results/sherlock_holmes.csv results/time_machine.csv Excellent: DATA has the names of the files we want to process and RESULTS automatically has the names of the corresponding result files. Why haven’t we included RUN_COUNT when assessing our variables’ values? This is another place we can streamline our script, by removing RUN_COUNT from the list of variables and changing our regeneration rule: # Regenerate result for any book. results/%.csv : data/%.txt $(COUNT) python $(COUNT) $&lt; &gt; $@ Since the phony target all depends on $(RESULTS) (i.e., all the files whose names appear in the variable RESULTS) we can regenerate all the results in one step: $ make clean rm -f results/*.csv $ make # Same as `make all` since &quot;all&quot; is the first target python bin/countwords.py \\ data/dracula.txt &gt; results/dracula.csv python bin/countwords.py \\ data/frankenstein.txt &gt; results/frankenstein.csv python bin/countwords.py \\ data/jane_eyre.txt &gt; results/jane_eyre.csv python bin/countwords.py \\ data/moby_dick.txt &gt; results/moby_dick.csv python bin/countwords.py \\ data/sense_and_sensibility.txt &gt; results/sense_and_sensibility.csv python bin/countwords.py \\ data/sherlock_holmes.txt &gt; results/sherlock_holmes.csv python bin/countwords.py \\ data/time_machine.txt &gt; results/time_machine.csv Our workflow is now just two steps: add a data file and run Make. This is a big improvement over running things manually, particularly as we start to add more steps like merging data files and generating plots. 9.8 Documenting a Makefile Every well-behaved program should tell people how to use it (Taschuk and Wilson 2017). If we run make --help, we get a (very) long list of options that Make understands, but nothing about our specific workflow. We could create another phony target called help that prints a list of available commands: .PHONY: all clean help settings # ...other definitions... # Show help. help : @echo &quot;all : regenerate all out-of-date results files.&quot; @echo &quot;results/*.csv : regenerate a particular results file.&quot; @echo &quot;clean : remove all generated files.&quot; @echo &quot;settings : show the values of all variables.&quot; @echo &quot;help : show this message.&quot; but sooner or later we will add a target or rule and forget to update this list. A better approach is to format some comments in a special way and then extract and display those comments when asked to. We’ll use ## (a double comment marker) to indicate the lines we want displayed and grep (Section 4.5) to pull these lines out of the file: .PHONY: all clean help settings COUNT=bin/countwords.py DATA=$(wildcard data/*.txt) RESULTS=$(patsubst data/%.txt,results/%.csv,$(DATA)) ## all : regenerate all results. all : $(RESULTS) ## results/%.csv : regenerate result for any book. results/%.csv : data/%.txt $(COUNT) python $(COUNT) $&lt; &gt; $@ ## clean : remove all generated files. clean : rm -f results/*.csv ## settings : show variables&#39; values. settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) @echo RESULTS: $(RESULTS) ## help : show this message. help : @grep &#39;^##&#39; ./Makefile Let’s test: $ make help ## all : regenerate all results. ## results/%.csv : regenerate result for any book. ## clean : remove all generated files. ## settings : show variables&#39; values. ## help : show this message. The exercises will explore how to format this more readably. 9.9 Automating Entire Analyses To finish our discussion of Make, let’s automatically generate a collated list of word frequencies. The target is a file called results/collated.csv that depends on the results generated by countwords.py. To create it, we add or change these lines in our Makefile: # ...phony targets and previous variable definitions... COLLATE=bin/collate.py ## all : regenerate all results. all : results/collated.csv ## results/collated.csv : collate all results. results/collated.csv : $(RESULTS) $(COLLATE) mkdir -p results python $(COLLATE) $(RESULTS) &gt; $@ # ...other rules... ## settings : show variables&#39; values. settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) @echo RESULTS: $(RESULTS) @echo COLLATE: $(COLLATE) # ...help rule... The first two lines tell Make about the collation program, while the change to all tells it what the final target of our pipeline is. Since this target depends on the results files for single novels, make all will regenerate all of those automatically. The rule to regenerate results/collated.csv should look familiar by now: it tells Make that all of the individual results have to be up-to-date and that the final result should be regenerated if the program used to create it has changed. One difference between the recipe in this rule and the recipes we’ve seen before is that this recipe uses $(RESULTS) directly instead of an automatic variable. We have written the rule this way because there isn’t an automatic variable that means “all but the last prerequisite”, so there’s no way to use automatic variables that wouldn’t result in us trying to process our program. Likewise, we can also add the plotcounts.py script to this workflow and update the all and settings rules accordingly. Note that there is no &gt; needed before the $@ because plotcounts.py default is to write to a file rather than to [standard output][stdout]. # ...phony targets and previous variable definitions... PLOT=bin/plotcounts.py ## all : regenerate all results. all : results/collated.png ## results/collated.png: plot the collated results. results/collated.png : results/collated.csv python $(PLOT) $&lt; --outfile $@ # ...other rules... ## settings : show variables&#39; values. settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) @echo RESULTS: $(RESULTS) @echo COLLATE: $(COLLATE) @echo PLOT: $(PLOT) # ...help... Running make all should now generate the new collated.png plot (Figure 9.2): $ make all python bin/collate.py results/time_machine.csv \\ results/moby_dick.csv results/jane_eyre.csv results/dracula.csv \\ results/sense_and_sensibility.csv results/sherlock_holmes.csv \\ results/frankenstein.csv &gt; results/collated.csv python bin/plotcounts.py results/collated.csv \\ --outfile results/collated.png alpha: 1.1712445413685917 Figure 9.2: Word count distribution for all books combined. Finally, we can update the clean target to only remove files created by the Makefile. It is a good habit to do this rather than using the asterisk wildcard to remove all files, since you might manually place files in the results directory and forget that these will be cleaned up when you run make clean. # ...phony targets and previous variable definitions... ## clean : remove all generated files. clean : rm $(RESULTS) results/collated.csv results/collated.png 9.10 Summary Make’s reliance on shell commands instead of direct calls to functions in Python or R sometimes makes it clumsy to use. However, that also makes it very flexible: a single Makefile can run shell commands and programs written in a variety of languages, which makes it a great way to assemble pipelines out of whatever is lying around. Programmers have created many replacements for Make in the 45 years since it was first created—so many, in fact, that none have attracted enough users to displace it. If you would like to explore them, check out Snakemake (for Python) and drake (for R). If you want to go deeper, Smith (2011) describes the design and implementation of several build managers. 9.11 Exercises Our Makefile currently reads as follows: .PHONY: all clean help settings COUNT=bin/countwords.py COLLATE=bin/collate.py PLOT=bin/plotcounts.py DATA=$(wildcard data/*.txt) RESULTS=$(patsubst data/%.txt,results/%.csv,$(DATA)) ## all : regenerate all results. all : results/collated.png ## results/collated.png: plot the collated results. results/collated.png : results/collated.csv python $(PLOT) $&lt; --outfile $@ ## results/collated.csv : collate all results. results/collated.csv : $(RESULTS) $(COLLATE) @mkdir -p results python $(COLLATE) $(RESULTS) &gt; $@ ## results/%.csv : regenerate result for any book. results/%.csv : data/%.txt $(COUNT) python $(COUNT) $&lt; &gt; $@ ## clean : remove all generated files. clean : rm $(RESULTS) results/collated.csv results/collated.png ## settings : show variables&#39; values. settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) @echo RESULTS: $(RESULTS) @echo COLLATE: $(COLLATE) @echo PLOT: $(PLOT) ## help : show this message. help : @grep &#39;^##&#39; ./Makefile A number of the exercises below ask you to make further edits to Makefile. 9.11.1 Report results that would change How can you get make to show the commands it would run without actually running them? (Hint: look at the manual page.) 9.11.2 Useful options What does Make’s -B option do and when is it useful? What about the -C option? What about the -f option? 9.11.3 Make sure the output directory exists One of our build recipes includes mkdir -p. What does this do and why is it useful? 9.11.4 Print the title and author The build rule for regenerating the result for any book is currently: ## results/%.csv : regenerate result for any book. results/%.csv : data/%.txt $(COUNT) python $(COUNT) $&lt; &gt; $@ Add an extra line to the recipe that uses the book_summary.sh script to print the title and author of the book to the screen. Use @bash so that the command itself isn’t printed to the screen and don’t forget to update the settings build rule to include the book_summary.sh script. If you’ve successfully made those changes, you should get the following output for Dracula: $ make -B results/dracula.csv Title: Dracula Author: Bram Stoker python bin/countwords.py data/dracula.txt &gt; results/dracula.csv 9.11.5 Create all results The default target of our final Makefile re-creates results/collated.csv. Add a target to Makefile so that make results creates or updates any result files that are missing or out of date, but does not regenerate results/collated.csv. 9.11.6 The perils of shell wildcards What is wrong with writing the rule for results/collated.csv like this: results/collated.csv : results/*.csv python $(COLLATE) $^ &gt; $@ (The fact that the result no longer depends on the program used to create it isn’t the biggest problem.) 9.11.7 Making documentation more readable We can format the documentation in our Makefile more readably using this command: ## help : show all commands. help : @grep -h -E &#39;^##&#39; ${MAKEFILE_LIST} | sed -e &#39;s/## //g&#39; \\ | column -t -s &#39;:&#39; Using man and online search, explain what every part of this recipe does. 9.11.8 Configuration A next step in automating this analysis might include moving the definitions of the COUNT, COLLATE, and PLOT variables into a separate file called config.mk: COUNT=bin/countwords.py COLLATE=bin/collate.py PLOT=bin/plotcounts.py and using the include command to access those definitions in the existing Makefile: .PHONY: results all clean help settings include config.mk # ... the rest of the Makefile ... Under what circumstances would this strategy be useful? 9.12 Key Points Make is a widely-used build manager. A build manager re-runs commands to update files that are out of date. A build rule has targets, prerequisites, and a recipe. A target can be a file or a phony target that simply triggers an action. When a target is out of date with respect to its prerequisites, Make executes the recipe associated with its rule. Make executes as many rules as it needs to when updating files, but always respects prerequisite order. Make defines automatic variables such as $@ (target), $^ (all prerequisites), and $&lt; (first prerequisite). Pattern rules can use % as a placeholder for parts of filenames. Makefiles can define variables using NAME=value. Makefiles can also use functions such as $(wildcard ...) and $(patsubst ...). Use specially-formatted comments to create self-documenting Makefiles. "],["config.html", "Chapter 10 Configuring Programs 10.1 Configuration File Formats 10.2 Matplotlib Configuration 10.3 The Global Configuration File 10.4 The User Configuration File 10.5 Adding Command-Line Options 10.6 A Job Control File 10.7 Summary 10.8 Exercises 10.9 Key Points", " Chapter 10 Configuring Programs Always be wary of any helpful item that weighs less than its operating manual. — Terry Pratchett In previous chapters we used command-line options to control our scripts and programs. If they are more complex, we may want to use up to four layers of configuration: A system-wide configuration file for general settings. A user-specific configuration file for personal preferences. A job-specific file with settings for a particular run. Command-line options to change things that commonly change. This is sometimes called overlay configuration because each level overrides the ones above it: the user’s configuration file overrides the system settings, the job configuration overrides the user’s defaults, and the command-line options overrides that. This is more complex than most research software needs initially (Xu et al. 2015), but being able to read a complete set of options from a file is a big boost to reproducibility. In this chapter, we’ll explore a number approaches for configuring our Zipf’s Law project, and ultimately decide to apply one of them. That project should now contain: zipf/ ├── .gitignore ├── CONDUCT.md ├── CONTRIBUTING.md ├── LICENSE.md ├── Makefile ├── README.md ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── script_template.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ └── ... └── results ├── collate.csv ├── collate.png ├── dracula.csv ├── dracula.png └── ... Be Careful When Applying Settings Outside Your Project This chapter’s examples modify files outside of the Zipf’s Law project in order to illustrate some concepts. If you alter these files while following along, remember to change them back later. 10.1 Configuration File Formats Programmers have invented far too many formats for configuration files; rather than creating one of your own, you should adopt some widely-used approach. One is to write the configuration as a Python module and load it as if it was a library. This is clever, but means that tools in other languages can’t process it. A second option is Windows INI format, which is laid out like this: [section1] key1=value1 key2=value2 [section2] key3=value3 key4=value4 INI files are simple to read and write, but the format is slowly falling out of use in favor of YAML. A simple YAML configuration file looks like this: # Standard settings for thesis. logfile: &quot;/tmp/log.txt&quot; quiet: false overwrite: false fonts: - Verdana - Serif Here, the keys logfile, quiet, and overwrite have the values /tmp/log.txt, false, and false respectively, while the value associated with the key fonts is a list containing Verdana and Serif. For more discussion of YAML, see Appendix H. 10.2 Matplotlib Configuration To see overlay configuration in action, let’s consider a common task in data science: changing the size of the labels on a plot. The labels on our Jane Eyre word frequency plot are fine for viewing on screen (Figure 10.1), but they will need to be bigger if we want to include the figure in a slideshow or report. Figure 10.1: Word frequency distribution for Jane Eyre with default label sizes. We could use any of the overlay options described above to change the size of the labels: Edit the system-wide Matplotlib configuration file (which would affect everyone using this computer). Create a user-specific Matplotlib style sheet. Create a job-specific configuration file to set plotting options in plotcounts.py. Add some new command-line options to plotcounts.py . Let’s consider these options one by one. 10.3 The Global Configuration File Our first configuration possibility is to edit the system-wide Matplotlib runtime configuration file, which is called matplotlibrc. When we import Matplotlib, it uses this file to set the default characteristics of the plot. We can find it on our system by running this command: import matplotlib as mpl mpl.matplotlib_fname() /Users/amira/anaconda3/lib/python3.7/site-packages/matplotlib/ mpl-data/matplotlibrc In this case the file is located in the Python installation directory (anaconda3). All the different Python packages installed with Anaconda live in a python3.7/site-packages directory, including Matplotlib. matplotlibrc lists all the default settings as comments. The default size of the X and Y axis labels is “medium”, as is the size of the tick labels: #axes.labelsize : medium ## fontsize of the x and y labels #xtick.labelsize : medium ## fontsize of the tick labels #ytick.labelsize : medium ## fontsize of the tick labels We can uncomment those lines and change the sizes to “large” and “extra large”: axes.labelsize : x-large ## fontsize of the x and y labels xtick.labelsize : large ## fontsize of the tick labels ytick.labelsize : large ## fontsize of the tick labels and then re-generate the Jane Eyre plot with bigger labels (Figure 10.2): $ python bin/plotcounts.py results/jane_eyre.csv --outfile results/jane_eyre.png Figure 10.2: Word frequency distribution for Jane Eyre with larger labels. This does what we want, but is usually the wrong approach. Since the matplotlibrc file sets system-wide defaults, we will now have big labels by default for all plotting we do in future, which we may not want. Secondly, we want to package our Zipf’s Law code and make it available to other people (Chapter 14). That package won’t include our matplotlibrc file, and we don’t have access to the one on their computer, so this solution isn’t as reproducible as others. A global options file is useful, though. If we are using Matplotlib with LaTeX to generate reports and the latter is installed in an unusual place on our computing cluster, a one-line change in matplotlibrc can prevent a lot of failed jobs. 10.4 The User Configuration File If we don’t want to change the configuration for everyone, we can change it for just ourself. Matplotlib defines several carefully-designed styles for plots: import matplotlib.pyplot as plt print(plt.style.available) [&#39;seaborn-dark&#39;, &#39;seaborn-darkgrid&#39;, &#39;seaborn-ticks&#39;, &#39;fivethirtyeight&#39;, &#39;seaborn-whitegrid&#39;, &#39;classic&#39;, &#39;_classic_test&#39;, &#39;fast&#39;, &#39;seaborn-talk&#39;, &#39;seaborn-dark-palette&#39;, &#39;seaborn-bright&#39;, &#39;seaborn-pastel&#39;, &#39;grayscale&#39;, &#39;seaborn-notebook&#39;, &#39;ggplot&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-muted&#39;, &#39;seaborn&#39;, &#39;Solarize_Light2&#39;, &#39;seaborn-paper&#39;, &#39;bmh&#39;, &#39;tableau-colorblind10&#39;, &#39;seaborn-white&#39;, &#39;dark_background&#39;, &#39;seaborn-poster&#39;, &#39;seaborn-deep&#39;] In order to make the labels bigger in all of our Zipf’s Law plots, we could create a custom Matplotlib style sheet. The convention is to store custom style sheets in a stylelib sub-directory in the Matplotlib configuration directory. That directory can be located by running the following command: mpl.get_configdir() /Users/amira/.matplotlib Once we’ve created the new sub-directory: $ mkdir /Users/amira/.matplotlib/stylelib we can add a new file called big-labels.mplstyle that has the same YAML format as the matplotlibrc file: axes.labelsize : x-large ## fontsize of the x and y labels xtick.labelsize : large ## fontsize of the tick labels ytick.labelsize : large ## fontsize of the tick labels To use this new style, we would just need to add one line to plotcounts.py: plt.style.use(&#39;big-labels&#39;) Using a custom style sheet leaves the system-wide defaults unchanged, and it’s a good way to achieve a consistent look across our personal data visualization projects. However, since each user has their own stylelib directory, it doesn’t solve the problem of ensuring that other people can reproduce our plots. 10.5 Adding Command-Line Options A third way to change the plot’s properties is to add some new command-line arguments to plotcounts.py. The choices parameter of add_argument lets us tell argparse that the user is only allowed to specify a value from a predefined list: mpl_sizes = [&#39;xx-small&#39;, &#39;x-small&#39;, &#39;small&#39;, &#39;medium&#39;, &#39;large&#39;, &#39;x-large&#39;, &#39;xx-large&#39;] parser.add_argument(&#39;--labelsize&#39;, type=str, default=&#39;x-large&#39;, choices=mpl_sizes, help=&#39;fontsize of the x and y labels&#39;) parser.add_argument(&#39;--xticksize&#39;, type=str, default=&#39;large&#39;, choices=mpl_sizes, help=&#39;fontsize of the x tick labels&#39;) parser.add_argument(&#39;--yticksize&#39;, type=str, default=&#39;large&#39;, choices=mpl_sizes, help=&#39;fontsize of the y tick labels&#39;) We can then add a few lines after the ax variable is defined in plotcounts.py to update the label sizes according to the user input: ax.xaxis.label.set_fontsize(args.labelsize) ax.yaxis.label.set_fontsize(args.labelsize) ax.xaxis.set_tick_params(labelsize=args.xticksize) ax.yaxis.set_tick_params(labelsize=args.yticksize) Alternatively, we can change the default runtime configuration settings before the plot is created. These are stored in a variable called matplotlib.rcParams: mpl.rcParams[&#39;axes.labelsize&#39;] = args.labelsize mpl.rcParams[&#39;xtick.labelsize&#39;] = args.xticksize mpl.rcParams[&#39;ytick.labelsize&#39;] = args.yticksize Adding extra command line arguments is a good solution if we only want to change a small number of plot characteristics. It also makes our work more reproducible: if we use a Makefile to regenerate our plots (Chapter 9), the settings will all be saved in one place. However, matplotlibrc has hundreds of parameters we could change, so the number of new arguments can quickly get out of hand if we want to tweak other aspects of the plot. 10.6 A Job Control File The final option for configuring our plots—the one we will actually adopt in this case— is to pass a YAML file full of Matplotlib parameters to plotcounts.py. First, we save the parameters we want to change in a file inside our project directory. We can call it anything, but plotparams.yml seems like it will be easy to remember. We’ll store it in bin with the scripts that will use it: # Plot characteristics axes.labelsize : x-large ## fontsize of the x and y labels xtick.labelsize : large ## fontsize of the tick labels ytick.labelsize : large ## fontsize of the tick labels Because this file is located in our project directory instead of the user-specific style sheet directory, we need to add one new option to plotcounts.py to load it: parser.add_argument(&#39;--plotparams&#39;, type=str, default=None, help=&#39;matplotlib parameters (YAML file)&#39;) We can use Python’s yaml library to read that file: with open(&#39;plotparams.yml&#39;, &#39;r&#39;) as reader: plot_params = yaml.load(reader, Loader=yaml.BaseLoader) print(plot_params) {&#39;axes.labelsize&#39;: &#39;x-large&#39;, &#39;xtick.labelsize&#39;: &#39;large&#39;, &#39;ytick.labelsize&#39;: &#39;large&#39;} and then loop over each item in plot_params to update Matplotlib’s mpl.rcParams: for (param, value) in param_dict.items(): mpl.rcParams[param] = value plotcounts.py now looks like this: &quot;&quot;&quot;Plot word counts.&quot;&quot;&quot; import argparse import yaml import numpy as np import pandas as pd import matplotlib as mpl from scipy.optimize import minimize_scalar def nlog_likelihood(beta, counts): # ...as before... def get_power_law_params(word_counts): # ...as before... def set_plot_params(param_file): &quot;&quot;&quot;Set the matplotlib parameters.&quot;&quot;&quot; if param_file: with open(param_file, &#39;r&#39;) as reader: param_dict = yaml.load(reader, Loader=yaml.BaseLoader) else: param_dict = {} for param, value in param_dict.items(): mpl.rcParams[param] = value def plot_fit(curve_xmin, curve_xmax, max_rank, beta, ax): # ...as before... def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; set_plot_params(args.plotparams) df = pd.read_csv(args.infile, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;rank&#39;, loglog=True, figsize=[12, 6], grid=True, xlim=args.xlim) word_counts = df[&#39;word_frequency&#39;].to_numpy() alpha = get_power_law_params(word_counts) print(&#39;alpha:&#39;, alpha) # Since the ranks are already sorted, we can take the last # one instead of computing which row has the highest rank max_rank = df[&#39;rank&#39;].to_numpy()[-1] # Use the range of the data as the boundaries # when drawing the power law curve curve_xmin = df[&#39;word_frequency&#39;].min() curve_xmax = df[&#39;word_frequency&#39;].max() plot_fit(curve_xmin, curve_xmax, max_rank, alpha, ax) ax.figure.savefig(args.outfile) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Word count csv file name&#39;) parser.add_argument(&#39;--outfile&#39;, type=str, default=&#39;plotcounts.png&#39;, help=&#39;Output image file name&#39;) parser.add_argument(&#39;--xlim&#39;, type=float, nargs=2, metavar=(&#39;XMIN&#39;, &#39;XMAX&#39;), default=None, help=&#39;X-axis limits&#39;) parser.add_argument(&#39;--plotparams&#39;, type=str, default=None, help=&#39;matplotlib parameters (YAML file)&#39;) args = parser.parse_args() main(args) 10.7 Summary Programs are only useful if they can be controlled, and work is only reproducible if those controls are explicit and shareable. If the number of controls needed is small, adding command-line options to programs and setting those options in Makefiles is usually the best solution. As the number of options grows, so too does the value of putting options in files of their own. And if we are installing the software on large systems that are used by many people, such as a research cluster, system-wide configuration files let us hide the details from people who just want to get their science done. More generally, the problem of configuring a program illustrates the difference between “works for me on my machine” and “works for everyone, everywhere”. From reproducible workflows (Chapter 9) to logging (Section 12.4), this difference influences every aspect of a research software engineer’s work. We don’t always have to design for large-scale re-use, but knowing what it entails allows us to make a conscious, thoughtful choice. 10.8 Exercises 10.8.1 Building with plotting parameters In the Makefile created in Chapter 9, the build rule involving plotcounts.py was defined as: ## results/collated.png: plot the collated results. results/collated.png : results/collated.csv python $(PLOT) $&lt; --outfile $@ Update that build rule to include the new --plotparams option. Make sure plotparams.yml is added as a second prerequisite in the updated build rule so that the appropriate commands will be re-run if the plotting parameters change. Hint: We use the automatic variable $&lt; to access the first prerequisite, but you’ll need $(word 2,$^) to access the second. Read about automatic variables (Section 9.5) and functions for string substitution and analysis to understand what that command is doing. 10.8.2 Using different plot styles There are a wide variety of pre-defined matplotlib styles (Section 10.4), as illustrated at the Python Graph Gallery. Add a new option --style to plotcounts.py that allows the user to pick a style from the list of pre-defined matplotlib styles. Hint: Use the choices parameter discussed in Section 10.5 to define the valid choices for the new --style option. Re-generate the plot of the Jane Eyre word count distribution using a bunch of different styles to decide which you like best. Matplotlib style sheets are designed to be composed together. (See the style sheets tutorial for details.) Use the nargs parameter to allow the user to pass any number of styles when using the --style option. 10.8.3 Saving configurations Add an option --saveconfig filename to plotcounts.py that writes all of its configuration to a file. Make sure this option saves all of the configuration, including any defaults that the user hasn’t changed. Add a new target test-saveconfig to the Makefile created in Chapter 9 to test that the new option is working. How would this new --saveconfig option make your work more reproducible? 10.8.4 Using INI syntax If we used Windows INI format instead of YAML for our plot parameters configuration file (i.e. plotparams.ini instead of plotparams.yml) that file would read as follows: [AXES] axes.labelsize=x-large [TICKS] xtick.labelsize=large ytick.labelsize=large The configparser library can be used to read and write INI files. Install that library by running pip install configparser at the command line. Using configparser, rewrite the set_plot_params function in plotcounts.py to handle a configuration file in INI rather than YAML format. Which file format do you find easier to work with? What other factors should influence your choice of a configuration file syntax? Note: the code modified in this exercise is not required for the rest of the book. 10.8.5 Configuration consistency In order for a data processing pipeline to work correctly, some of the configuration parameters for Program A and Program B must be the same. However, the programs were written by different teams, and each has its own configuration file. What steps could you take to ensure the required consistency? 10.9 Key Points Overlay configuration specifies settings for a program in layers, each of which overrides previous layers. Use a system-wide configuration file for general settings. Use a user-specific configuration file for personal preferences. Use a job-specific configuration file with settings for a particular run. Use command-line options to change things that commonly change. Use YAML or some other standard syntax to write configuration files. Save configuration information to make your research reproducible. "],["testing.html", "Chapter 11 Testing Software 11.1 Assertions 11.2 Unit Testing 11.3 Testing Frameworks 11.4 Testing Floating-Point Values 11.5 Integration Testing 11.6 Regression Testing 11.7 Test Coverage 11.8 Continuous Integration 11.9 When to Write Tests 11.10 Summary 11.11 Exercises 11.12 Key Points", " Chapter 11 Testing Software Opera happens because a large number of things amazingly fail to go wrong. — Terry Pratchett We have written software to count and analyze the words in classic texts, but how can we be sure it’s producing reliable results? The short is answer is that we can’t—not completely—but we can test its behavior against our expectations to decide if we are sure enough. This chapter explores ways to do this, including assertions, unit tests, integration tests, and regression tests. A Scientist’s Nightmare Why is testing research software important? A successful early career researcher in protein crystallography, Geoffrey Chang, had to retract five published papers—three from the journal Science—because his code had inadvertently flipped two columns of data (Miller 2006). More recently, a simple calculation mistake in a paper by Reinhart and Rogoff contributed to making the financial crash of 2008 even worse for millions of people (Borwein and Bailey 2013). Testing helps to catch errors like these. Here’s the current structure of our Zipf’s Law project files: zipf/ ├── .gitignore ├── CONDUCT.md ├── CONTRIBUTING.md ├── LICENSE.md ├── Makefile ├── README.md ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── plotparams.yml │ ├── script_template.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ └── ... └── results ├── collated.csv ├── collated.png ├── dracula.csv ├── dracula.png └── ... 11.1 Assertions The first step in building confidence in our programs is to assume that mistakes will happen and guard against them. This is called defensive programming, and the most common way to do it is to add assertions to our code so that it checks itself as it runs. An assertion is a statement that something must be true at a certain point in a program. When Python sees an assertion, it checks the assertion’s condition. If it’s true, Python does nothing; if it’s false, Python halts the program immediately and prints a user-defined error message. For example, this code halts as soon as the loop encounters an impossible word frequency: frequencies = [13, 10, 2, -4, 5, 6, 25] total = 0.0 for freq in frequencies[:5]: assert freq &gt;= 0.0, &#39;Word frequencies must be non-negative&#39; total += freq print(&#39;total frequency of first 5 words:&#39;, total) ----------------------------------------------------------------- AssertionError Traceback (most recent call last) &lt;ipython-input-19-33d87ea29ae4&gt; in &lt;module&gt;() 2 total = 0.0 3 for freq in frequencies[:5]: ----&gt; 4 assert freq &gt;= 0.0, &#39;Word frequencies must be non-negative&#39; 5 total += freq 6 print(&#39;total frequency of first 5 words:&#39;, total) AssertionError: Word frequencies must be non-negative Programs intended for widespread use are full of assertions: 10-20% of the code they contain are there to check that the other 80-90% are working correctly. Broadly speaking, assertions fall into three categories: A precondition is something that must be true at the start of a function in order for it to work correctly. For example, a function might check that the list it has been given has at least two elements and that all of its elements are integers. A postcondition is something that the function guarantees is true when it finishes. For example, a function could check that the value being returned is an integer that is greater than zero, but less than the length of the input list. An invariant is something that is true for every iteration in a loop. The invariant might be a property of the data (as in the example above), or it might be something like, “the value of highest is less than or equal to the current loop index”. The function get_power_law_params in our plotcounts.py script is a good example of the need for a precondition. Its docstring does not say that its word_counts parameter must be a list of numeric word counts; even if we add that, a user might easily pass in a list of the words themselves instead. Adding an assertion makes the requirement clearer, and also guarantees that the function will fail as soon as it is called rather than returning an error from scipy.optimize.minimize_scalar that would be more difficult to interpret/debug. def get_power_law_params(word_counts): &quot;&quot;&quot; Get the power law parameters. References ---------- Moreno-Sanchez et al (2016) define alpha (Eq. 1), beta (Eq. 2) and the maximum likelihood estimation (mle) of beta (Eq. 6). Moreno-Sanchez I, Font-Clos F, Corral A (2016) Large-Scale Analysis of Zipf&#39;s Law in English Texts. PLoS ONE 11(1): e0147073. https://doi.org/10.1371/journal.pone.0147073 &quot;&quot;&quot; assert type(word_counts) == np.ndarray, \\ &#39;Input must be a numerical (numpy) array of word counts&#39; mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4), args=word_counts, method=&#39;brent&#39;) beta = mle.x alpha = 1 / (beta - 1) return alpha Update plotcounts.py with the assertion described above. You’ll see additional examples of assertions throughout this chapter. 11.2 Unit Testing Catching errors is good, but preventing them is better, so responsible programmers test their code. As the name suggests, a unit test checks the correctness of a single unit of software. Exactly what constitutes a “unit” is subjective, but it typically means the behavior of a single function in one situation. In our Zipf’s Law software, the count_words function in countwords.py is a good candidate for unit testing: def count_words(reader): &quot;&quot;&quot;Count the occurrence of each word in a string.&quot;&quot;&quot; text = reader.read() chunks = text.split() stripped = [word.strip(string.punctuation) for word in chunks] word_list = [word.lower() for word in stripped if word] word_counts = Counter(word_list) return word_counts A single unit test will typically have: a fixture, which is the thing being tested (e.g., an array of numbers); an actual result, which is what the code produces when given the fixture; and an expected result that the actual result is compared to. The fixture is typically a subset or smaller version of the data the function will typically process. For instance, in order to write a unit test for the count_words function, we could use a piece of text small enough for us to count word frequencies by hand. Let’s add the poem Risk by Anaïs Nin to our data: $ cd ~/zipf $ mkdir test_data $ cat test_data/risk.txt And then the day came, when the risk to remain tight in a bud was more painful than the risk it took to blossom. We can then count the words by hand to construct the expected result: from collections import Counter risk_poem_counts = {&#39;the&#39;: 3, &#39;risk&#39;: 2, &#39;to&#39;: 2, &#39;and&#39;: 1, &#39;then&#39;: 1, &#39;day&#39;: 1, &#39;came&#39;: 1, &#39;when&#39;: 1, &#39;remain&#39;: 1, &#39;tight&#39;: 1, &#39;in&#39;: 1, &#39;a&#39;: 1, &#39;bud&#39;: 1, &#39;was&#39;: 1, &#39;more&#39;: 1, &#39;painful&#39;: 1, &#39;than&#39;: 1, &#39;it&#39;: 1, &#39;took&#39;: 1, &#39;blossom&#39;: 1} expected_result = Counter(risk_poem_counts) We then generate the actual result by calling count_words, and use an assertion to check if it is what we expected: import sys sys.path.append(&#39;/Users/amira/zipf/bin&#39;) import countwords with open(&#39;test_data/risk.txt&#39;, &#39;r&#39;) as reader: actual_result = countwords.count_words(reader) assert actual_result == expected_result There’s no output, which means the assertion (and test) passed. (Remember, assertions only do something if the condition is false.) Appending the Import Path The last code chunk included sys.path.append, which allowed us to import scripts from elsewhere than our current working directory. This isn’t a perfect solution, but works well enough for quick tests while developing code, especially since the tool we’ll use for the rest of the chapter handles this issue for us. Import statements are notoriously tricky, but we’ll learn better methods for organizing our code for function imports in Chapter 14. 11.3 Testing Frameworks Writing one unit test is easy enough, but we should check other cases as well. To manage them, we can use a test framework (also called a test runner). The most widely-used test framework for Python is called pytest, which structures tests as follows: Tests are put in files whose names begin with test_. Each test is a function whose name also begins with test_. These functions use assert to check results. Following these rules, we can create a test_zipfs.py script in your bin directory that contains the test we just developed: from collections import Counter import countwords def test_word_count(): &quot;&quot;&quot;Test the counting of words. The example poem is Risk, by Anaïs Nin. &quot;&quot;&quot; risk_poem_counts = {&#39;the&#39;: 3, &#39;risk&#39;: 2, &#39;to&#39;: 2, &#39;and&#39;: 1, &#39;then&#39;: 1, &#39;day&#39;: 1, &#39;came&#39;: 1, &#39;when&#39;: 1, &#39;remain&#39;: 1, &#39;tight&#39;: 1, &#39;in&#39;: 1, &#39;a&#39;: 1, &#39;bud&#39;: 1, &#39;was&#39;: 1, &#39;more&#39;: 1, &#39;painful&#39;: 1, &#39;than&#39;: 1, &#39;it&#39;: 1, &#39;took&#39;: 1, &#39;blossom&#39;: 1} expected_result = Counter(risk_poem_counts) with open(&#39;test_data/risk.txt&#39;, &#39;r&#39;) as reader: actual_result = countwords.count_words(reader) assert actual_result == expected_result The pytest library comes with a command-line tool that is also called pytest. When we run it with no options, it searches for all files in or below the working directory whose names match the pattern test_*.py. It then runs the tests in these files and summarizes their results. (If we only want to run the tests in a particular file, we can use the command pytest path/to/test_file.py.) $ pytest ===================== test session starts ====================== platform darwin -- Python 3.7.6, pytest-6.2.0, py-1.10.0, pluggy-0.13.1 rootdir: /Users/amira collected 1 item bin/test_zipfs.py . [100%] ====================== 1 passed in 0.02s ======================= To add more tests, we simply write more test_ functions in test_zipfs.py. For instance, besides counting words, the other critical part of our code is the calculation of the \\(\\alpha\\) parameter. Earlier we defined a power law relating \\(\\alpha\\) to the word frequency \\(f\\), the word rank \\(r\\), and a constant of proportionality \\(c\\) (Section 7.3):\\[ r = cf^{\\frac{-1}{\\alpha}} \\] We also noted that Zipf’s Law holds exactly when \\(\\alpha\\) is equal to one. Setting \\(\\alpha\\) to one and re-arranging the power law gives us:\\[ c = f/r \\] We can use this formula to generate synthetic word counts data (i.e. our test fixture) with a constant of proportionality set to a hypothetical maximum word frequency of 600 (and thus \\(r\\) ranges from 1 to 600): import numpy as np max_freq = 600 word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) print(word_counts) [600. 300. 200. 150. 120. 100. 85. 75. 66. 60. 54. 50. 46. 42. 40. 37. 35. 33. 31. 30. 28. 27. 26. 25. ... 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] (We use np.floor to round down to the nearest whole number, because we can’t have fractional word counts.) Passing this test fixture to get_power_law_params in plotcounts.py should give us a value of 1.0. To test this, we can add a second test to test_zipfs.py: from collections import Counter import numpy as np import plotcounts import countwords def test_alpha(): &quot;&quot;&quot;Test the calculation of the alpha parameter. The test word counts satisfy the relationship, r = cf**(-1/alpha), where r is the rank, f the word count, and c is a constant of proportionality. To generate test word counts for an expected alpha value of 1.0, a maximum word frequency of 600 is used (i.e. c = 600 and r ranges from 1 to 600) &quot;&quot;&quot; max_freq = 600 word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) actual_alpha = plotcounts.get_power_law_params(word_counts) expected_alpha = 1.0 assert actual_alpha == expected_alpha def test_word_count(): #...as before... Let’s re-run both of our tests: $ pytest ===================== test session starts ====================== platform darwin -- Python 3.7.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 rootdir: /Users/amira collected 2 items bin/test_zipfs.py F. [100%] =========================== FAILURES =========================== __________________________ test_alpha __________________________ def test_alpha(): &quot;&quot;&quot;Test the calculation of the alpha parameter. The test word counts satisfy the relationship, r = cf**(-1/alpha), where r is the rank, f the word count, and c is a constant of proportionality. To generate test word counts for an expected alpha value of 1.0, a maximum word frequency of 600 is used (i.e. c = 600 and r ranges from 1 to 600) &quot;&quot;&quot; max_freq = 600 word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) actual_alpha = plotcounts.get_power_law_params(word_counts) expected_alpha = 1.0 &gt; assert actual_alpha == expected_alpha E assert 0.9951524579316625 == 1.0 bin/test_zipfs.py:26: AssertionError ==================== short test summary info ===================== FAILED bin/test_zipfs.py::test_alpha - assert 0.99515245793 == 1.0 ================== 1 failed, 1 passed in 3.98s =================== The output tells us that one test failed but the other test passed. This is a very useful feature of test runners like pytest: they continue on and complete all the tests rather than stopping at the first assertion failure as a regular Python script would. 11.4 Testing Floating-Point Values The output above shows that that while test_alpha failed, the actual_alpha value of 0.9951524579316625 was very close to the expected value of 1.0. After a bit of thought, we decide that this isn’t actually a failure: the value produced by get_power_law_params is an estimate, and being off by half a percent is good enough. This example shows that testing scientific software almost always requires us to make the same kind of judgment calls that scientists have to make when doing any other sort of experimental work. If we are measuring the mass of a proton, we might expect ten decimal places of accuracy. If we are measuring the weight of a baby penguin, on the other hand, we’ll probably be satisfied if we’re within five grams. What matters most is that we are explicit about the bounds we used so that other people can tell what we actually did. Degrees of Difficulty There’s an old joke that physicists worry about decimal places, astronomers worry about powers of ten, and economists are happy if they’ve got the sign right. So how should we write tests when we don’t know precisely what the right answer is? The best approach is to write tests that check if the actual value is within some tolerance of the expected value. The tolerance can be expressed as the absolute error, which is the absolute value of the difference between two, or the relative error, which the ratio of the absolute error to the value we’re approximating (Goldberg 1991). For example, if we add 9+1 and get 11, the absolute error is 1 (i.e., 11-10), and the relative error is 10%. If we add 99+1 and get 101, on the other hand, the absolute error is still 1, but the relative error is only 1%. For test_alpha, we might decide that an absolute error of 0.01 in the estimation of \\(\\alpha\\) is acceptable. If we are using pytest, we can check that values lie within this tolerance using pytest.approx: from collections import Counter import pytest import numpy as np import plotcounts import countwords def test_alpha(): &quot;&quot;&quot;Test the calculation of the alpha parameter. The test word counts satisfy the relationship, r = cf**(-1/alpha), where r is the rank, f the word count, and c is a constant of proportionality. To generate test word counts for an expected alpha value of 1.0, a maximum word frequency of 600 is used (i.e. c = 600 and r ranges from 1 to 600) &quot;&quot;&quot; max_freq = 600 word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) actual_alpha = plotcounts.get_power_law_params(word_counts) expected_alpha = pytest.approx(1.0, abs=0.01) assert actual_alpha == expected_alpha def test_word_count(): #...as before... When we re-run pytest, both tests now pass: $ pytest ===================== test session starts ====================== platform darwin -- Python 3.7.6, pytest-6.2.0, py-1.10.0, pluggy-0.13.1 rootdir: /Users/amira collected 2 items bin/test_zipfs.py .. [100%] ====================== 2 passed in 0.69s ======================= Testing Visualizations Testing visualizations is hard: any change to the dimension of the plot, however small, can change many pixels in a raster image, and cosmetic changes such as moving the legend up a couple of pixels will cause all of our tests to fail. The simplest solution is therefore to test the data used to produce the image rather than the image itself. Unless we suspect that the plotting library contains bugs, the correct data should always produce the correct plot. 11.5 Integration Testing Our Zipf’s Law analysis has two steps: counting the words in a text and estimating the \\(\\alpha\\) parameter from the word count. Our unit tests give us some confidence that these components work in isolation, but do they work correctly together? Checking that is called integration testing. Integration tests are structured the same way as unit tests: a fixture is used to produce an actual result that is compared against the expected result. However, creating the fixture and running the code can be considerably more complicated. For example, in the case of our Zipf’s Law software an appropriate integration test fixture might be a text file with a word frequency distribution that has a known \\(\\alpha\\) value. In order to create this text fixture, we need a way to generate random words. Fortunately, a Python library called RandomWordGenerator exists to do just that. We can install it using pip, the Python Package Installer: $ pip install Random-Word-Generator Borrowing from the word count distribution we created for test_alpha, we can use the following code to create a text file full of random words with a frequency distribution that corresponds to an \\(\\alpha\\) of approximately 1.0: import numpy as np from RandomWordGenerator import RandomWord max_freq = 600 word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) rw = RandomWord() random_words = rw.getList(num_of_words=max_freq) writer = open(&#39;test_data/random_words.txt&#39;, &#39;w&#39;) for index in range(max_freq): count = int(word_counts[index]) word_sequence = f&#39;{random_words[index]} &#39; * count writer.write(word_sequence + &#39;\\n&#39;) writer.close() We can confirm this code worked by checking the resulting file: tail -n 5 test_data/random_words.txt lZnkzoBHRb djiroplqrJ HmAUGOncHg DGLpfTIitu KALSfPkrga We can then add this integration test to test_zipfs.py: def test_integration(): &quot;&quot;&quot;Test the full word count to alpha parameter workflow.&quot;&quot;&quot; with open(&#39;test_data/random_words.txt&#39;, &#39;r&#39;) as reader: word_counts_dict = countwords.count_words(reader) counts_array = np.array(list(word_counts_dict.values())) actual_alpha = plotcounts.get_power_law_params(counts_array) expected_alpha = pytest.approx(1.0, abs=0.01) assert actual_alpha == expected_alpha Finally, we re-run pytest to check that the integration test passes: $ pytest ===================== test session starts ====================== platform darwin -- Python 3.7.6, pytest-6.2.0, py-1.10.0, pluggy-0.13.1 rootdir: /Users/amira collected 3 items bin/test_zipfs.py ... [100%] ======================= 3 passed in 0.48s ====================== 11.6 Regression Testing So far we have tested two simplified texts: a short poem and a collection of random words with a known frequency distribution. The next step is to test with real data, i.e., an actual book. The problem is, we don’t know the expected result: it’s not practical to count the words in Dracula by hand, and even if we tried, the odds are good that we’d make a mistake. For this kind of situation we can use regression testing. Rather than assuming that the test author knows what the expected result should be, regression tests compares today’s answer with a previous one. This doesn’t guarantee that the answer is right—if the original answer is wrong, we could carry that mistake forward indefinitely—but it does draw attention to any changes (or “regressions”). In Section 7.4 we calculated an \\(\\alpha\\) of 1.0866646252515038 for Dracula. Let’s use that value to add a regression test to test_zipfs.py: def test_regression(): &quot;&quot;&quot;Regression test for Dracula.&quot;&quot;&quot; with open(&#39;data/dracula.txt&#39;, &#39;r&#39;) as reader: word_counts_dict = countwords.count_words(reader) counts_array = np.array(list(word_counts_dict.values())) actual_alpha = plotcounts.get_power_law_params(counts_array) expected_alpha = pytest.approx(1.087, abs=0.001) assert actual_alpha == expected_alpha $ pytest ===================== test session starts ====================== platform darwin -- Python 3.7.6, pytest-6.2.0, py-1.10.0, pluggy-0.13.1 rootdir: /Users/amira collected 4 items bin/test_zipfs.py .... [100%] ====================== 4 passed in 0.56s ======================= 11.7 Test Coverage How much of our code do the tests we have written check? More importantly, what parts of our code being tested (yet)? To find out, we can use a tool to check their code coverage. Most Python programmers use the coverage library, which we can once again install using pip: $ pip install coverage Once we have it, we can use it to run pytest on our behalf: $ coverage run -m pytest ===================== test session starts ====================== platform darwin -- Python 3.7.6, pytest-6.2.0, py-1.10.0, pluggy-0.13.1 rootdir: /Users/amira collected 4 items bin/test_zipfs.py .... [100%] ====================== 4 passed in 0.72s ======================= The coverage command doesn’t display any information of its own, since mixing that in with our program’s output would be confusing. Instead, it puts coverage data in a file called .coverage (with a leading .) in the current directory. To display that data, we run: $ coverage report -m bin/countwords.py 20 7 65% 25-26, 30-38 bin/plotcounts.py 58 37 36% 48-55, 75-77, 82-83, 88-118, 122-140 bin/test_zipfs.py 31 0 100% bin/utilities.py 8 5 38% 18-22 ------------------------------------------------- TOTAL 117 49 58% This summary shows us that some lines of countwords.py or plotcounts.py were not executed when we ran the tests: in fact, only 65% and 36% of the lines were run respectively. This makes sense, since much of the code in those scripts is devoted to handling command line arguments or file I/O rather than the word counting and parameter estimation functionality that our unit, integration and regression tests focus on. To make sure that’s the case, we can get a more complete report by running coverage html at the command line and opening htmlcov/index.html. Clicking on the name of our countwords.py script, for instance, produces the colorized line-by-line display shown in Figure 11.1. Figure 11.1: Example of Python code coverage report. This output confirms that all lines relating to word counting were tested, but not any of the lines related to argument handling or I/O. Commit our Coverage? At this point, you’re probably wondering if you should use version control to track the files reporting your code’s coverage. While it won’t necessarily harm your code, the reports will become inaccurate unless you continue updating your coverage reports as your code changes. Therefore, we recommend adding .coverage and htmlcov/ to your .gitignore file. In the next section, we’ll explore an approach that can help you automate tasks like assessing coverage. Is this good enough? The answer depends on what the software is being used for and by whom. If it is for a safety-critical application such as a medical device, we should aim for 100% code coverage, i.e., every single line in the application should be tested. In fact, we should probably go further and aim for 100% path coverage to ensure that every possible path through the code has been checked. Similarly, if the software has become popular and is being used by thousands of researchers all over the world, we should probably check that it’s not going to embarrass us. But most of us don’t write software that people’s lives depend on, or that is in a “top 100” list, so requiring 100% code coverage is like asking for ten decimal places of accuracy when checking the voltage of a household electrical outlet. We always need to balance the effort required to create tests against the likelihood that those tests will uncover useful information. We also have to accept that no amount of testing can prove a piece of software is completely correct. A function with only two numeric arguments has 2128 possible inputs. Even if we could write the tests, how could we be sure we were checking the result of each one correctly? Luckily, we can usually put test cases into groups. For example, when testing a function that summarizes a table full of data, it’s probably enough to check that it handles tables with: no rows only one row many identical rows rows having keys that are supposed to be unique, but aren’t rows that contain nothing but missing values Some projects develop checklists like this one to remind programmers what they ought to test. These checklists can be a bit daunting for newcomers, but they are a great way to pass on hard-earned experience. 11.8 Continuous Integration Now that we have a set of tests, we could run pytest every now and again to check our code. This is probably sufficient for short-lived projects, but if several people are involved, or if we are making changes over weeks or months, we might forget to run the tests or it might be difficult to identify which change is responsible for a test failure. The solution is continuous integration (CI), which runs tests automatically whenever a change is made. CI tells developers immediately if changes have caused problems, which makes them much easier to fix. CI can also be set up to run tests with several different configurations of the software or on several different operating systems, so that a programmer using Windows can be warned that a change breaks things for Mac users and vice versa. One popular CI tool is Travis CI, which integrates well with GitHub. If Travis CI has been set up, then every time a change is committed to a GitHub repository, Travis CI creates a fresh environment, makes a fresh clone of the repository (Section 7.8), and runs whatever commands the project’s managers have set up. Before setting up our account with Travis CI, however, we need to prepare our repository to be recognized by the tool. We’ll first add a file called .travis.yml to our repository, which includes instructions for Travis CI. (The leading . in the name hides the file from casual listings on Mac or Linux, but not on Windows.) This file must be in the root directory of the repository, and is written in YAML (Section 10.1 and Appendix H). For our project, we add the following lines: language: python python: - &quot;3.6&quot; install: - pip install -r requirements.txt script: - pytest Python 3.6? An early draft of the book used Python 3.6, and we later updated to 3.7. Over the life of a real project, software versions change, and it’s not always feasible or possible to re-do all parts of the project. In this case, our first build in Travis CI used Python 3.6, so that’s what we’ve shown here, and later checked that updating to 3.7 didn’t break anything. We can’t expect our software will be perfect the first time around, but we can do our best to document what’s changed and confirm our results are still accurate. The language key tells Travis CI which programming language to use, so that it knows which of its standard virtual machines to use as a starting point for the project. The python key specifies the version or versions of Python to use, while the install key indicates the name of the file (requirements.txt) listing the libraries that need to be installed. The script key lists the commands to run—in this case, pytest— which in turn executes our project scripts (i.e. test_zipfs.py, plotcounts.py, countwords.py and utilities.py). These scripts import a number of packages that don’t come with the Python Standard Library, so these are what comprises requirements.txt: numpy pandas matplotlib scipy pytest pyyaml Be sure to save both requirements.txt and .travis.yml to your project’s root directory, commit to your repository, and push to GitHub, or Travis CI will not be able to recognize your project. We’re now ready to set up CI for our project. The basic steps are as follows: Create an account on Travis CI (if we don’t already have one). Link our Travis CI account to our GitHub account (if we haven’t done so already). Tell Travis CI to watch the repository that contains our project. Creating an account with an online service is probably a familiar process, but linking our Travis CI account to our GitHub account may be something new. We only have to do this once to allow Travis CI to access all our GitHub repositories, but we should always be careful when giving sites access to other sites, and only trust well-established and widely-used services. We can tell Travis CI which repository we want it to watch by clicking the “+” next to the “My Repositories” link on the left-hand side of the Travis CI homepage (Figure 11.2). Figure 11.2: Click to add a new GitHub repository to Travis CI. To add the GitHub repository we have been using throughout this book, find it in the repository list (Figure 11.3). If the repository doesn’t show up, re-synchronize the list using the green “Sync account” button on the left sidebar. If it still doesn’t appear, the repository may belong to someone else or be private, or your required files may be incorrect. Click the “Trigger a build” button to initiate your first test using Travis CI. Figure 11.3: Find Zipf’s Law repository and trigger a build. Once your repository has been activated, Travis CI follows the instructions in .travis.yml and reports whether the build passed (shown in green) or produced warnings or errors (shown in red). To create this report, Travis CI has: Created a new Linux virtual machine. Installed the desired version of Python. Run the commands below the script key. Reported the results at https://travis-ci.com/USER/REPO, where USER/REPO identifies the repository for a given user. Our tests pass and the build completes successfully. We can view additional details about the test by clicking on the repository name (Figure 11.4). Figure 11.4: Travis build overview (build succeeded). This example shows one of the other benefits of CI: it forces us to be explicit about what we are doing and how we do it, just as writing a Makefile forces us to be explicit about exactly how we produce results (Zampetti et al. 2020). 11.9 When to Write Tests We have now met the three major types of test: unit, integration and regression. At what point in the code development process should we write these? The answer depends on who you ask. Many programmers are passionate advocates of a practice called test-driven development (TDD). Rather than writing code and then writing tests, they write the tests first and then write just enough code to make those tests pass. Once the code is working, they clean it up (Appendix F.4) and then move on to the next task. TDD’s advocates claim that this leads to better code because: Writing tests clarifies what the code is actually supposed to do. It eliminates confirmation bias. If someone has just written a function, they are predisposed to want it to be right, so they will bias their tests towards proving that it is correct instead of trying to uncover errors. Writing tests first ensures that they actually get written. These arguments are plausible. However, studies such as Fucci et al. (2016) and Fucci et al. (2017) don’t support them: in practice, writing tests first or last doesn’t appear to affect productivity. What does have an impact is working in small, interleaved increments, i.e., writing just a few lines of code and testing it before moving on rather than writing several pages of code and then spending hours on testing. So how do most data scientists figure out if their software is doing the right thing? The answer is spot checks: each time they produce an intermediate or final result, they scan a table, create a chart, or inspect some summary statistics to see if everything looks OK. Their heuristics are usually easy to state, like “there shouldn’t be NAs at this point” or “the age range should be reasonable”, but applying those heuristics to a particular analysis always depends on their evolving insight into the data in question. By analogy with test-driven development, we could call this process “checking-driven development”. Each time we add a step to our pipeline and look at its output, we can also add a check of some kind to the pipeline to ensure that what we are checking for remains true as the pipeline evolves or is run on other data. Doing this helps reusability—it’s amazing how often a one-off analysis winds up being used many times—but the real goal is comprehensibility. If someone can get our code and data, then runs the code on the data, and gets the same result that we did, then our computation is reproducible, but that doesn’t mean they can understand it. Comments help (either in the code or as blocks of prose in a computational notebook), but they won’t check that assumptions and invariants hold. And unlike comments, runnable assertions can’t fall out of step with what the code is actually doing. 11.10 Summary Testing data analysis pipelines is often harder than testing mainstream software applications, since data analysts often don’t know what the right answer is (Braiek and Khomh 2018). (If we did, we would have submitted our report and moved on to the next problem already.) The key distinction is the difference between validation, which asks whether the specification is correct, and verification, which asks whether we have met that specification. The difference between them is the difference between building the right thing and building something right; the practices introduced in this chapter will help with both. 11.11 Exercises 11.11.1 Explaining assertions Given a list of a numbers, the function total returns the total: total([1, 2, 3, 4]) 10 total only works on numbers: total([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]) ValueError: invalid literal for int() with base 10: &#39;a&#39; Explain in words what the assertions in this function check, and for each one, give an example of input that will make that assertion fail. def total(values): assert len(values) &gt; 0 for element in values: assert int(element) values = [int(element) for element in values] total = sum(values) assert total &gt; 0 return total 11.11.2 Rectangle normalization A rectangle can be described using a tuple of four cartesian coordinates (x0, y0, x1, y1), where (x0, y0) represents the lower left corner and (x1, y1) the upper right. In order to do some calculations, suppose we need to be able to normalize rectangles so that the lower left corner is at the origin (i.e. (x0, y0) = (0, 0)) and the longest side is 1.0 units long. This function does that: def normalize_rectangle(rect): &quot;&quot;&quot;Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis. Input should be of the format (x0, y0, x1, y1). (x0, y0) and (x1, y1) define the lower left and upper right corners of the rectangle, respectively.&quot;&quot;&quot; # insert preconditions x0, y0, x1, y1 = rect # insert preconditions dx = x1 - x0 dy = y1 - y0 if dx &gt; dy: scaled = float(dx) / dy upper_x, upper_y = 1.0, scaled else: scaled = float(dx) / dy upper_x, upper_y = scaled, 1.0 # insert postconditions here return (0, 0, upper_x, upper_y) In order to answer the following questions, cut and paste the normalize_rectangle function into a new file called geometry.py (outside of your zipf project) and save that file in a new directory called exercises. To ensure that the inputs to normalize_rectangle are valid, add preconditions to check that: rect contains 4 coordinates, the width of the rectangle is a positive, non-zero value (i.e. x0 &lt; x1), and the height of the rectangle is a positive, non-zero value (i.e. y0 &lt; y1). If the normalization calculation has worked correctly, the new x1 coordinate will lie between 0 and 1 (i.e. 0 &lt; upper_x &lt;= 1.0). Add a postcondition to check that this is true. Do the same for the new y1 coordinate, upper_y. Running normalize_rectangle for a short, wide rectangle should pass your new preconditions and postconditions: import geometry geometry.normalize_rectangle([2, 5, 3, 10]) (0, 0, 0.2, 1.0) but will fail for a tall, skinny rectangle: geometry.normalize_rectangle([20, 15, 30, 20]) AssertionError Traceback (most recent call last) &lt;ipython-input-3-f4e8cdf7f69d&gt; in &lt;module&gt; ----&gt; 1 geometry.normalize_rectangle([20, 15, 30, 20]) ~/Desktop/exercises/geometry.py in normalize_rectangle(rect) 19 20 assert 0 &lt; upper_x &lt;= 1.0, \\ 21 &#39;Calculated upper X coordinate invalid&#39; ---&gt; 22 assert 0 &lt; upper_y &lt;= 1.0, \\ 23 &#39;Calculated upper Y coordinate invalid&#39; 24 25 return (0, 0, upper_x, upper_y) AssertionError: Calculated upper Y coordinate invalid Find and correct the source of the error in normalize_rectangle. Once fixed, you should be able to successfully run geometry.normalize_rectangle([20, 15, 30, 20]). Write a unit test for tall, skinny rectangles and save it in a new file called test_geometry.py. Run pytest to make sure the test passes. Add a couple more unit tests to test_geometry.py. Explain the rationale behind each test. 11.11.3 Testing with randomness Programs that rely on random numbers are impossible to test because there’s (deliberately) no way to predict their output. Luckily, computer programs don’t actually use random numbers: they use a pseudo-random number generator (PRNG) that produces values in a repeatable but unpredictable way. Given the same initial seed, a PRNG will always produce the same sequence of values. How can we use this fact when testing programs that rely on pseudo-random numbers? 11.11.4 Testing with relative error If E is the expected result of a function and A is the actual value it produces, the relative error is abs((A-E)/E). This means that if we expect the results of tests to be 2, 1, and 0, and we actually get 2.1, 1.1, and 0.1 the relative errors are 5%, 10%, and infinity. Why does this seem counter-intuitive, and what might be a better way to measure error in this case? 11.12 Key Points Test software to convince people (including yourself) that software is correct enough and to make tolerances on “enough” explicit. Add assertions to code so that it checks itself as it runs. Write unit tests to check indivdiual pieces of code. Write integration tests to check that those pieces work together correctly. Write regression tests to check if things that used to work no longer do. A test framework finds and runs tests written in a prescribed fashion and reports their results. Test coverage is the fraction of lines of code that are executed by a set of tests. Continuous integration re-builds and/or re-tests software every time something changes. "],["errors.html", "Chapter 12 Handling Errors 12.1 Exceptions 12.2 Writing Useful Error Messages 12.3 Testing Error Handling 12.4 Reporting Errors 12.5 Summary 12.6 Exercises 12.7 Key Points", " Chapter 12 Handling Errors “When Mister Safety Catch Is Not On, Mister Crossbow Is Not Your Friend.” — Terry Pratchett We are imperfect people living in an imperfect world. People will misunderstand how to use our programs, and even if we test thoroughly as described in the previous chapter, those programs might still contain bugs. We should therefore plan from the start to detect and handle errors. Something that goes wrong while a program is running is sometimes referred to as an exception from normal behavior. Generally speaking, we distinguish between two types of errors/exceptions. Internal errors, are mistakes in the program itself, such as calling a function with None instead of a list. External errors are usually caused by interactions between the program and the outside world: a user may mis-type a filename, the network might be down, and so on. When an internal error occurs, the only thing we can do in most cases is report it and halt the program. If a function has been passed None instead of a valid list, for example, the odds are good that one of our data structures is corrupted. We can try to guess what the problem is and take corrective action, but our guess will often be wrong and our attempt to correct the problem might actually make things worse. When an external error occurs on the other hand, we don’t always want the program to stop. If a user mis-types her password, handling the error by prompting her to try again would be friendlier than halting and requiring her to restart the program. This chapter looks at how we can raise, catch and handle errors. We consider how to write useful error messages, and how to make our programs log those messages along with other useful information as they are running, so that it’s easier to figure out what happened when something goes wrong. The Zipf’s Law project should now include: zipf/ ├── .gitignore ├── .travis.yml ├── CONDUCT.md ├── CONTRIBUTING.md ├── LICENSE.md ├── Makefile ├── README.md ├── requirements.txt ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── plotparams.yml │ ├── script_template.py │ ├── test_zipfs.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ └── ... ├── results │ ├── dracula.csv │ ├── dracula.png │ └── ... └── test_data ├── random_words.txt └── risk.txt 12.1 Exceptions Most modern programming languages use exceptions for error handling. As the name suggests, an exception is a way to represent an exceptional or unusual occurrence that doesn’t fit neatly into the program’s expected operation. The code below uses exceptions to report attempts to divide by zero: for denom in [-5, 0, 5]: try: result = 1/denom print(f&#39;1/{denom} == {result}&#39;) except: print(f&#39;Cannot divide by {denom}&#39;) 1/-5 == -0.2 Cannot divide by 0 1/5 == 0.2 try/except looks like if/else and works in a similar fashion. If nothing unexpected happens inside the try block, the except block isn’t run (Figure 12.1). If something goes wrong inside the try, on the other hand, the program jumps immediately to the except. This is why the print statement inside the try doesn’t run when denom is 0: as soon as Python tries to calculate 1/denom, it skips directly to the code under except. Figure 12.1: Exception control flow. We often want to know exactly what went wrong, so Python and other languages store information about the error in an object (which is also called an exception). We can catch an exception and inspect it as follows: for denom in [-5, 0, 5]: try: result = 1/denom print(f&#39;1/{denom} == {result}&#39;) except Exception as error: print(f&#39;{denom} has no reciprocal: {error}&#39;) 1/-5 == -0.2 0 has no reciprocal: division by zero 1/5 == 0.2 We can use any variable name we like instead of error; Python will assign the exception object to that variable so that we can do things with it in the except block. Python also allows us to specify what kind of exception we want to catch. For example, we can write code to handle out-of-range indexing and division by zero separately: numbers = [-5, 0, 5] for i in [0, 1, 2, 3]: try: denom = numbers[i] result = 1/denom print(f&#39;1/{denom} == {result}&#39;) except IndexError as error: print(f&#39;index {i} out of range&#39;) except ZeroDivisionError as error: print(f&#39;{denom} has no reciprocal: {error}&#39;) 1/-5 == -0.2 0 has no reciprocal: division by zero 1/5 == 0.2 index 3 out of range Exceptions are organized in a hierarchy: for example, FloatingPointError, OverflowError, and ZeroDivisionError are all special cases of ArithmeticError, so an except that catches the latter will catch all three of the former, but an except that catches an OverflowError won’t catch a ZeroDivisionError. The Python documentation describes all of the built-in exception types; in practice, the ones that people handle most often are: ArithmeticError: something has gone wrong in a calculation. IndexError and KeyError: something has gone wrong indexing a list or lookup something up in a dictionary. OSError: thrown when a file is not found, the program doesn’t have permission to read it, and so on. So where do exceptions come from? The answer is that programmers can raise them explicitly: for number in [1, 0, -1]: try: if number &lt; 0: raise ValueError(f&#39;no negatives: {number}&#39;) print(number) except ValueError as error: print(f&#39;exception: {error}&#39;) 1 0 exception: no negatives: -1 We can define our own exception types, and many libraries do, but the built-in types are enough to cover common cases. One final note is that exceptions don’t have to be handled where they are raised. In fact, their greatest strength is that they allow long-range error handling. If an exception occurs inside a function and there is no except for it there, Python checks to see if whoever called the function is willing to handle the error. It keeps working its way up through the call stack until it finds a matching except. If there isn’t one, Python takes care of the exception itself. The example below relies on this: the second call to sum_reciprocals tries to divide by zero, but the exception is caught in the calling code rather than in the function. def sum_reciprocals(values): result = 0 for v in values: result += 1/v return result numbers = [-1, 0, 1] try: one_over = sum_reciprocals(numbers) except ArithmeticError as error: print(f&#39;Error trying to sum reciprocals: {error}&#39;) Error trying to sum reciprocals: division by zero This behavior is designed to support a pattern called “throw low, catch high”: write most of your code without exception handlers, since there’s nothing useful you can do in the middle of a small utility function, but put a few handlers in the uppermost functions of your program to catch and report all errors. We can now go ahead and add error handling to our Zipf’s Law code. Some is already built in: for example, if we try to read a file that does not exist, the open function throws a FileNotFoundError: $ python bin/collate.py results/none.csv results/dracula.csv Traceback (most recent call last): File &quot;bin/collate.py&quot;, line 35, in &lt;module&gt; main(args) File &quot;bin/collate.py&quot;, line 23, in main with open(fname, &#39;r&#39;) as reader: FileNotFoundError: [Errno 2] No such file or directory: &#39;results/none.csv&#39; But what happens if we try to read a file that exists, but was not created by countwords.py? $ python bin/collate.py Makefile Traceback (most recent call last): File &quot;bin/collate.py&quot;, line 35, in &lt;module&gt; main(args) File &quot;bin/collate.py&quot;, line 24, in main update_counts(reader, word_counts) File &quot;bin/collate.py&quot;, line 15, in update_counts for word, count in csv.reader(reader): ValueError: not enough values to unpack (expected 2, got 1) This error is hard to understand, even if we are familiar with the code’s internals. Our program should therefore check that the input files are CSV files, and if not, raise an error with a useful explanation to what went wrong. We could achieve this by wrapping the call to open in a try/except clause: for fname in args.infiles: try: with open(fname, &#39;r&#39;) as reader: update_counts(reader, word_counts) except ValueError as e: print(f&#39;{fname} is not a CSV file.&#39;) print(f&#39;ValueError: {e}&#39;) $ python bin/collate.py Makefile Makefile is not a CSV file. ValueError: not enough values to unpack (expected 2, got 1) This is definitely more informative than before. However, all ValueErrors that are raised when trying to open a file will result in this error message, including those raised when we actually do use a CSV file as input. A more precise approach in this case would be to throw an exception only if some other kind of file is specified as an input: for fname in args.infiles: if fname[-4:] != &#39;.csv&#39;: raise OSError(f&#39;{fname} is not a CSV file.&#39;) with open(fname, &#39;r&#39;) as reader: update_counts(reader, word_counts) $ python bin/collate.py Makefile Traceback (most recent call last): File &quot;bin/collate.py&quot;, line 37, in &lt;module&gt; main(args) File &quot;bin/collate.py&quot;, line 24, in main raise OSError(f&#39;{fname} is not a CSV file.&#39;) OSError: Makefile is not a CSV file. This approach is still not perfect: we are checking that the file’s suffix is .csv instead of checking the content of the file and confirming that it is what we require. What we should do is check that there are two columns separated by a comma, that the first column contains strings, and that the second is numerical. Kinds of Errors The “if then raise” approach is sometimes referred to as “look before you leap”, while the try/except approach obeys the old adage that “it’s easier to ask for forgiveness than permission”. The first approach is more precise, but has the shortcoming that programmers can’t anticipate everything that can go wrong when running a program, so there should always be an except somewhere to deal with unexpected cases. The one rule we should always follow is to check for errors as early as possible so that we don’t waste the user’s time. Few things are as frustrating as being told at the end of an hour-long calculation that the program doesn’t have permission to write to an output directory. It’s a little extra work to check things like this up front, but the larger your program or the longer it runs, the more useful those checks will be. 12.2 Writing Useful Error Messages Figure 12.2: An unhelpful error message. The error message shown in Figure 12.2 is not helpful. Having collate.py print the message below would be equally unfriendly: OSError: Something went wrong, try again. This message doesn’t provide any information on what went wrong, so it is difficult to know what to change for next time. A slightly better message would be: OSError: Unsupported file type. This tells us the problem is with the type of file we’re trying to process, but it still doesn’t tell us what file types are supported, which means we have to rely on guesswork or read the source code. Telling the user “filename is not a CSV file” (as we did in the previous section) makes it clear that the program only works with CSV files, but since we don’t actually check the content of the file, this message could confuse someone who has comma-separated values saved in a .txt file. An even better message would therefore be: OSError: File must end in .csv This message tells us exactly what the criteria are to avoid the error. Error messages are often the first thing people read about a piece of software, so they should therefore be the most carefully written documentation for that software. A web search for “writing good error messages” turns up hundreds of hits, but recommendations are often more like gripes than guidelines and are usually not backed up by evidence. What research there is gives us the following rules (Becker et al. 2016): Tell the user what they did, not what the program did. Putting it another way, the message shouldn’t state the effect of the error, it should state the cause. Be spatially correct, i.e., point at the actual location of the error. Few things are as frustrating as being pointed at line 28 when the problem is really on line 35. Be as specific as possible without being or seeming wrong from a user’s point of view. For example, “file not found” is very different from “don’t have permissions to open file” or “file is empty”. Write for your audience’s level of understanding. For example, error messages should never use programming terms more advanced than those you would use to describe the code to the user. Do not blame the user, and do not use words like fatal, illegal, etc. The former can be frustrating—in many cases, “user error” actually isn’t—and the latter can make people worry that the program has damaged their data, their computer, or their reputation. Do not try to make the computer sound like a human being. In particular, avoid humor: very few jokes are funny on the dozenth re-telling, and most users are going to see error messages at least that often. Use a consistent vocabulary. This rule can be hard to enforce when error messages are written by several different people, but putting them all in one module makes review easier. That last suggestion deserves a little elaboration. Most people write error messages directly in their code: if fname[-4:] != &#39;.csv&#39;: raise OSError(f&#39;{fname}: File must end in .csv&#39;) A better approach is to put all the error messages in a dictionary: ERRORS = { &#39;not_csv_suffix&#39; : &#39;{fname}: File must end in .csv&#39;, &#39;config_corrupted&#39; : &#39;{config_name} corrupted&#39;, # ...more error messages... } and then only use messages from that dictionary: if fname[-4:] != &#39;.csv&#39;: raise OSError(ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname)) Doing this makes it much easier to ensure that messages are consistent. It also makes it much easier to give messages in the user’s preferred language: ERRORS = { &#39;en&#39; : { &#39;not_csv_suffix&#39; : &#39;{fname}: File must end in .csv&#39;, &#39;config_corrupted&#39; : &#39;{config_name} corrupted&#39;, # ...more error messages in English... }, &#39;fr&#39; : { &#39;not_csv_suffix&#39; : &#39;{fname}: Doit se terminer par .csv&#39;, &#39;config_corrupted&#39; : f&#39;{config_name} corrompu&#39;, # ...more error messages in French... } # ...other languages... } The error report is then looked up and formatted as: ERRORS[user_language][&#39;not_csv_suffix&#39;].format(fname=fname) where user_language is a two-letter code for the user’s preferred language. 12.3 Testing Error Handling An alarm isn’t much use if it doesn’t go off when it’s supposed to. Equally, if a function doesn’t raise an exception when it should then errors can easily slip past us. If we want to check that a function called func raises an ExpectedError exception we can use the following template for a unit test: #...set up fixture... try: actual = func(fixture) assert False, &#39;Expected function to raise exception&#39; except ExpectedError as error: pass except Exception as error: assert False, &#39;Function raised the wrong exception&#39; This template has three cases: If the call to func returns a value without throwing an exception then something has gone wrong, so we assert False (which always fails). If func raises the error it’s supposed to then we go into the first except branch without triggering the assert immediately below the function call. The code in this except branch could check that the exception contains the right error message, but in this case it does nothing (which in Python is written pass). Finally, if the function raises the wrong kind of exception we also assert False. Checking this case might seem overly cautious, but if the function raises the wrong kind of exception, users could easily fail to catch it. This pattern is so common that pytest provides support for it. Instead of the eight lines in our original example, we can write: import pytest #...set up fixture... with pytest.raises(ExpectedError): actual = func(fixture) The argument to pytest.raises is the type of exception we expect; the call to the function then goes in the body of the with statement. We will explore pytest.raises further in the exercises. 12.4 Reporting Errors Programs should report things that go wrong; they should also sometimes report things that go right so that people can monitor their progress. Adding print statements is a common approach, but removing them or commenting them out when the code goes into production is tedious and error-prone. A better approach is to use a logging framework, such as Python’s logging library. This lets us leave debugging statements in our code and turn them on or off at will. It also lets us send output to any of several destinations, which is helpful when our data analysis pipeline has several stages and we are trying to figure out which one contains a bug. To understand how logging frameworks work, suppose we want to turn print statements in our collate.py program on or off without editing the program’s source code. We would probably wind up with code like this: if LOG_LEVEL &gt;= 0: print(&#39;Processing files...&#39;) for fname in args.infiles: if LOG_LEVEL &gt;= 1: print(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: if LOG_LEVEL &gt;= 1: print(f&#39;Computing word counts...&#39;) update_counts(reader, word_counts) LOG_LEVEL acts as a threshold: any debugging output at a lower level than its value isn’t printed. As a result, the first log message will always be printed, but the other two only in case the user has requested more details by setting LOG_LEVEL higher than zero. A logging framework combines the if and print statements in a single function call and defines standard names for the logging levels. In order of increasing severity, the usual levels are: DEBUG: very detailed information used for localizing errors. INFO: confirmation that things are working as expected. WARNING: something unexpected happened, but the program will keep going. ERROR: something has gone badly wrong, but the program hasn’t hurt anything. CRITICAL: potential loss of data, security breach, etc. Each of these has a corresponding function: we can use logging.debug, logging.info, etc. to write messages at these levels. By default, only WARNING and above are displayed; messages appear on standard error so that the flow of data in pipes isn’t affected. The logging framework also displays the source of the message, which is called root by default. Thus, if we run the small program shown below, only the warning message appears: import logging logging.warning(&#39;This is a warning.&#39;) logging.info(&#39;This is just for information.&#39;) WARNING:root:This is a warning. Rewriting the collate.py example above using logging yields code that is less cluttered: import logging logging.info(&#39;Processing files...&#39;) for fname in args.infiles: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) We can also configure logging to send messages to a file instead of standard error using logging.basicConfig. (This has to be done before we make any logging calls—it’s not retroactive.) We can also use that function to set the logging level: everything at or above the specified level is displayed. import logging logging.basicConfig(level=logging.DEBUG, filename=&#39;logging.log&#39;) logging.debug(&#39;This is for debugging.&#39;) logging.info(&#39;This is just for information.&#39;) logging.warning(&#39;This is a warning.&#39;) logging.error(&#39;Something went wrong.&#39;) logging.critical(&#39;Something went seriously wrong.&#39;) DEBUG:root:This is for debugging. INFO:root:This is just for information. WARNING:root:This is a warning. ERROR:root:Something went wrong. CRITICAL:root:Something went seriously wrong. By default, basicConfig re-opens the file we specify in append mode; we can use filemode='w' to overwrite the existing log data. Overwriting is useful during debugging, but we should think twice before doing it in production, since the information we throw away often turns out to be exactly what we need to find a bug. Many programs allow users to specify logging levels and log file names as command-line parameters. At its simplest, this is a single flag -v or --verbose that changes the logging level from WARNING (the default) to DEBUG (the noisiest level). There may also be a corresponding flag -q or --quiet that changes the level to ERROR, and a flag -l or --logfile that specifies a log file name. To log messages to a file while also printing them, we can tell logging to use two handlers simultaneously: import logging logging.basicConfig( level=logging.DEBUG, handlers=[ logging.FileHandler(&quot;logging.log&quot;), logging.StreamHandler()]) logging.debug(&#39;This is for debugging.&#39;) The string 'This is for debugging' is both printed to standard error and appended to logging.log. Libraries like logging can send messages to many destinations; in production, we might send them to a centralized logging server that collates logs from many different systems. We might also use rotating files so that the system always has messages from the last few hours but doesn’t fill up the disk. We don’t need any of these when we start, but the data engineers and system administrators who eventually have to install and maintain your programs will be very grateful if we use logging instead of print statements, because it allows them to set things up the way they want with very little work. Logging Configuration Chapter 10 explained why and how to save the configuration that produced a particular result. We clearly also want this information in the log, so we have three options: Write the configuration values into the log one at a time. Save the configuration as a single record in the log (e.g., as a single entry containing JSON). Write the configuration to a separate file and save the filename in the log. Option 1 usually means writing a lot of extra code to reassemble the configuration. Option 2 also often requires us to write extra code (since we need to be able to save and restore configurations as JSON as well as in whatever format we normally use), so on balance we recommend option 3. 12.5 Summary Most programmers spend as much time debugging as they do writing new code, but most courses and textbooks only show working code, and never discuss how to prevent, diagnose, report, and handle errors. Raising our own exceptions instead of using the system’s, writing useful error messages, and logging problems systematically can save us and our users a lot of needless work. 12.6 Exercises This chapter suggested several edits to collate.py. If our script now reads as follows: &quot;&quot;&quot; Combine multiple word count CSV-files into a single cumulative count. &quot;&quot;&quot; import csv import argparse from collections import Counter import logging import utilities as util ERRORS = { &#39;not_csv_suffix&#39; : &#39;{fname}: File must end in .csv&#39;, } def update_counts(reader, word_counts): &quot;&quot;&quot;Update word counts with data from another reader/file.&quot;&quot;&quot; for word, count in csv.reader(reader): word_counts[word] += int(count) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; word_counts = Counter() logging.info(&#39;Processing files...&#39;) for fname in args.infiles: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infiles&#39;, type=str, nargs=&#39;*&#39;, help=&#39;Input file names&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) The following exercises will ask you to make further edits to collate.py. 12.6.1 Set the logging level Define a new command line flag for collate.py called --verbose (or -v) that changes the logging level from WARNING (the default) to DEBUG (the noisiest level). Hint: the following command changes the logging level to DEBUG: logging.basicConfig(level=logging.DEBUG) Once finished, running collate.py with and without the -v flag should produce the following output: $ python bin/collate.py results/dracula.csv results/moby_dick.csv -n 5 the,22559 and,12306 of,10446 to,9192 a,7629 $ python bin/collate.py results/dracula.csv results/moby_dick.csv -n 5 -v INFO:root:Processing files... DEBUG:root:Reading in results/dracula.csv... DEBUG:root:Computing word counts... DEBUG:root:Reading in results/moby_dick.csv... DEBUG:root:Computing word counts... the,22559 and,12306 of,10446 to,9192 a,7629 12.6.2 Send the logging output to file In Exercise 12.6.1, logging information is printed to the screen when the verbose flag is activated. This is problematic if we want to re-direct the output from collate.py to a CSV file, because the logging information will appear in the CSV file as well as the words and their counts. Edit collate.py so that the logging information is sent to a log file called collate.log instead. (HINT: logging.basicConfig has an argument called filename.) Create a new command line option -l or --logfile so that the user can specify a different name for the log file if they don’t like the default name of collate.log. 12.6.3 Handling exceptions Modify the script collate.py so that it catches any exceptions that are raised when it tries to open files and records them in the log file. When you are finished, the program should collate all the files it can rather than halting as soon as it encounters a problem. Modify your first solution to handle nonexistent files and permission problems separately. 12.6.4 Testing error handling In our suggested solution to the previous exercise we modified collate.py to handle different types of errors associated with reading input files. If the main function in collate.py now reads: def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; log_lev = logging.DEBUG if args.verbose else logging.WARNING logging.basicConfig(level=log_lev, filename=args.logfile) word_counts = Counter() logging.info(&#39;Processing files...&#39;) for fname in args.infiles: try: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format( fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) except FileNotFoundError: msg = f&#39;{fname} not processed: File does not exist&#39; logging.warning(msg) except PermissionError: msg = f&#39;{fname} not processed: No permission to read&#39; logging.warning(msg) except Exception as error: msg = f&#39;{fname} not processed: {error}&#39; logging.warning(msg) util.collection_to_csv(word_counts, num=args.num) It is difficult to write a simple unit test for the lines of code dedicated to reading input files, because main is a long function that requires command line arguments as input. Edit collate.py so that the six lines of code responsible for processing an input file appear in their own function that reads as follows (i.e. once you are done, main should call process_file in place of the existing code): def process_file(fname, word_counts): &quot;&quot;&quot;Read file and update word counts&quot;&quot;&quot; logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format( fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) Add a unit test to test_zipfs.py that uses pytest.raises to check that the new collate.process_file function raises an OSError if the input file does not end in .csv. Run pytest to check that the new test passes. Add a unit test to test_zipfs.py that uses pytest.raises to check that the new collate.process_file function raises a FileNotFoundError if the input file does not exist. Run pytest to check that the new test passes. Use the coverage library (Section 11.7) to check that the relevant commands in process_file (specifically raise OSError and open(fname, 'r')) were indeed tested. 12.6.5 Error catalogs In Section 12.2 we started to define an error catalog called ERRORS. Read Appendix F.1 and explain why we have used capital letters for the name of the catalog. Python has three ways to format strings: the % operator, the str.format method, and f-strings (where the “f” stands for “format”). Look up the documentation for each and explain why we have to use str.format rather than f-strings for formatting error messages in our catalog/lookup table. There’s a good chance we will eventually want to use the error messages we’ve defined in other scripts besides collate.py. To avoid duplication, move ERRORS to the utilities module that was first created in Section 5.8. 12.6.6 Tracebacks Run the following code: try: 1/0 except Exception as e: help(e.__traceback__) What kind of object is e.__traceback__? What useful information can you get from it? 12.7 Key Points Signal errors by raising exceptions. Use try/except blocks to catch and handle exceptions. Python organizes its standard exceptions in a hierarchy so that programs can catch and handle them selectively. “Throw low, catch high”, i.e., raise exceptions immediately but handle them at a higher level. Write error messages that help users figure out what to do to fix the problem. Store error messages in a lookup table to ensure consistency. Use a logging framework instead of print statements to report program activity. Separate logging messages into DEBUG, INFO, WARNING, ERROR, and CRITICAL levels. Use logging.basicConfig to define basic logging parameters. "],["provenance.html", "Chapter 13 Tracking Provenance 13.1 Data Provenance 13.2 Code Provenance 13.3 Summary 13.4 Exercises 13.5 Key Points", " Chapter 13 Tracking Provenance The most important problem is that we are trying to understand the fundamental workings of the universe via a language devised for telling one another when the best fruit is. — Terry Pratchett We have now developed, automated, and tested a workflow for plotting the word count distribution for classic novels. In the normal course of events, we would include the outputs from that workflow (e.g., our figures and \\(\\alpha\\) values) in a research paper or a report for a consulting client. But modern publishing involves much more than producing a PDF and making it available on a preprint server such as arXiv or bioRxiv. It also entails providing the data underpinning the report and the code used to do the analysis: An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures. — Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in Buckheit and Donoho (1995) While some reports, datasets, software packages, and analysis scripts can’t be published without violating personal or commercial confidentiality, every researcher’s default should be to make all these as widely available as possible. Publishing it under an open license (Section 8.4) is the first step; the sections below describe what else we can do to capture the provenance of our data analysis. Our Zipf’s Law project files are structured as they were at the end of the previous chapter: zipf/ ├── .gitignore ├── .travis.yml ├── CONDUCT.md ├── CONTRIBUTING.md ├── LICENSE.md ├── Makefile ├── README.md ├── requirements.txt ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── plotparams.yml │ ├── script_template.py │ ├── test_zipfs.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ └── ... ├── results │ ├── dracula.csv │ ├── dracula.png │ └── ... └── test_data ├── random_words.txt └── risk.txt Identifying Reports and Authors Before publishing anything, we need to understand how authors and their works are identified. A Digital Object Identifier (DOI) is a unique identifier for a particular version of a particular digital artifact such as a report, a dataset, or a piece of software. DOIs are written as doi:prefix/suffix, but are often also represented as URLs like http://dx.doi.org/prefix/suffix. In order to be allowed to issue a DOI, an academic journal, data archive, or other organization must guarantee a certain level of security, longevity and access. An ORCID is an Open Researcher and Contributor ID. Anyone can get an ORCID for free, and should include it in publications because people’s names and affiliations change over time. 13.1 Data Provenance The first step in documenting the data associated with a report is to determine what (if anything) needs to be published. If the report involved the analysis of a publicly available dataset that is maintained and documented by a third party, it’s not necessary to publish a duplicate of the dataset: the report simply needs to document where to access the data and what version was analyzed. This is the case for our Zipf’s Law analysis, since the texts we analyze are available at Project Gutenberg. It’s not strictly necessary to publish intermediate data produced during the analysis of a publicly available dataset either (e.g., the CSV files produced by countwords.py), so long as readers have access to the original data and the code/software used to process it. However, making intermediate data available can save people time and effort, particularly if it takes a lot of computing power to reproduce it (or if installing the required software is complicated). For example, NASA has published the Goddard Institute for Space Studies Surface Temperature Analysis, which estimates the global average surface temperature based on thousands of land and ocean weather observations, because a simple metric of global warming is expensive to produce and is useful in many research contexts. If a report involves a new dataset, such as observations collected during a field experiment, then that data needs to be published in its raw (unprocessed) form. The publication of a dataset, whether raw or intermediate, should follow the FAIR Principles. 13.1.1 The FAIR Principles The FAIR Principles describe what research data should look like. They are still aspirational for most researchers, but tell us what to aim for (Goodman et al. 2014; Michener 2015; Hart et al. 2016; Brock 2019; Tierney and Ram 2020). The most immediately important elements of the FAIR Principles are outlined below. 13.1.1.1 Data should be findable. The first step in using or re-using data is to find it. We can tell we’ve done this if: (Meta)data is assigned a globally unique and persistent identifier (i.e. a DOI). Data is described with rich metadata Metadata clearly and explicitly includes the identifier of the data it describes. (Meta)data is registered or indexed in a searchable resource, such as the data sharing platforms described in Section 13.1.2. 13.1.1.2 Data should be accessible. People can’t use data if they don’t have access to it. In practice, this rule means the data should be openly accessible (the preferred solution) or that authenticating in order to view or download it should be free. We can tell we’ve done this if: (Meta)data is retrievable by its identifier using a standard communications protocol like HTTP. Metadata is accessible even when the data is no longer available. 13.1.1.3 Data should be interoperable. Data usually needs to be integrated with other data, which means that tools need to be able to process it. We can tell we’ve done this if: (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation. (Meta)data uses vocabularies that follow FAIR principles. (Meta)data includes qualified references to other (meta)data. 13.1.1.4 Data should be reusable. This is the ultimate purpose of the FAIR Principles and much other work. We can tell we’ve done this if: Meta(data) is described with accurate and relevant attributes. (Meta)data is released with a clear and accessible data usage license. (Meta)data has detailed provenance. (Meta)data meets domain-relevant community standards. 13.1.2 Where to archive data Small datasets (i.e., anything under 500 MB) can be stored in version control. If the data is being used in several projects, it may make sense to create one repository to hold only the data; the R community refers to these as data packages, and they are often accompanied by small scripts to clean up and query the data. For medium-sized datasets (between 500 MB and 5 GB), it’s better to put the data on platforms like the Open Science Framework, Dryad, and Figshare, which will give the dataset a DOI. Big datasets (i.e., anything more than 5 GB) may not be ours in the first place, and probably need the attention of a professional archivist. Data Journals While archiving data at a site like Dryad or Figshare (following the FAIR Principles) is usually the end of the data publishing process, there is the option of publishing a journal paper to describe the dataset in detail. Some research disciplines have journals devoted to describing particular types of data (e.g., the Geoscience Data Journal) and there are also generic data journals (e.g., Scientific Data). 13.2 Code Provenance Our Zipf’s Law analysis represents a typical data science project in that we’ve written some code that leverages other pre-existing software packages in order to produce the key results of a report. To make a computational workflow like this open, transparent and reproducible we must archive three key items: A copy of any analysis scripts or notebooks used to produce the key results presented in the report. A detailed description of the software environment in which those analysis scripts or notebooks ran. A description of the data processing steps taken in producing each key result, i.e., a step-by-step account of which scripts were executed in what order for each key result. Unfortunately, librarians, publishers, and regulatory bodies are still trying to determine the best way to document and archive material like this, so a widely accepted set of FAIR Principles for research software are still under development (Lamprecht et al. 2020). In the meantime, the best advice we can give is presented below. It involves adding information about the software environment and data processing steps to a GitHub repository that contains the analysis scripts/notebooks, before creating a new release of that repository and archiving it (with a DOI) with Zenodo. 13.2.1 Software environment In order to document the software packages used in our analysis, we should archive a list of the names and version numbers of each software package. We can get version information for the Python packages we are using by running: $ pip freeze alabaster==0.7.12 anaconda-client==1.7.2 anaconda-navigator==1.9.12 anaconda-project==0.8.3 appnope==0.1.0 appscript==1.0.1 asn1crypto==1.0.1 ... Other command line tools will often have an option like --version or --status to access the version information. Archiving a list of package names and version numbers would mean that our software environment is technically reproducible, but it would be left up to the reader of the report to figure out how to get all those packages installed and working together. This might be fine for a small number of packages with very few dependencies, but in more complex cases we probably want to make life easier for the reader (and for our future selves looking to re-run the analysis). One way to make things easier is to export a description of a complete conda environment (Section 14.2; Appendix I.2), which can be saved as YAML using: $ conda env export &gt; environment.yml $ cat environment.yml name: base channels: - conda-forge - defaults dependencies: - _ipyw_jlab_nb_ext_conf=0.1.0=py37_0 - alabaster=0.7.12=py37_0 - anaconda=2019.10=py37_0 - anaconda-client=1.7.2=py37_0 - anaconda-navigator=1.9.12=py37_0 - anaconda-project=0.8.3=py_0 - appnope=0.1.0=py37_0 - appscript=1.1.0=py37h1de35cc_0 - asn1crypto=1.0.1=py37_0 ... That software environment can be recreated on another computer with one line of code: $ conda env create -f environment.yml Since this environment.yml file is an important part of reproducing the analysis, remember to add it to your GitHub repository. Container Images More complex tools like Docker can install our entire environment (down to the precise version of the operating system) on a different computer (Nüst et al. 2020). However, their complexity can be daunting, and there is a lot of debate about how well (or whether) they actually make research more reproducible in practice. 13.2.2 Data processing steps The second item that needs to be added to our GitHub repository is a description of the data processing steps involved in each key result. Assuming the author list on our report is Amira Khan and Sami Virtanen (Section 0.2), we could add a new Markdown file called KhanVirtanen2020.md to the repository to describe the steps: The code in this repository was used in generating the results for the following paper: Khan A and Virtanen S (2020). Zipf&#39;s Law in classic english texts. *Journal of Important Research*, 27, 134-139. The code was executed in the software environment described by `environment.yml`. It can be installed using [conda](https://docs.conda.io/en/latest/): $ conda env create -f environment.yml Figure 1 in the paper was created by running the following at the command line: $ make all We should also add this information as an appendix to the report itself. 13.2.3 Analysis scripts Later in this book we will package and release our Zipf’s Law code so that it can be downloaded and installed by the wider research community, just like any other Python package (Chapter 14). Doing this is especially helpful if other people might be interested in using and/or extending it, but often the scripts and notebooks we write to produce a particular figure or table are too case-specific to be of broad interest. To fully capture the provenance of the results presented in a report, these analysis scripts and/or notebooks (along with the details of the associated software environment and data processing steps) can simply be archived with a repository like Figshare or Zenodo, which specialize in storing the long tail of research projects (i.e., supplementary figures, data, and code). Uploading a zip file of analysis scripts to the repository is a valid option, but more recently the process has been streamlined via direct integration between GitHub and Zenodo. As described in this tutorial, the process involves creating a new release of our repository in GitHub that Zenodo copies and then issues a DOI for (Figure 13.1). Figure 13.1: A new code release on GitHub. 13.2.4 Reproducibility versus inspectability In most cases, documenting our software environment, analysis scripts, and data processing steps will ensure that our computational analysis is reproducible/repeatable at the time our report is published. But what about five or ten years later? As we have discussed, data analysis workflows usually depend on a hierarchy of packages. Our Zipf’s Law analysis depends on a collection of Python libraries, which in turn depend on the Python language itself. Some workflows also depend critically on a particular operating system. Over time some of these dependencies will inevitably be updated or no longer supported, meaning our workflow will be documented but not reproducible. Fortunately, most readers are not looking to exactly re-run a decade old analysis: they just want to be able to figure out what was run and what the important decisions were, which is sometimes referred to as inspectability (Gil et al. 2016; Brown 2017). While exact repeatability has a short shelf-life, inspectability is the enduring legacy of a well-documented computational analysis. Your Future Self Will Thank You Data and code provenance is often promoted for the good of people trying to reproduce your work, who were not part of creating the work in the first place. Prioritizing their needs can be difficult: how can we justify spending time for other people when our current projects need work for the good of the people working on them right now? Instead of thinking about people who are unknown and unrelated, we can think about newcomers to our team and the time we will save ourselves in onboarding them. We can also think about the time we will save ourselves when we come back to this project five months or five years from now. Documentation that serves these two groups well will almost certainly serve the needs of strangers as well. 13.3 Summary The Internet started a revolution in scientific publishing that shows no sign of ending. Where an inter-library loan once took weeks to arrive and data had to be transcribed from published papers (if it could be found at all), we can now download one another’s work in minutes: if we can find it and make sense of it. Organizations like Our Research are building tools to help with both; by using DOIs and ORCIDs, publishing on preprint servers, following the FAIR Principles, and documenting our workflow, we help ensure that everyone can pursue their ideas as we did. 13.4 Exercises 13.4.1 ORCID If you don’t already have an ORCID, go to the website and register now. If you do have an ORCID, log in and make sure that your details and publication record are up-to-date. 13.4.2 A FAIR test An online questionnaire for measuring the extent to which datasets are FAIR has been created by the Australian Research Data Commons. Fill in the questionnaire for a dataset you have published or that you use often. 13.4.3 Evaluate a project’s data provenance This exercise is modified from Wickes and Stein (2016) and explores the dataset from Meili (2016). Go to the dataset’s page http://doi.org/10.3886/E17507V2 and download the files. You will need to make an ICPSER account and agree to their data agreement before you can download. Review the dataset’s main page to get a sense of the study, then review the spreadsheet file and the coded response file. Who are the participants of this study? What types of data was collected and used for analysis? Can you find information on the demographics of the interviewees? This dataset is clearly in support of an article. What information can you find about it, and can you find a link to it? 13.4.4 Evaluate a project’s code provenance The GitHub repository borstlab/reversephi_paper provides the code and data for the paper Leonhardt et al. (2017). Browse the repository and answer the following questions: Where is the software environment described? What files would you need to recreate the software environment? Where are the data processing steps described? How could you re-create the results included in the manuscript? How are the scripts and data archived? i.e. Where can you download the version of the code and data as it was when the manuscript was published? To get a feel for the different approaches to code provenance, repeat steps 1-3 with the following: The figshare page that accompanies the paper Irving, Wijffels, and Church (2019). The GitHub repo blab/h3n2-reassortment that accompanies the paper Potter et al. (2019). 13.4.5 Making permanent links The link to the UK Home Office’s accessibility guideline posters might change in future. Use the Wayback Machine to find a link that is more likely to be usable in the long run. 13.4.6 Create an archive of your Zipf’s analysis A slightly less permanent alternative to having a DOI for your analysis code is to provide a link to a GitHub release. Follow the instructions on GitHub to create a release for the current state of your zipf/ project. Once you’ve created the release, read about how to link to it. What is the URL that allows direct download of the zip archive of your release? What about getting a DOI? Creating a GitHub release is also a necessary step to get a DOI through the Zenodo/GitHub integration (Section 13.2.3). We are stopping short of getting the DOI here, since nobody reading this book needs to formally cite or archive the example Zipf’s Law software we’ve been developing. Also, if every reader of the book generated a DOI, we’d have many DOIs pointing to the same code! 13.4.7 Publishing your code Think about a project that you’re currently working on. How would you go about publishing the code associated with that project (i.e., the software description, analysis scripts, and data processing steps)? 13.5 Key Points Publish data and code as well as papers. Use DOIs to identify reports, datasets, or software release. Use an ORCID to identify yourself as an author of a report, dataset, or software release. Data should be FAIR: findable, accessible, interoperable, and reusable. Put small datasets in version control repositories; store large ones on data sharing sites. Describe your software environment, analysis scripts, and data processing steps in reproducible ways. Make your analyses inspectable as well as reproducible. "],["packaging.html", "Chapter 14 Creating Packages with Python 14.1 Creating a Python Package 14.2 Virtual Environments 14.3 Installing a Development Package 14.4 What Installation Does 14.5 Distributing Packages 14.6 Documenting Packages 14.7 Software Journals 14.8 Summary 14.9 Exercises 14.10 Key Points", " Chapter 14 Creating Packages with Python Another response of the wizards, when faced with a new and unique situation, was to look through their libraries to see if it had ever happened before. This was…a good survival trait. It meant that in times of danger you spent the day sitting very quietly in a building with very thick walls. — Terry Pratchett The more software we write, the more we come to think of programming languages as a way to build and combine libraries. Every widely-used language now has an online repository from which people can download and install those libraries. This lesson shows you how to use Python’s tools to create and share libraries of your own. We will continue with our Zipf’s Law project, which should include the following files: zipf/ ├── .gitignore ├── .travis.yml ├── CONDUCT.md ├── CONTRIBUTING.md ├── KhanVirtanen2020.md ├── LICENSE.md ├── Makefile ├── README.md ├── environment.yml ├── requirements.txt ├── bin │ ├── book_summary.sh │ ├── collate.py │ ├── countwords.py │ ├── plotcounts.py │ ├── plotparams.yml │ ├── script_template.py │ ├── test_zipfs.py │ └── utilities.py ├── data │ ├── README.md │ ├── dracula.txt │ └── ... ├── results │ ├── dracula.csv │ ├── dracula.png │ └── ... └── test_data ├── random_words.txt └── risk.txt 14.1 Creating a Python Package A package consists of one or more Python source files in a specific directory structure combined with installation instructions for the computer. Python packages can come from various sources: some are distributed with Python itself as part of the language’s standard library, but anyone can create one, and there are thousands that can be downloaded and installed from online repositories. Terminology People sometimes refer to packages as modules. Strictly speaking, a module is a single source file, while a package is a directory structure that contains one or more modules. A generic package folder hierarchy looks like this: pkg_name ├── pkg_name │ ├── module1.py │ └── module2.py ├── README.md └── setup.py The top-level directory is named after the package. It contains a directory that is also named after the package, and that contains the package’s source files. It is initially a little confusing to have two directories with the same name, but most Python projects follow this convention because it makes it easier to set up the project for installation. __init__.py Python packages often contain a file with a special name: __init__.py (two underscores before and after init). Just as importing a module file executes the code in the module, importing a package executes the code in __init__.py. Packages had to have this file before Python 3.3, even if it was empty, but since Python 3.3 it is only needed if we want to run some code as the package is being imported. If we want to make our Zipf’s Law software available as a Python package, we need to follow the generic folder hierarchy. A quick search of the Python Package Index (PyPI) reveals that the package name zipf is already taken, so we will need to use something different. Let’s use pyzipf and update our directory names accordingly: $ mv ~/zipf ~/pyzipf $ cd ~/pyzipf $ mv bin pyzipf Updating GitHub’s Repository Name We won’t do it in this case (because it would break links/references from earlier in the book), but now that we’ve decided to name our package is pyzipf, we would normally update the name of our GitHub repository to match. After changing the name at the GitHub website, we would need to update our git remote so that our local repository could still be synchronized with GitHub: $ git remote set-url origin https://github.com/amira-khan/pyzipf.git Python has several ways to build an installable package. We will show how to use setuptools, which is the lowest common denominator and will allow everyone, regardless of what Python distribution they have, to use our package. To use setuptools, we must create a file called setup.py in the directory above the root directory of the package. (This is why we require the two-level directory structure described earlier.) setup.py must have exactly that name, and must contain lines like these: from setuptools import setup setup( name=&#39;pyzipf&#39;, version=&#39;0.1.0&#39;, author=&#39;Amira Khan&#39;, packages=[&#39;pyzipf&#39;]) The name and author parameters are self-explanatory. Most software projects use semantic versioning for software releases. A version number consists of three integers X.Y.Z, where X is the major version, Y is the minor version, and Z is the patch version. Major version zero (0.Y.Z) is for initial development, so we have started with 0.1.0. The first stable public release would be version 1.0.0, and in general, the version number is incremented as follows: Increment major every time there’s an incompatible externally-visible change Increment minor when adding new functionality in a backwards-compatible manner (i.e. without breaking any existing code) Increment patch for backwards-compatible bug fixes that don’t add any new features Finally, we specify the name of the directory containing the code to be packaged with the packages parameter. This is straightforward in our case because we only have a single package directory. For more complex projects, the find_packages function from setuptools can automatically find all packages by recursively searching the current directory. 14.2 Virtual Environments We can add additional information to our package later, but this is enough to be able to build it for testing purposes. Before we do that, though, we should create a virtual environment to test how our package installs without breaking anything in our main Python installation. We exported details of our environment in Chapter 13 as a way to document the software we’re using; in this section, we’ll use environments to make the software we’re creating more robust. A virtual environment is a layer on top of an existing Python installation. Whenever Python needs to find a package, it looks in the virtual environment before checking the main Python installation. This gives us a place to install packages that only some projects need without affecting other projects. Virtual environments also help with package development: We want to be able to easily test install and uninstall our package, without affecting the entire Python environment. We want to answer problems people have with our package with something more helpful than “I don’t know, it works for me”. By installing and running our package in a completely empty environment, we can ensure that we’re not accidentally relying on other packages being installed. We can manage virtual environments using conda (Appendix I). To create a new virtual environment called pyzipf we run conda create, specifying the environment’s name with the -n or --name flag and including pip and our current version of python in the new environment: $ conda create -n pyzipf pip python=3.7.6 Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /Users/amira/anaconda3/envs/pyzipf added / updated specs: - pip - python=3.7.6 The following packages will be downloaded: ...list of packages... The following NEW packages will be INSTALLED: ...list of packages... Proceed ([y]/n)? y ... Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate pyzipf # # To deactivate an active environment, use # # $ conda deactivate conda creates the directory /Users/amira/anaconda3/envs/pyzipf, which contains the subdirectories needed for a minimal Python installation, such as bin and lib. It also creates /Users/amira/anaconda3/envs/pyzipf/bin/python, which checks for packages in these directories before checking the main installation. conda Variations As with many of the other tools we’ve explored in this book, the behavior of some conda commands differ depending on operating system. There are multiple ways to accomplish some of the tasks we present in this chapter. The options we present here represent the approaches most likely to work across multiple platforms. Additionally, the path for Anaconda differs among operating systems. Our examples show the default path for Anaconda installed via the Unix shell on MacOS (/Users/amira/anaconda3), but for the MacOS graphical installer it is /Users/amira/opt/anaconda3, for Linux it is /home/amira/anaconda3 and on Windows it is C:\\Users\\amira\\Anaconda3. During the installation process users can also choose a custom location if they like (Section 1.3). We can switch to the pyzipf environment by running: $ conda activate pyzipf Once we have done this, the python command runs the interpreter in pyzipf/bin: (pyzipf)$ which python /Users/amira/anaconda3/envs/pyzipf/bin/python Notice that every shell command displays (pyzipf) when that virtual environment is active. Between Git branches and virtual environments, it can be very easy to lose track of what exactly we are working on and with. Prompts like this can make it a little less confusing; using virtual environment names that match the names of your projects (and branches, if you’re testing different environments on different branches) quickly becomes essential. We can now install packages safely. Everything we install will go into the pyzipf virtual environment without affecting the underlying Python installation. When we are done, we can switch back to the default environment using conda deactivate: (pyzipf)$ conda deactivate $ which python /usr/bin/python 14.3 Installing a Development Package Let’s install our package in this virtual environment. First we re-activate it: $ conda activate pyzipf Next, we go into the upper pyzipf directory that contains our setup.py file and install our package: (pyzipf)$ cd ~/pyzipf (pyzipf)$ pip install -e . Obtaining file:///Users/amira/pyzipf Installing collected packages: pyzipf Running setup.py develop for pyzipf Successfully installed pyzipf The -e option indicates that we want to install the package in “editable” mode, which means that any changes we make in the package code are directly available to use without having to reinstall the package; the . means “install from the current directory”. If we look in the location containing package installations (e.g., /Users/amira/anaconda3/envs/pyzipf/lib/python3.7/site-packages/), we can see the pyzipf package beside all the other locally-installed packages. If we try to use the package at this stage, though, Python will complain that some of the packages it depends on, such as pandas, are not installed. We could install these manually, but it is more reliable to automate this process by listing everything that our package depends on using the install_requires parameter in setup.py: from setuptools import setup setup( name=&#39;pyzipf&#39;, version=&#39;0.1&#39;, author=&#39;Amira Khan&#39;, packages=[&#39;pyzipf&#39;], install_requires=[ &#39;matplotlib&#39;, &#39;pandas&#39;, &#39;scipy&#39;, &#39;pyyaml&#39;, &#39;pytest&#39;]) We don’t have to list numpy explicitly because it will be installed as a dependency for pandas and scipy. Versioning Dependencies It is good practice to specify the versions of our dependencies and even better to specify version ranges. For example, if we have only tested our package on pandas version 1.0.1, we could put pandas==1.1.2 or pandas&gt;=1.1.2 instead of just pandas in the list argument passed to the install_requires parameter. Next, we can install our package using the modified setup.py file: (pyzipf)$ cd ~/pyzipf (pyzipf)$ pip install -e . Obtaining file:///Users/amira/pyzipf Collecting matplotlib Downloading matplotlib-3.3.3-cp37-cp37m-macosx_10_9_x86_64.whl |████████████████████████████████| 8.5 MB 3.1 MB/s Collecting cycler&gt;=0.10 Using cached cycler-0.10.0-py2.py3-none-any.whl Collecting kiwisolver&gt;=1.0.1 Downloading kiwisolver-1.3.1-cp37-cp37m-macosx_10_9_x86_64.whl |████████████████████████████████| 61 kB 2.0 MB/s Collecting numpy&gt;=1.15 Downloading numpy-1.19.4-cp37-cp37m-macosx_10_9_x86_64.whl |████████████████████████████████| 15.3 MB 8.9 MB/s Collecting pillow&gt;=6.2.0 Downloading Pillow-8.0.1-cp37-cp37m-macosx_10_10_x86_64.whl |████████████████████████████████| 2.2 MB 6.3 MB/s Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.3 Using cached pyparsing-2.4.7-py2.py3-none-any.whl Collecting python-dateutil&gt;=2.1 Using cached python_dateutil-2.8.1-py2.py3-none-any.whl Collecting six Using cached six-1.15.0-py2.py3-none-any.whl Collecting pandas Downloading pandas-1.1.5-cp37-cp37m-macosx_10_9_x86_64.whl |████████████████████████████████| 10.0 MB 1.4 MB/s Collecting pytz&gt;=2017.2 Using cached pytz-2020.4-py2.py3-none-any.whl Collecting pytest Using cached pytest-6.2.1-py3-none-any.whl Collecting attrs&gt;=19.2.0 Using cached attrs-20.3.0-py2.py3-none-any.whl Collecting importlib-metadata&gt;=0.12 Downloading importlib_metadata-3.3.0-py3-none-any.whl Collecting pluggy&lt;1.0.0a1,&gt;=0.12 Using cached pluggy-0.13.1-py2.py3-none-any.whl Collecting py&gt;=1.8.2 Using cached py-1.10.0-py2.py3-none-any.whl Collecting typing-extensions&gt;=3.6.4 Downloading typing_extensions-3.7.4.3-py3-none-any.whl Collecting zipp&gt;=0.5 Downloading zipp-3.4.0-py3-none-any.whl Collecting iniconfig Using cached iniconfig-1.1.1-py2.py3-none-any.whl Collecting packaging Using cached packaging-20.8-py2.py3-none-any.whl Collecting pyyaml Using cached PyYAML-5.3.1.tar.gz Collecting scipy Downloading scipy-1.5.4-cp37-cp37m-macosx_10_9_x86_64.whl |████████████████████████████████| 28.7 MB 10.7 MB/s Collecting toml Using cached toml-0.10.2-py2.py3-none-any.whl Building wheels for collected packages: pyyaml Building wheel for pyyaml (setup.py) ... done Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-macosx_10_9_x86_64.whl size=44626 sha256=5a59ccf08237931e7946ec6b526922e4 f0c8ee903d43671f50289431d8ee689d Stored in directory: /Users/amira/Library/Caches/pip/wheels/5e/ 03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653 Successfully built pyyaml Installing collected packages: zipp, typing-extensions, six, pyparsing, importlib-metadata, toml, pytz, python-dateutil, py, pluggy, pillow, packaging, numpy, kiwisolver, iniconfig, cycler, attrs, scipy, pyyaml, pytest, pandas, matplotlib, pyzipf Attempting uninstall: pyzipf Found existing installation: pyzipf 0.1.0 Uninstalling pyzipf-0.1.0: Successfully uninstalled pyzipf-0.1.0 Running setup.py develop for pyzipf Successfully installed attrs-20.3.0 cycler-0.10.0 importlib-metadata-3.3.0 iniconfig-1.1.1 kiwisolver-1.3.1 matplotlib-3.3.3 numpy-1.19.4 packaging-20.8 pandas-1.1.5 pillow-8.0.1 pluggy-0.13.1 py-1.10.0 pyparsing-2.4.7 pytest-6.2.1 python-dateutil-2.8.1 pytz-2020.4 pyyaml-5.3.1 pyzipf scipy-1.5.4 six-1.15.0 toml-0.10.2 typing-extensions-3.7.4.3 zipp-3.4.0 (The precise output of this command will change depending on which versions of our dependencies get installed.) We can now import our package in a script or a Jupyter notebook just as we would any other package. For example, to use the function in utilities, we would write: from pyzipf import utilities as util util.collection_to_csv(...) To allow our functions to continue accessing utilities.py, we need to change that line in both countwords.py and collate.py. However, the useful command-line scripts that we used to count and plot word counts are no longer accessible directly from the Unix shell. Fortunately there is an alternative to changing the function import as described above. The setuptools package allows us to install programs along with the package. These programs are placed beside those of other packages. We tell setuptools to do this by defining entry points in setup.py: from setuptools import setup setup( name=&#39;pyzipf&#39;, version=&#39;0.1&#39;, author=&#39;Amira Khan&#39;, packages=[&#39;pyzipf&#39;], install_requires=[ &#39;matplotlib&#39;, &#39;pandas&#39;, &#39;scipy&#39;, &#39;pyyaml&#39;, &#39;pytest&#39;], entry_points={ &#39;console_scripts&#39;: [ &#39;countwords = pyzipf.countwords:main&#39;, &#39;collate = pyzipf.collate:main&#39;, &#39;plotcounts = pyzipf.plotcounts:main&#39;]}) The right side of the = operator is the location of a function, written as package.module:function; the left side is the name we want to use to call this function from the command line. In this case, we want to call each module’s main function, which as it stands requires an input argument args containing the command-line arguments given by the user (Section 5.2). For example, the relevant section of our countwords.py program is: def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; word_counts = count_words(args.infile) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Input file name&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() main(args) We can’t pass any arguments to main when we define entry points in our setup.py file, so we need to change our script slightly: def parse_command_line(): &quot;&quot;&quot;Parse the command line for input arguments.&quot;&quot;&quot; parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Input file name&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) args = parser.parse_args() return args def main(): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; args = parse_command_line() word_counts = count_words(args.infile) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: main() The new parse_command_line function handles the command line arguments, so that main() no longer requires any input arguments. Once we have made the corresponding change in collate.py and plotcounts.py, we can re-install our package: (pyzipf)$ pip install -e . Defaulting to user installation because normal site-packages is not writeable Obtaining file:///Users/amira/pyzipf Requirement already satisfied: matplotlib in /usr/lib/python3.7/site-packages (from pyzipf==0.1) (3.2.1) Requirement already satisfied: pandas in /Users/amira/.local/lib/python3.7/site-packages (from pyzipf==0.1) (1.0.3) Requirement already satisfied: scipy in /usr/lib/python3.7/site-packages (from pyzipf==0.1) (1.4.1) Requirement already satisfied: pyyaml in /usr/lib/python3.7/site-packages (from pyzipf==0.1) (5.3.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/lib/python3.7/site-packages (from matplotlib-&gt;pyzipf==0.1) (0.10.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/lib/python3.7/site-packages (from matplotlib-&gt;pyzipf==0.1) (1.1.0) Requirement already satisfied: numpy&gt;=1.11 in /usr/lib/python3.7/site-packages (from matplotlib-&gt;pyzipf==0.1) (1.18.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2, !=2.1.6,&gt;=2.0.1 in /usr/lib/python3.7/site-packages (from matplotlib-&gt;pyzipf==0.1) (2.4.6) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/lib/python3.7/site-packages (from matplotlib-&gt;pyzipf==0.1) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/lib/python3.7/site-packages (from pandas-&gt;pyzipf==0.1) (2019.3) Requirement already satisfied: six in /usr/lib/python3.7/site-packages (from cycler&gt;=0.10-&gt;matplotlib-&gt;pyzipf==0.1) (1.14.0) Requirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;pyzipf==0.1) (46.1.3) Installing collected packages: pyzipf Running setup.py develop for pyzipf Successfully installed pyzipf The output looks slightly different than the first run because pip could re-use some packages saved locally by the previous install rather than re-fetching them from online repositories. (If we hadn’t used the -e option to make the package immediately editable, we would have to uninstall it before reinstalling it during development.) We can now use our commands directly from the Unix shell without writing the full path to the file and without prefixing it with python. (pyzipf)$ countwords data/dracula.txt -n 5 the,8036 and,5896 i,4712 to,4540 of,3738 Tracking pyzipf.egg-info? Using setuptools automatically creates a new folder in your project directory named pyzipf.egg-info. This folder is another example of information generated by a script that is also included in the repository, so it should be included in the .gitignore file to avoid tracking with Git. 14.4 What Installation Does Now that we have created and installed a Python package, let’s explore what actually happens during installation. The short version is that the contents of the package are copied into a directory that Python will search when it imports things. In theory we can “install” packages by manually copying source code into the right places, but it’s much more efficient and safer to use a tool specifically made for this purpose, such as conda or pip. Most of the time, these tools copy packages into the Python installation’s site-packages directory, but this is not the only place Python searches. Just as the PATH environment in the shell contains a list of directories that the shell searches for programs it can execute (Section 4.6), the Python variable sys.path contains a list of the directories it searches (Section 11.2). We can look at this list inside the interpreter: import sys sys.path [&#39;&#39;, &#39;/Users/amira/anaconda3/envs/pyzipf/lib/python37.zip&#39;, &#39;/Users/amira/anaconda3/envs/pyzipf/lib/python3.7&#39;, &#39;/Users/amira/anaconda3/envs/pyzipf/lib/python3.7/lib-dynload&#39;, &#39;/Users/amira/.local/lib/python3.7/site-packages&#39;, &#39;/Users/amira/anaconda3/envs/pyzipf/lib/python3.7/site-packages&#39;, &#39;/Users/amira/pyzipf&#39;] The empty string at the start of the list means “the current directory”. The rest are system paths for our Python installation, and will vary from computer to computer. 14.5 Distributing Packages Look But Don’t Execute In this section we upload the pyzipf package to TestPyPI and PyPI: https://test.pypi.org/project/pyzipf https://pypi.org/project/pyzipf/ You won’t be able to execute the twine upload commands below exactly as shown (because Amira has already uploaded the pyzipf package), but the general sequence of commands in this section is an excellent resource to refer back to when you are uploading your own packages. If you want to try uploading your own pyzipf package via twine, you could edit the project name to include your name (e.g. pyzipf-yourname) and use the TestPyPI repository for the upload. An installable package is most useful if we distribute it so that anyone who wants it can run pip install pyzipf and get it. To make this possible, we need to use setuptools to create a source distribution (known as an sdist in Python packaging jargon): (pyzipf)$ python setup.py sdist running sdist running egg_info creating pyzipf.egg-info writing pyzipf.egg-info/PKG-INFO writing dependency_links to pyzipf.egg-info/dependency_links.txt writing entry points to pyzipf.egg-info/entry_points.txt writing requirements to pyzipf.egg-info/requires.txt writing top-level names to pyzipf.egg-info/top_level.txt writing manifest file &#39;pyzipf.egg-info/SOURCES.txt&#39; package init file &#39;pyzipf/__init__.py&#39; not found (or not a regular file) reading manifest file &#39;pyzipf.egg-info/SOURCES.txt&#39; writing manifest file &#39;pyzipf.egg-info/SOURCES.txt&#39; running check warning: check: missing required meta-data: url warning: check: missing meta-data: if &#39;author&#39; supplied, &#39;author_email&#39; must be supplied too creating pyzipf-0.1 creating pyzipf-0.1/pyzipf creating pyzipf-0.1/pyzipf.egg-info copying files to pyzipf-0.1... copying README.md -&gt; pyzipf-0.1 copying setup.py -&gt; pyzipf-0.1 copying pyzipf/collate.py -&gt; pyzipf-0.1/pyzipf copying pyzipf/countwords.py -&gt; pyzipf-0.1/pyzipf copying pyzipf/plotcounts.py -&gt; pyzipf-0.1/pyzipf copying pyzipf/script_template.py -&gt; pyzipf-0.1/pyzipf copying pyzipf/test_zipfs.py -&gt; pyzipf-0.1/pyzipf copying pyzipf/utilities.py -&gt; pyzipf-0.1/pyzipf copying pyzipf.egg-info/PKG-INFO -&gt; pyzipf-0.1/pyzipf.egg-info copying pyzipf.egg-info/SOURCES.txt -&gt; pyzipf-0.1/pyzipf.egg-info copying pyzipf.egg-info/dependency_links.txt -&gt; pyzipf-0.1/pyzipf.egg-info copying pyzipf.egg-info/entry_points.txt -&gt; pyzipf-0.1/pyzipf.egg-info copying pyzipf.egg-info/requires.txt -&gt; pyzipf-0.1/pyzipf.egg-info copying pyzipf.egg-info/top_level.txt -&gt; pyzipf-0.1/pyzipf.egg-info Writing pyzipf-0.1/setup.cfg creating dist Creating tar archive removing &#39;pyzipf-0.1&#39; (and everything under it) This creates a file named pyzipf-0.1.tar.gz, located in a new directory in our project, dist/ (another directory to add to .gitignore). These distribution files can now be distributed via PyPI, the standard repository for Python packages. Before doing that, though, we can put pyzipf on TestPyPI, which lets us test the distribution of our package without having things appear in the main PyPI repository. We must have an account, but they are free to create. The preferred tool for uploading packages to PyPI is called twine, which we can install with: (pyzipf)$ pip install twine Following the Python Packaging User Guide, and then upload our distributions from the dist/ folder using the --repository option to specify the TestPyPI repository: $ twine upload --repository testpypi dist/* Uploading distributions to https://test.pypi.org/legacy/ Enter your username: amira-khan Enter your password: ********* Uploading pyzipf-0.1.0.tar.gz 100%|█████████████████| 5.59k/5.59k [00:01&lt;00:00, 3.27kB/s] View at: https://test.pypi.org/project/pyzipf/0.1/ Figure 14.1: Our new project on TestPyPI. and view the results at the new test project webpage (Figure 14.1). In the exercises, we will explore additional metadata that can be added to setup.py so that it appears on the project webpage. We have now uploaded both types of distribution, allowing people to use the wheel distribution if their system supports it or the source distribution if it does not. We can test that this has worked by creating a virtual environment and installing our package from TestPyPI and PyPI (since not all of our dependencies are available on TestPyPI): (pyzipf)$ conda create -n pyzipf-test pip python=3.7.6 (pyzipf)$ conda activate pyzipf-test (pyzipf-test)$ pip install --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple pyzipf Looking in indexes: https://test.pypi.org/simple, https://pypi.org/simple Collecting pyzipf Downloading pyzipf-0.1.tar.gz (5.5 kB) Collecting matplotlib Using cached matplotlib-3.3.3-cp37-cp37m-macosx_10_9_x86_64.whl ...collecting other packages... Building wheels for collected packages: pyzipf Building wheel for pyzipf (setup.py) ... done Created wheel for pyzipf: filename=pyzipf-0.1-py3-none-any.whl size=6836 sha256=62a23715379b71ad5a6b124444fab194 596d094c7df293c4019d33bdd648aff1 Stored in directory: /Users/amira/Library/Caches/pip/wheels/c6/ d6/08/f16cf80ec82a9c70ab8a5d9c8acc7ab35c9a01009539aeb2be Successfully built pyzipf Installing collected packages: zipp, typing-extensions, six, pyparsing, importlib-metadata, toml, pytz, python-dateutil, py, pluggy, pillow, packaging, numpy, kiwisolver, iniconfig, cycler, attrs, scipy, pyyaml, pytest, pandas, matplotlib, pyzipf Successfully installed attrs-20.3.0 cycler-0.10.0 importlib-metadata-3.3.0 iniconfig-1.1.1 kiwisolver-1.3.1 matplotlib-3.3.3 numpy-1.19.4 packaging-20.8 pandas-1.1.5 pillow-8.0.1 pluggy-0.13.1 py-1.10.0 pyparsing-2.4.7 pytest-6.2.1 python-dateutil-2.8.1 pytz-2020.4 pyyaml-5.3.1 pyzipf-0.1 scipy-1.5.4 six-1.15.0 toml-0.10.2 typing-extensions-3.7.4.3 zipp-3.4.0 Once again, pip takes advantage of the fact that some packages already existing on our system (e.g., they are cached from our previous installs) and doesn’t download them again. Once we are happy with our package at TestPyPI, we can go through the same process to put it on the main PyPI repository. conda Installation Packages Given the widespread use of conda for package management, it can be a good idea to post a conda installation package to Anaconda Cloud. The conda documentation has instructions for quickly building a conda package for a Python module that is already available on PyPI. See Appendix I for more information about conda and Anaconda Cloud. 14.6 Documenting Packages Now that our package has been distributed, we need to think about whether we have provided sufficient documentation. Docstrings (Section 5.3) and READMEs are sufficient to describe most simple packages, but as our code base grows larger we will want to complement these manually written sections with automatically generated content, references between functions, and search functionality. For most large Python packages, such documentation is generated using a documentation generator called Sphinx, which is often used in combination with a free online hosting service called Read The Docs. In this section we will update our README file with some basic package-level documentation, before using Sphinx and Read The Docs to host that information online along with more detailed function-level documentation. For further advice on writing documentation for larger and more complex packages, see Appendix G. 14.6.1 Including package-level documentation in the README When a user first encounters a package, they usually want to know what the package is meant to do, instructions on how to install it, and examples of how to use it. We can include these elements in the README.md file we started in Chapter 7. At the moment it reads as follows: $ cat README.md # Zipf&#39;s Law These Zipf&#39;s Law scripts tally the occurrences of words in text files and plot each word&#39;s rank versus its frequency. ... This file is currently written in Markdown, but Sphinx uses a format called reStructuredText (reST), so we will switch to that. Like Markdown, reST is a plain-text markup format that can be rendered into HTML or PDF documents with complex indices and cross-links. GitHub recognizes files ending in .rst as reST files and displays them nicely, so our first task is to rename our existing file: $ git mv README.md README.rst We then make a few edits to the file formatting: titles are underlined and overlined, section headings are underlined, and code blocks are set off with two colons (::) and indented. We can also add some context about why to use the package, as well as updated information about package installation: The ``pyzipf`` package tallies the occurrences of words in text files and plots each word&#39;s rank versus its frequency together with a line for the theoretical distribution for Zipf&#39;s Law. Motivation ---------- Zipf&#39;s Law is often stated as an observational pattern seen in the relationship between the frequency and rank of words in a text: `&quot;…the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.&quot;` — `wikipedia &lt;https://en.wikipedia.org/wiki/Zipf%27s_law&gt;`_ Many books are available to download in plain text format from sites such as `Project Gutenberg &lt;https://www.gutenberg.org/&gt;`_, so we created this package to qualitatively explore how well different books align with the word frequencies predicted by Zipf&#39;s Law. Installation ------------ ``pip install pyzipf`` Usage ----- After installing this package, the following three commands will be available from the command line - ``countwords`` for counting the occurrences of words in a text. - ``collate`` for collating multiple word count files together. - ``plotcounts`` for visualizing the word counts. A typical usage scenario would include running the following from your terminal:: countwords dracula.txt &gt; dracula.csv countwords moby_dick.txt &gt; moby_dick.csv collate dracula.csv moby_dick.csv &gt; collated.csv plotcounts collated.csv --outfile zipf-drac-moby.jpg Additional information on each function can be found in their docstrings and appending the ``-h`` flag, e.g. ``countwords -h``. Contributing ------------ Interested in contributing? Check out the CONTRIBUTING.md file for guidelines on how to contribute. Please note that this project is released with a Contributor Code of Conduct (CONDUCT.md). By contributing to this project, you agree to abide by its terms. Both of these files can be found in our `GitHub repository. &lt;https://github.com/amira-khan/zipf&gt;`_ 14.6.2 Creating a web page for documentation Now that we’ve added package-level documentation to our README file, we need to think about function-level documentation. We want to provide users with a list of all the functions available in our package along with a short description of what they do and how to use them. We could achieve this by manually cutting and pasting function names and docstrings from our Python modules (i.e. countwords.py, plotcounts.py, etc), but that would be a time consuming process prone to errors as more functions are added over time. Instead, we can use a documentation generator called Sphinx that is capable of scanning Python code for function names and docstrings and can export that information to HTML format for hosting on the web. To start, let’s install Sphinx and create a docs/ directory at the top of our repository: $ pip install sphinx $ mkdir docs $ cd docs We can then run Sphinx’s quickstart tool to create a minimal set of documentation that includes the package-level information in the README.rst file we just created and the function-level information in the docstrings we’ve written along the way. It asks us to specify the project’s name, the name of the project’s author, and a release; we can use the default settings for everything else. $ sphinx-quickstart Welcome to the Sphinx 3.1.1 quickstart utility. Please enter values for the following settings (just press Enter to accept a default value, if one is given in brackets). Selected root path: . You have two options for placing the build directory for Sphinx output. Either, you use a directory &quot;_build&quot; within the root path, or you separate &quot;source&quot; and &quot;build&quot; directories within the root path. &gt; Separate source and build directories (y/n) [n]: n The project name will occur in several places in the built documentation. &gt; Project name: pyzipf &gt; Author name(s): Amira Khan &gt; Project release []: 0.1 If the documents are to be written in a language other than English, you can select a language here by its language code. Sphinx will then translate text that it generates into that language. For a list of supported codes, see https://www.sphinx-doc.org/en/master/usage/configuration.html &gt; Project language [en]: Creating file /Users/amira/pyzipf/docs/conf.py. Creating file /Users/amira/pyzipf/docs/index.rst. Creating file /Users/amira/pyzipf/docs/Makefile. Creating file /Users/amira/pyzipf/docs/make.bat. Finished: An initial directory structure has been created. You should now populate your master file /Users/amira/pyzipf/docs/index.rst and create other documentation source files. Use the Makefile to build the docs, like so: make builder where &quot;builder&quot; is one of the supported builders, e.g. HTML, LaTeX or linkcheck. quickstart creates a file called conf.py in the docs directory that configures Sphinx. We will make two changes to that file so that another tool called autodoc can find our modules (and their docstrings). The first change relates to the “path setup” section near the head of the file: # -- Path setup ------------------------------------------------- # If extensions (or modules to document with autodoc) are in # another directory, add these directories to sys.path here. If the # directory is relative to the documentation root, use # os.path.abspath to make it absolute, like shown here. Relative to the docs/ directory, our modules (i.e. countwords.py, utilities.py, etc) are located in the ../pyzipf directory. We therefore need to uncomment the relevant lines of the path setup section in conf.py to tell Sphinx where those modules are: import os import sys sys.path.insert(0, os.path.abspath(&#39;../pyzipf&#39;)) We will also change the “general configuration” section to add autodoc to the list of Sphinx extensions we want: extensions = [&#39;sphinx.ext.autodoc&#39;] With those edits complete, we can now generate a Sphinx autodoc script that generates information about each of our modules and puts it in corresponding .rst files in the docs/source directory: sphinx-apidoc -o source/ ../pyzipf Creating file source/collate.rst. Creating file source/countwords.rst. Creating file source/plotcounts.rst. Creating file source/test_zipfs.rst. Creating file source/utilities.rst. Creating file source/modules.rst. At this point, we are ready to generate our webpage. The docs sub-directory contains a Makefile that was generated by sphinx-quickstart. If we run make html and open docs/_build/index.html in a web browser we’ll have a website landing page with minimal documentation (Figure 14.2). If we click on the Module Index link we can access the documentation for the individual modules (Figures 14.3 and 14.4). Figure 14.2: The default website landing page. Figure 14.3: The module index. Figure 14.4: The countwords documentation. The landing page for the website is the perfect place for the content of our README file, so we can add the line .. include:: ../README.rst to the docs/index.rst file to insert it: Welcome to pyzipf&#39;s documentation! ================================== .. include:: ../README.rst .. toctree:: :maxdepth: 2 :caption: Contents: Indices and tables ================== * :ref:`genindex` * :ref:`modindex` * :ref:`search` If we re-run make html, we now get an updated set of web pages that re-uses our README as the introduction to the documentation (Figure 14.5). Figure 14.5: The new landing page showing the contents of README.rst. Before going on, note that Sphinx is not included in the installation requirements in requirements.txt (Section 11.8). Sphinx isn’t needed to run, develop, or even test our package, but it is needed for building the documentation. To note this requirement, but without requiring everyone installing the package to install Sphinx, let’s create a requirements_docs.txt file that contains this line (where the version number is found by running pip freeze): Sphinx&gt;=1.7.4 Anyone wanting to build the documentation (including us, on another computer) now only needs run pip install -r requirements_docs.txt 14.6.3 Hosting documentation online Now that we have generated our package documentation, we need to host it online. A common option for Python projects is Read The Docs, which is a community-supported site that hosts software documentation free of charge. Just as continuous integration systems automatically re-test things (Section 11.8), Read The Docs integrates with GitHub so that documentation is automatically re-built every time updates are pushed to the project’s GitHub repository. If we register for Read The Docs with our GitHub account, we can login at the Read The Docs website and import a project from our GitHub repository. Read The Docs will then build the documentation (using make html just as we did earlier) and host the resulting files. For this to work, all of the source files generated by Sphinx need to be checked into your GitHub repository: in our case, this means docs/source/*.rst, docs/Makefile, docs/conf.py, and docs/index.rst. We also need to create and save a Read the Docs configuration file in the root directory of our pyzipf package: $ cd ~/pyzipf $ cat .readthedocs.yml # .readthedocs.yml # Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html # for details # Required version: 2 # Build documentation in the docs/ directory with Sphinx sphinx: configuration: docs/conf.py # Optionally set the version of Python and requirements required # to build your docs python: version: 3.7 install: - requirements: requirements.txt The configuration file uses the now-familiar YAML format (Section 10.1 and Appendix H) to specify the location of the Sphinx configuration script (docs/conf.py) and the dependencies for our package (requirements.txt). Amira has gone through this process and the documentation is now available at: https://pyzipf.readthedocs.io/ 14.7 Software Journals As a final step to releasing our new package, we might want to give it a DOI so that it can be cited by researchers. As we saw in Section 13.2.3, GitHub integrates with Zenodo for precisely this purpose. While creating a DOI using a site like Zenodo is often the end of the software publishing process, there is the option of publishing a journal paper to describe the software in detail. Some research disciplines have journals devoted to describing particular types of software (e.g., Geoscientific Model Development), and there are also a number of generic software journals such as the Journal of Open Research Software and the Journal of Open Source Software. Packages submitted to these journals are typically assessed against a range of criteria relating to how easy the software is to install and how well it is documented, so the peer review process can be a great way to get critical feedback from people who have seen many research software packages come and go over the years. Once you have obtained a DOI and possibly published with a software journal, the last step is to tell users how to cite your new software package. This is traditionally done by adding a CITATION file to the associated GitHub repository (alongside README, LICENSE, CONDUCT and similar files discussed in Section 1.1.1), containing a plain text citation that can be copied and pasted into email as well as entries formatted for various bibliographic systems like BibTeX. $ cat CITATION.md # Citation If you use the pyzipf package for work/research presented in a publication, we ask that you please cite: Khan, A., and Virtanen, S., 2020. pyzipf: A Python package for word count analysis. *Journal of Important Software*, 5(51), 2317, https://doi.org/10.21105/jois.02317 ### BibTeX entry @article{Khan2020, title={pyzipf: A Python package for word count analysis.}, author={Khan, Amira and Virtanen, Sami}, journal={Journal of Important Software}, volume={5}, number={51}, eid={2317}, year={2020}, doi={10.21105/jois.02317}, url={https://doi.org/10.21105/jois.02317}, } 14.8 Summary Thousands of people have helped write the software that our Zipf’s Law example relies on, but their work is only useful because they packaged it and documented how to use it. Doing this is increasingly recognized as a credit-worthy activity by universities, government labs, and other organizations, particularly for research software engineers. It is also deeply satisfying to make strangers’ lives better, if only in small ways. 14.9 Exercises 14.9.1 Package metadata In a number of places on our TestPyPI webpage, it says that no project description was provided (Figure 14.1). How could we edit our setup.py file to include a description? What other metadata would you add? Hint: The setup() args documentation might be useful. 14.9.2 Separating requirements As well as requirements_docs.txt, developers often create a requirements_dev.txt file to list packages that are not needed by the package’s users, but are required for its development and testing. Pull pytest out of requirements.txt and put it in a new requirements_dev.txt file. 14.9.3 Software review The Journal of Open Source Software has a checklist that reviewers must follow when assessing a submitted software paper. Run through the checklist (skipping the criteria related to the software paper) and see how the Zipf’s Law package would rate on each criteria. 14.9.4 Packaging quotations Each chapter in this book opens with a quote from the British author Terry Pratchett. This script quotes.py contains a function random_quote which prints a random Pratchett quote: import random quote_list = [&quot;It&#39;s still magic even if you know how it&#39;s done.&quot;, &quot;Everything starts somewhere, &quot;\\ &quot;though many physicists disagree.&quot;, &quot;Ninety percent of most magic merely consists &quot;\\ &quot;of knowing one extra fact.&quot;, &quot;Wisdom comes from experience. &quot;\\ &quot;Experience is often a result of lack of wisdom.&quot;, &quot;There isn&#39;t a way things should be. &quot;\\ &quot;There&#39;s just what happens, and what we do.&quot;, &quot;Multiple exclamation marks are a sure sign &quot;\\ &quot;of a diseased mind.&quot;, &quot;+++ Divide By Cucumber Error. &quot;\\ &quot;Please Reinstall Universe And Reboot +++&quot;, &quot;It&#39;s got three keyboards and a hundred extra &quot;\\ &quot;knobs, including twelve with ‘?&#39; on them.&quot;, ] def random_quote(): &quot;&quot;&quot;Print a random Pratchett quote.&quot;&quot;&quot; print(random.choice(quote_list)) Create a new conda development environment called pratchett and use pip to install a new package called pratchett into that environment. The package should contain quotes.py, and once the package has been installed the user should be able to run: from pratchett import quotes quotes.random_quote() 14.10 Key Points Use setuptools to build and distribute Python packages. Create a directory named mypackage containing a setup.py script as well as a subdirectory also called mypackage containing the package’s source files. Use semantic versioning for software releases. Use a virtual environment to test how your package installs without disrupting your main Python installation. Use pip to install Python packages. The default respository for Python packages is PyPI. Use TestPyPI to test the distribution of your package. Use a README file for package-level documentation. Use Sphinx to generate documentation for a package. Use Read The Docs to host package documentation online. Create a DOI for your package using GitHub’s Zenodo integration. Publish the details of your package in a software journal so that others can cite it. "],["finale.html", "Chapter 15 Finale 15.1 Why We Wrote This Book", " Chapter 15 Finale We have come a long way since we first met Amira, Jun, and Sami in Section 0.2. Amira now has her scripts, data sets and reports organized, in version control, and on GitHub. Her work has already paid off: a colleague spotted a problem in an analysis step, that impacted five figures in one of her reports. Because Amira has embraced GitHub, the colleague could easily suggest a fix through a pull request. It took Amira some time to regenerate all of the affected figures, because she had to recall which code needed to be rerun. She recognized this as a great reason to embrace Make, and has added implementing it to her to-do list. She used to be intimidated by the Unix shell, but now Amira finds it an essential part of her everyday work: she uses shell scripts to automate data processing tasks, she runs her own command line tools she wrote in Python, and issues countless commands to Git. Her comfort with the shell is also helping as she learns how to run her analyses on a remote computing cluster from Sami. Sami had experience with Git and Make in software projects from their undergraduate studies, but they have a new appreciation for their importance in research projects. They’ve reworked their Git and Make workshops to use examples of data pipelines, and have been getting rave reviews from participants. Sami has also gained an appreciation for the importance of provenance of both data and code in research. They’ve been helping their users by suggesting ways they can make their research more accessible, inspectable and reproducible. Jun is now working on his first Python package. He’s added better error handling so he and his users get better information when something goes wrong, he’s implemented some testing strategies to give him and his users confidence his code works, and he’s improved his documentation. Jun has also added a license, a Code of Conduct and contributing guidelines to his project repo, and has already had a contribution that fixes some typos in the documentation. 15.1 Why We Wrote This Book Shell scripts, branching, automated workflows, healthy team dynamics—they all take time to learn, but they enable researchers to get more done in less time and with less pain, and that’s why we wrote this book. The climate crisis is the greatest threat our species has faced since civilization began. The COVID-19 pandemic has shown just how ill-prepared we are to deal with problems of that magnitude, or with the suspicion and disinformation that now poisons every public discussion online. Every hour that a researcher doesn’t waste wrestling with software is an hour they can spend solving a new problem; every meeting that ends early with a clear decision frees up time to explain to the public what we actually know and why it matters. We don’t expect this book to change the world, but we hope that knowing how to write, test, document, package, and share your work will give you a slightly better chance of doing so. We hope you have enjoyed reading what we have written; if so, we would enjoy hearing from you. Damien Irving (https://damienirving.github.io/) Kate Hertweck (https://katehertweck.com/) Luke Johnston (https://lukewjohnston.com/) Joel Ostblom (https://joelostblom.com/) Charlotte Wickham (https://www.cwick.co.nz/) Greg Wilson (https://third-bit.com) So much universe, and so little time. — Terry Pratchett "],["solutions.html", "A Solutions Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapter 7 Chapter 8 Chapter 9 Chapter 10 Chapter 11 Chapter 12 Chapter 13 Chapter 14", " A Solutions The exercises included in this book represent a wide variety of problems, from multiple choice questions to larger coding tasks. It’s relatively straightforward to indicate a correct answer for the former, though there may be unanticipated cases in which the specific software version you’re using leads to alternative answers being preferable. It’s even more difficult to identify the “right” answer for the latter, since there are often many ways to accomplish the same task with code. Here we present possible solutions that the authors generally agree represent “good” code, but we encourage you to explore additional approaches. Commits noted in a solution reference Amira’s zipf repository on GitHub, which allow you to see the specific lines of a file modified to arrive at the answer. Chapter 2 Exercise 2.10.1 The -l option makes ls use a long listing format, showing not only the file/directory names but also additional information such as the file size and the time of its last modification. If you use both the -h option and the -l option, this makes the file size “human readable”, i.e. displaying something like 5.3K instead of 5369. Exercise 2.10.2 The command ls -R -t results in the contents of each directory sorted by time of last change. Exercise 2.10.3 No: . stands for the current directory. No: / stands for the root directory. No: Amira’s home directory is /Users/Amira. No: This goes up two levels, i.e. ends in /Users. Yes: ~ stands for the user’s home directory, in this case /Users/amira. No: This would navigate into a directory home in the current directory if it exists. Yes: Starting from the home directory ~, this command goes into data then back (using ..) to the home directory. Yes: Shortcut to go back to the user’s home directory. Yes: Goes up one level. Yes: Same as the previous answer, but with an unnecessary . (indicating current directory) Exercise 2.10.4 No: There is a directory backup in /Users. No: This is the content of Users/sami/backup, but with .. we asked for one level further up. No: Same as previous explanation, but results shown as directories (which is what the -F option specifies). Yes: ../backup/ refers to /Users/backup/. Exercise 2.10.5 No: pwd is not the name of a directory. Yes: ls without directory argument lists files and directories in the current directory. Yes: Uses the absolute path explicitly. Exercise 2.10.6 The touch command updates a file’s timestamp. If no file exists with the given name, touch will create one. Assuming you don’t already have my_file.txt in your working directory, touch my_file.txt will create the file. When you inspect the file with ls -l, note that the size of my_file.txt is 0 bytes. In other words, it contains no data. If you open my_file.txt using your text editor it is blank. Some programs do not generate output files themselves, but instead require that empty files have already been generated. When the program is run, it searches for an existing file to populate with its output. The touch command allows you to efficiently generate a blank text file to be used by such programs. Exercise 2.10.7 $ remove my_file.txt? y The -i option will prompt before (every) removal (use y to confirm deletion or n to keep the file). The Unix shell doesn’t have a trash bin, so all the files removed will disappear forever. By using the -i option, we have the chance to check that we are deleting only the files that we want to remove. Exercise 2.10.8 $ mv ../data/chapter1.txt ../data/chapter2.txt . Recall that .. refers to the parent directory (i.e. one above the current directory) and that . refers to the current directory. Exercise 2.10.9 No: While this would create a file with the correct name, the incorrectly named file still exists in the directory and would need to be deleted. Yes: This would work to rename the file. No: The period (.) indicates where to move the file, but does not provide a new file name; identical file names cannot be created. No: The period (.) indicates where to copy the file, but does not provide a new file name; identical file names cannot be created. Exercise 2.10.10 We start in the /Users/amira/data directory, containing a single file, books.dat. We create a new folder called doc, move (mv) the file books.dat to the new folder (doc). Then we make a copy (cp) of the file we just moved named books-saved.dat. The tricky part here is the location of the copied file. Recall that .. means “go up a level”, so the copied file is now in /Users/amira. Notice that .. is interpreted with respect to the current working directory, not with respect to the location of the file being copied. So, the only thing that will show using ls (in /Users/amira/data) is the doc folder. No: books-saved.dat is located at /Users/amira Yes. No: books.dat is located at /Users/amira/data/doc No: books-saved.dat is located at /Users/amira Exercise 2.10.11 If given more than one file name followed by a directory name (i.e. the destination directory must be the last argument), cp copies the files to the named directory. If given three file names, cp throws an error because it is expecting a directory name as the last argument. Exercise 2.10.12 Yes: Shows all files whose names contain two different characters (?) followed by the letter n, then zero or more characters (*) followed by txt. No: Shows all files whose names start with zero or more characters (*) followed by e_, zero or more characters (*), then txt. The output includes the two desired books, but also time_machine.txt. No: Shows all files whose names start with zero or more characters (*) followed by n, zero or more characters (*), then txt. The output includes the two desired books, but also frankenstein.txt and time_machine.txt. No: Shows all files whose names start with zero or more characters (*) followed by n, a single character ?, e, zero or more characters (*), then txt. The output shows frankenstein.txt and sense_and_sensibility.txt. Exercise 2.10.13 $ mv *.txt data Amira needs to move her files books.txt and titles.txt to the data directory. The shell will expand *.txt to match all .txt files in the current directory. The mv command then moves the list of .txt files to the data directory. Exercise 2.10.14 Yes: This accurately recreates the directory structure. Yes: This accurately recreates the directory structure. No: The first line of this code set gives an error: mkdir: 2016-05-20/data: No such file or directory mkdir won’t create a subdirectory for a directory that doesn’t yet exist (unless you use an option like -p that explicitly creates parent directories). No: This creates raw and processed directories at the same level as data: 2016-05-20/ ├── data ├── processed └── raw Exercise 2.10.15 A solution using two wildcard expressions: $ ls s*.txt $ ls t*.txt When there are no files beginning with s and ending in .txt, or when there are no files beginning with t and ending in .txt, Exercise 2.10.16 No: This would remove only .csv files with one-character names. Yes: This removes only files ending in .csv. No: The shell would expand * to match everything in the current directory, so the command would try to remove all matched files and an additional file called .csv. No: The shell would expand *.* to match all files with any extension, so this command would delete all files in the current directory. Exercise 2.10.17 novel-????-[ab]*.{txt,pdf} matches: Files whose names start with novel-, which is then followed by exactly four characters (since each ? matches one character), followed by another literal -, followed by either the letter a or the letter b, followed by zero or more other characters (the *), followed by .txt or .pdf. Chapter 3 Exercise 3.8.1 echo hello &gt; testfile01.txt writes the string “hello” to testfile01.txt, but the file gets overwritten each time we run the command. echo hello &gt;&gt; testfile02.txt writes “hello” to testfile02.txt, but appends the string to the file if it already exists (i.e. when we run it for the second time). Exercise 3.8.2 No: This results from only running the first line of code (head). No: This results from only running the second line of code (tail). Yes: The first line writes the first three lines of dracula.txt, the second line appends the last two lines of dracula.txt to the same file. No: We would need to pipe the commands to obtain this answer (head -n 3 dracula.txt | tail -n 2 &gt; extracted.txt). Exercise 3.8.3 Try running each line of code in the data directory. No: This incorrectly uses redirect (&gt;), and will result in an error. No: The number of lines desired for head is reported incorrectly; this will result in an error. No: This will extract the first three files from the wc results, which have not yet been sorted into length of lines. Yes: This output correctly orders and connects each of the commands. Exercise 3.8.4 To obtain a list of unique results from these data, we need to run: $ sort genres.txt | uniq It makes sense that uniq is almost always run after using sort, because that allows a computer to compare only adjacent lines. If uniq did not compare only adjacent lines, it would require comparing each line to all other lines, For a small set of comparisons, this doesn’t matter much, but this isn’t always possible for large files. Exercise 3.8.5 When used on a single file, cat prints the contents of that file to the screen. In this case, the contents of titles.txt are sent as input to head -n 5, so the first five lines of titles.txt is output. These five lines are used as the input for tail -n 3, which results in lines 3-5 as output. This is used as input to the final command, which sorts them in reverse order. These results are written to the file final.txt, the contents of which are: Sense and Sensibility,1811 Moby Dick,1851 Jane Eyre,1847 Exercise 3.8.6 cut selects substrings from a line by: breaking the string into pieces wherever it finds a separator (-d ,), which in this case is a comma, and keeping one or more of the resulting fields/columns (-f 2). In this case, the output is only the dates from titles.txt, since this is in the second column. $ cut -d , -f 2 titles.txt 1897 1818 1847 1851 1811 1892 1897 1895 1847 Exercise 3.8.7 No: This sorts by the book title. No: This results in an error because sort is being used incorrectly. No: There are duplicate dates in the output because they have not been sorted first. Yes: This results in the output shown below. No: This extracts the desired data (below), but then counts the number of lines, resulting in the incorrect answer. 1 1811 1 1818 2 1847 1 1851 1 1892 1 1895 2 1897 If you have difficulty understanding the answers above, try running the commands or sub-sections of the pipelines (e.g., the code between pipes). Exercise 3.8.8 The difference between the versions is whether the code after echo is inside quotation marks. Version 2 will allow us to preview the commands. This prints to screen everything enclosed in the quotation marks, expanding the loop variable name (prefixed with $). The first version redirects the output from echo analyze $file to a file (analyzed-$file). This doesn’t allow us to preview the commands, but instead creates files (analyzed-$file), containing the text analyze $file. Try both versions for yourself to see the output. Be sure to open the analyzed-* files to view their contents. Exercise 3.8.9 Version 1 gives the same output on each iteration through the loop. Bash expands the wildcard *.txt to match all files ending in .txt and then lists them using ls. The expanded loop would look like this (we’ll only show the first two data files): $ for datafile in dracula.txt frankenstein.txt ... &gt; do &gt; ls dracula.txt frankenstein.txt ... dracula.txt frankenstein.txt ... dracula.txt frankenstein.txt ... ... Version 2 lists a different file on each loop iteration. The value of the datafile variable is evaluated using $datafile, and then listed using ls. dracula.txt frankenstein.txt jane_eyre.txt moby_dick.txt sense_and_sensibility.txt sherlock_holmes.txt time_machine.txt Exercise 3.8.10 Version 1 results in only dracula.txt output, because it is the only file beginning in “d”. Version 2 results in the following, because these files all contain a “d” with zero or more characters before and after: README.md dracula.txt moby_dick.txt sense_and_sensibility.txt Exercise 3.8.11 Both versions write the first 16 lines (head -n 16) of each book to a file (headers.txt). Version 1 results in the text from each file being overwritten in each iteration because of use of &gt; as a redirect. Version 2 uses &gt;&gt;, which appends the lines to the existing file. This is preferable because the final headers.txt includes the first 16 lines from all files. Exercise 3.8.12 If a command causes something to crash or hang, it might be useful to know what that command was, in order to investigate the problem. Were the command only be recorded after running it, we would not have a record of the last command run in the event of a crash. Chapter 4 Exercise 4.8.1 $ cd ~/zipf Change into the zipf directory, which is located in the home directory (designated by ~). $ for file in $(find . -name &quot;*.bak&quot;) &gt; do &gt; rm $file &gt; done Find all the files ending in .bak and remove them one by one. $ rm bin/summarize_all_books.sh Remove the summarize_all_books.sh script. $ rm -r results Recursively remove each file in the results directory and then remove the directory itself. (It is necessary to remove all the files first because you cannot remove a non-empty directory.) Exercise 4.8.2 Running this script with the given parameters will print the first and last line from each file in the directory ending in .txt. No: This answer misinterprets the lines printed. Yes. No: This answer includes the wrong files. No: Leaving off the quotation marks would result in an error. Exercise 4.8.3 One possible script (longest.sh) to accomplish this task: # Shell script which takes two arguments: # 1. a directory name # 2. a file extension # and prints the name of the file in that directory # with the most lines which matches the file extension. # # Usage: bash longest.sh directory/ txt wc -l $1/*.$2 | sort -n | tail -n 2 | head -n 1 Exercise 4.8.4 script1.sh will print the names of all files in the directory on a single line, e.g. README.md dracula.txt frankenstein.txt jane_eyre.txt moby_dick.txt script1.sh sense_and_sensibility.txt sherlock_holmes.txt time_machine.txt. Although *.txt is included when running the script, the commands run by the script do not reference $1. script2.sh will print the contents of the first three files ending in .txt; the three variables ($1, $2, $3) refer to the first, second, and third argument entered after the script, respectively. script3.sh will print the name of each file ending in .txt, since $@ refers to all the arguments (e.g. filenames) given to a shell script. The list of files would be followed by .txt: dracula.txt frankenstein.txt jane_eyre.txt moby_dick.txt sense_and_sensibility.txt sherlock_holmes.txt time_machine.txt.txt Exercise 4.8.5 No: This command extracts any line containing “he”, either as a word or within a word. No: This results in the same output as the answer for #1. -E allows the search term to represent an extended regular expression, but the search term is simple enough that it doesn’t make a difference in the result. Yes: -w means to return only matches for the word “he”. No: -i means to invert the search result; this would return all lines except the one we desire. Exercise 4.8.6 # Obtain unique years from multiple comma-delimited # lists of titles and publication years # # Usage: bash year.sh file1.txt file2.txt ... for filename in $* do cut -d , -f 2 $filename | sort -n | uniq done Exercise 4.8.7 One possible solution: for sister in Elinor Marianne do echo $sister: grep -o -w $sister sense_and_sensibility.txt | wc -l done The -o option prints only the matching part of a line. An alternative (but possibly less accurate) solution is: for sister in Elinor Marianne do echo $sister: grep -o -c -w $sister sense_and_sensibility.txt done This solution is potentially less accurate because grep -c only reports the number of lines matched. The total number of matches reported by this method will be lower if there is more than one match per line. Exercise 4.8.8 Yes: This returns data/jane_eyre.txt. Maybe: This option may work on your computer, but may not behave consistently across all shells because expansion of the wildcard (*e.txt) may prevent piping from working correctly. We recommend enclosing *e.txt in quotation marks, as in answer 1. No: This searches the contents of files for lines matching “machine”, rather than the file names. See above. Exercise 4.8.9 Find all files with a .dat extension recursively from the current directory. Count the number of lines each of these files contains. Sort the output from step 2 numerically. Exercise 4.8.10 The following command works if your working directory is Desktop/ and you replace “username” with that of your current computer. -mtime needs to be negative because it is referencing a day prior to the current date. $ find . -type f -mtime -1 -user username Chapter 5 Exercise 5.11.1 Running a Python statement directly from the command line is useful as a basic calculator and for simple string operations, since these commands occur in one line of code. More complicated commands will require multiple statements; when run using python -c, statements must be separated by semi-colons: $ python -c &quot;import math; print(math.log(123))&quot; Multiple statements, therefore, quickly become more troublesome to run in this manner. Exercise 5.11.2 The my_ls.py script could read as follows: &quot;&quot;&quot;List the files in a given directory with a given suffix.&quot;&quot;&quot; import argparse import glob def main(args): &quot;&quot;&quot;Run the program.&quot;&quot;&quot; dir = args.dir if args.dir[-1] == &#39;/&#39; else args.dir + &#39;/&#39; glob_input = dir + &#39;*.&#39; + args.suffix glob_output = sorted(glob.glob(glob_input)) for item in glob_output: print(item) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;dir&#39;, type=str, help=&#39;Directory&#39;) parser.add_argument(&#39;suffix&#39;, type=str, help=&#39;File suffix (e.g. py, sh)&#39;) args = parser.parse_args() main(args) Exercise 5.11.3 The sentence_endings.py script could read as follows: &quot;&quot;&quot;Count the occurrence of different sentence endings.&quot;&quot;&quot; import argparse def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; text = args.infile.read() for ending in [&#39;.&#39;, &#39;?&#39;, &#39;!&#39;]: count = text.count(ending) print(f&#39;Number of {ending} is {count}&#39;) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Input file name&#39;) args = parser.parse_args() main(args) Exercise 5.11.4 While there may be other ways for plotcounts.py to meet the requirements of the exercise, we’ll be using this script in subsequent chapters so we recommend that the script reads as follows: &quot;&quot;&quot;Plot word counts.&quot;&quot;&quot; import argparse import pandas as pd def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; df = pd.read_csv(args.infile, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) df[&#39;rank&#39;] = df[&#39;word_frequency&#39;].rank(ascending=False, method=&#39;max&#39;) df[&#39;inverse_rank&#39;] = 1 / df[&#39;rank&#39;] ax = df.plot.scatter(x=&#39;word_frequency&#39;, y=&#39;inverse_rank&#39;, figsize=[12, 6], grid=True, xlim=args.xlim) ax.figure.savefig(args.outfile) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infile&#39;, type=argparse.FileType(&#39;r&#39;), nargs=&#39;?&#39;, default=&#39;-&#39;, help=&#39;Word count csv file name&#39;) parser.add_argument(&#39;--outfile&#39;, type=str, default=&#39;plotcounts.png&#39;, help=&#39;Output image file name&#39;) parser.add_argument(&#39;--xlim&#39;, type=float, nargs=2, metavar=(&#39;XMIN&#39;, &#39;XMAX&#39;), default=None, help=&#39;X-axis limits&#39;) args = parser.parse_args() main(args) Chapter 6 Exercise 6.11.1 Amira does not need to make the heaps-law subdirectory a Git repository because the zipf repository will track everything inside it regardless of how deeply nested. Amira shouldn’t run git init in heaps-law because nested Git repositories can interfere with each other. If someone commits something in the inner repository, Git will not know whether to record the changes in that repository, the outer one, or both. Exercise 6.11.2 git status now shows: On branch master Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) example.txt nothing added to commit but untracked files present (use &quot;git add&quot; to track) Nothing has happened to the file; it still exists but Git no longer has it in the staging area. git rm --cached is equivalent to git restore --staged. With newer versions of Git, older commands will still work, and you may encounter references to them when reading help documentation. If you created this file in your zipf project, we recommend removing it before proceeding. Exercise 6.11.3 If we make a few changes to .gitignore such that it now reads: __pycache__ this is a change this is another change Then git diff would show: diff --git a/.gitignore b/.gitignore index bee8a64..5c83419 100644 --- a/.gitignore +++ b/.gitignore @@ -1 +1,3 @@ -__pycache__ +__pycache__ this is a change + +this is another change Whereas git diff --word-diff shows: diff --git a/.gitignore b/.gitignore index bee8a64..5c83419 100644 --- a/.gitignore +++ b/.gitignore @@ -1 +1,3 @@ __pycache__ {+this is a change+} {+this is another change+} Depending on the nature of the changes you are viewing, the latter may be easier to interpret since it shows exactly what has been changed. Exercise 6.11.4 Maybe: would only create a commit if the file has already been staged. No: would try to create a new repository, which results in an error if a repository already exists. Yes: first adds the file to the staging area, then commits. No: would result in an error, as it would try to commit a file “my recent changes” with the message “myfile.txt”. Exercise 6.11.5 Go into your home directory with cd ~. Create a new folder called bio with mkdir bio. Make the repository your working directory with cd bio. Turn it into a repository with git init. Create your biography using nano or another text editor. Add it and commit it in a single step with git commit -a -m \"Some message\". Modify the file. Use git diff to see the differences. Exercise 6.11.6 Create employment.txt using an editor like Nano. Add both me.txt and employment.txt to the staging area with git add *.txt. Check that both files are there with git status. Commit both files at once with git commit. Exercise 6.11.7 GitHub displays timestamps in a human-readable relative format (i.e. “22 hours ago” or “three weeks ago”), since this makes it easy for anyone in any time zone to know what changes have been made recently. However, if we hover over the timestamp we can see the exact time at which the last change to the file occurred. Exercise 6.11.8 The answer is 1. The command git add motivation.txt adds the current version of motivation.txt to the staging area. The changes to the file from the second echo command are only applied to the working copy, not the version in the staging area. As a result, when git commit -m \"Motivate project\" is executed, the version of motivation.txt committed to the repository is the content from the first echo. However, the working copy still has the output from the second echo; git status would show that the file is modified. git restore HEAD motivation.txt therefore replaces the working copy with the most recently committed version of motivation.txt (the content of the first echo), so cat motivation.txt prints: Zipf&#39;s Law describes the relationship between the frequency and rarity of words. Exercise 6.11.9 Add this line to .gitignore: results/plots/ Exercise 6.11.10 Add the following two lines to .gitignore: *.dat # ignore all data files !final.dat # except final.data The exclamation point ! includes a previously-excluded entry. Note also that if we have previously committed .dat files in this repository they will not be ignored once these rules are added to .gitignore. Only future .dat files will be ignored. Exercise 6.11.11 The left button (with the picture of a clipboard) copies the full identifier of the commit to the clipboard. In the shell, git log shows the full commit identifier for each commit. The middle button (with seven letters and numbers) shows all of the changes that were made in that particular commit; green shaded lines indicate additions and red lines indicate removals. We can show the same thing in the shell using git diff or git diff FROM..TO (where FROM and TO are commit identifiers). The right button lets us view all of the files in the repository at the time of that commit. To do this in the shell, we would need to check out the repository as it was at that commit using git checkout ID, where ID is the tag, branch name, or commit identifier. If we do this, we need to remember to put the repository back to the right state afterward. Exercise 6.11.12 Committing updates our local repository. Pushing sends any commits we have made locally that aren’t yet in the remote repository to the remote repository. Exercise 6.11.13 When GitHub creates a README.md file while setting up a new repository, it actually creates the repository and then commits the README.md file. When we try to pull from the remote repository to our local repository, Git detects that their histories do not share a common origin and refuses to merge them. $ git pull origin master warning: no common commits remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. From https://github.com/frances/eniac * branch master -&gt; FETCH_HEAD * [new branch] master -&gt; origin/master fatal: refusing to merge unrelated histories We can force Git to merge the two repositories with the option --allow-unrelated-histories. Please check the contents of the local and remote repositories carefully before doing this. Exercise 6.11.14 The checkout command restores files from the repository, overwriting the files in our working directory. HEAD indicates the latest version. No: this can be dangerous; without a filename, git checkout will restore all files in the current directory (and all directories below it) to their state at the commit specified. This command will restore data_cruncher.sh to the latest commit version, but will also reset any other files we have changed to that version, which will erase any unsaved changes you may have made to those files. Yes: this restores the latest version of only the desired file. No: this gets the version of data_cruncher.sh from the commit before HEAD, which is not what we want. Yes: the unique ID (identifier) of the last commit is what HEAD means. Yes: this is equivalent to the answer to 2. No: git restore assumes HEAD, so Git will assume you’re trying to restore a file called HEAD, resulting in an error. Exercise 6.11.15 Compares what has changed between the current bin/plotcounts.py and the same file nine commits ago. It returns an error: fatal: ambiguous argument 'HEAD~9': unknown revision or path not in the working tree. We don’t have enough commits in history for the command to properly execute. It compares changes (either staged or unstaged) to the most recent commit. Exercise 6.11.16 No, using git checkout on a staged file does not unstage it. The changes are in the staging area and checkout would affect the working directory. Exercise 6.11.17 Each line of output corresponds to a line in the file, and includes the commit identifier, who last modified the line, when that change was made, and what is included on that line. Note that the edit you just committed is not present here; git blame only shows the current lines in the file, and doesn’t report on lines that have been removed. Chapter 7 Exercise 7.12.1 --oneline shows each commit on a single line with the short identifier at the start and the title of the commit beside it. -n NUMBER limits the number of commits to show. --since and --after can be used to show commits in a range of dates or times; --author can be used to show commits by a particular person; and -w tells Git to ignore whitespace when comparing commits. Exercise 7.12.2 An online search for “show Git branch in Bash prompt” turns up several approaches, one of the simplest of which is to add this line to our ~/.bashrc file: export PS1=&quot;\\\\w + \\$(git branch 2&gt;/dev/null | grep &#39;^*&#39; | colrm 1 2) \\$ &quot; Breaking it down: Setting the PS1 variable defines the primary shell prompt. \\\\w in a shell prompt string means “the current directory”. The + is a literal + sign between the current directory and the Git branch name. The command that gets the name of the current Git branch is in $(...). (We need to escape the $ as \\$ so Bash doesn’t just run it once when defining the string.) The git branch command shows all the branches, so we pipe that to grep and select the one marked with a *. Finally, we remove the first column (i.e., the one containing the *) to leave just the branch name. So what’s 2&gt;/dev/null about? That redirects any error messages to /dev/null, a special “file” that consumes input without saving it. We need that because sometimes we will be in a directory that isn’t inside a Git repository, and we don’t want error messages showing up in our shell prompt. None of this is obvious, and we didn’t figure it out ourselves. Instead, we did a search and pasted various answers into explainshell.com until we had something we understood and trusted. Exercise 7.12.3 https://github.com/github/gitignore/blob/master/Python.gitignore ignores 76 files or patterns. Of those, we recognized less than half. Searching online for some of these, like \"*.pot file\", turns up useful explanations. Searching for others like var/ does not; in that case, we have to look at the category (in this case, “Python distribution”) and set aside time to do more reading. Exercise 7.12.4 git diff master..same does not print anything because there are no differences between the two branches. git merge same master prints merging because Git combines histories even when the files themselves do not differ. After running this command, git history shows a commit for the merge. Exercise 7.12.5 Git refuses to delete a branch with unmerged commits because it doesn’t want to destroy our work. Using the -D (capital-D) option to git branch will delete the branch anyway. This is dangerous because any content that exists only in that branch will be lost. Even with -D, git branch will not delete the branch we are currently on. Exercise 7.12.6 Chartreuse has repositories on GitHub and their desktop containing identical copies of README.md and nothing else. Fuchsia has repositories on GitHub and their desktop with exactly the same content as Chartreuse’s repositories. fuchsia.txt is in both of Fuchsia’s repositories but not in Chartreuse’s repositories. fuchsia.txt is still in both of Fuchsia’s repositories but still not in Chartreuse’s repositories. chartreuse.txt is in both of Chartreuse’s repositories but not yet in either of Fuchsia’s repositories. chartreuse.txt is in Fuchsia’s desktop repository but not yet in their GitHub repository. chartreuse.txt is in both of Fuchsia’s repositories. fuchsia.txt is in Chartreuse’s GitHub repository but not in their desktop repository. All four repositories contain both fuchsia.txt and chartreuse.txt. Chapter 8 Exercise 8.14.1 Our license is at https://github.com/merely-useful/py-rse/blob/book/LICENSE.md. Our contribution guidelines are at https://github.com/merely-useful/py-rse/blob/book/CONTRIBUTING.md. Exercise 8.14.2 The CONDUCT.md file should have contents that mimic those given in Section 8.3. Exercise 8.14.3 The newly created LICENSE.md should look something like the example MIT License shown in Section 8.4.1. Exercise 8.14.4 The text in the README.md might look something like: ## Contributing Interested in contributing? Check out the [CONTRIBUTING.md](CONTRIBUTING.md) file for guidelines on how to contribute. Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By contributing to this project, you agree to abide by its terms. Your CONTRIBUTING.md file might look something like the following: # Contributing Thank you for your interest in contributing to the Zipf&#39;s Law package! If you are new to the package and/or collaborative code development on GitHub, feel free to discuss any suggested changes via issue or email. We can then walk you through the pull request process if need be. As the project grows, we intend to develop more detailed guidelines for submitting bug reports and feature requests. We also have a code of conduct (see [`CONDUCT.md`](CONDUCT.md)). Please follow it in all your interactions with the project. Exercise 8.14.5 Be sure to tag the new issue as a feature request to help triage. Exercise 8.14.6 We often delete the duplicate label: when we mark an issue that way, we (almost) always add a comment saying which issue it’s a duplicate of, in which case it’s just as sensible to label the issue wontfix. Exercise 8.14.7 Some solutions could be: Give the team member their own office space so they don’t distract others. Buy noise cancelling headphones for the employees that find it distracting. Re-arrange the work spaces so that there is a “quiet office” and a regular office space and have the team member with the attention disorder work in the regular office. Exercise 8.14.8 Possible solutions: Change the rule so that anyone who contributes to the project, in any way, gets included as a co-author. Update the rule to include a contributor list on all projects with descriptions of duties, roles, and tasks the contributor provided for the project. Exercise 8.14.9 We obviously can’t say which description fits you best, but: Use three sticky notes and interruption bingo to stop Anna from cutting people off. Tell Bao that the devil doesn’t need more advocates, and that he’s only allowed one “but what about” at a time. Hediyeh’s lack of self-confidence will take a long time to remedy. Keeping a list of the times she’s been right and reminding her of them frequently is a start, but the real fix is to create and maintain a supportive environment. Unmasking Kenny’s hitchhiking wll feel like nit-picking, but so does the accounting required to pin down other forms of fraud. The most important thing is to have the discussion in the open so that everyone realizes he’s taking credit for everyone else’s work as well as theirs. Melissa needs a running partner—someone to work beside her so that she starts when she should and doesn’t get distracted. If that doesn’t work, the project may need to assign everything mission-critical to someone else (which will probably lead to her leaving). Petra can be managed with a one-for-one rule: each time she builds or fixes something that someone else needs, she can then work on something she thinks is cool. However, she’s only allowed to add whatever it is to the project if someone else will publicly commit to maintaining it. Get Frank and Raj off your project as quickly as you can. Chapter 9 Exercise 9.11.1 make -n target will show commands without running them. Exercise 9.11.2 The -B option makes everything, even files that aren’t out of date. The -C option tells Make to change directories before executing, so that make -C ~/myproject runs Make in ~/myproject regardless of the directory it is invoked from. By default, Make looks for (and runs) a file called Makefile or makefile. If you use another name for your Makefile (which is necessary if you have multiple Makefiles in the same directory), then you need to specify the name of that Makefile using the -f option. Exercise 9.11.3 mkdir -p some/path makes one or more nested directories if they don’t exist, and does nothing (without complaining) if they already exist. It is useful for creating the output directories for build rules. Exercise 9.11.4 The build rule for generated the result for any book should now be: ## results/%.csv : regenerate result for any book. results/%.csv : data/%.txt $(COUNT) @bash $(SUMMARY) $&lt; Title @bash $(SUMMARY) $&lt; Author python $(COUNT) $&lt; &gt; $@ where SUMMARY is defined earlier in the Makefile as SUMMARY=bin/book_summary.sh and the settings build rule now includes: @echo SUMMARY: $(SUMMARY) Exercise 9.11.5 Since we already have a variable RESULTS that contains all of the results files, all we need is a phony target that depends on them: .PHONY: results # and all the other phony targets ## results : regenerate result for all books. results : ${RESULTS} Exercise 9.11.6 If we use a shell wildcard in a rule like this: results/collated.csv : results/*.csv python $(COLLATE) $^ &gt; $@ then if results/collated.csv already exists, the rule tells Make that the file depends on itself. Exercise 9.11.7 Our rule is: help : @grep -h -E &#39;^##&#39; ${MAKEFILE_LIST} | sed -e &#39;s/## //g&#39; \\ | column -t -s &#39;:&#39; The -h option to grep tells it not to print filenames, while the -E option tells it to interprets ^## as a pattern. MAKEFILE_LIST is an automatically-defined variable with the names of all the Makefiles in play. (There might be more than one because Makefiles can include other Makefiles.) sed can be used to do string substitution. column formats text nicely in columns. Exercise 9.11.8 This strategy would be advantageous if in future we intended to write a number of different Makefiles that all use the countwords.py, collate.py and plotcounts.py scripts. We discuss configuration strategies in more detail in Chapter 10. Chapter 10 Exercise 10.8.1 The build rule involving plotcounts.py should now read: ## results/collated.png: plot the collated results. results/collated.png : results/collated.csv $(PARAMS) python $(PLOT) $&lt; --outfile $@ --plotparams $(word 2,$^) where PARAMS is defined earlier in the Makefile along with all the other variables and also included later in the settings build rule: COUNT=bin/countwords.py COLLATE=bin/collate.py PARAMS=bin/plotparams.yml PLOT=bin/plotcounts.py SUMMARY=bin/book_summary.sh DATA=$(wildcard data/*.txt) RESULTS=$(patsubst data/%.txt,results/%.csv,$(DATA)) ## settings : show variables&#39; values. settings : @echo COUNT: $(COUNT) @echo DATA: $(DATA) @echo RESULTS: $(RESULTS) @echo COLLATE: $(COLLATE) @echo PARAMS: $(PARAMS) @echo PLOT: $(PLOT) @echo SUMMARY: $(SUMMARY) Exercise 10.8.2 Make the following additions to plotcounts.py: import matplotlib.pyplot as plt parser.add_argument(&#39;--style&#39;, type=str, choices=plt.style.available, default=None, help=&#39;matplotlib style&#39;) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; if args.style: plt.style.use(args.style) Add nargs='*' to the definition of the --style option: parser.add_argument(&#39;--style&#39;, type=str, nargs=&#39;*&#39;, choices=plt.style.available, default=None, help=&#39;matplotlib style&#39;) Exercise 10.8.3 The first step is to add a new command-line argument to tell plotcount.py what we want to do: if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) # ...other options as before... parser.add_argument(&#39;--saveconfig&#39;, type=str, default=None, help=&#39;Save configuration to file&#39;) args = parser.parse_args() main(args) Next, we add three lines to main to act on this option after all of the plotting parameters have been set. For now we use return to exit from main as soon as the parameters have been saved; this lets us test our change without overwriting any of our actual plots. def save_configuration(fname, params): &quot;&quot;&quot;Save configuration to a file.&quot;&quot;&quot; with open(fname, &#39;w&#39;) as reader: yaml.dump(params, reader) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; if args.style: plt.style.use(args.style) set_plot_params(args.plotparams) if args.saveconfig: save_configuration(args.saveconfig, mpl.rcParams) return df = pd.read_csv(args.infile, header=None, names=(&#39;word&#39;, &#39;word_frequency&#39;)) # ...carry on producing plot... Finally, we add a target to Makefile to try out our change. We do the test this way so that we can be sure that we’re testing with the same options we use with the real program; if we were to type in the whole command ourselves, we might use something different. We also save the configuration to /tmp rather than to our project directory to keep it out of version control’s way: ## test-saveconfig : save plot configuration. test-saveconfig : python $(PLOT) --saveconfig /tmp/test-saveconfig.yml \\ --plotparams $(PARAMS) The output is over 400 lines long, and includes settings for everything from the animation bit rate and the size of y-axis ticks: !!python/object/new:matplotlib.RcParams dictitems: _internal.classic_mode: false agg.path.chunksize: 0 animation.avconv_args: [] animation.avconv_path: avconv animation.bitrate: -1 ... ytick.minor.size: 2.0 ytick.minor.visible: false ytick.minor.width: 0.6 ytick.right: false The beautiful thing about this file is that the entries are automatically sorted alphabetically, which makes it easy for both human beings and the diff command to spot differences. This helps reproducibility because any one of these settings might change in a new release of matplotlib, and any of those changes might affect our plots. Saving the settings allows us to compare what we had when we did our work to what we have when we’re trying to recreate it, which in turn gives us a starting point for debugging if we need to. Exercise 10.8.4 import configparser def set_plot_params(param_file): &quot;&quot;&quot;Set the matplotlib parameters.&quot;&quot;&quot; if param_file: config = configparser.ConfigParser() config.read(param_file) for section in config.sections(): for param in config[section]: value = config[section][param] mpl.rcParams[param] = value Most people seem to find Windows INI files easier to write and read, since it’s easier to see what’s a heading and what’s a value. However, Windows INI files only provide one level of sectioning, so complex configurations are harder to express. Thus, while YAML may be a bit more difficult to get started with, it will take us further. Exercise 10.8.5 The answer depends on whether we are able to make changes to Program A and Program B. If we can, we can modify them to use overlay configuration and put the shared parameters in a single file that both programs load. If we can’t do that, the next best thing is to create a small helper program that reads their configuration files and checks that common parameters have consistent values. The first solution prevents the problem; the second detects it, which is a lot better than nothing. Chapter 11 Exercise 11.11.1 The first assertion checks that the input sequence values is not empty. An empty sequence such as [] will make it fail. The second assertion checks that each value in the list can be turned into an integer. Input such as [1, 2,'c', 3] will make it fail. The third assertion checks that the total of the list is greater than 0. Input such as [-10, 2, 3] will make it fail. Exercise 11.11.2 Remove the comments about inserting preconditions and add the following: assert len(rect) == 4, &#39;Rectangles must contain 4 coordinates&#39; x0, y0, x1, y1 = rect assert x0 &lt; x1, &#39;Invalid X coordinates&#39; assert y0 &lt; y1, &#39;Invalid Y coordinates&#39; Remove the comment about inserting postconditions and add the following assert 0 &lt; upper_x &lt;= 1.0, \\ &#39;Calculated upper X coordinate invalid&#39; assert 0 &lt; upper_y &lt;= 1.0, \\ &#39;Calculated upper Y coordinate invalid&#39; The problem is that the following section of normalize_rectangle should read float(dy) / dx, not float(dx) / dy. if dx &gt; dy: scaled = float(dx) / dy test_geometry.py should read as follows: import geometry def test_tall_skinny(): &quot;&quot;&quot;Test normalization of a tall, skinny rectangle.&quot;&quot;&quot; rect = [20, 15, 30, 20] expected_result = (0, 0, 1.0, 0.5) actual_result = geometry.normalize_rectangle(rect) assert actual_result == expected_result Other tests might include (but are not limited to): def test_short_wide(): &quot;&quot;&quot;Test normalization of a short, wide rectangle.&quot;&quot;&quot; rect = [2, 5, 3, 10] expected_result = (0, 0, 0.2, 1.0) actual_result = geometry.normalize_rectangle(rect) assert actual_result == expected_result def test_negative_coordinates(): &quot;&quot;&quot;Test rectangle normalization with negative coordinates.&quot;&quot;&quot; rect = [-2, 5, -1, 10] expected_result = (0, 0, 0.2, 1.0) actual_result = geometry.normalize_rectangle(rect) assert actual_result == expected_result Exercise 11.11.3 There are three approaches to testing when pseudo-random numbers are involved: Run the function once with a known seed, check and record its output, and then compare the output of subsequent runs to that saved output. (Basically, if the function does the same thing it did the first time, we trust it.) Replace the pseudo-random number generator with a function of our own that generates a predictable series of values. For example, if we are randomly partitioning a list into two equal halves, we could instead use a function that puts odd-numbered values in one partition and even-numbered values in another (which is a legal but unlikely outcome of truly random partitioning). Instead of checking for an exact result, check that the result lies within certain bounds, just as we would with the result of a physical experiment. Exercise 11.11.4 This result seems counter-intuitive to many people because relative error is a measure of a single value, but in this case we are looking at a distribution of values: each result is off by 0.1 compared to a range of 0–2, which doesn’t “feel” infinite. In this case, a better measure might be the largest absolute error divided by the standard deviation of the data. Chapter 12 Exercise 12.6.1 Add a new command line argument to collate.py: parser.add_argument(&#39;-v&#39;, &#39;--verbose&#39;, action=&quot;store_true&quot;, default=False, help=&quot;Set logging level to DEBUG&quot;) and two new lines to the beginning of the main function: log_level = logging.DEBUG if args.verbose else logging.WARNING logging.basicConfig(level=log_level) such that the full collate.py script now reads as follows: &quot;&quot;&quot; Combine multiple word count CSV-files into a single cumulative count. &quot;&quot;&quot; import csv import argparse from collections import Counter import logging import utilities as util ERRORS = { &#39;not_csv_suffix&#39; : &#39;{fname}: File must end in .csv&#39;, } def update_counts(reader, word_counts): &quot;&quot;&quot;Update word counts with data from another reader/file.&quot;&quot;&quot; for word, count in csv.reader(reader): word_counts[word] += int(count) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; log_lev = logging.DEBUG if args.verbose else logging.WARNING logging.basicConfig(level=log_lev) word_counts = Counter() logging.info(&#39;Processing files...&#39;) for fname in args.infiles: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infiles&#39;, type=str, nargs=&#39;*&#39;, help=&#39;Input file names&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) parser.add_argument(&#39;-v&#39;, &#39;--verbose&#39;, action=&quot;store_true&quot;, default=False, help=&quot;Set logging level to DEBUG&quot;) args = parser.parse_args() main(args) Exercise 12.6.2 Add a new command line argument to collate.py: parser.add_argument(&#39;-l&#39;, &#39;--logfile&#39;, type=str, default=&#39;collate.log&#39;, help=&#39;Name of the log file&#39;) and pass the name of the log file to logging.basicConfig using the filename argument: logging.basicConfig(level=log_lev, filename=args.logfile) such that the collate.py script now reads as follows: &quot;&quot;&quot; Combine multiple word count CSV-files into a single cumulative count. &quot;&quot;&quot; import csv import argparse from collections import Counter import logging import utilities as util ERRORS = { &#39;not_csv_suffix&#39; : &#39;{fname}: File must end in .csv&#39;, } def update_counts(reader, word_counts): &quot;&quot;&quot;Update word counts with data from another reader/file.&quot;&quot;&quot; for word, count in csv.reader(reader): word_counts[word] += int(count) def main(args): &quot;&quot;&quot;Run the command line program.&quot;&quot;&quot; log_lev = logging.DEBUG if args.verbose else logging.WARNING logging.basicConfig(level=log_lev, filename=args.logfile) word_counts = Counter() logging.info(&#39;Processing files...&#39;) for fname in args.infiles: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) util.collection_to_csv(word_counts, num=args.num) if __name__ == &#39;__main__&#39;: parser = argparse.ArgumentParser(description=__doc__) parser.add_argument(&#39;infiles&#39;, type=str, nargs=&#39;*&#39;, help=&#39;Input file names&#39;) parser.add_argument(&#39;-n&#39;, &#39;--num&#39;, type=int, default=None, help=&#39;Output only n most frequent words&#39;) parser.add_argument(&#39;-v&#39;, &#39;--verbose&#39;, action=&quot;store_true&quot;, default=False, help=&quot;Set logging level to DEBUG&quot;) parser.add_argument(&#39;-l&#39;, &#39;--logfile&#39;, type=str, default=&#39;collate.log&#39;, help=&#39;Name of the log file&#39;) args = parser.parse_args() main(args) Exercise 12.6.3 The loop in collate.py that reads/processes each input file should now read as follows: for fname in args.infiles: try: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format(fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) except Exception as error: logging.warning(f&#39;{fname} not processed: {error}&#39;) The loop in collate.py that reads/processes each input file should now read as follows: for fname in args.infiles: try: logging.debug(f&#39;Reading in {fname}...&#39;) if fname[-4:] != &#39;.csv&#39;: msg = ERRORS[&#39;not_csv_suffix&#39;].format( fname=fname) raise OSError(msg) with open(fname, &#39;r&#39;) as reader: logging.debug(&#39;Computing word counts...&#39;) update_counts(reader, word_counts) except FileNotFoundError: msg = f&#39;{fname} not processed: File does not exist&#39; logging.warning(msg) except PermissionError: msg = f&#39;{fname} not processed: No permission to read&#39; logging.warning(msg) except Exception as error: msg = f&#39;{fname} not processed: {error}&#39; logging.warning(msg) Exercise 12.6.4 The try/except block in collate.py should read begin as follows: try: process_file(fname, word_counts) except FileNotFoundError: # ... the other exceptions The following additions need to be made to test_zipfs.py. import collate def test_not_csv_error(): &quot;&quot;&quot;Error handling test for csv check&quot;&quot;&quot; fname = &#39;data/dracula.txt&#39; word_counts = Counter() with pytest.raises(OSError): collate.process_file(fname, word_counts) The following unit test needs to be added to test_zipfs.py. def test_missing_file_error(): &quot;&quot;&quot;Error handling test for missing file&quot;&quot;&quot; fname = &#39;fake_file.csv&#39; word_counts = Counter() with pytest.raises(FileNotFoundError): collate.process_file(fname, word_counts) The following sequence of commands is required to test the code coverage. $ coverage run -m pytest $ coverage html Open htmlcov/index.html and click on bin/collate.py to view a coverage summary. The lines of process_files that include the raise OSError and open(fname, 'r') commands should appear in green after clicking the green “run” box in the top left hand corner of the page. Exercise 12.6.5 The convention is to use ALL_CAPS_WITH_UNDERSCORES when defining global variables. Python’s f-strings interpolate variables that are in scope: there is no easy way to interpolate values from a lookup table. In contrast, str.format can be given any number of named keyword arguments (Appendix F), so we can look up a string and then interpolate whatever values we want. Once ERRORS has been moved to the utilities module all references to it in collate.py must be updated to util.ERRORS. Exercise 12.6.6 A traceback is an object that records where an exception was raised), what stack frames were on the call stack when the error occurred, and other details that are helpful for debugging. Python’s traceback library can be used to get and print information from these objects. Chapter 13 Exercise 13.4.1 You can get an ORCID by registering here. Please add this 16-digit identifier to all of your published works and to your online profiles. Exercise 13.4.2 If possible, compare your answers with those of a colleague who works with the same data. Where did you agree and disagree, and why? Exercise 13.4.3 51 solicitors were interviewed as the participants. Interview data and a data from a database on court decisions. This information is not available within the documentation. Information on their jobs and opinions are there, but the participant demographics are only described within the associated article. The difficulty is that the article is not linked within the documentation or the metadata. We can search the dataset name and authorname trying to find this. A search for “National Science Foundation (1228602)”, which is the grant information, finds the grant page. Two articles are linked there, but both the DOI links are broken. We can search with the citation for each paper to find them. The Forced Migration article uses a different subset of interviews and does not mention demographics nor links to the deposited dataset. The Boston College Law Review article has the same two problems of different data and no dataset citation. Searching more broadly through Meili’s work, we can find Meili (2015). This lists the dataset as a footnote and reports the 51 interviews with demographic data on reported gender of the interviewees. This paper lists data collection as 2010-2014, while the other two say 2010-2013. We might come to a conclusion that this extra year is where the extra 9 interviews come in, but that difference is not explained anywhere. Exercise 13.4.4 For borstlab/reversephi_paper: The software requirements are documented in README.md. In addition to the tools used in the zipf/ project (Python, Make and Git), the project also requires ImageMagick. No information on installing ImageMagick or a required version of ImageMagick is provided. To re-create the conda environment you would need the file my_environment.yml. Instructions for creating and using the environment are provided in README.md. Like zipf the data processing and analysis steps are documented in a Makefile. The README includes instructions for re-creating the results using make all. There doesn’t seem to be a DOI for the archived code and data, but the GitHub repo does have a release v1.0 with the description “Published manuscript (1.0)”. A zip file of this release could be downloaded from GitHub. For the figshare page that accompanies the paper Irving, Wijffels, and Church (2019): The figshare page includes a “Software environment” section. To re-create the conda environment you would need the file environment.yml. figure*_log.txt are log files for each figure in the paper. These files show the computational steps performed in generating the figure, in the form of a list of commands executed at the command line. code.zip is a version controlled (using git) file directory containing the code written to perform the analysis (i.e. it contains the scripts referred to in the log files). This code can also be found on GitHub. The figshare page itself is the archive, and includes a version history for the contents. For the GitHub repo blab/h3n2-reassortment: README.md includes an “Install requirements” section that describes setting up the conda environment using the file h3n2_reassortment.yaml. The analysis also depends on components from Nextstrain. Instructions for cloning them from GitHub are provided. The code seems to be spread across the directories jupyter_notebooks, hyphy, flu_epidemiology, and src, but it isn’t clear what order the code should be run in, or how the components depend on each other. The data itself is not archived, but links are provided in the “Install requirements” section of README.md to documents that describe how to obtain the data. Some intermediate data is also provided in the data/ directory. The GitHub repo has a release with files “that are up-to-date with the version of the manuscript that was submitted to Virus Evolution on 31 January 2019.” Exercise 13.4.5 https://web.archive.org/web/20191105173924/https://ukhomeoffice.github.io/accessibility-posters/posters/accessibility-posters.pdf Exercise 13.4.6 You’ll know you’ve completed this exercise when you have a URL that points to zip archive for a specific release of your repository on GitHub, e.g. https://github.com/amira-khan/zipf/archive/KhanVirtanen2020.zip Exercise 13.4.7 Some steps to publishing your project’s code would be: Upload the code on GitHub. Use a standard folder and file structure as taught in this book. Include README, CONTRIBUTING, CONDUCT, and LICENSE files. Make sure these files explain how to install and configure the required software and tells people how to run the code in the project. Include a requirements.txt file for Python package dependencies. Chapter 14 Exercise 14.9.1 A description and long_description argument need to be provided when the setup function is called in setup.py. On the TestPyPI webpage, the user interface displays description in the grey banner and long_description in the section named “Project Description”. Other metadata that might be added includes the author email address, software license details and a link to the documentation at Read The Docs. Exercise 14.9.2 The new requirements_dev.txt file will have this inside it: pytest Exercise 14.9.3 The answers to the relevant questions from the checklist are shown below. Repository: Is the source code for this software available at the repository url? Yes. The source code is available at PyPI. License: Does the repository contain a plain-text LICENSE file with the contents of an OSI approved software license? Yes. Our GitHub repository contains LICENSE.md (Section 8.4.1). Installation: Does installation proceed as outlined in the documentation? Yes. Our README says the package can be installed via pip. Functionality: Have the functional claims of the software been confirmed? Yes. The command line programs countwords, collate, and plotcounts perform as described in the README. A statement of need: Do the authors clearly state what problems the software is designed to solve and who the target audience is? Yes. The “Motivation” section of the README explains this. Installation instructions: Is there a clearly-stated list of dependencies? Ideally these should be handled with an automated package management solution. Yes. In our setup.py file the install_requires argument lists dependencies. Example usage: Do the authors include examples of how to use the software (ideally to solve real-world analysis problems). Yes. There are examples in the README. Functionality documentation: Is the core functionality of the software documented to a satisfactory level (e.g., API method documentation)? Yes. This information is available on Read The Docs. Automated tests: Are there automated tests or manual steps described so that the functionality of the software can be verified? We have units tests written and available (test_zipfs.py), but our documentation needs to be updated to tell people to run pytest in order to manually run those tests. Community guidelines: Are there clear guidelines for third parties wishing to 1) Contribute to the software 2) Report issues or problems with the software 3) Seek support? Yes. Our CONTRIBUTING file explains this (Section 8.11). Exercise 14.9.4 The directory tree for the pratchett package is: pratchett ├── pratchett │ └── quotes.py ├── README.md └── setup.py README.md should contain a basic description of the package and how to install/use it, while setup.py should contain: from setuptools import setup setup( name=&#39;pratchett&#39;, version=&#39;0.1&#39;, author=&#39;Amira Khan&#39;, packages=[&#39;pratchett&#39;], ) The following sequence of commands will create the development environment, activate it, and then install the package: $ conda create -n pratchett python $ conda activate pratchett (pratchett)$ cd pratchett (pratchett)$ pip install -e . "],["objectives.html", "B Learning Objectives B.1 Getting Started B.2 The Basics of the Unix Shell B.3 Building Tools with the Unix Shell B.4 Going Further with the Unix Shell B.5 Building Command-Line Tools with Python B.6 Using Git at the Command Line B.7 Going Further with Git B.8 Working in Teams B.9 Automating Analyses with Make B.10 Configuring Programs B.11 Testing Software B.12 Handling Errors B.13 Tracking Provenance B.14 Creating Packages with Python", " B Learning Objectives This appendix lists the learning objectives for each chapter, and is intended to help instructors who want to use this curriculum. B.1 Getting Started Identify the few standard files that should be present in every research software project. Explain the typical directory structure used in small and medium-sized data analysis projects. Download the required data. Install the required software. B.2 The Basics of the Unix Shell Explain how the shell relates to the keyboard, the screen, the operating system, and users’ programs. Explain when and why a command-line interface should be used instead of graphical user interfaces. Explain the steps in the shell’s read-evaluate-print loop. Identify the command, options, and filenames in a command-line call. Explain the similarities and differences between files and directories. Translate an absolute path into a relative path and vice versa. Construct absolute and relative paths that identify specific files and directories. Delete, copy, and move files and directories. B.3 Building Tools with the Unix Shell Redirect a command’s output to a file. Use redirection to process a file instead of keyboard input. Construct pipelines with two or more stages. Explain Unix’s “small pieces, loosely joined” philosophy. Write a loop that applies one or more commands separately to each file in a set of files. Trace the values taken on by a loop variable during execution of the loop. Explain the difference between a variable’s name and its value. Demonstrate how to see recently-executed commands. Re-run recently executed commands without retyping them. B.4 Going Further with the Unix Shell Write a shell script that uses command-line arguments. Create pipelines that include shell scripts as well as built-in commands. Create and use variables in shell scripts with correct quoting. Use grep to select lines from text files that match simple patterns. Use find to find files whose names match simple patterns. Edit the .bashrc file to change default shell variables. Create aliases for commonly-used commands. B.5 Building Command-Line Tools with Python Explain the benefits of writing Python programs that can be executed at the command line. Create a command-line Python program that respects Unix shell conventions for reading input and writing output. Use the argparse library to handle command-line arguments in a program. Explain how to tell if a module is being run directly or being loaded by another program. Write docstrings for programs and functions. Explain the difference between optional arguments and positional arguments. Create a module that contains functions used by multiple programs and import that module. B.6 Using Git at the Command Line Explain the advantages and disadvantages of using Git at the command line. Demonstrate how to configure Git on a new computer. Create a local Git repository at the command line. Demonstrate the modify-add-commit cycle for one or more files. Synchronize a local repository with a remote repository. Explain what the HEAD of a repository is and demonstrate how to use it in commands. Identify and use Git commit identifiers. Demonstrate how to compare revisions to files in a repository. Restore old versions of files in a repository. Explain how to use .gitignore to ignore files and identify files that are being ignored. B.7 Going Further with Git Explain why branches are useful. Demonstrate how to create a branch, make changes on that branch, and merge those changes back into the original branch. Explain what conflicts are and demonstrate how to resolve them. Explain what is meant by a branch-per-feature workflow. Define the terms fork, clone, remote, and pull request. Demonstrate how to fork a repository and submit a pull request to the original repository. B.8 Working in Teams Explain how a project lead can be a good ally. Explain the purpose of a Code of Conduct and add one to a project. Explain why every project should include a license and add one to a project. Describe different kinds of licenses for software and written material. Explain what an issue tracking system does and what it should be used for. Describe what a well-written issue should contain. Explain how to label issues to manage work. Submit an issue to a project. Describe common approaches to prioritizing tasks. Describe some common-sense rules for running meetings. Explain why every project should include contribution guidelines and add some to a project. Explain how to handle conflict between project participants. B.9 Automating Analyses with Make Explain what a build manager is and how they aid reproducible research. Name and describe the three parts of a build rule. Write a Makefile that re-runs a multi-stage data analysis. Explain and trace how Make chooses an order in which to execute rules. Explain what phony targets are and define a phony target. Explain what automatic variables are and identify three commonly-used automatic variables. Write Make rules that use automatic variables. Explain why and how to write pattern rules in a Makefile. Write Make rules that use patterns. Define variables in a Makefile explicitly and by using functions. Make a self-documenting Makefile. B.10 Configuring Programs Explain what overlay configuration is. Describe the four levels of configuration typically used by robust software. Create a configuration file using YAML. B.11 Testing Software Explain three different goals for testing software. Add assertions to a program to check that it is operating correctly. Write and run unit tests using pytest. Determine the coverage of those tests and identify untested portions of code. Explain continuous integration and implement it using Travis CI. Describe and contrast test-driven development and checking-driven development. B.12 Handling Errors Explain how to use exceptions to signal and handle errors in programs. Write try/except blocks to raise and catch exceptions. Explain what is meant by “throw low, catch high”. Describe the most common built-in exception types in Python and how they relate to each other. Explain what makes a useful error message. Create and use a lookup table for common error messages. Explain the advantages of using a logging framework rather than print statements. Describe the five standard logging levels and explain what each should be used for. Create, configure, and use a simple logger. B.13 Tracking Provenance Explain what a DOI is and how to get one. Explain what an ORCID is and get one. Describe the FAIR Principles and determine whether a dataset conforms to them. Explain where to archive small, medium, and large datasets. Describe good practices for archiving analysis code and determine whether a report conforms to them. Explain the difference between reproducibility and inspectability. B.14 Creating Packages with Python Create a Python package using setuptools. Create and use a virtual environment to manage Python package installations. Install a Python package using pip. Distribute that package via TestPyPI. Write a README file for a Python package. Use Sphinx to create and preview documentation for a package. Explain where and how to obtain a DOI for a software release. Describe some academic journals that publish software papers. "],["keypoints.html", "C Key Points C.1 Getting Started C.2 The Basics of the Unix Shell C.3 Building Tools with the Unix Shell C.4 Going Further with the Unix Shell C.5 Building Command-Line Programs in Python C.6 Using Git at the Command Line C.7 Going Further with Git C.8 Working in Teams C.9 Automating Analyses with Make C.10 Configuring Programs C.11 Testing Software C.12 Handling Errors C.13 Tracking Provenance C.14 Creating Packages with Python", " C Key Points This appendix lists the key points for each chapter. C.1 Getting Started Make tidiness a habit, rather than cleaning up your project files later. Include a few standard files in all your projects, such as README, LICENSE, CONTRIBUTING, CONDUCT and CITATION. Put runnable code in a bin/ directory. Put raw/original data in a data/ directory and never modify it. Put results in a results/ directory. This includes cleaned-up data and figures (i.e. everything created using what’s in bin and data). Put documentation and manuscripts in a docs/ directory. Refer to The Carpentries software installation guide if you’re having trouble. C.2 The Basics of the Unix Shell A shell is a program that reads commands and runs other programs. The filesystem manages information stored on disk. Information is stored in files, which are located in directories (folders). Directories can also store other directories, which forms a directory tree. pwd prints the user’s current working directory. / on its own is the root directory of the whole filesystem. ls prints a list of files and directories. An absolute path specifies a location from the root of the filesystem. A relative path specifies a location in the filesystem starting from the current directory. cd changes the current working directory. .. means the parent directory. . on its own means the current directory. mkdir creates a new directory. cp copies a file. rm removes (deletes) a file. mv moves (renames) a file or directory. * matches zero or more characters in a filename. ? matches any single character in a filename. wc counts lines, words, and characters in its inputs. man displays the manual page for a given command; some commands also have a --help option. C.3 Building Tools with the Unix Shell cat displays the contents of its inputs. head displays the first few lines of its input. tail displays the last few lines of its input. sort sorts its inputs. Use the up-arrow key to scroll up through previous commands to edit and repeat them. Use history to display recent commands and !number to repeat a command by number. Every process in Unix has an input channel called standard input and an output channel called standard output. &gt; redirects a command’s output to a file, overwriting any existing content. &gt;&gt; appends a command’s output to a file. &lt; operator redirects input to a command. A pipe | sends the output of the command on the left to the input of the command on the right. A for loop repeats commands once for every thing in a list. Every for loop must have a variable to refer to the thing it is currently operating on and a body containing commands to execute. Use $name or ${name} to get the value of a variable. C.4 Going Further with the Unix Shell Save commands in files (usually called shell scripts) for re-use. bash filename runs the commands saved in a file. $@ refers to all of a shell script’s command-line arguments. $1, $2, etc., refer to the first command-line argument, the second command-line argument, etc. Place variables in quotes if the values might have spaces or other special characters in them. find lists files with specific properties or whose names match patterns. $(command) inserts a command’s output in place. grep selects lines in files that match patterns. Use the .bashrc file in your home directory to set shell variables each time the shell runs. Use alias to create shortcuts for things you type frequently. C.5 Building Command-Line Programs in Python Write command-line Python programs that can be run in the Unix shell like other command-line tools. If the user does not specify any input files, read from standard input. If the user does not specify any output files, write to standard output. Place all import statements at the start of a module. Use the value of __name__ to determine if a file is being run directly or being loaded as a module. Use argparse to handle command-line arguments in standard ways. Use short options for common controls and long options for less common or more complicated ones. Use docstrings to document functions and scripts. Place functions that are used across multiple scripts in a separate file that those scripts can import. C.6 Using Git at the Command Line Use git config with the --global option to configure your user name, email address, and other preferences once per machine. git init initializes a repository. Git stores all repository management data in the .git subdirectory of the repository’s root directory. git status shows the status of a repository. git add puts files in the repository’s staging area. git commit saves the staged content as a new commit in the local repository. git log lists previous commits. git diff shows the difference between two versions of the repository. Synchronize your local repository with a remote repository on a forge such as GitHub. git remote manages bookmarks pointing at remote repositories. git push copies changes from a local repository to a remote repository. git pull copies changes from a remote repository to a local repository. git restore and git checkout recover old versions of files. The .gitignore file tells Git what files to ignore. C.7 Going Further with Git Use a branch-per-feature workflow to develop new features while leaving the master branch in working order. git branch creates a new branch. git checkout switches between branches. git merge merges changes from another branch into the current branch. Conflicts occur when files or parts of files are changed in different ways on different branches. Version control systems do not allow people to overwrite changes silently; instead, they highlight conflicts that need to be resolved. Forking a repository makes a copy of it on a server. Cloning a repository with git clone creates a local copy of a remote repository. Create a remote called upstream to point to the repository a fork was derived from. Create pull requests to submit changes from your fork to the upstream repository. C.8 Working in Teams Welcome and nurture community members proactively. Create an explicit Code of Conduct for your project modelled on the Contributor Covenant. Include a license in your project so that it’s clear who can do what with the material. Create issues for bugs, enhancement requests, and discussions. Label issues to identify their purpose. Triage issues regularly and group them into milestones to track progress. Include contribution guidelines in your project that specify its workflow and its expectations of participants. Make rules about governance explicit. Use common-sense rules to make project meetings fair and productive. Manage conflict between participants rather than hoping it will take care of itself. C.9 Automating Analyses with Make Make is a widely-used build manager. A build manager re-runs commands to update files that are out of date. A build rule has targets, prerequisites, and a recipe. A target can be a file or a phony target that simply triggers an action. When a target is out of date with respect to its prerequisites, Make executes the recipe associated with its rule. Make executes as many rules as it needs to when updating files, but always respects prerequisite order. Make defines automatic variables such as $@ (target), $^ (all prerequisites), and $&lt; (first prerequisite). Pattern rules can use % as a placeholder for parts of filenames. Makefiles can define variables using NAME=value. Makefiles can also use functions such as $(wildcard ...) and $(patsubst ...). Use specially-formatted comments to create self-documenting Makefiles. C.10 Configuring Programs Overlay configuration specifies settings for a program in layers, each of which overrides previous layers. Use a system-wide configuration file for general settings. Use a user-specific configuration file for personal preferences. Use a job-specific configuration file with settings for a particular run. Use command-line options to change things that commonly change. Use YAML or some other standard syntax to write configuration files. Save configuration information to make your research reproducible. C.11 Testing Software Test software to convince people (including yourself) that software is correct enough and to make tolerances on “enough” explicit. Add assertions to code so that it checks itself as it runs. Write unit tests to check indivdiual pieces of code. Write integration tests to check that those pieces work together correctly. Write regression tests to check if things that used to work no longer do. A test framework finds and runs tests written in a prescribed fashion and reports their results. Test coverage is the fraction of lines of code that are executed by a set of tests. Continuous integration re-builds and/or re-tests software every time something changes. C.12 Handling Errors Signal errors by raising exceptions. Use try/except blocks to catch and handle exceptions. Python organizes its standard exceptions in a hierarchy so that programs can catch and handle them selectively. “Throw low, catch high”, i.e., raise exceptions immediately but handle them at a higher level. Write error messages that help users figure out what to do to fix the problem. Store error messages in a lookup table to ensure consistency. Use a logging framework instead of print statements to report program activity. Separate logging messages into DEBUG, INFO, WARNING, ERROR, and CRITICAL levels. Use logging.basicConfig to define basic logging parameters. C.13 Tracking Provenance Publish data and code as well as papers. Use DOIs to identify reports, datasets, or software release. Use an ORCID to identify yourself as an author of a report, dataset, or software release. Data should be FAIR: findable, accessible, interoperable, and reusable. Put small datasets in version control repositories; store large ones on data sharing sites. Describe your software environment, analysis scripts, and data processing steps in reproducible ways. Make your analyses inspectable as well as reproducible. C.14 Creating Packages with Python Use setuptools to build and distribute Python packages. Create a directory named mypackage containing a setup.py script as well as a subdirectory also called mypackage containing the package’s source files. Use semantic versioning for software releases. Use a virtual environment to test how your package installs without disrupting your main Python installation. Use pip to install Python packages. The default respository for Python packages is PyPI. Use TestPyPI to test the distribution of your package. Use a README file for package-level documentation. Use Sphinx to generate documentation for a package. Use Read The Docs to host package documentation online. Create a DOI for your package using GitHub’s Zenodo integration. Publish the details of your package in a software journal so that others can cite it. "],["tree.html", "D Project Tree", " D Project Tree The final directory tree for the Zipf’s Law project looks as follows: pyzipf/ ├── .gitignore ├── CITATION.md ├── CONDUCT.md ├── CONTRIBUTING.md ├── KhanVirtanen2020.md ├── LICENSE.md ├── Makefile ├── README.rst ├── environment.yml ├── requirements.txt ├── requirements_docs.txt ├── setup.py ├── data │ ├── README.md │ ├── dracula.txt │ ├── frankenstein.txt │ ├── jane_eyre.txt │ ├── moby_dick.txt │ ├── sense_and_sensibility.txt │ ├── sherlock_holmes.txt │ └── time_machine.txt ├── docs │ ├── Makefile │ ├── conf.py │ ├── index.rst │ └── source │ │ ├── collate.rst │ │ ├── countwords.rst │ │ ├── modules.rst │ │ ├── plotcounts.rst │ │ ├── test_zipfs.rst │ │ └── utilities.rst ├── results │ ├── collated.csv │ ├── collated.png │ ├── dracula.csv │ ├── dracula.png │ ├── frankenstein.csv │ ├── jane_eyre.csv │ ├── jane_eyre.png │ ├── moby_dick.csv │ ├── sense_and_sensibility.csv │ ├── sherlock_holmes.csv │ └── time_machine.csv ├── test_data │ ├── random_words.txt │ └── risk.txt └── pyzipf ├── book_summary.sh ├── collate.py ├── countwords.py ├── plotcounts.py ├── plotparams.yml ├── script_template.py ├── test_zipfs.py └── utilities.py You can view the complete project, including the version history, in Amira’s zipf repository on GitHub. Each file was introduced and subsequently modified in the following chapters, sections and exercises: pyzipf/: Introduced as zipf/ in Section 1.2 and changes name to pyzipf/ in Section 14.1. pyzipf/.gitignore: Introduced in Section 6.9, and updated in various other chapters following GitHub’s .gitignore templates. pyzipf/CITATION.md: Introduced in Section 14.7. pyzipf/CONDUCT.md: Introduced in Section 8.3 and committed to the repository in Exercise 8.14.2. pyzipf/CONTRIBUTING.md: Introduced in Section 8.11 and committed to the repository in Exercise 8.14.4. pyzipf/KhanVirtanen2020.md: Introduced in Section 13.2.2. pyzipf/LICENSE.md: Introduced in Section 8.4.1 and committed to the repository in Exercise 8.14.3. pyzipf/Makefile: Introduced and updated throughout Chapter 9. Updated again in Exercise 10.8.1. pyzipf/README.rst: Introduced as a .md file in Section 7.6, updated in Section 7.8 and then converted to a .rst file with further updates in Section 14.6.1. pyzipf/environment.yml: Introduced in Section 13.2.1. pyzipf/requirements.txt: Introduced in Section 11.8. pyzipf/requirements_docs.txt: Introduced in Section 14.6.2. pyzipf/setup.py: Introduced and updated throughout Chapter 14. pyzipf/data/* : Downloaded as part of the setup instructions (Section 1.2). pyzipf/docs/*: Introduced in Section 14.6.2. pyzipf/results/collated.*: Generated in Section 9.9. pyzipf/results/dracula.csv: Generated in Section 5.7. pyzipf/results/dracula.png: Generated in Section 6.5 and updated in Section 7.4. pyzipf/results/jane_eyre.csv: Generated in Section 5.7. pyzipf/results/jane_eyre.png: Generated in Section 5.9. pyzipf/results/moby_dick.csv: Generated in Section 5.7. pyzipf/results/frankenstein.csv: Generated in Section 9.7. pyzipf/results/sense_and_sensibility.csv: Generated in Section 9.7. pyzipf/results/sherlock_holmes.csv: Generated in Section 9.7. pyzipf/results/time_machine.csv: Generated in Section 9.7. pyzipf/test_data/random_words.txt: Generated in Section 11.5. pyzipf/test_data/risk.txt: Introduced in Section 11.2. pyzipf/pyzipf/: Introduced as bin/ in Section 1.1.2 and changes name to pyzipf/ in Section 14.1. pyzipf/pyzipf/book_summary.sh: Introduced and updated throughout Chapter 4. pyzipf/pyzipf/collate.py: Introduced in Section 5.7 and updated in Section 5.8, throughout Chapter 12 and in Section 14.1. pyzipf/pyzipf/countwords.py: Introduced in Section 5.4 and updated in Sections 5.8 and 14.1. pyzipf/pyzipf/plotcounts.py: Introduced in Exercise 5.11.4 and updated throughout Chapters 6, 7 and 10. pyzipf/pyzipf/plotparams.yml: Introduced in Section 10.6. pyzipf/pyzipf/script_template.py: Introduced in Section 5.2 and updated in Section 5.3. pyzipf/pyzipf/test_zipfs.py: Introduced and updated throughout Chapter 11. pyzipf/pyzipf/utilities.py: Introduced in Section 5.8. "],["ssh.html", "E Working Remotely E.1 Logging In E.2 Copying Files E.3 Running Commands E.4 Creating Keys E.5 Dependencies", " E Working Remotely When the Internet was young, people didn’t encrypt anything except the most sensitive information when sending it over a network. However, this meant that villains could steal usernames and passwords. The SSH protocol was invented to prevent this (or at least slow it down). It uses several sophisticated (and heavily tested) encryption protocols to ensure that outsiders can’t see what’s in the messages going back and forth between different computers. To understand how it works, let’s take a closer look at what happens when we use the shell on a desktop or laptop computer. The first step is to log in so that the operating system knows who we are and what we’re allowed to do. We do this by typing our username and password; the operating system checks those values against its records, and if they match, runs a shell for us. As we type commands, characters are sent from our keyboard to the shell. It displays those characters on the screen to represent what we type, and then executes the command and displays its output (if any). If we want to run commands on another machine, such as the server in the basement that manages our database of experimental results, we have to log in to that machine so that our commands will go to it instead of to our laptop. We call this a remote login. E.1 Logging In In order for us to be able to login, the remote computer must run a remote login server and we must run a program that can talk to that server. The client program passes our login credentials to the remote login server; if we are allowed to login, that server then runs a shell for us on the remote computer (Figure E.1). Figure E.1: The local client connects to the remote login server, which starts a remote shell (solid lines). Commands from the local client are passed to the remote shell and output is passed back (dashed lines). Once our local client is connected to the remote server, everything we type into the client is passed on, by the server, to the shell running on the remote computer. That remote shell runs those commands on our behalf, just as a local shell would, then sends back output, via the server, to our client, for our computer to display. The remote login server which accepts connections from client programs is known as the SSH daemon, or sshd. The client program we use to login remotely is the secure shell, or ssh. It has a companion program called scp that allows us to copy files to or from a remote computer using the same kind of encrypted connection. To make a remote login, we issue the command ssh username@computer which tries to make a connection to the SSH daemon running on the remote computer we have specified. After we log in, we can use the remote shell to use the remote computer’s files and directories. Typing exit or Control-D terminates the remote shell, and the local client program, and returns us to our previous shell. In the example below, the remote machine’s command prompt is moon&gt; instead of $ to make it clearer which machine is doing what. $ pwd /Users/amira $ ssh amira@moon.euphoric.edu Password: ******** moon&gt; hostname moon moon&gt; pwd /Users/amira moon&gt; ls -F bin/ cheese.txt dark_side/ rocks.cfg moon&gt; exit $ pwd /Users/amira E.2 Copying Files To copy a file, we specify the source and destination paths, either of which may include computer names. If we leave out a computer name, scp assumes we mean the machine we’re running on. For example, this command copies our latest results to the backup server in the basement, printing out its progress as it does so: $ scp results.dat amira@backupserver:backups/results-2019-11-11.dat Password: ******** results.dat 100% 9 1.0 MB/s 00:00 Note the colon :, separating the hostname of the server and the pathname of the file we are copying to. It is this character that informs scp that the source or target of the copy is on the remote machine and the reason it is needed can be explained as follows: In the same way that the default directory into which we are placed when running a shell on a remote machine is our home directory on that machine, the default target, for a remote copy, is also the home directory. This means that: $ scp results.dat amira@backupserver: would copy results.dat into our home directory on backupserver, however, if we did not have the colon to inform scp of the remote machine, we would still have a valid command: $ scp results.dat amira@backupserver but now we have merely created a file called amira@backupserver on our local machine, as we would have done with cp. $ cp results.dat amira@backupserver Copying a whole directory between remote machines uses the same syntax as the cp command: we just use the -r option to signal that we want copying to be recursively. For example, this command copies all of our results from the backup server to our laptop: $ scp -r amira@backupserver:backups ./backups Password: ******** results-2019-09-18.dat 100% 7 1.0 MB/s 00:00 results-2019-10-04.dat 100% 9 1.0 MB/s 00:00 results-2019-10-28.dat 100% 8 1.0 MB/s 00:00 results-2019-11-11.dat 100% 9 1.0 MB/s 00:00 E.3 Running Commands Here’s one more thing the ssh client program can do for us. Suppose we want to check whether we have already created the file backups/results-2019-11-12.dat on the backup server. Instead of logging in and then typing ls, we could do this: $ ssh amira@backupserver &quot;ls results*&quot; Password: ******** results-2019-09-18.dat results-2019-10-28.dat results-2019-10-04.dat results-2019-11-11.dat Here, ssh takes the argument after our remote username and passes them to the shell on the remote computer. (We have to put quotes around it to make it look like a single argument.) Since those arguments are a legal command, the remote shell runs ls results for us and sends the output back to our local shell for display. E.4 Creating Keys Typing our password over and over again is annoying, especially if the commands we want to run remotely are in a loop. To remove the need to do this, we can create an SSH key to tell the remote machine that it should always trust us. SSH keys come in pairs, a public key that gets shared with services like GitHub, and a private key that is stored only on our computer. If the keys match, we are granted access. The cryptography behind SSH keys ensures that no one can reverse engineer our private key from the public one. We might already have an SSH key pair on your machine. We can check by moving to your .ssh directory and listing the contents. $ cd ~/.ssh $ ls If we see id_rsa.pub, we already have a key pair and don’t need to create a new one. If we don’t see id_rsa.pub, this command will generate a new key pair. (Make sure to replace your@email.com with your own email address.) $ ssh-keygen -t rsa -C &quot;your@email.com&quot; When asked where to save the new key, press enter to accept the default location. Generating public/private rsa key pair. Enter file in which to save the key (/Users/username/.ssh/id_rsa): We will then be asked to provide an optional passphrase. This can be used to make your key even more secure, but if we want to avoid typing our password every time we can skip it by pressing enter twice: Enter passphrase (empty for no passphrase): Enter same passphrase again: When key generation is complete, we should see the following confirmation: Your identification has been saved in /Users/username/.ssh/id_rsa. Your public key has been saved in /Users/username/.ssh/id_rsa.pub. The key fingerprint is: 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your@email.com The key&#39;s randomart image is: +--[ RSA 2048]----+ | | | | | . E + | | . o = . | | . S = o | | o.O . o | | o .+ . | | . o+.. | | .+=o | +-----------------+ (The random art image is an alternate way to match keys.) We now need to place a copy of our public key on any servers we would like to to connect to. Display the contents of our public key file with cat: $ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA879BJGYlPTLIuc9/R5MYiN4yc/YiCL cdBpSdzgK9Dt0Bkfe3rSz5cPm4wmehdE7GkVFXrBJ2YHqPLuM1yx1AUxIebpwlIl9f /aUHOts9eVnVh4NztPy0iSU/Sv0b2ODQQvcy2vYcujlorscl8JjAgfWsO3W4iGEe6Q wBpVomcME8IU35v5VbylM9ORQa6wvZMVrPECBvwItTY8cPWH3MGZiK/74eHbSLKA4P Y3gM4GHI450Nie16yggEg2aTQfWA1rry9JYWEoHS9pJ1dnLqZU3k/8OWgqJrilwSoC 5rGjgp93iu0H8T6+mEHGRQe84Nk1y5lESSWIbn6P636Bl3uQ== your@email.com Copy the contents of the output, then log in to the remote server as usual: $ ssh amira@moon.euphoric.edu Password: ******** Paste the copied content at the end of ~/.ssh/authorized_keys. moon&gt; nano ~/.ssh/authorized_keys After appending the content, log out of the remote machine and try to log in again. If we set up the SSH key correctly we won’t need to type our password: moon&gt; exit $ ssh amira@moon.euphoric.edu E.5 Dependencies The example of copying our public key to a remote machine, so that it can then be used when we next SSH into that remote machine, assumed that we already had a directory ~/.ssh/. While a remote server may support the use of SSH to login, your home directory there may not contain a .ssh directory by default. We have already seen that we can use SSH to run commands on remote machines, so we can ensure that everything is set up as required before we place the copy of our public key on a remote machine. Walking through this process allows us to highlight some of the typical requirements of the SSH protocol itself, as documented in the man-page for the ssh command. Firstly, we check that we have a .ssh/ directory on another remote machine, comet $ ssh amira@comet &quot;ls -ld ~/.ssh&quot; Password: ******** ls: cannot access /Users/amira/.ssh: No such file or directory Oops: we should create the directory and check that it’s there: $ ssh amira@comet &quot;mkdir ~/.ssh&quot; Password: ******** $ ssh amira@comet &quot;ls -ld ~/.ssh&quot; Password: ******** drwxr-xr-x 2 amira amira 512 Jan 01 09:09 /Users/amira/.ssh Now we have a .ssh directory, into which to place SSH-related files, but we can see that the default permissions allow anyone to inspect the files within that directory. For a protocol that is supposed to be secure, this is not considered a good thing and so the recommended permissions are read/write/execute for the user, and not accessible by others. Let’s alter the permissions on the directory: $ ssh amira@comet &quot;chmod 700 ~/.ssh; ls -ld ~/.ssh&quot; Password: ******** drwx------ 2 amira amira 512 Jan 01 09:09 /Users/amira/.ssh That looks much better. In the above example, it was suggested that we paste the content of our public key at the end of ~/.ssh/authorized_keys, however as we didn’t have a ~/.ssh/ on this remote machine, we can simply copy our public key over as the initial ~/.ssh/authorized_keys, and of course, we will use scp to do this, even though we don’t yet have passwordless SSH access set up. $ scp ~/.ssh/id_rsa.pub amira@comet:.ssh/authorized_keys Password: ******** Note that the default target for the scp command on a remote machine is the home directory, so we have not needed to use the shorthand ~/.ssh/ or even the full path /Users/amira/.ssh/ to our home directory there. Checking the permissions of the file we have just created on the remote machine, also serves to indicate that we no longer need to use our password, because we now have what’s needed to use SSH without it. $ ssh amira@comet &quot;ls -l ~/.ssh&quot; -rw-r--r-- 2 amira amira 512 Jan 01 09:11 /Users/amira/.ssh/ authorized_keys While the authorized keys file is not considered to be highly sensitive, (after all, it contains public keys), we alter the permissions to match the man page’s recommendations $ ssh amira@comet &quot;chmod go-r ~/.ssh/authorized_keys; ls -l ~/.ssh&quot; -rw------- 2 amira amira 512 Jan 01 09:11 /Users/amira/.ssh/ authorized_keys "],["style.html", "F Writing Readable Code F.1 Python Style F.2 Order F.3 Checking Style F.4 Refactoring F.5 Code Reviews F.6 Python Features F.7 Summary", " F Writing Readable Code Nothing in biology makes sense except in light of evolution (Dobzhansky 1973). Similarly, nothing in software development makes sense except in light of human psychology. This is particularly true when we look at programming style. Computers don’t need to understand programs in order to execute them, but people do if they are to create, debug, and extend them. Throughout this book we have written code to analyze word counts in classic novels using good Python style. This appendix discusses the style choices we made, presents guidelines for good Python programming style, and introduces some language features that can make programs more flexible and more readable. F.1 Python Style The single most important rule of style is to be consistent, both internally and with other programs (Kernighan and Pike 1999). Python’s standard style is called PEP-8; the acronym “PEP” is short for “Python Enhancement Proposal”, and PEP-8 lays out the rules that Python’s own libraries use. Some of its rules are listed below, along with others borrowed from “Code Smells and Feels”: F.1.1 Spacing Always indent code blocks using 4 spaces, and use spaces instead of tabs. Python doesn’t actually require consistent indentation so long as each block is indented the same amount, which means that this is legal: def transpose(original): result = Matrix(original.numRow, original.numCol) for row in range(original.numRow): for col in range(original.numCol): result[row, col] = original[col, row] return result The same block of code is much more readable when written as: def transpose(original): result = Matrix(original.numRow, original.numCol) for row in range(original.numRow): for col in range(original.numCol): result[row, col] = original[col, row] return result The use of 4 spaces is a compromise between 2 (which we find perfectly readable, but some people find too crowded) and 8 (which most people agree uses up too much horizontal space). As for the use of spaces rather than tabs, the original reason was that the most common interpretation of tabs by the editors of the 1980s was 8 spaces, which again was more than most people felt necessary. Today, almost all editors will auto-indent or auto-complete when the tab key is pressed (or insert spaces, if configured to do so), but the legacy of those ancient times lives on. Do not put spaces inside parentheses. Write (1+2) instead of ( 1+2 ). This applies to function calls as well: write max(a, b) rather than max( a, b ). (We will see a related rule when we discussed default parameter values in Section F.6.) Always use spaces around comparisons like &gt; and &lt;=. Python automatically interprets a+b&lt;c+d as (a+b)&lt;(c+d), but that’s a lot of punctuation crowded together. Using spaces around comparison operators makes it easier to see what’s being compared to what. However, we should use our own judgment for spacing around arithmetic operators like + and /. For example, a+b+c is perfectly readable, but substrate[i, j] + overlay[i, j] is easier for the eye to follow than the spaceless: substrate[i, j]+overlay[i, j] Most programmers would also write: a*b + c*d instead of: a*b+c*d or: (a*b)+(c*d) Adding spaces makes simple expressions more readable, but does not change the way Python interprets them—when it encounters a * b+c, for example, Python still does the multiplication before the addition. Put two blank lines between each function definition. This helps the eye see where one ends and the next begins, though the fact that functions always start in the first column helps as well. Add an empty line at the end of the script. Ending a file in a newline character is required for some other programming languages. Although it’s not required for Python code to function, it does make it easier to view and edit code. F.1.2 Naming Use ALL_CAPS_WITH_UNDERSCORES for global variables. This convention is inherited from C, which was used to write the first version of Python. In that language, upper case was used to indicate a constant whose value couldn’t be modified; Python doesn’t enforce that rule, but SHOUTING_AT_PROGRAMMERS helps remind them that some things shouldn’t be messed with. Use lower_case_with_underscores for the names of functions and variables. Research on naming conventions has produced mixed results (Binkley et al. 2012; Schankin et al. 2018) but Python has (mostly) settled on underscored names for most things. This style is called snake case or pothole case; we should only use CamelCase for classes, which are outside the scope of this lesson. Avoid abbreviations in function and variable names. Abbreviations and acronyms can be ambiguous (does xcl mean “Excel”, “exclude”, or “excellent”?), and can be be hard for non-native speakers to understand. Following this rule doesn’t necessarily require more typing: a good programming editor will auto-complete names for us. Use short names for short-lived local variables and longer names for things with wider scope. Using i and j for loop indices is perfectly readable provided the loop is only a few lines long (Beniamini et al. 2017). Anything that is used at a greater distance or whose purpose isn’t immediately clear (such as a function) should have a longer name. Do not comment and uncomment sections of code to change behavior. If we need to do something in some runs of the program and not in others, use an if statement to enable or disable that block of code: it eliminates the risk of accidentally commenting out one too many lines. If the lines we were removing or commenting out print debugging information, we should replace them with logging calls (Section 12.4). If they are operations that we want to execute, we can add a configuration option (Chapter 10), and if we are sure we don’t need the code, we should take it out completely: we can always get it back from version control (Section 6.11.14). F.2 Order The order of items in each file should be: The shebang line (because it has to be first to work). The file’s documentation string (Section 5.3). All of the import statements, one per line. Global variable definitions (especially things that would be constants in languages that support them). Function definitions. If the file can be run as a program, the if __name__ == '__main__' statement discussed in Section 5.1. That much is clear, but programmers disagree (strongly) on whether high-level functions should come first or last, i.e., whether main should be the first function in the file or the last one. Our scripts put it last, so that it is immediately before the check on __name__. Wherever it goes, main tends to follow one of three patterns: Figure out what the user has asked it to do (Chapter 10). Read all input data. Process it. Write output. or: Figure out what the user has asked for. For each input file: Read. Process. Write file-specific output (if any). Write summary output (if any). or: Figure out what the user has asked for. Repeatedly: Wait for user input. Do what the user has asked. Exit when a “stop” command of some sort is received. Each step in each of the outlines above usually becomes a function. Those functions depend on others, some of which are written to break code into comprehensible chunks and are then called just once, others of which are utilities that may be called many times from many different places. We put all of the single-use functions in the first half of the file in the order in which they are likely to be called, and then put all of the multi-use utility functions in the bottom of the file in alphabetical order. If any of those utility functions are used by other scripts or programs, they should go in a file of their own (Section 5.8). In fact, this is a good practice even if those functions are only used by one program, since it signals even more clearly which are specific to this program and which are likely to be reused elsewhere. This is why we create collate.py in Section 5.7: we could have kept all of our code in countwords.py, but collating felt like something we might want to do separately. F.3 Checking Style Checking that code conforms to guidelines like PEP-8 would be time consuming if we had to do it manually, but most languages have tools that will check style rules for us. These tools are often called linters, after an early tool called lint that found lint (or fluff) in C code. Python’s linter used to be called pep8 and is now called pycodestyle. To see how it works, let’s look at this program, which is supposed to count the number of stop words in a document: stops = [&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;] def count(ln): n = 0 for i in range(len(ln)): line = ln[i] stuff = line.split() for word in stuff: # print(word) j = stops.count(word) if (j &gt; 0) == True: n = n + 1 return n import sys lines = sys.stdin.readlines() # print(&#39;number of lines&#39;, len(lines)) n = count(lines) print(&#39;number&#39;, n) When we run: $ pycodestyle count_stops.py it prints: src/style/count_stops_before.py:3:1: E302 expected 2 blank lines, found 1 src/style/count_stops_before.py:11:24: E712 comparison to True should be &#39;if cond is True:&#39; or &#39;if cond:&#39; src/style/count_stops_before.py:12:13: E101 indentation contains mixed spaces and tabs src/style/count_stops_before.py:12:13: W191 indentation contains tabs src/style/count_stops_before.py:15:1: E305 expected 2 blank lines after class or function definition, found 1 src/style/count_stops_before.py:15:1: E402 module level import not at top of file which tells us that: We should use two blank lines before the function definition on line 3 and after it on line 15. Using == True or == False is redundant (because x == True is the same as x and x == False is the same as not x). Line 12 uses tabs instead of just spaces. The import on line 15 should be at the top of the file. Fixing these issues gives us: import sys stops = [&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;] def count(ln): n = 0 for i in range(len(ln)): line = ln[i] stuff = line.split() for word in stuff: # print(word) j = stops.count(word) if j &gt; 0: n = n + 1 return n lines = sys.stdin.readlines() # print(&#39;number of lines&#39;, len(lines)) n = count(lines) print(&#39;number&#39;, n) F.4 Refactoring Once a program gets a clean bill of health from pycodestyle, it’s worth having a human being look it over and suggest improvements. To refactor code means to change its structure without changing what it does, like simplifying an equation. It is just as much a part of programming as writing code in the first place: nobody gets things right the first time (Brand 1995), and needs or insights can change over time. Most discussions of refactoring focus on object-oriented programming, but many patterns can and should be used to clean up procedural code. Knowing a few of these patterns helps us create better software and makes it easier to communicate with our peers. F.4.1 Do not repeat values The first and simplest refactoring is “replace value with name”. It tells us to replace magic numbers with names, i.e., to define constants. This can seem ridiculous in simple cases (why define and use inches_per_foot instead of just writing 12?). However, what may be obvious to us when we’re writing code won’t be obvious to the next person, particularly if they are working in a different context (most of the world uses the metric system and doesn’t know how many inches are in a foot). It is also a matter of habit: if we write numbers without explanation in our code for simple cases, we are more likely to do so in complex cases, and more likely to regret it afterward. Using names instead of raw values also makes it easier to understand code when we read it aloud, which is always a good test of its style. Finally, a single value defined in one place is much easier to change than a bunch of numbers scattered throughout our program. We may not think we will have to change it, but then people want to use our software on Mars and we discover that constants aren’t (Mak 2006). # ...before... seconds_elapsed = num_days * 24 * 60 * 60 # ...after... SECONDS_PER_DAY = 24 * 60 * 60 # ...other code... seconds_elapsed = num_days * SECONDS_PER_DAY F.4.2 Do not repeat calculations in loops It’s inefficient to calculate the same value over and over again. It also makes code less readable: if a calculation is inside a loop or a function, readers will assume that it might change each time the code is executed. Our second refactoring, “hoist repeated calculation out of loop”, tells us to move the repeated calculation out of the loop or function. Doing this signals that its value is always the same. And naming that common value helps readers understand what its purpose is. # ...before... for sample in signals: output.append(2 * pi * sample / weight) # ...after... scaling = 2 * pi / weight for sample in signals: output.append(sample * scaling) F.4.3 Replace tests with flags to clarify repeated tests Novice programmers frequently write conditional tests like this: if (a &gt; b) == True: # ...do something... The comparison to True is unnecessary because a &gt; b is a Boolean value that is itself either True or False. Like any other value, Booleans can be assigned to variables, and those variables can then be used directly in tests: was_greater = estimate &gt; 0.0 # ...other code that might change estimate... if was_greater: # ...do something... This refactoring is “replace repeated test with flag”. Again, there is no need to write if was_greater == True: that always produces the same result as if was_greater. Similarly, the equality tests in if was_greater == False is redundant: the expression can simply be written if not was_greater. Creating and using a flag instead of repeating the test is therefore like moving a calculation out of a loop: even if that value is only used once, it makes our intention clearer. # ...before... def process_data(data, scaling): if len(data) &gt; THRESHOLD: scaling = sqrt(scaling) # ...process data to create score... if len(data) &gt; THRESHOLD: score = score ** 2 # ...after... def process_data(data, scaling): is_large_data = len(data) &gt; THRESHOLD if is_large_data: scaling = sqrt(scaling) # ...process data to create score... if is_large_data: score = score ** 2 If it takes many lines of code to process data and create a score, and the test then needs to change from &gt; to &gt;=, we are more likely to get the refactored version right the first time, since the test only appears in one place and its result is given a name. F.4.4 Use in-place operators to avoid duplicating expression An in-place operator, sometimes called an update operator, does a calculation with two values and overwrites one of the values. For example, instead of writing: step = step + 1 we can write: step += 1 In-place operators save us some typing. They also make the intention clearer, and most importantly, they make it harder to get complex assignments wrong. For example: samples[least_factor_index, max(current_offset, offset_limit)] *= scaling_factor is less difficult to read than the equivalent expression: samples[least_factor_index, max(current_offset, offset_limit)] = \\ scaling_factor * samples[least_factor_index, max(current_limit, offset_limit)] (The proof of this claim is that you probably didn’t notice that the long form uses different expressions to index samples on the left and right of the assignment.) The refactoring “use in-place operator” does what its name suggests: converts normal assignments into their briefer equivalents. # ...before... for least_factor in all_factors: samples[least_factor] = \\ samples[least_factor] * bayesian_scaling # ...after... for least_factor in all_factors: samples[least_factor] *= bayesian_scaling F.4.5 Handle special cases first A short circuit test is a quick check to handle a special case, such as checking the length of a list of values and returning math.nan for the average if the list is empty. “Place short circuits early” tells us to put short-circuit tests near the start of functions so that readers can mentally remove special cases from their thinking while reading the code that handles the usual case. # ...before... def rescale_by_average(values, factors, weights): a = 0.0 for (f, w) in zip(factors, weights): a += f * w if a == 0.0: return a /= len(f) if not values: return else: for (i, v) in enumerate(values): values[i] = v / a # ...after... def rescale_by_average(values, factors, weights): if (not values) or (not factors) or (not weights): return a = 0.0 for (f, w) in zip(factors, weights): a += f * w a /= len(f) for (i, v) in enumerate(values): values[i] = v / a Return Consistently PEP-8 says, “Be consistent in return statements,” and goes on to say that either all return statements in a function should return a value, or none of them should. If a function contains any explicit return statements at all, it should end with one as well. A related refactoring pattern is “default and override”. To use it, assign a default or most common value to a variable unconditionally, and then override it in a special case. The result is fewer lines of code and clearer control flow; however, it does mean executing two assignments instead of one, so it shouldn’t be used if the common case is expensive (e.g., involves a database lookup or a web request). # ...before.. if configuration[&#39;threshold&#39;] &gt; UPPER_BOUND: scale = 0.8 else: scale = 1.0 # ...after... scale = 1.0 if configuration[&#39;threshold&#39;] &gt; UPPER_BOUND: scale = 0.8 In simple cases, people will sometimes put the test and assignment on a single line: scale = 1.0 if configuration[&#39;threshold&#39;] &gt; UPPER_BOUND: scale = 0.8 Some programmers take this even further and use a conditional expression: scale = 0.8 if configuration[&#39;threshold&#39;] &gt; UPPER_BOUND else 1.0 However, this puts the default last instead of first, which is less clear. A Little Jargon X if test else Y is called a ternary expression. Just as a binary expression like A + B has two parts, a ternary expression has three. Conditional expressions are the only ternary expression in most programming languages. F.4.6 Use functions to make code more comprehensible Functions were created so that programmers could re-use common operations, but moving code into functions also reduces cognitive load by reducing the number of things that have to be understood simultaneously. A common rule of thumb is that no function should be longer than a printed page (about 80 lines) or have more than four levels of indentation because of nested loops and conditionals. Anything longer or more deeply nested is hard for readers to understand, so we should moves pieces of long functions into small ones. # ...before... def check_neighbors(grid, point): if (0 &lt; point.x) and (point.x &lt; grid.width-1) and \\ (0 &lt; point.y) and (point.y &lt; grid.height-1): # ...look at all four neighbors # ...after.. def check_neighbors(grid, point): if in_interior(grid, point): # ...look at all four neighbors... def in_interior(grid, point): return \\ (0 &lt; point.x) and (point.x &lt; grid.width-1) and \\ (0 &lt; point.y) and (point.y &lt; grid.height-1) We should always extract functions when code can be re-used. Even if they are only used once, multi-part conditionals, long equations, and the bodies of loops are good candidates for extraction. If we can’t think of a plausible name, or if a lot of data has to be passed into the function after it’s extracted, the code should probably be left where it is. Finally, it’s often helpful to keep using the original variable names as parameter names during refactoring to reduce typing. F.4.7 Combine operations in functions “Combine functions” is the opposite of “extract function”. If operations are always done together, it can sometimes be be more efficient to do them together, and might be easier to understand. However, combining functions often reduces their reusability and readability. (One sign that functions shouldn’t have been combined is people using the combination and throwing some of the result away.) The fragment below shows how two functions can be combined: # ...before... def count_vowels(text): num = 0 for char in text: if char in VOWELS: num += 1 return num def count_consonants(text): num = 0 for char in text: if char in CONSONANTS: num += 1 return num # ...after... def count_vowels_and_consonants(text): num_vowels = 0 num_consonants = 0 for char in text: if char in VOWELS: num_vowels += 1 elif char in CONSONANTS: num_consonants += 1 return num_vowels, num_consonants F.4.8 Replace code with data It is easier to understand and maintain lookup tables than complicated conditionals, so the “create lookup table” refactoring tells us to turn the latter into the former: # ...before.. def count_vowels_and_consonants(text): num_vowels = 0 num_consonants = 0 for char in text: if char in VOWELS: num_vowels += 1 elif char in CONSONANTS: num_consonants += 1 return num_vowels, num_consonants # ...after... IS_VOWEL = {&#39;a&#39; : 1, &#39;b&#39; : 0, &#39;c&#39; : 0, ... } IS_CONSONANT = {&#39;a&#39; : 0, &#39;b&#39; : 1, &#39;c&#39; : 1, ... } def count_vowels_and_consonants(text): num_vowels = num_consonants = 0 for char in text: num_vowels += IS_VOWEL[char] num_consonants += IS_CONSONANT[char] return num_vowels, num_consonants The more cases there are, the greater the advantage lookup tables have over multi-part conditionals. Those advantages multiply when items can belong to more than one category, in which case the table is often best written as a dictionary with items as keys and sets of categories as values: LETTERS = { &#39;A&#39; : {&#39;vowel&#39;, &#39;upper_case&#39;}, &#39;B&#39; : {&#39;consonant&#39;, &#39;upper_case&#39;}, # ...other upper-case letters... &#39;a&#39; : {&#39;vowel&#39;, &#39;lower_case&#39;}, &#39;b&#39; : {&#39;consonant&#39;, &#39;lower_case&#39;}, # ...other lower-case letters... &#39;+&#39; : {&#39;punctuation&#39;}, &#39;@&#39; : {&#39;punctuation&#39;}, # ...other punctuation... } def count_vowels_and_consonants(text): num_vowels = num_consonants = 0 for char in text: num_vowels += int(&#39;vowel&#39; in LETTERS[char]) num_consonants += int(&#39;consonant&#39; in LETTERS[char]) return num_vowels, num_consonants The expressions used to update num_vowels and num_consonants make use of the fact that in produces either True or False, which the function int converts to either 1 or 0. We will explore ways of making this code more readable in the exercises. F.5 Code Reviews At the end of Section F.3, our stop-word program looked like this: import sys stops = [&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;] def count(ln): n = 0 for i in range(len(ln)): line = ln[i] stuff = line.split() for word in stuff: # print(word) j = stops.count(word) if j &gt; 0: n = n + 1 return n lines = sys.stdin.readlines() # print(&#39;number of lines&#39;, len(lines)) n = count(lines) print(&#39;number&#39;, n) This passes a PEP-8 style check, but based on our coding guidelines and our discussion of refactoring, these things should be changed: The commented-out print statements should either be removed or turned into logging statements (Section 12.4). The variables ln, i, and j should be given clearer names. The outer loop in count loops over the indices of the line list rather than over the lines. It should do the latter (which will allow us to get rid of the variable i). Rather than counting how often a word occurs in the list of stop words with stops.count, we can turn the stop words into a set and use in to check words. This will be more readable and more efficient. There’s no reason to store the result of line.split in a temporary variable: the inner loop of count can use it directly. Since the set of stop words is a global variable, it should be written in upper case. We should use += to increment the counter n. Rather than reading the input into a list of lines and then looping over that, we can give count a stream and have it process the lines one by one. Since we might want to use count in other programs some day, we should put the two lines at the bottom that handle input into a conditional so that they aren’t executed when this script is imported. After making all these changes, our program looks like this: import sys STOPS = {&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;} def count(reader): n = 0 for line in reader: for word in line.split(): if word in STOPS: n += 1 return n if __name__ == &#39;__main__&#39;: n = count(sys.stdin) print(&#39;number&#39;, n) Reading code in order to find bugs and suggest improvements like these is called code review. Multiple studies over more than 40 years have shown that code review is the most effective way to find bugs in software (Fagan 1976, 1986; Cohen 2010; Bacchelli and Bird 2013). It is also a great way to transfer knowledge between programmers: reading someone else’s code critically will give us lots of ideas about what we could do better, and highlight things that we should probably stop doing as well. Despite this, code review still isn’t common in research software development. This is partly a chicken-and-egg problem: people don’t do it because other people don’t do it (Segal 2005). Code review is also more difficult to do in specialized scientific fields: in order for review to be useful, reviewers need to understand the problem domain well enough to comment on algorithms and design choices rather than indentation and variable naming, and the number of people who can do that for a research project is often very small (Petre and Wilson 2014). Section 7.9 explained how to create and merge pull requests. How we review these is just as important as what we look for: being dismissive or combative are good ways to ensure that people don’t pay attention to our reviews, or avoid having us review their work (Bernhardt 2018). Equally, being defensive when someone offers suggestions politely and sincerely is very human, but can stunt our development as a programmer. Lots of people have written guidelines for doing reviews that avoid these traps (Quenneville 2018; Sankarram 2018). A few common points are: Work in small increments. As Cohen (2010) and others have found, code review is most effective when done in short bursts. That means that change requests should also be short: anything that’s more than a couple of screens long should be broken into smaller pieces. Look for algorithmic problems first. Code review isn’t just (or even primarily) about style: its real purpose is to find bugs before they can affect anyone. The first pass over any change should therefore look for algorithmic problems. Are the calculations right? Are any rare cases going to be missed? Are errors being caught and handled (Chapter 12)? Using a consistent style helps reviewers focus on these issues. Use a checklist. Linters are great, but can’t decide when someone should have used a lookup table instead of conditionals. A list of things to check for can make review faster and more comprehensible, especially when we can copy-and-paste or drag-and-drop specific comments onto specific lines (something that GitHub unfortunately doesn’t yet support). Ask for clarification. If we don’t understand something, or don’t understand why the author did it, we should ask. (When the author explains it, we might suggest that the explanation should be documented somewhere.) Offer alternatives. Telling authors that something is wrong is helpful; telling them what they might do instead is more so. Don’t be sarcastic or disparaging. “Did you maybe think about testing this garbage?” is a Code of Conduct violation in any well-run project. Don’t present opinions as facts. “Nobody uses X any more” might be true. If it is, the person making the claim ought to be able to point at download statistics or a Google Trends search; if they can’t, they should say, “I don’t think we use X any more” and explain why they think that. Don’t feign surprise or pass judgment. “Gosh, didn’t you know [some obscure fact]?” isn’t helpful; neither is, “Geez, why don’t you [some clever trick] here?” Don’t overwhelm people with details. If someone has used the letter x as a variable name in several places, and they shouldn’t have, comment on the first two or three and simply put a check beside the others—the reader won’t need the comment repeated. Don’t try to sneak in feature requests. Nobody enjoys fixing bugs and style violations. Asking them to add entirely new functionality while they’re at it is rude. How we respond to reviews is just as important: Be specific in replies to reviewers. If someone has suggested a better variable name, we can probably simply fix it. If someone has suggested a major overhaul to an algorithm, we should reply to their comment to point at the commit that includes the fix. Thank our reviewers. If someone has taken the time to read our code carefully, thank them for doing it. And finally: Don’t let anyone break these rules just because they’re frequent contributors or in positions of power. The culture of any organization is shaped by the worst behavior it is willing to tolerate Gruenert and Whitaker (2015). The main figures in a project should be more respectful than everyone else in order to show what standards everyone else is expected to meet. F.6 Python Features Working memory can only hold a few items at once: initial estimates in the 1950s put the number at 7±2 (Miller 1956), and more recent estimates put it as low as 4 or 5. High-level languages from Fortran to Python are essentially a way to reduce the number of things programmers have to think about at once so that they can fit what the computer is doing into this limited space. The sections below describe some of these features; as we become more comfortable with Python we will find and use others. But beware: the things that make programs more compact and comprehensible for experienced programmers can make them less comprehensible for novices. For example, suppose we want to create this matrix as a list of lists: [[0, 1, 2, 3, 4], [1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7], [4, 5, 6, 7, 8]] One way is to use loops: matrix = [] for i in range(5): row = [] for j in range(5): row.append(i+j) matrix.append(row) Another is to use a nested list comprehension: [[i+j for j in range(5)] for i in range(5)] An experienced programmer might recognize what the latter is doing; the rest of us are probably better off reading and writing the more verbose solution. F.6.1 Provide default values for parameters If our function requires two dozen parameters, the odds are very good that users will frequently forget them or put them in the wrong order. One solution is to bundle parameters together so that (for example) people pass three point objects instead of nine separate x, y, and z values. A second approach (which can be combined with the previous one) is to specify default values for some of the parameters. Doing this gives users control over everything while also allowing them to ignore details; it also indicates what we consider “normal” for the function. For example, suppose we are comparing images to see if they are the same or different. We can specify two kinds of tolerance: how large a difference in color value to notice, and how many differences above that threshold to tolerate as a percentage of the total number of pixels. By default, any color difference is considered significant, and only 1% of pixels are allowed to differ: def image_diff(left, right, per_pixel=0, fraction=0.01): # ...implementation... When this function is called using image_diff(old, new), those default values apply. However, it can also be called like this: image_diff(old, new, per_pixel=2) allows pixels to differ slightly without those differences being significant. image_diff(old, new, fraction=0.05) allows more pixels to differ. image_diff(old, new, per_pixel=1, fraction=0.005) raises the per-pixel threshold but decrease number of allowed differences. Note that we do not put spaces around the = when defining a default parameter value. This is consistent with PEP-8’s rules about spacing in function definitions and calls (Section F.1). Default parameter values make code easier to understand and use, but there is a subtle trap. When Python executes a function definition like this: def collect(new_value, accumulator=set()): accumulator.add(new_value) return accumulator it calls set() to create a new empty set when it is reading the function definition, and then uses that set as the default value for accumulator every time the function is called. It does not call set() once for each call, so all calls using the default will share the same set: &gt;&gt;&gt; collect(&#39;first&#39;) {&#39;first&#39;} &gt;&gt;&gt; collect(&#39;second&#39;) {&#39;first&#39;, &#39;second&#39;} A common way to avoid this is to pass None to the function to signal that the user didn’t provide a value: def collect(new_value, accumulator=None): if accumulator is None: accumulator = set() accumulator.add(new_value) return accumulator F.6.2 Handle a variable number of arguments We can often make programs simpler by writing functions that take a variable number of arguments, just like print and max. One way to to require user to stuff those arguments into a list, e.g., to write find_limits([a, b, c, d]). However, Python can do this for us. If we declare a single argument whose name starts with a single *, Python will put all “extra” arguments into a tuple and pass that as the argument. By convention, this argument is called args: def find_limits(*args): print(args) find_limits(1, 3, 5, 2, 4) (1, 3, 5, 2, 4) This catch-all parameter can be used with regular parameters, but must come last in the parameter list to avoid ambiguity: def select_outside(low, high, *values): result = [] for v in values: if (v &lt; low) or (v &gt; high): result.add(v) return result print(select_outside(0, 1.0, 0.3, -0.2, -0.5, 0.4, 1.7)) [-0.2, -0.5, 1.7] An equivalent special form exists for keyword arguments: the catch-all variable’s name is prefixed with ** (i.e., two asterisks instead of one), and it is conventionally called kwargs (for “keyword arguments”). When this is used, the function is given a dictionary of names and values rather than a list: def set_options(tag, **kwargs): result = f&#39;&lt;{tag}&#39; for key in kwargs: result += f&#39; {key}=&quot;{kwargs[key]}&quot;&#39; result += &#39;/&gt;&#39; return result print(set_options(&#39;h1&#39;, color=&#39;blue&#39;)) print(set_options(&#39;p&#39;, align=&#39;center&#39;, size=&#39;150%&#39;)) &lt;h1 color=&quot;blue&quot;/&gt; &lt;p align=&quot;center&quot; size=&quot;150%&quot;/&gt; Notice that the names of parameters are not quoted: we pass color='blue' to the function, not 'color'='blue'. F.6.3 Unpacking variable arguments We can use the inverse of *args and **kwargs to match a list of values to arguments. In this case, we put the * in front of a list and ** in front of a dictionary when calling the function, rather than in front of the parameter when defining it: def trim_value(data, low, high): print(data, &quot;with&quot;, low, &quot;and&quot;, high) parameters = [&#39;some matrix&#39;, &#39;lower bound&#39;] named_parameters = {&#39;high&#39;: &#39;upper bound&#39;} trim_value(*parameters, **named_parameters) some matrix with lower bound and upper bound F.6.4 Use destructuring to assign multiple values at once One last feature of Python is destructuring assignment. Suppose we have a nested list such as [1, [2, 3]], and we want to assign its numbers to three variables called first, second, and third. Instead of writing this: first = values[0] second = values[1][0] third = values[1][1] we can write this: [first, [second, third]] = [1, [2, 3]] In general, if the variables on the left are arranged in the same way as the values on the right, Python will automatically unpack the values and assign them correctly. This is particularly useful when looping over lists of structured values: people = [ [[&#39;Kay&#39;, &#39;McNulty&#39;], &#39;mcnulty@eniac.org&#39;], [[&#39;Betty&#39;, &#39;Jennings&#39;], &#39;jennings@eniac.org&#39;], [[&#39;Marlyn&#39;, &#39;Wescoff&#39;], &#39;mwescoff@eniac.org&#39;] ] for [[first, last], email] in people: print(&#39;{first} {last} &lt;{email}&gt;&#39;) Kay McNulty &lt;mcnulty@eniac.org&gt; Betty Jennings &lt;jennings@eniac.org&gt; Marlyn Wescoff &lt;mwescoff@eniac.org&gt; F.7 Summary George Orwell laid out six rules for good writing, the last and most important of which is, “Break any of these rules sooner than say anything outright barbarous.” PEP-8 conveys the same message: there will always be cases where your code will be easier to understand if you don’t do the things described in this lesson, but there are probably fewer of them than you think. "],["documentation.html", "G Documenting Programs G.1 Writing Good Docstrings G.2 Defining Your Audience G.3 Creating a FAQ", " G Documenting Programs An old proverb says, “Trust, but verify.” The equivalent in programming is, “Be clear, but document.” No matter how well software is written, it always embodies decisions that aren’t explicit in the final code or accommodates complications that aren’t going to be obvious to the next reader. Putting it another way, the best function names in the world aren’t going to answer the questions “Why does the software do this?” and “Why doesn’t it do this in a simpler way?” In this appendix we address some issues that commonly arise when people start to think about documenting their software. We start with tips for writing good docstrings, before considering more detailed documentation like tutorials, cookbooks and frequently-asked question (FAQ) lists. Clearly defining your audience can make it easier to determine which types of documentation you need to provide, and there are a number of things you can do to reduce the work involved in providing support for FAQs. G.1 Writing Good Docstrings If we are doing exploratory programming, a short docstring to remind ourselves of each function’s purpose is probably as much documentation as we need. (In fact, it’s probably better than what most people do.) That one- or two-liner should begin with an active verb and describe either how inputs are turned into outputs, or what side effects the function has; as we discuss below, if we need to describe both, we should probably rewrite our function. An active verb is something like “extract”, “normalize”, or “plot”. For example, here’s a function that calculates someone’s age from their birthday, which uses a one-line docstring beginning with the active verb “find”: import re from datetime import date def calculate_age(birthday): &quot;&quot;&quot;Find current age from birth date.&quot;&quot;&quot; valid_date = &#39;([0-9]{4})-([0-9]{2})-([0-9]{2})$&#39; if not bool(re.search(valid_date, birthday)): message = &#39;Birthday must be in YYYY-MM-DD format&#39; raise ValueError(message) today = date.today() born = date.fromisoformat(birthday) no_bday_this_year_yet = (today.month, today.day) &lt; (born.month, born.day) age = today.year - born.year - no_bday_this_year_yet return age Other examples of good one-line docstrings include: “Create a list of capital cities from a list of countries.” “Clip signals to lie in [0…1].” “Reduce the red component of each pixel.” We can tell our one-liners are useful if we can read them aloud in the order the functions are called in place of the function’s name and parameters. Once we start writing code for other people (or our future selves) our docstrings should include: The name and purpose of every public class, function, and constant in our code. The name, purpose, and default value (if any) of every parameter to every function. Any side effects the function has. The type of value returned by every function. What exceptions those functions can raise and when. The word “public” in the first rule is important. We don’t have to write full documentation for helper functions that are only used inside our package and aren’t meant to be called by users, but these should still have at least a comment explaining their purpose. Here’s the previous function with a more complete docstring: def calculate_age(birthday): &quot;&quot;&quot;Find current age from birth date. :param birthday str: birth date :returns: age in years :rtype: int :raises ValueError: if birthday not in YYYY-MM-DD format &quot;&quot;&quot; We could format and organise the information in the docstring any way we like, but here we’ve decided to use reStructuredText, which is the default plain-text markup format supported by Sphinx (Section 14.6). The Sphinx documentation describes the precise syntax for parameters, returns, exceptions and other items that are typically included in a docstring. While the reStructuredText docstring format suggested in the Sphinx documentation looks nice once Sphinx parses and converts it to HTML for the web, the code itself is somewhat dense and hard to read. To address this issue, a number of different formats have been proposed. Two of the most prominent are Google style: def calculate_age(birthday): &quot;&quot;&quot;Find current age from birth date. Args: birthday (str): birth date. Returns: Age in years. Raises: ValueError: If birthday not in YYYY-MM-DD format. &quot;&quot;&quot; and numpydoc style: def calculate_age(birthday): &quot;&quot;&quot;Find current age from birth date. Parameters ---------- birthday : string birth date Returns ------- integer age in years Raises ------ ValueError if birthday not in YYYY-MM-DD format &quot;&quot;&quot; These two formats have become so popular that a Sphinx extension called Napoleon has been released. It parses numpydoc and Google style docstrings and converts them to reStructuredText before Sphinx attempts to parse them. This happens in an intermediate step while Sphinx is processing the documentation, so it doesn’t modify any of the docstrings in your actual source code files. The choice between the numpydoc and Google styles is largely aesthetic, but the two styles should not be mixed. Choose one style for your project and be consistent with it. G.2 Defining Your Audience It’s important to consider who documentation is for. There are three kinds of people in any domain: novices, competent practitioners, and experts (Wilson 2019a). A novice doesn’t yet have a mental model of the domain: they don’t know what the key terms are, how they relate, what the causes of their problems are, or how to tell whether a solution to their problem is appropriate or not. Competent practitioners know enough to accomplish routine tasks with routine effort: they may need to check Stack Overflow every few minutes, but they know what to search for and what “done” looks like. Finally, experts have such a deep and broad understanding of the domain that they can solve routine problems at a glance and are able to handle the one-in-a-thousand cases that would baffle the merely competent. Each of these three groups needs a different kind of documentation: A novice needs a tutorial that introduces her to key ideas one by one and shows how they fit together. A competent practitioner needs reference guides, cookbooks, and Q&amp;A sites; these give her solutions close enough to what she needs that she can tweak them the rest of the way. Experts need this material as well—nobody’s memory is perfect—but they may also paradoxically want tutorials. The difference between them and novices is that experts want tutorials on how things work and why they were designed that way. The first thing to decide when writing documentation is therefore to decide which of these needs we are trying to meet. Tutorials like this book should be long-form prose that contain code samples and diagrams. They should use authentic tasks to motivate ideas, i.e., show people things they actually want to do rather than printing the numbers from 1 to 10, and should include regular check-ins so that learners and instructors alike can tell if they’re making progress. Tutorials help novices build a mental model, but competent practitioners and experts will be frustrated by their slow pace and low information density. They will want single-point solutions to specific problems like how to find cells in a spreadsheet that contain a certain string or how to configure the web server to load an access control module. They can make use of an alphabetical list of the functions in a library, but are much happier if they can search by keyword to find what they need; one of the signs that someone is no longer a novice is that they’re able to compose useful queries and tell if the results are on the right track or not. False Beginners A false beginner is someone who appears not to know anything, but who has enough prior experience in other domains to be able to piece things together much more quickly than a genuine novice. Someone who is proficient with MATLAB, for example, will speed through a tutorial on Python’s numerical libraries much more quickly than someone who has never programmed before. Creating documentation for false beginners is especially challenging; if resources permit, the best option is often a translation guide that shows them how they would do a task with the system they know well and then how to do the equivalent task with the new system. In an ideal world, we would satisfy these needs with a chorus of explanations (Caulfield 2016), some long and detailed, others short and to the point. In our world, though, time and resources are limited, so all but the most popular packages must make do with single explanations. G.3 Creating a FAQ As projects grow, documentation within functions alone may be insufficient for users to apply code to their own problems. One strategy to assist other people with understanding a project is with an FAQ: a list of frequently-asked questions and corresponding answers. A good FAQ uses the terms and concepts that people bring to the software rather than the vocabulary of its authors; putting it another way, the questions should be things that people might search for online, and the answers should give them enough information to solve their problem. Creating and maintaining a FAQ is a lot of work, and unless the community is large and active, a lot of that effort may turn out to be wasted, because it’s hard for the authors or maintainers of a piece of software to anticipate what newcomers will be mystified by. A better approach is to leverage sites like Stack Overflow, which is where most programmers are going to look for answers anyway: Post every question that someone actually asks us, whether it’s online, by email, or in person. Be sure to include the name of the software package in the question so that it’s findable. Answer the question, making sure to mention which version of the software we’re talking about (so that people can easily spot and discard stale answers in the future). Stack Overflow’s guide to asking a good question has been refined over many years, and is a good guide for any project: Write the most specific title we can. “Why does division sometimes give a different result in Python 2.7 and Python 3.5?” is much better than, “Help! Math in Python!!” Give context before giving sample code. A few sentences to explain what are are trying to do and why will help people determine if their question is a close match to ours or not. Provide a minimal reprex. Section 8.6 explains the value of a reproducible example, and why reprexes should be as short as possible. Readers will have a much easier time figuring out if this question and its answers are for them if they can see and understand a few lines of code. Tag, tag, tag. Keywords make everything more findable, from scientific papers to left-handed musical instruments. Use “I” and question words (how/what/when/where/why). Writing this way forces us to think more clearly about what someone might actually be thinking when they need help. Keep each item short. The “minimal manual” approach to instructional design (Carroll 2014) breaks everything down into single-page steps, with half of that page devoted to troubleshooting. This may feel trivializing to the person doing the writing, but is often as much as a person searching and reading can handle. It also helps writers realize just how much implicit knowledge they are assuming. Allow for a chorus of explanations. As discussed earlier, users are all different from one another, and are therefore best served by a chorus of explanations. Do not be afraid of providing multiple explanations to a single question that suggest different approaches or are written for different prior levels of understanding. "],["yaml.html", "H YAML", " H YAML YAML is a way to write nested data structures in plain text that is often used to specify configuration options for software. The acronym stands for “YAML Ain’t Markup Language”, but that’s a lie: YAML doesn’t use &lt;tags&gt; like HTML, but can still be quite fussy about what is allowed to appear where. Throughout this book we use YAML to configure our plotting script (Chapter 10), continuous integration with Travis CI (Section 11.8), software environment with conda (Section 13.2.1), and a documentation website with Read the Docs (Section 14.6.3). While you don’t need to be an expert in YAML to complete tasks like these, this appendix outlines some basics that can help make things easier. A simple YAML file has one key-value pair on each line with a colon separating the key from the value: project-name: planet earth purpose: science fair moons: 1 Here, the keys are \"project-name\", \"purpose\", and \"moons\", and the values are \"planet earth\", \"science fair\", and (hopefully) the number 1, since most YAML implementations try to guess the type of data. If we want to create a list of values without keys, we can write it either using square brackets (like a Python array) or dashed items (like a Markdown list), so: rotation-time: [&quot;1 year&quot;, &quot;12 months&quot;, &quot;365.25 days&quot;] and: rotation-time: - 1 year - 12 months - 365.25 days are equivalent. (The indentation isn’t absolutely required in this case, but helps make the intention clear.) If we want to write entire paragraphs, we can use a marker to show that a value spans multiple lines: feedback: | Neat molten core concept. Too much water. Could have used more imaginative ending. We can also add comments using # just as we do in many programming languages. YAML is easy to understand when used this way, but it starts to get tricky as soon as sub-lists and sub-keys appear. For example, this is part of the YAML configuration file for formatting this book: bookdown::gitbook: highlight: tango config: download: [&quot;pdf&quot;, &quot;epub&quot;] toc: collapse: section before: | &lt;li&gt;&lt;a href=&quot;./&quot;&gt;Merely Useful&lt;/a&gt;&lt;/li&gt; sharing: no It corresponds to the following Python data structure: { &#39;bookdown::gitbook&#39;: { &#39;highlight&#39;: &#39;tango&#39;, &#39;config&#39;: { &#39;download&#39;: [ &#39;pdf&#39;, &#39;epub&#39; ], &#39;toc&#39;: { &#39;collapse&#39;: &#39;section&#39;, &#39;before&#39;: &#39;&lt;li&gt;&lt;a href=&quot;./&quot;&gt;Merely Useful&lt;/a&gt;&lt;/li&gt;\\n&#39; } &#39;sharing&#39;: False } } } "],["anaconda.html", "I Anaconda I.1 Package Management With conda I.2 Environment Management With conda", " I Anaconda When people first started using Python for data science, installing the relevant libraries could be difficult. The main problem was that the Python package installer (pip) only worked for libraries written in pure Python. Many scientific Python libraries have C and/or Fortran dependencies, so it was left up to data scientists (who often do not have a background in system administration) to figure out how to install those dependencies themselves. To overcome this problem, a number of scientific Python distributions have been released over the years. These come with the most popular data science libraries and their dependencies pre-installed, and some also come with a package manager to assist with installing additional libraries that weren’t pre-installed. Today the most popular distribution for data science is Anaconda, which comes with a package (and environment) manager called conda. In this appendix we look at some of the most important features of conda for research software engineers. I.1 Package Management With conda According to the latest documentation, Anaconda comes with over 250 of the most widely used data science libraries (and their dependencies) pre-installed. In addition, there are several thousand libraries available via the conda install command, which can be executed at the command line or by using the Anaconda Navigator graphical user interface. A package manager like conda greatly simplifies the software installation process by identifying and installing compatible versions of software and all required dependencies. It also handles the process of updating software as more recent versions become available. If you don’t want to install the entire Anaconda distribution, you can install Miniconda instead. It essentially comes with conda and nothing else. I.1.1 Anaconda Cloud What happens if we want to install a Python package that isn’t on the list of the few thousand or so most popular data science packages (i.e. the ones that are automatically available via the conda install command)? The answer is the Anaconda Cloud website, where the community can post conda installation packages. The utility of the Anaconda Cloud for research software engineers is best illustrated by an example. A few years ago, an atmospheric scientist by the name of Andrew Dawson wrote a Python package called windspharm for performing computations on global wind fields in spherical geometry. While many of Andrew’s colleagues routinely process global wind fields, atmospheric science is a relatively small field and thus the windspharm package will never have a big enough user base to make the list of popular data science packages supported by Anaconda. Andrew has therefore posted a conda installation package to Anaconda Cloud (Figure I.1) so that users can install windspharm using conda: $ conda install -c ajdawson windspharm Figure I.1: The windspharm conda installation package on Anaconda Cloud. The conda documentation has instructions for quickly building a conda package for a Python module that is already available on PyPI (Section 14.5). I.1.2 conda-forge It turns out there are often multiple installation packages for the same library up on Anaconda Cloud (e.g. Figure I.2). To try and address this duplication problem conda-forge was launched, which aims to be a central repository that contains just a single, up-to-date (and working) version of each installation package on Anaconda Cloud. You can therefore expand the selection of packages available via conda install beyond the chosen few thousand by adding the conda-forge channel to your conda configuration: $ conda config --add channels conda-forge Figure I.2: Search results for the windspharm package on Anaconda Cloud. The conda-forge website has instructions for adding a conda installation package to the conda-forge repository. I.2 Environment Management With conda If you are working on several data science projects at once, installing all the libraries you need in the same place (i.e. the system default location) can become problematic. This is especially true if the projects rely on different versions of the same package, or if you are developing a new package and need to try new things. The way to avoid these issues is to create different virtual environments for different projects/tasks. The original environment manager for Python development was virtualenv, which has been more recently superseded by pipenv. The advantage that conda has over these options is that it is language agnostic (i.e. you can isolate non-Python packages in your environments too) and supports binary packages (i.e. you don’t need to compile the source code after installing), so it has become the environment manager of choice in data science. In this book conda is used to export the details of an environment when documenting the computational methodology for a report (Section 13.2) and to test how a new package installs without disturbing anything in our main Python installation (Section 14.2). "],["glossary.html", "J Glossary", " J Glossary abandonware Software that is no longer being maintained. absolute error The absolute value of the difference between the observed and the correct value. Absolute error is usually less useful than relative error. absolute import In Python, an import that specifies the full location of the file to be imported. absolute path A path that points to the same location in the filesystem regardless of where it is evaluated. An absolute path is the equivalent of latitude and longitude in geography. See also: relative path actual result (of test) The value generated by running code in a test. If this matches the expected result, the test passes; if the two are different, the test fails. agile development A software development methodology that emphasizes lots of small steps and continuous feedback instead of up-front planning and long-term scheduling. Exploratory programming is often agile. ally Someone who actively promotes and supports inclusivity. append mode To add data to the end of an existing file instead of overwriting the previous contents of that file. Overwriting is the default, so most programming languages require programs to be explicit about wanting to append instead. assertion A Boolean expression that must be true at a certain point in a program. Assertions may be built into the language (e.g., Python’s assert statement) or provided as functions (e.g., R’s stopifnot). They are often used in testing, but are also put in production code to check that it is behaving correctly. In many languages, assertions should not be used to perform data-validation as they may be silently dropped by compilers and interpreters under optimization conditions. Using assertions for data validation can therefore introduce security risks. Unlike many languages, R does not have an assert statement which can be disabled, and so use of package such as assertr for data validation does not create security holes. authentic task A task which contains important elements of things that learners would do in real (non-classroom situations). auto-completion A feature that allows the user to finish a word or code quickly through the use of pressing the TAB key to list possible words or code from which the user can select. automatic variable A variable that is automatically given a value in a build rule. For example, Make automatically assigns the name of a rule’s target to the automatic variable $@. Automatic variables are frequently used when writing pattern rules. See also: Makefile boilerplate Standard text that is included in legal contracts, licenses, and so on. branch-per-feature workflow A common strategy for managing work with Git and other version control systems in which a separate branch is created for work on each new feature or each bug fix and merged when that work is completed. This isolates changes from one another until they are completed. bug report A collection of files, logs, or related information that describes either an unexpected output of some code or program, or an unexpected error or warning. This information is used to help find and fix a bug in the program or code. bug tracker A system that tracks and manages reported bugs for a software program, to make it easier to address and fix the bugs. build manager A program that keeps track of how files depend on one another and runs commands to update any files that are out-of-date. Build managers were invented to compile only those parts of programs that had changed, but are now often used to implement workflows in which plots depend on results files, which in turn depend on raw data files or configuration files. See also: build rule, Makefile build recipe The part of a build rule that describes how to update something that has fallen out-of-date. build rule A specification for a build manager that describes how some files depend on others and what to do if those files are out-of-date. build target The file(s) that a build rule will update if they are out-of-date compared to their dependencies. See also: Makefile, default target byte code A set of instructions designed to be executed efficiently by an interpreter. call stack A data structure that stores information about the active subroutines executed. camel case A style of writing code that involves naming variables and objects with no space, underscore (_), dot (.), or dash (-) characters, with each word being capitalized. Examples include CalculateSum and findPattern. See also: kebab case, pothole case catch (an exception) To accept responsibility for handling an error or other unexpected event. R prefers “handling a condition” to “catching an exception”. Python, on the other hand, encourages raising and catching exceptions, and in some situations, requires it. Creative Commons license A set of licenses that can be applied to published work. Each license is formed by concatenating one or more of -BY (Attribution): users must cite the original source; -SA (ShareAlike): users must share their own work under a similar license; -NC (NonCommercial): work may not be used for commercial purposes without the creator’s permission; -ND (NoDerivatives): no derivative works (e.g., translations) can be created without the creator’s permission. Thus, CC-BY-NC means “users must give attribution and cannot use commercially without permission”. The term CC-0 (zero, not letter ‘O’) is sometimes used to mean “no restrictions”, i.e., the work is in the public domain. checklist A list of things to be checked or completed when doing a task. command-line interface A user interface that relies solely on text for commands and output, typically running in a shell. code coverage (in testing) How much of a library or program is executed when tests run. This is normally reported as a percentage of lines of code: for example, if 40 out of 50 lines in a file are run during testing, those tests have 80% code coverage. code review To check a program or a change to a program by inspecting its source code. cognitive load The amount of working memory needed to accomplish a set of simultaneous tasks. command history An automatically-created list of previously-executed commands. Most read-eval-print loops (REPLs), including the Unix shell, record history and allow users to play back recent commands. command-line argument A filename or control flag given to a command-line program when it is run. command line flag See command-line argument command line option See command-line argument command line switch See command-line argument comment Text written in a script that is not treated as code to be run, but rather as text that describes what the code is doing. These are usually short notes, often beginning with a # (in many programming languages). commit As a verb, the act of saving a set of changes to a database or version control repository. As a noun, the changes saved. commit message A comment attached to a commit that explains what was done and why. commons Something managed jointly by a community according to rules they themselves have evolved and adopted. competent practitioner Someone who can do normal tasks with normal effort under normal circumstances. See also: novice, expert computational notebook A combination of a document format that allows users to mix prose and code in a single file, and an application that executes that code interactively and in place. The Jupyter Notebook and R Markdown files are both examples of computational notebooks. conditional expression A ternary expression that serves the role of an if/else statement. For example, C and similar languages use the syntax test : ifTrue ? ifFalse to mean “choose the value ifTrue if test is true, or the value ifFalse if it is not”. confirmation bias The tendency to analyze information or make decisions in ways that reinforce existing beliefs. continuation prompt A prompt that indicates that the command currently being typed is not yet complete, and will not be run until it is. continuous integration A software development practice in which changes are automatically merged as soon as they become available. current working directory The folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory. data package A software package that, mostly, contains only data. Is used to make it simpler to disseminate data for easier use. default target The build target that is used when none is specified explicitly. default value A value assigned to a function parameter when the caller does not specify a value. Default values are specified as part of the function’s definition. defensive programming A set of programming practices that assumes mistakes will happen and either reports or corrects them, such as inserting assertions to report situations that are not ever supposed to occur. destructuring assignment Unpacking values from data structures and assigning them to multiple variables in a single statement. dictionary A data structure that allows items to be looked up by value, sometimes called an associative array. Dictionaries are often implemented using hash tables. docstring Short for “documentation string”, a string appearing at the start of a module, class, or function in Python that automatically becomes that object’s documentation. documentation generator A software tool that extracts specially-formatted comments or dostrings from code and generates cross-referenced developer documentation. Digital Object Identifier A unique persistent identifier for a book, paper, report, dataset, software release, or other digital artefact. See also: ORCID down-vote A vote against something. See also: up-vote entry point Where a program or function starts executing, or the first commands in a file that run. exception An object that stores information about an error or other unusual event in a program. One part of a program will create and raise an exception to signal that something unexpected has happened; another part will catch it. expected result (of test) The value that a piece of software is supposed to produce when tested in a certain way, or the state in which it is supposed to leave the system. See also: actual result (of test) expert Someone who can diagnose and handle unusual situations, knows when the usual rules do not apply, and tends to recognize solutions rather than reasoning to them. See also: competent practitioner, novice explicit relative import In Python, an import that specifies a path relative to the current location. exploratory programming A software development methodology in which requirements emerge or change as the software is being written, often in response to results from early runs. export a variable To make a variable defined inside a shell script available outside that script. external error An error caused by something outside a program, such as trying to open a file that doesn’t exist. false beginner Someone whose previous knowledge allows them to learn (or re-learn) something more quickly. False beginners start at the same point as true beginners (i.e., a pre-test will show the same proficiency) but can move much more quickly. Frequently Asked Questions A curated list of questions commonly asked about a subject, along with answers. feature request A request to the maintainers or developers of a software program to add a specific functionality (a feature) to that program. filename extension The last part of a filename, usually following the ‘.’ symbol. Filename extensions are commonly used to indicate the type of content in the file, though there is no guarantee that this is correct. filename stem The part of the filename that does not include the extension. For example, the stem of glossary.yml is glossary. filesystem The part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored (as in “the Unix filesystem”). filter As a verb, to choose a set of records (i.e., rows of a table) based on the values they contain. As a noun, a command-line program that reads lines of text from files or standard input, performs some operation on them (such as filtering), and writes to a file or stdout. fixture The thing on which a test is run, such as the parameters to the function being tested or the file being processed. flag variable A variable that changes state exactly once to show that something has happened that needs to be dealt with later. folder Another term for a directory. forge A website that integrates version control, issue tracking, and other tools for software development. full identifier (of a commit) A unique 160-bit identifier for a commit in a Git repository, usually written as a 20-character hexadecimal character string. Git A version control tool to record and manage changes to a project. Git branch A snapshot of a version of a Git repository. Multiple branches can capture multiple versions of the same repository. Git clone Copies (and usually downloads) of a Git remote repository on a local computer. Git conflict A situation in which incompatible or overlapping changes have been made on different branches that are now being merged. Git fork To make a new copy of a Git repository on a server, or the copy that is made. See also: Git clone Git merge Merging branches in Git incorporates development histories of two branches in one. If changes are made to similar parts of the branches on both branches a conflict will occur and this must be resolved before the merge will be completed. Git pull Downloads and synchronizes changes between a remote repository and a local repository. Git push Uploads and synchronizes changes between a local repository and a remote repository. Git remote A short name for a remote repository (like a bookmark). Git stage To put changes in a “holding area” from which they can be committed. governance The process by which an organization manages itself, or the rules used to do so. GNU Public License A license that allows people to re-use software as long as they distribute the source of their changes. graphical user interface A user interface that relies on windows, menus, pointers, and other graphical elements, as opposed to a command-line interface or voice-driven interface. hitchhiker Someone who is part of a project but does not actually do any work on it. home directory A directory that contains a user’s files. Each user on a multi-user computer will have their own home directory; a personal computer will often only have one home directory. impact/effort matrix A tool for prioritizing work in which every task is placed according to its importance and the effort required to complete it. implicit relative import In Python, an import that does not specify a path (and hence may be ambiguous). impostor syndrome The false belief that one’s successes are a result of accident or luck rather than ability. in-place operator An operator that updates one of its operands. For example, the expression x += 2 uses the in-place operator += to add 2 to the current value of x and assign the result back to x. inspectability The degree to which a third party can figure out what was done and why. Work can be reproducible without being inspectable. integration test A test that checks whether the parts of a system work properly when put together. See also: unit test internal error An error caused by a fault in a program, such as trying to access elements beyond the end of an array. interruption bingo A technique for managing interruptions in meetings. Everyone’s name is placed on each row and each column of a grid; each time person A interrupts person B, a mark is added to the appropriate grid cell. invariant Something that is must be true at all times inside of a program or during the lifecycle of an object. Invariants are often expressed using assertions. If an invariant expression is not true, this is indicative of a problem, and may result in failure or early temrination of the program. issue A bug report, feature request, or other to-do item associated with a project. Also called a ticket. label (an issue) A short textual tag associated with an issue to categorize it. Common labels include `bug` and feature request. issue tracking system Similar to a bug tracking system in that it tracks \"issues\" made to a repository, usually in the form of feature requests, bug reports, or some other to-do item. JavaScript Object Notation A way to represent data by combining basic values like numbers and character strings in lists and key/value structures. The acronym stands for “JavaScript Object Notation”; unlike better-defined standards like XML, it is unencumbered by a syntax for comments or ways to define a schema. kebab case A naming convention in which the parts of a name are separated with dashes, as in first-second-third. See also: camel case, pothole case LaTeX A typesetting system for document preparation that uses a specialized markup language to define a document structure (e.g. headings), stylise text, insert mathematical equations, and manage citations and cross-references. LaTeX is widely used in academia, in particular for scientific papers and theses in mathematics, physics, engineering, and computer science. linter A program that checks for common problems in software, such as violations of indentation rules or variable naming conventions. The name comes from the first tool of its kind, called lint. list comprehension In Python, an expression that creates a new list in place. For example, [2*x for x in values] creates a new list whose items are the doubles of those in values. logging framework A software library that manages internal reporting for programs. logging level A setting that controls how much information is generated by a logging framework. Typical logging levels include DEBUG, WARNING, and ERROR. long option A full-word identifier for a command line argument. While most common flags are a single letter preceded by a dash, such as -v, long options typically use two dashes and a readable name, such as --verbose. See also: short option loop body The statement or statements executed by a loop. magic number An unnamed numerical constant that appears in a program without explanation. Makefile A file containing commands for Make, often actually called Makefile. Martha’s Rules A simple set of rules for making decisions in small groups. maximum likelihood estimation To choose the parameters for a probability distribution in order to maximize the likelihood of obtaining observed data. mental model A simplified representation of the key elements and relationships of some problem domain that is good enough to support problem solving. milestone A target that a project is trying to meet, often represented as a set of issues that all have to be resolved by a certain time. MIT License A license that allows people to re-use software with no restrictions. Nano (editor) A very simple text editor found on most Unix systems. non-governmental organization An organization that is not affiliated with the government, but does the sorts of public service work that governments often do. novice Someone who has not yet built a usable mental model of a domain. See also: competent practitioner, expert object-oriented programming A style of programming in which functions and data are bound together in objects that only interact with each other through well-defined interfaces. open license A license that permits general re-use, such as the MIT License or GPL for software and CC-BY or CC-0 for data, prose, or other creative outputs. open science A generic term for making scientific software, data, and publications generally available. operating system A program that provides a standard interface to whatever hardware it is running on. Theoretically, any program that only interacts with the operating system should run on any computer that operating system runs on. oppression A form of injustice in which one social group is marginalized or deprived while another is privileged. optional argument An argument to a function or a command that may be omitted. ORCID An Open Researcher and Contributor ID that uniquely and persistently identifies an author of scholarly works. ORCIDs are for people what DOIs are for documents. orthogonality The ability to use various features of software in any order or combination. Orthogonal systems tend to be easier to understand, since features can be combined without worrying about unexpected interactions. overlay configuration A technique for configuring programs in which several layers of configuration are used, each overriding settings in the ones before. pager A program that displays a few lines of text at a time. parent directory The directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem. See also: subdirectory patch A single file containing a set of changes to a set of files, separated by markers that indicate where each individual change should be applied. path (in filesystem) A string that specifies a location in a filesystem. In Unix, the directories in a path are joined using /. See also: absolute path, relative path path coverage The fraction of possible execution paths in a piece of software that have been executed by tests. Software can have complete code coverage without having complete path coverage. pattern rule A generic build rule that describes how to update any file whose name matches a pattern. Pattern rules often use automatic variables to represent the actual filenames. phony target A build target that does not correspond to an actual file. Phony targets are often used to store commonly-used commands in a Makefile. pipe (in the Unix shell) The | used to make the output of one command the input of the next. positional argument An argument to a function that gets its value according to its place in the function’s definition, as opposed to a named argument that is explicitly matched by name. postcondition Something that is guaranteed to be true after a piece of software finishes executing. See also: invariant, precondition pothole case A naming style that separates the parts of a name with underscores, as in first_second_third. See also: camel case, kebab case power law A mathematical relationship in which one quantity changes in proportion to a constant raised to the power of another quantity. precondition Something that must be true before a piece of software runs in order for that software to run correctly. See also: invariant, postcondition prerequisite Something that a build target depends on. privilege An unearned advantage, typically as a result of belonging to a dominant social class or group. pseudo-random number generator A function that can generate pseudo-random numbers. See also: seed procedural programming A style of programming in which functions operate on data that is passed into them. The term is used in contrast to other programming styles, such as object-oriented programming and functional programming. process An operating system’s representation of a running program. A process typically has some memory, the identity of the user who is running it, and a set of connections to open files. product manager The person responsible for defining what features a product should have. project manager The person responsible for ensuring that a project moves forward. prompt The text printed by an REPL or shell that indicates it is ready to accept another command. The default prompt in the Unix shell is usually $, while in Python it is &gt;&gt;&gt;, and in R it is &gt;. See also: continuation prompt provenance A record of where data originally came from and what was done to process it. pull request The request to merge a new feature or correction created on a user’s fork of a Git repositoryrepository into the upstream repository. The developer will be notified of the change, review it, make or suggest changes, and potentially merge it. raise (an exception) To signal that something unexpected or unusual has happened in a program by creating an exception and handing it to the error-handling system, which then tries to find a point in the program that will catch it. raster image An image stored as a matrix of pixels. recursion Calling a function from within a call to that function, or defining a term using a simpler version of the same term. redirection To send a request for a web page or web service to a different page or service. refactoring Reorganizing software without changing its behavior. regression testing Testing software to ensure that things which used to work have not been broken. regular expression A pattern for matching text, written as text itself. Regular expressions are sometimes called “regexp”, “regex”, or “RE”, and are powerful tools for working with text. relative error The absolute value of the difference between the actual and correct value divided by the correct value. For example, if the actual value is 9 and the correct value is 10, the relative error is 0.1. Relative error is usually more useful than absolute error. relative path A path whose destination is interpreted relative to some other location, such as the current working directory. A relative path is the equivalent of giving directions using terms like “straight” and “left”. See also: absolute path remote login Starting an interactive session on one computer from another computer, e.g., by using SSH. remote login server A process that handles requests to log in to a computer from other computers. See also: ssh daemon remote repository A repository located on another computer. Tools such as Git are designed to synchronize changes between local and remote repositories in order to share work. read-eval-print loop An interactive program that reads a command typed in by a user, executes it, prints the result, and then waits patiently for the next command. REPLs are often used to explore new ideas, or for debugging. repository A place where a version control system stores the files that make up a project and the metadata that describes their history. See also: Git reprex A reproducible example. When asking questions about coding problems online or filing issues on GitHub, you should always include a reprex so others can reproduce your problem and help. The reprex package can help! reproducible research The practice of describing and documenting research results in such a way that another researcher or person can re-run the analysis code on the same data to obtain the same result. reStructured Text A plaintext markup format used primarily in Python documentation. revision See commit. root directory The directory that contains everything else, either directly or indirectly. The root directory is written / (a bare forward slash). rotating file A set of files used to store recent information. For example, there might be one file with results for each day of the week, so that results from last Tuesday are overwritten this Tuesday. research software engineer Someone whose primary responsibility is to build the specialized software that other researchers depend on. script Originally, a program written in a language too user-friendly for “real” programmers to take seriously; the term is now synonymous with program. search path The list of directories that a program searches to find something. For example, the Unix shell uses the search path stored in the PATH variable when trying to find a program whose name it has been given. seed A value used to initialize a pseudo-random number generator. semantic versioning A standard for identifying software releases. In the version identifier major.minor.patch, major changes when a new version of software is incompatible with old versions, minor changes when new features are added to an existing version, and patch changes when small bugs are fixed. sense vote A preliminary vote used to determine whether further discussion is needed in a meeting. See also: Martha’s Rules shebang In Unix, a character sequence such as #!/usr/bin/python in the first line of an executable file that tells the shell what program to use to run that file. shell A command-line interface that allows a user to interact with the operating system, such as Bash (for Unix and MacOS) or PowerShell (for Windows). shell script A set of commands for the shell stored in a file so that they can be re-executed. A shell script is effectively a program. shell variable A variable set and used in the Unix shell. Commonly-used shell variables include HOME (the user’s home directory) and PATH (their search path). short circuit test A logical test that only evaluates as many arguments as it needs to. For example, if A is false, then most languages never evaluate B in the expression A and B. short identifier (of commit) The first few characters of a full identifier. Short identifiers are easy for people to type and say aloud, and are usually unique within a repository's recent history. short option A single-letter identifier for a command line argument. Most common flags are a single letter preceded by a dash, such as -v. See also: long option snake case See pothole case. See also: camel case, kebab case software distribution A set of programs that are built, tested, and distributed as a collection so that they can run together. source distribution A software distribution that includes the source code, typically so that programs can be recompiled on the target computer when they are installed. sprint A short, intense period of work on a project. ssh daemon A remote login server that handles SSH connections. SSH key A string of random bits stored in a file that is used to identify a user for SSH. Each SSH key has separate public and private parts; the public part can safely be shared, but if the private part becomes known, the key is compromised. SSH protocol A formal standard for exchanging encrypted messages between computers and for managing remote logins. stack frame A section of the call stack that records details of a single call to a specific function. standard error A predefined communication channel for a process, typically used for error messages. See also: standard input, standard output standard input A predefined communication channel for a process, typically used to read input from the keyboard or from the previous process in a pipe. See also: standard error, standard output standard output A predefined communication channel for a process, typically used to send output to the screen or to the next process in a pipe. See also: standard error, standard input stop word Common words that are filtered out of text before processing it, such as “the” and “an”. subcommand A command that is part of a larger family of commands. For example, git commit is a subcommand of Git. subdirectory A directory that is below another directory. See also: parent directory sustainable software Software that its users can afford to keep up to date. Sustainability depends on the quality of the software, the skills of the potential maintainers, and how much the community is willing to invest. tag (in version control) A readable label attached to a specific commit so that it can easily be referenced to later. test-driven development A programming practice in which tests are written before a new feature is added or a bug is fixed in order to clarify the goal. ternary expression An expression that has three parts. Conditional expressions are the only ternary expressions in most languages. test framework See test runner. test runner A program that finds and runs software tests and reports their results. three stickies A technique for ensuring that everyone in a meeting gets a chance to speak. Everyone is given three sticky notes (or other tokens). Each time someone speaks, it costs them a sticky; when they are out of stickies they cannot speak until everyone has used at least one, at which point everyone gets all of their stickies back. ticket See issue. ticketing system See issue tracking system. timestamp A digital identifier showing the time at which something was created or accessed. Timestamps should use ISO date format for portability. tolerance How closely the actual result of a test must agree with the expected result in order for the test to pass. Tolerances are usually expressed in terms of relative error. traceback In Python, an object that records where an exception was raised, what stack frames were on the call stack, and other details. transitive dependency If A depends on B and B depends on C, C is a transitive dependency of A. triage To go through the issues associated with a project and decide which are currently priorities. Triage is one of the key responsibilities of a project manager. tuple A data type that has a fixed number of parts, such as the three color components of a red-green-blue color specification. Tuples are immutable (their values can not be reset.) unit test A test that exercises one function or feature of a piece of software and produces pass, fail, or error. See also: integration test up-vote A vote in favor of something. See also: down-vote update operator See in-place operator. validation Checking that a piece of software does what its users want, i.e., “are we building the right thing”? See also: verification variable arguments In a function, the ability to take any number of arguments. R uses ... to capture the “extra” arguments. Python uses *args and **kwargs to capture unnamed, and named, “extra” arguments, respectively. verification Checking that a piece of software works as intended, i.e., “did we build the thing right?” See also: validation version control system A system for managing changes made to software during its development. See also: Git virtual environment In Python, the virtualenv package allows you to create virtual, disposable, Python software environments containing only the packages and versions of packages you want to use for a particular project or task, and to install new packages into the environment without affecting other virtual environments, or the system-wide default environment. virtual machine A program that pretends to be a computer. This may seem a bit redundant, but VMs are quick to create and start up, and changes made inside the virtual machine are contained within that VM so we can install new packages or run a completely different operating system without affecting the underlying computer. whitespace The space, newline, carriage return, and horizontal and vertical tab characters that take up space but do not create a visible mark. The name comes from their appearance on a printed page in the era of typewriters. wildcard A character expression that can match text, such as the * in *.csv (which matches any filename whose name ends with .csv). working memory The part of memory that briefly stores information that can be directly accessed by consciousness. "],["references.html", "K References", " K References "]]
