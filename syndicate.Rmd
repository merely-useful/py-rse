# Syndicating Data {#syndicate}

## Questions {#syndicate-questions}

```{r, child="questions/syndicate.md"}
```

## Objectives {#syndicate-objectives}

```{r, child="objectives/syndicate.md"}
```

## Introduction {#syndicate-intro}

A growing number of organizations make data sets available on the web in a style called [REST](glossary.html#rest),
which stands for REpresentational State Transfer.
When REST is used,
every data set is identified by a URL
and can be accessed through a set of functions
called an [application programming interface](glossary.html#api) (API).
This lesson will look at how to use these interfaces,
and how to provide data through them ourselves.

## How can I fetch data data from a website? {#syndicate-fetch}

The World Bank's [Climate Data API][climate-api]
provides data generated by 15 global circulation models.
According to the API's [home page][climate-api],
the data sets containing yearly averages for various values are identified by URLs of the form:

<code>http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/<em>VAR</em>/year/<em>ISO3</em>.<em>EXT</em></code>

where:

-   *VAR* is either `pr` (for precipitation) or `tas` (for "temperature at surface");
-   *ISO3* is the International Standards Organization (ISO)
    [3-letter code for a country][wikipedia-iso-country],
    such as "CAN" for Canada or "BRA" for Brazil;
    and
-   *EXT* (short for "extension") specifies the format we want the data in.
    There are several choices for format,
    but the simplest is [comma-separated values](glossary.html#csv) (CSV),
    in which each record is a row,
    and the values in each row are separated by commas.
    (CSV is frequently used for spreadsheet data.)

For example, if we want the average annual temperature in Canada as a CSV file, the URL is:

```text
http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv
```

If we paste that URL into a browser, it displays:

```text
year,data
1901,-7.67241907119751
1902,-7.862711429595947
1903,-7.910782814025879
...
2007,-6.819293975830078
2008,-7.2008957862854
2009,-6.997011661529541
```

> **Behind the Scenes**
>
> This particular data set might be stored in a file on the World Bank's server,
> or that server might:
>
> 1.  Receive our URL.
> 2.  Break it into pieces.
> 3.  Extract the three key fields (the variable, the country code, and the desired format).
> 4.  Fetch the desired data from a database.
> 5.  Format the data as CSV.
> 6.  Send that to our browser.
>
> As long as the World Bank doesn't change its URLs,
> we don't need to know which method it's using
> and it can switch back and forth between them without breaking our programs.

If we only wanted to look at data for a couple of countries,
we could just download those files one by one.
But we want to compare data for many different pairs of countries,
so we should write a program.

Python has a library called [Requests][requests] for running HTTP requests.
To install it, run:

```shell
$ pip install requests
```

(Note that `pip` is a program in its own right,
so the command above must be run in the shell,
and *not* from within Python itself.)
Once it is installed,
we can get the data we want like this:

<!-- src="syndicate/get_tas_can.py" -->
```python
import requests

url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv'
response = requests.get(url)
if response.status_code != 200:
    print('Failed to get data:', response.status_code)
else:
    print('First 100 characters of data are')
    print(response.text[:100])
```
```text
First 100 characters of data are
year,data
1901,-7.67241907119751
1902,-7.862711429595947
1903,-7.910782814025879
1904,-8.15572929382
```

`requests.get` actually gets our data. More specifically, it:

-   creates a connection to the `climatedataapi.worldbank.org` server;
-   sends the URL `/climateweb/rest/v1/country/cru/tas/year/CAN.csv` to that server;
-   creates an object in memory on our computer to hold the response;
-   assigns a number to that object's `status_code` member variable to tell us whether the request succeeded or not; and
-   assigns the data sent back by the web server to the object's `text` member variable.

(We could just pass this URL as an argument to the `requests.get` call,
but assigning it to a variable makes it easier to find.)

The server can return many different [status codes](glossary.html#http-status-code);
the most common are:

| Code | Name                  | Meaning                                                                    |
|------|-----------------------|----------------------------------------------------------------------------|
| 200  | OK                    | The request has succeeded.                                                 |
| 204  | No Content            | The server has completed the request, but doesn't need to return any data. |
| 400  | Bad Request           | The request is badly formatted.                                            |
| 401  | Unauthorized          | The request requires authentication.                                       |
| 404  | Not Found             | The requested resource could not be found.                                 |
| 408  | Timeout               | The server gave up waiting for the client.                                 |
| 418  | I'm a teapot          | No, really...                                                              |
| 500  | Internal Server Error | An error occurred in the server.                                           |

200 (OK) is the one we want;
if we get anything else, the response probably doesn't contain actual data
(though it might contain an error message).

## How can I handle tabular data? {#syndicate-csv}

Our little program gets the data we want,
but returns it as one long character string rather than as a list of numbers.
There are two ways we could convert the former to the latter:

-   Write a function to split that string on newline characters to create lines,
    then split the lines on commas and convert the second part of each to a number.
-   Use a Python library to do this for us.

Most experienced programmers would say that the second approach is easier,
but "easy" is relative:
using standard libraries is only easier if we know that those libraries exist and how to use them.

Let's try the first approach.
To begin,
we create a file called `test_01.csv` that contains the following three lines:

<!-- src="syndicate/test_01.csv" -->
```
1901,12.3
1902,45.6
1903,78.9
```

It's easy to read this file line by line and (for example) report the length of each line:

<!-- src="syndicate/read_csv_01.py" -->
```python
with open('test_01.csv', 'r') as reader:
    for line in reader:
        print(len(line))
```
```text
10
10
10
```

We can also split each line on commas to turn each one into a list of string fragments:

<!-- src="syndicate/read_csv_02.py" -->
```python
with open('test_01.csv', 'r') as reader:
    for line in reader:
        fields = line.split(',')
        print(fields)
```
```text
['1901', '12.3\n']
['1902', '45.6\n']
['1903', '78.9\n']
```

The dates are correct,
but the values all end with `\n`.
This is an [escape sequence](glossary.html#escape-sequence) that represents
the newline character at the end of each line.
To get rid of it,
we should strip leading and trailing whitespace from each line before splitting it on commas:

<!-- src="syndicate/read_csv_03.py" -->
```python
with open('test_01.csv', 'r') as reader:
    for line in reader:
        fields = line.strip().split(',')
        print(fields)
```
```text
['1901', '12.3']
['1902', '45.6']
['1903', '78.9']
```

Now let's have a look at how we could parse the data using standard Python libraries instead.
The library we'll use is called `csv`.
It doesn't read data itself:
instead, it takes the lines read by something else and turns them into lists of values by splitting on commas.
Here's one way to use it:

<!-- src="syndicate/read_csv_04.py" -->
```python
import csv

with open('test_01.csv', 'r') as raw:
    cooked = csv.reader(raw)
    for record in cooked:
        print(record)
```
```text
['1901', '12.3']
['1902', '45.6']
['1903', '78.9']
```

Here,
`raw` reads data in the normal way,
while `cooked` is a [wrapper](glossary.html#wrapper)
that takes a line of text and turns it into a list of fields.

We can equally well give a `csv.reader` a list of strings rather than a file:

<!-- src="syndicate/read_csv_05.py" -->
```python
import csv

with open('test_01.csv', 'r') as raw:
    lines = raw.readlines()
cooked = csv.reader(lines)
for record in cooked:
    print(record)
```
```text
['1901', '12.3']
['1902', '45.6']
['1903', '78.9']
```

Using the `csv` library doesn't seem any simpler than just splitting strings,
but look at what happens when we have data like this:

<!-- src="syndicate/test_02.csv" -->
```
"Meltzer, Marlyn Wescoff",1922,2008
"Spence, Frances Bilas",1922,2012
"Teitelbaum,Ruth Lichterman",1924,1986
```

With simple string splitting, our output is:

```text
['"Meltzer', ' Marlyn Wescoff"', '1922', '2008']
['"Spence', ' Frances Bilas"', '1922', '2012']
['"Teitelbaum', 'Ruth Lichterman"', '1924', '1986']
```

The double quotes are still there,
and the field containing each person's name has been split into pieces.
If we use the `csv` library,
on the other hand,
the output is:

```text
['Meltzer, Marlyn Wescoff', '1922', '2008']
['Spence, Frances Bilas', '1922', '2012']
['Teitelbaum,Ruth Lichterman', '1924', '1986']
```

because the library understands how to handle text fields containing commas
(and a lot more).

We need to do one more thing before using `csv` with the climate data.
When we use the World Bank's API to get data for a particular country,
it comes back as one long string:

```text
year,data
1901,-7.67241907119751
1902,-7.862711429595947
1903,-7.910782814025879
...
```

We have to break this into lines before giving it to `csv.reader`,
and we can do that by splitting the string on the same `\n` escape sequence
we encountered a few moments ago.
To see how this works,
let's read `test_01.csv` into memory and split it into pieces:

<!-- src="syndicate/read_csv_06.py" -->
```python
with open('test_01.csv', 'r') as reader:
    data = reader.read()
    lines = data.split('\n')
    print(lines)
```
```text
['1901,12.3', '1902,45.6', '1903,78.9', '']
```

That's *almost* right, but why is there an empty string at the end of the list?
The answer is that the last line of the file ends in a newline,
so Python does the same thing it does in the example below:

```python
fields = 'a-b-'.split('-')
print(fields)
```
```text
['a', 'b', '']
```

The solution once again is to strip leading and trailing whitespace before splitting:

<!-- src="syndicate/read_csv_07.py" -->
```python
with open('test_01.csv', 'r') as reader:
    data = reader.read()
    lines = data.strip().split('\n')
    print(lines)
```
```text
['1901,12.3', '1902,45.6', '1903,78.9']
```

Putting this all together, we can get data for Canada like this:

<!-- src="syndicate/get_tas_can_csv.py" -->
```python
import requests
import csv

url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv'
response = requests.get(url)
if response.status_code != 200:
    print('Failed to get data:', response.status_code)
else:
    wrapper = csv.reader(response.text.strip().split('\n'))
    for record in wrapper:
        print(record)
```
```text
['year', 'data']
['1901', '-7.67241907119751']
['1902', '-7.862711429595947']
['1903', '-7.910782814025879']
['1904', '-8.155729293823242']
['1905', '-7.547311305999756']
...
```

That looks like progress,
so let's convert the data from strings to the numbers we actually want:

<!-- src="syndicate/get_tas_can_convert.py" -->
```python
import requests
import csv

url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv'
response = requests.get(url)
if response.status_code != 200:
    print('Failed to get data:', response.status_code)
else:
    wrapper = csv.reader(response.text.strip().split('\n'))
    for record in wrapper:
        year = int(record[0])
        value = float(record[1])
        print(year, value)
```
```text
Traceback (most recent call last):
  File "api_with_naive_converting.py", line 11, in <module>
    year = int(record[0])
ValueError: invalid literal for int() with base 10: 'year'
```

The error occurs because the first line of data is:

```text
year,data
```

When we try to convert the string `'year'` to an integer,
Python quite rightly complains.
The fix is straightforward:
we just need to ignore lines that start with the word `year`:

<!-- src="syndicate/get_tas_can_clean.py" -->
```python
import requests
import csv

url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv'
response = requests.get(url)
if response.status_code != 200:
    print('Failed to get data:', response.status_code)
else:
    wrapper = csv.reader(response.text.strip().split('\n'))
    results = []
    for record in wrapper:
        if record[0] != 'year':
            year = int(record[0])
            value = float(record[1])
            print(year, value)
```
```text
1901 -7.67241907119751
1902 -7.862711429595947
1903 -7.910782814025879
1904 -8.155729293823242
1905 -7.547311305999756
...
```

## How can I generalize data fetching and handle errors? {#syndicate-generalize}

Now that we know how to get the data for Canada,
let's create a function that will do the same thing for an arbitrary country.
The steps are simple:

1.  copy the code we've written into a function that takes a 3-letter country code as a parameter,
2.  insert that country code into the URL at the appropriate place, and
3.  return the result as a list instead of printing it.

The resulting function looks like:

<!-- src="syndicate/get_without_check.py" -->
```python
def annual_mean_temp(country):
    '''Get the annual mean temperature for a country given its 3-letter ISO code (such as "CAN").'''
    url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/' + country + '.csv'
    response = requests.get(url)
    if response.status_code != 200:
        print('Failed to get data:', response.status_code)
    else:
        wrapper = csv.reader(response.text.strip().split('\n'))
        results = []
        for record in wrapper:
            if record[0] != 'year':
                year = int(record[0])
                value = float(record[1])
                results.append([year, value])
        return results
```

This works:

<!-- src="syndicate/get_without_check.py" -->
```python
canada = annual_mean_temp('CAN')
print('first three entries for Canada:', canada[:3])
```
```text
first three entries for Canada: [[1901, -7.67241907119751], [1902, -7.862711429595947], [1903, -7.910782814025879]]
```

However,
there's a problem.
Look what happens when we pass in an invalid country identifier:

<!-- src="syndicate/get_without_check.py" -->
```python
latveria = annual_mean_temp('LTV')
print('first three entries for Latveria:', latveria[:3])
```
```text
first three entries for Latveria: []
```

Latveria doesn't exist,
so why is our function returning an empty list rather than printing an error message?
The non-appearance of an error message must mean that the response code was 200;
if it was anything else,
we would have gone into the `if` branch,
printed a message,
and returned `None`
(which is what functions do when they're not told to return anything specific).

So if the response code was 200 and there was no data, that would explain what we're seeing.
Let's check:

<!-- src="syndicate/get_with_minimal_check.py" -->
```python
def annual_mean_temp(country):
    '''Get the annual mean temperature for a country given its 3-letter ISO code (such as "CAN").'''
    url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/' + country + '.csv'
    print('url used is', url)
    response = requests.get(url)
    print('response code:', response.status_code)
    print('length of data:', len(response.text))
    if response.status_code != 200:
        print('Failed to get data:', response.status_code)
    else:
        wrapper = csv.reader(response.text.strip().split('\n'))
        results = []
        for record in wrapper:
            if record[0] != 'year':
                year = int(record[0])
                value = float(record[1])
                results.append([year, value])
        return results

latveria = annual_mean_temp('LTV')
print('number of records for Latveria:', len(latveria))
```
```text
url used is http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/LTV.csv
response code: 200
length of data: 0
number of records for Latveria: 0
```

In other words,
the World Bank is always saying,
"I was able to answer your query,"
even when it actually can't.
After a bit more experimenting, we discover that the site *always* returns a 200 status code.
The only way to tell if there's real data or not is to check if `response.text` is empty.
Here's the updated function:

<!-- src="syndicate/get_with_check.py" -->
```python
def annual_mean_temp(country):
    '''
    Get the annual mean temperature for a country given its 3-letter ISO code (such as "CAN").
    Returns an empty list if the country code is invalid.
    '''
    url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/' + country + '.csv'
    response = requests.get(url)
    results = []
    if len(response.text) > 0:
        wrapper = csv.reader(response.text.strip().split('\n'))
        for record in wrapper:
            if record[0] != 'year':
                year = int(record[0])
                value = float(record[1])
                results.append([year, value])
    return results
```

<!-- == noindent -->
and here's a quick check:

<!-- src="syndicate/get_with_check.py" -->
```python
print('number of records for Canada:', len(annual_mean_temp('CAN')))
print('number of records for Latveria:', len(annual_mean_temp('LTV')))
```

```text
number of records for Canada: 109
number of records for Latveria: 0
```

## How can I compare different data sets? {#syndicate-compare}

Now that we can get surface temperatures for different countries,
we can write a function to compare those values.
Here's our first attempt:

<!-- src="syndicate/compare_01.py" -->
```python
def diff_records(left, right):
    '''Given lists of [year, value] pairs, return list of [year, difference] pairs.'''
    num_years = len(left)
    results = []
    for i in range(num_years):
        left_year, left_value = left[i]
        right_year, right_value = right[i]
        difference = left_value - right_value
        results.append([left_year, difference])
    return results
```

Here, we're using the number of entries in `left` (which we find with `len(left)`) to control our loop.
Inside the loop we unpack the left and right years and values from the list entries,
then append a pair containing a year and a difference to `results`,
which we return at the end.
To see if this function works, we can run a couple of tests on made-up data:

<!-- src="syndicate/compare_01.py" -->
```python
print('one record:', diff_records([[1900, 1.0]],
                                  [[1900, 2.0]]))
print('two records:', diff_records([[1900, 1.0], [1901, 10.0]],
                                   [[1900, 2.0], [1901, 20.0]]))
```
```text
one record: [[1900, -1.0]]
two records: [[1900, -1.0], [1901, -10.0]]
```

That looks pretty good, but what about these cases?

<!-- src="syndicate/compare_01.py" -->
```python
print('mis-matched years:', diff_records([[1900, 1.0]],
                                         [[1999, 2.0]]))
print('left is shorter', diff_records([[1900, 1.0]],
                                      [[1900, 10.0], [1901, 20.0]]))
print('right is shorter', diff_records([[1900, 1.0], [1901, 2.0]],
                                       [[1900, 10.0]]))
```
```text
IndexError: list index out of rangemis-matched years: [[1900, -1.0]]
left is shorter [[1900, -9.0]]
right is shorter
```

The first test gives us an answer even though the years didn't match:
we get a result, but it's meaningless.
The second case gives us a partial result,
again without telling us there's a problem,
while the third crashes because we're using `left` to determine the number of records,
but `right` doesn't have that many.

The first two problems are actually worse than the third
because they are [silent failures](glossary.html#silent-failure):
the function does the wrong thing, but doesn't indicate that in any way.
Let's fix that:

<!-- src="syndicate/compare_02.py" -->
```python
def diff_records(left, right):
    '''
    Given lists of [year, value] pairs, return list of [year, difference] pairs.
    Fails if the inputs are not for exactly corresponding years.
    '''
    assert len(left) == len(right), \
           'Inputs have different lengths.'
    num_years = len(left)
    results = []
    for i in range(num_years):
        left_year, left_value = left[i]
        right_year, right_value = right[i]
        assert left_year == right_year, \
               'Record {0} is for different years: {1} vs {2}'.format(i, left_year, right_year)
        difference = left_value - right_value
        results.append([left_year, difference])
    return results
```

Do our "good" tests pass?

<!-- src="syndicate/compare_02.py" -->
```python
print('one record:', diff_records([[1900, 1.0]],
                                  [[1900, 2.0]]))
print('two records:', diff_records([[1900, 1.0], [1901, 10.0]],
                                   [[1900, 2.0], [1901, 20.0]]))
```
```text
one record: [[1900, -1.0]]
two records: [[1900, -1.0], [1901, -10.0]]
```

What about our the three tests that we now expect to fail?

<!-- src="syndicate/compare_02.py" -->
```python
print('mis-matched years:', diff_records([[1900, 1.0]],
                                         [[1999, 2.0]]))
```
```text
AssertionError: Record 0 is for different years: 1900 vs 1999mis-matched years:
```
<!-- src="syndicate/compare_02.py" -->
```python
print('left is shorter', diff_records([[1900, 1.0]],
                                      [[1900, 10.0], [1901, 20.0]]))
```
```text
AssertionError: Inputs have different lengths. left is shorter
```
<!-- src="syndicate/compare_02.py" -->
```python
print('right is shorter', diff_records([[1900, 1.0], [1901, 2.0]],
                                       [[1900, 10.0]]))
```
```text
AssertionError: Inputs have different lengths. right is shorter
```

Excellent:
the assertions we've added will now alert us if we try to work with badly-formatted or inconsistent data.

## How can I publish data so that others can use it? {#syndicate-publish}

We now have functions to download temperature data for different countries and find annual differences.
The next step is to share our findings with the world by publishing the data sets we generate.
To do this, we have to answer three questions:

-   How are we going to store the data?
-   How are people going to download it?
-   How are people going to find it?

The first question is the easiest to answer:
`diff_records` returns a list of (year, difference) pairs that we can write out as a CSV file:

<!-- src="syndicate/save_records_01.py" -->
```python
import csv

def save_records(filename, records):
    '''Save a list of [year, temp] pairs as CSV.'''
    with open(filename, 'w') as raw:
        writer = csv.writer(raw)
        writer.writerows(records)
```

We use the `csv` library to write data
for the same reason we use it to read:
it correctly handles special cases (such as text containing commas).
Let's test it:

<!-- src="syndicate/save_records_01.py" -->
```python
save_records('temp.csv', [[1, 2], [3, 4]])
```

If we then look in the file `temp.csv`, we find:

```text
1,2
3,4
```

<!-- == noindent -->
as desired.

Now, where should this file go?
The answer is clearly "a server",
since data on our laptop is only accessible when we're online
(and probably not even then, since most people don't run a web server on their laptop).
But where on the server, and what should we call it?

The answer to those questions depends on how the server is set up.
On many multi-user Linux machines,
users can create a directory called like `public_html` under their home directory,
and the web server will automatically search in those directories.
For example,
if Nelle has a file called `thesis.pdf` in her `public_html` directory,
the web server will find it when it gets the URL `http://the.server.name/~nelle/thesis.pdf`.
(In this case,
the tilde `~` in front of Nelle's name tells the web server
to look in Nelle's `public_html` directory.)
The specifics differ from one machine to the next,
but the basic idea stays the same.

As for what we should call it, here we return to the key idea in REST:
every data set should be identified by a "guessable" URL.
In our case we'll use a name  like `left_right.csv`,
where `left` and `right` are the three-letter codes of the countries whose temperatures we are differencing.
We can then tell people that if they want to compare Australia and Brazil,
they should look for `http://the.server.name/~nelle/AUS_BRA.csv`.
(We use upper case to be consistent with the World Bank's API.)

But what's to prevent someone from creating a badly-named (and therefore unfindable) file?
Someone could, for example, call `save_records('aus+bra.csv', records)`.
To reduce the odds of this happening,
let's modify `save_records` to take country identifiers as parameters:

<!-- src="syndicate/save_records_02.py" -->
```python
import csv

def save_records(left, right, records):
    '''Save a list of [year, temp] pairs as CSV.'''
    filename = left + '_' + right + '.csv'
    with open(filename, 'w') as raw:
        writer = csv.writer(raw)
        writer.writerows(records)
```

We can now call it like this:

<!-- src="syndicate/save_records_02.py" -->
```python
save_records('AUS', 'BRA', [[1, 2], [3, 4]])
```

and then check that the right output file has been created.
We are bound to have the country codes anyway (having used them to look up our data),
so this should seem natural to our users.

## How can I make it easy for other people to find my data? {#syndicate-findable}

It's not enough to tell people what the rule is for creating filenames,
since that doesn't tell them what data sets we've actually generated.
The final step in this lesson is therefore
to make the data we generate findable
by creating an [index](glossary.html#index) to tell people what files exist.
Here's the format we will use:

```text
2014-05-26,FRA,TCD,FRA_TCD.csv
2014-05-27,AUS,BRA,AUS_BRA.csv
2014-05-27,AUS,CAN,AUS_CAN.csv
2014-05-28,BRA,CAN,BRA_CAN.csv
```

The columns are the date the data set was generated,
the identifiers of the two countries being compared,
and the name of the data file.
We include the date to make it easy for people to see what's been updated when,
but why do we bother to include the filename?
After all, we can re-generate it easily given the two country codes.
The answer is that while *we* know the rule for generating filenames,
other people's programs shouldn't have to.

Here's a function that updates the index file every time we generate a new data file:

<!-- src="syndicate/make_index.py" -->
```python
import time

def update_index(index_filename, left, right):
    '''Append a record to the index.'''

    # Read existing data.
    with open(index_filename, 'r') as raw:
        reader = csv.reader(raw)
        records = []
        for r in reader:
            records.append(r)
    
    # Create new record.
    timestamp = time.strftime('%Y-%m-%d')
    data_filename = left + '_' + right + '.csv'
    new_record = (timestamp, left, right, data_filename)
    
    # Save.
    records.append(new_record)
    with open(index_filename, 'w') as raw:
        writer = csv.writer(raw)
        writer.writerows(records)
```

Let's test it.
If our index file contains:

```text
2014-05-26,FRA,TCD,FRA_TCD.csv
2014-05-27,AUS,BRA,AUS_BRA.csv
2014-05-27,AUS,CAN,AUS_CAN.csv
2014-05-28,BRA,CAN,BRA_CAN.csv
```

and we run:

<!-- src="syndicate/make_index.py" -->
```python
update_index('data/index.csv', 'TCD', 'CAN')
```

then our index file now contains:

```text
2014-05-26,FRA,TCD,FRA_TCD.csv
2014-05-27,AUS,BRA,AUS_BRA.csv
2014-05-27,AUS,CAN,AUS_CAN.csv
2014-05-28,BRA,CAN,BRA_CAN.csv
2014-05-29,TCD,CAN,TCD_CAN.csv
```

Now that all of this is in place,
it's easy for us---and more importantly,
for other people---to do new and exciting things with our data.
For example,
we can easily write a small program that tells us what data sets include information about a particular country
*and* have been published since we last checked:

<!-- src="syndicate/available.py" -->
```python
def what_is_available(index_file, country, after):
    '''What data files include a country and have been published since 'after'?'''
    with open(index_file, 'r') as raw:
        reader = csv.reader(raw)
        filenames = []
        for record in reader:
            if (after <= record[0]) and (country in (record[1], record[2])):
                filenames.append(record[3])
    return filenames

print(what_is_available('data/index.csv', 'BRA', '2014-05-27'))
```
```text
['AUS_BRA.csv', 'BRA_CAN.csv']
```

This may not seem like a breakthrough,
but it is actually an example of how the web helps researchers do new kinds of science.
With a little bit more work,
we could create a file on *our* machine to record when we last ran `what_is_available`
for each of several different sites that are producing data.
Each time we run it, our program would:

-   read our local "what to check" file;
-   ask each data source whether it had any new data for us;
-   download and process that data; and
-   present us with a summary of the results.

This is exactly how blogs work.
Every blog reader keeps a list of blog URLs that it's supposed to check.
When it is run, it goes to each of those sites and asks them for their index file
(which is typically called something like `feed.xml`).
It then checks the articles listed in that index against its local record of what has already been seen,
then downloads any articles that are new.
By automating this process, blogging tools help us focus attention on things that are actually worth looking at.

## Summary {#syndicate-summary}

```{r syndicate-concept, echo=FALSE, fig.cap="Syndication Concept Map"}
insert_graphic("figures/syndicate/concept.pdf")
```

## Exercises {#syndicate-exercises}

### Enumerating

Python includes a function called `enumerate` that's often used in `for` loops.
Look at its documentation, then rewrite `diff_records` to use it.

### When to complain?

We committed the same mistake as the World Bank in our differencing code:
if someone gives `annual_mean_temp` an invalid country identifier,
it doesn't report an error,
but instead returns an empty list,
so the caller has to somehow know to look for that.
Should it use an assertion to fail if it doesn't get data?
Why or why not?

### Deciding what to check

Should `save_records` check that every record in its input has exactly two fields?
Why or why not?
What about country codes:
should it contain a list of those that match actual countries
and check that `left` and `right` are in that list?

### Metadata for metadata

Should the first line of the index file be a header giving the names of the columns?
Why or why not?

### To automate or not

Should `update_index` be called inside `save_records`
so that the index is automatically updated every time a new data set is generated?
Why or why not?

### Removing redundant redundancy

`update_index` and `save_records` both construct the name of the data file.
Refactor them to remove this redundancy.

## Key Points {#syndicate-keypoints}

```{r, child="keypoints/syndicate.md"}
```

```{r, child="etc/links.md"}
```
