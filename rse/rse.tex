\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{Nemilov}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Merely Useful: RSE},
            pdfauthor={Madeleine Bonsma-Fisher, Kate Hertweck, Damien Irving, Luke Johnston, Christina Koch, Sara Mahallati, Brandeis Marshall, Joel Ostblom, Elizabeth Wickes, Charlotte Wickham, and Greg Wilson},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Merely Useful: RSE}
\author{Madeleine Bonsma-Fisher, Kate Hertweck, Damien Irving, Luke Johnston, Christina Koch, Sara Mahallati, Brandeis Marshall, Joel Ostblom, Elizabeth Wickes, Charlotte Wickham, and Greg Wilson}
\date{2019-11-07}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{rse-intro}{%
\chapter{Introduction}\label{rse-intro}}

\begin{quote}
It's still magic even if you know how it's done.

--- Terry Pratchett
\end{quote}

Computers are now as essential for research as telescopes, test tubes, and libraries,
which means that researchers need need to know how to build, use, and share software.
However,
most introductions to programming focus on developing commercial applications,
not on exploring problems and answering questions.
This series of books will show you how to do the latter
both on your own and as part of a team.

The prequels to this book introduce three core skills:
how to write short programs to clean and analyze data in a reproducible way,
how to keep track of what you have done,
and how to share your software and your results with other people.
But just as some astronomers spend their careers designing telescopes,
some researchers focus on building software to make their colleagues' lives easier.
People who do this are called \href{glossary.html\#rse}{research software engineers},
and the aim of this book is to get you ready for this role---to help you go from writing code for your own use
to creating tools to help your entire field advance.
All of this material can be freely re-used under the terms of the Creative Commons Attribution License (CC-BY 4.0);
please see Appendix~\ref{license} for details.

\hypertarget{rse-intro-personas}{%
\section{Who are these lessons for?}\label{rse-intro-personas}}

\begin{description}
\tightlist
\item[Amira]
completed a master's in library science five years ago
and has since worked for a small aid organization.
She did some statistics during her degree,
and has learned some R and Python by doing data science courses online,
but has no formal training in programming.
Amira would like to tidy up the scripts, data sets, and reports she has created
in order to share them with her colleagues.
These lessons will show her how to do this and what ``done'' looks like.
\item[Jun]
completed an \href{https://www.insightdatascience.com/}{Insight Data Science} fellowship last year after doing a PhD in Geology
and now works for a company that does forensic audits.
He uses a variety of machine learning and visualization packages,
and would now like to turn some of his own work into an open source project.
This book will show him how such a project should be organized
and how to encourage people to contribute to it.
\item[Sami]
became a competent programmer during a bachelor's degree in applied math
and was then hired by the university's research computing center.
The kinds of applications they are being asked to support
have shifted from fluid dynamics to data analysis;
this guide will teach them how to build and run data pipelines
so that they can pass those skills on to their users.
\end{description}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

For researchers and data scientists who can write functions to create programs that are several pages long,
and who want to be more productive and have more confidence in their results,
this book provides a pragmatic, tools-based introduction to research software engineering.
Unlike material courses aimed at computer scientists and professional software developers,
this book uses data analysis as a motivating example
and assumes that the learner's ultimate goal is to answer questions rather than ship products.

\hypertarget{prerequisites}{%
\subsection{Prerequisites}\label{prerequisites}}

Learners must be able to:

\begin{itemize}
\tightlist
\item
  Write reusable functions to analyze tabular data using \href{https://cran.r-project.org/}{R} or \href{glossary.html\#python}{Python}.
\item
  Keep track of their work using \href{glossary.html\#git}{Git} and \href{http://github.com}{GitHub}.
\item
  Create reproducible reports using \href{https://rmarkdown.rstudio.com/}{R~Markdown} or \href{https://jupytext.readthedocs.io/en/latest/introduction.html}{Jupytext} (in \href{http://jupyter.org/}{JupyterLab Notebooks}).
\item
  Publish those reports with appropriate metadata such as a \href{glossary.html\#doi}{DOI} and \href{https://orcid.org/}{ORCID}.
\end{itemize}

Learners will need:

\begin{itemize}
\tightlist
\item
  A personal computer with Internet access.
\item
  A \href{glossary.html\#unix-shell}{Unix shell} (see Appendix~\ref{install} for how to set this up on Windows).
\item
  \href{glossary.html\#python}{Python~3} and \href{http://jupyter.org/}{Jupyter}.
\item
  \href{https://cran.r-project.org/}{R} and \href{https://rstudio.com/}{RStudio}.
\item
  A \href{http://github.com}{GitHub} account.
\end{itemize}

If you are using Windows,
please install \href{https://chocolatey.org/}{Chocolatey}:
you can use it to install other programs as you go along.

\hypertarget{rse-intro-big-picture}{%
\section{What's the big picture?}\label{rse-intro-big-picture}}

To make sense of what follows,
we must distinguish two key ideas:

\begin{itemize}
\tightlist
\item
  \href{glossary.html\#open-science}{Open science}
  focuses on making data, methods, and results freely available to all
  by publishing them under \href{glossary.html\#open-license}{open licenses}.
\item
  \href{glossary.html\#reproducible-research}{Reproducible research}
  means ensuring that anyone with access to data and software can feasibly reproduce results,
  both to check them and to build on them.
\end{itemize}

People often conflate these terms,
but they are distinct:

\begin{itemize}
\tightlist
\item
  If you share your data and the programs that analyze it,
  but don't document what steps to take in what order,
  your work is open but not reproducible.
\item
  If you completely automate your analysis,
  but your data is only available to people in your lab,
  your work is reproducible but not open.
\end{itemize}

The third key idea is \href{glossary.html\#sustainable-software}{sustainability}.
Software is sustainable if it's easier for people to maintain it and extend it than to replace it.
However,
sustainability isn't just a property of the software:
it also depends on the skills and culture of its users.
If a software package is being maintained by a couple of post-docs
who are being paid a fraction of what they could earn in industry
and have no realistic hope of promotion because their field doesn't value tool building,
it doesn't matter whether the package is well tested and easy to install:
sooner or later (probably sooner)
it will become \href{glossary.html\#abandonware}{abandonware}.

\hypertarget{why-isnt-all-of-this-already-normal}{%
\subsection{Why isn't all of this already normal?}\label{why-isnt-all-of-this-already-normal}}

Nobody argues that research should be irreproducible or unsustainable,
but ``not against it'' and actively supporting it are very different things.
Academia doesn't yet know how to reward people for writing useful software,
so while you may be thanked,
the effort you put in may not translate into job security or decent pay.

And some people still worry that if they make their data and code generally available,
someone else will use it and publish a result they have come up with themselves.
This is almost unheard of in practice,
but that doesn't stop people using it as a scare tactic.
Other people are afraid of looking foolish or incompetent by sharing code that might contain bugs.
This isn't just \href{glossary.html\#impostor-syndrome}{impostor syndrome}:
members of marginalized groups are frequently judged more harshly than others,
so being wrong in public is much riskier for them.

\hypertarget{syllabus}{%
\subsection{Syllabus}\label{syllabus}}

This book covers the tools and practices needed to create work that is open, reproducible, and sustainable.
You can use this book for self-study
or as the foundation of a one-semester course for graduate students or senior undergraduates.
By its end,
you will be able to:

\begin{itemize}
\tightlist
\item
  Use \href{glossary.html\#branch}{branches} to coordinate development of complex software.
\item
  Enable users to configure software without modifying it.
\item
  Re-run analyses and rebuild packages reproducibly with a single command.
\item
  Test your software and know which parts have not yet been tested.
\item
  Keep track of what still needs to be done and who's working on it.
\item
  Work on remote computers such as computing clusters or the cloud.
\item
  Document software so it can be easily used, maintained, and extended.
\item
  Create packages that can be installed in standard ways and be used by others.
\item
  Work productively in a small team where everyone is welcome.
\end{itemize}

\hypertarget{rse-intro-ack}{%
\section{Acknowledgments}\label{rse-intro-ack}}

This book owes its existence to
everyone we met through \href{http://software-carpentry.org}{Software Carpentry}, \href{https://datacarpentry.org/}{Data Carpentry}, and \href{https://librarycarpentry.org}{Library Carpentry}.
We are also grateful to \href{https://www.insightdatascience.com/}{Insight Data Science} for sponsoring the early stages of this work
and to everyone who has contributed,
particularly Jonathan Dursi
and the authors of (Noble, \protect\hyperlink{ref-Nobl2009}{2009}; Haddock and Dunn, \protect\hyperlink{ref-Hadd2010}{2010}; Wilson et al., \protect\hyperlink{ref-Wils2014}{2014}, \protect\hyperlink{ref-Wils2017}{2017}; Scopatz and Huff, \protect\hyperlink{ref-Scop2015}{2015}; Taschuk and Wilson, \protect\hyperlink{ref-Tasc2017}{2017}; Brown and Wilson, \protect\hyperlink{ref-Brow2018}{2018}; Devenyi et al., \protect\hyperlink{ref-Deve2018}{2018}; Sholler et al., \protect\hyperlink{ref-Shol2019}{2019}; Wilson, \protect\hyperlink{ref-Wils2019}{2019}\protect\hyperlink{ref-Wils2019}{b}).

\hypertarget{rse-bash-basics}{%
\chapter{The Basics of the Unix Shell}\label{rse-bash-basics}}

At a high level, computers do four things:
run programs,
store data,
communicate with each other,
and interact with people.
They can do the last of these in many different ways,
of which a \href{glossary.html\#gui}{graphical user interface} (GUI) is the most widely used way.
The computer displays icons to show us our files and programs,
and we tell it to copy or run those by clicking with a mouse.
This style of interaction is easy to learn but hard to automate,
and doesn't create a record of what we did.

In contrast,
when we use a \href{glossary.html\#cli}{command-line interface} (CLI)
we communicate with the computer by typing commands,
and the computer responds by displaying text.
CLIs existed long before GUIs,
and have survived because they are efficient,
easy to automate,
and automatically create a record of what we did.

The heart of every CLI is a \href{glossary.html\#repl}{read-evaluate-print loop}.
Its name comes from its basic operating cycle:
when we type a command and press Return (also known as Enter)
the CLI reads the command,
evaluates it (i.e., executes it),
prints its output,
then loops around and waits for another command.

If you have used an interactive console for R or Python,
then you have already used a REPL and a simple CLI.
This lesson will introduce another kind of CLI that lets us interact with our computer's operating system.
It is called a \href{glossary.html\#command-shell}{command shell},
or just ``shell'' for short,
and in essence is a program that runs other programs on our behalf (Figure~\ref{fig:rse-bash-basics-repl}).
Those ``other programs'' can do things as simple as telling us what time it is
or as complex as modeling global climate change;
as long as they obey a few simple rules,
the shell can run them without having to know what language they are written in
or how they do what they do.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-repl}The Shell}
\end{figure}

Programmers have written many different shells over the last forty years.
The most popular on Unix systems today,
and in most packages that provide Unix-like tools for Windows,
is called Bash
(an acronym of Bourne Again SHell,
which is a pun on the name of its predecessor,
the Bourne shell).
Like other shells,
it allows us to combine programs to create pipelines
that can handle large volumes of data.
Sequences of commands can be saved in a \href{glossary.html\#script}{script},
just as commands for R or Python can be saved in programs,
which makes our workflows more reproducible.
Finally,
the command line is often the easiest way to interact with remote machines and supercomputers---in fact,
the shell is practically essential for working with clusters and the cloud.

\hypertarget{rse-bash-basics-explore}{%
\section{How can I explore my files and directories using the shell?}\label{rse-bash-basics-explore}}

When Bash runs it presents us with a \href{glossary.html\#prompt}{prompt} to indicate that it is waiting for input.
By default,
this prompt is a simple dollar sign:

\begin{verbatim}
$
\end{verbatim}

However,
different shells may use a different symbol,
and as we'll see later,
we can customize the prompt to give us more information.

\begin{quote}
\textbf{Don't Type the Dollar Sign}

We show the \texttt{\$} prompt so that it's clear what you are supposed to type,
particularly when several commands appear in a row,
but you should \emph{not} type it yourself.
\end{quote}

Let's run a command to find out who the shell thinks we are:

\begin{verbatim}
$ whoami
\end{verbatim}

\begin{verbatim}
amira
\end{verbatim}

Amira is one of the learners described in Section~\ref{rse-intro-personas};
we will use her in the examples that follow.

Now that we know who we are,
we can explore where we are and what we have.
The part of the operating system responsible for managing files and directories (also called \href{glossary.html\#folder}{folders})
is called the \href{glossary.html\#filesystem}{filesystem}.
Some of the most commonly-used commands in the shell create, inspect, rename, and delete files and directories.
Let's start exploring them by running the command \texttt{pwd},
which stands for print working directory.
The ``print'' part of its name is straightforward;
the ``working directory'' part refers to the fact that
the shell keeps track of our \href{glossary.html\#current-working-directory}{current working directory} at all times.
Most commands read and write files in the current working directory
unless we tell them to do something else,
so knowing where we are before running a command is important.

\begin{verbatim}
$ pwd
\end{verbatim}

\begin{verbatim}
/Users/amira
\end{verbatim}

Here,
the computer's response is \texttt{/Users/amira},
which tells us that we are in a directory called \texttt{amira} that is contained in a top-level directory called \texttt{User}.
This directory is Amira's \href{glossary.html\#home-directory}{home directory},
and to understand what that phrase means,
we must first understand how the filesystem is organized.
On Amira's computer, it looks like Figure~\ref{fig:rse-bash-basics-filesystem}.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-filesystem}Sample Filesystem}
\end{figure}

At the top is the \href{glossary.html\#root-directory}{root directory} that holds everything else,
which we can refer to using a slash character, \texttt{/} on its own.
Inside that directory are several other directories,
including \texttt{bin} (where some built-in programs are stored),
\texttt{data} (for miscellaneous data files),
\texttt{tmp} (for temporary files that don't need to be stored long-term),
and \texttt{Users} (where users' personal directories are located).
We know that \texttt{/Users} is stored inside the root directory \texttt{/} because its name begins with \texttt{/},
and that our current working directory \texttt{/Users/amira} is stored inside \texttt{/Users}
because \texttt{/Users} is the first part of its name.
A string like this is called a \href{glossary.html\#path}{path} because it tells us
how to get from one place in the filesystem (in this case, the root directory)
to another (in this case, Amira's home directory).

\begin{quote}
\textbf{Slashes}

The \texttt{/} character means two different things in a path.
At the front of a path or on its own,
it refers to the root directory.
When it appears inside a name, it is a separator.
Windows uses backslashes (\texttt{\textbackslash{}\textbackslash{}}) instead of forward slashes as separators.
\end{quote}

Underneath \texttt{/Users},
we find one directory for each user with an account on this machine.
Jun's files are stored in \texttt{/Users/jun},
Sami's in \texttt{/Users/sami},
and Amira's in \texttt{/Users/amira}.
This is where the name ``home directory'' comes from:
when we log in as a particular user,
the shell starts us off in the directory that holds that user's files.

\begin{quote}
\textbf{Home Directory Variations}

Our home directory will be different on different operating systems.
On Linux it may look like \texttt{/home/amira},
and on Windows it may be \texttt{C:\textbackslash{}Documents\ and\ Settings\textbackslash{}amira} or \texttt{C:\textbackslash{}Users\textbackslash{}amira}
(depending on the version of Windows).
Our examples show what we would see on MacOS.
\end{quote}

Now that we know where we are,
let's see what we have using the command \texttt{ls},
which stands for ``listing'':

\begin{verbatim}
$ ls
\end{verbatim}

\begin{verbatim}
Applications Documents    Library      Music        Public         todo.txt
Desktop      Downloads    Movies       Pictures     climate-data
\end{verbatim}

\texttt{ls} prints the names of the files and directories in the current directory.
(Again,
our results may be different depending on our operating system
and what files or directories we have.)

We can make the output of \texttt{ls} more comprehensible by using the \texttt{-F} \href{glossary.html\#command-option}{option}
(also sometimes called a \href{glossary.html\#command-switch}{switch} or a \href{glossary.html\#command-flag}{flag}).
Options are exactly like arguments to a function in R or Python;
in this case,
\texttt{-F} tells \texttt{ls} to add markers to its output to tell us what things are.
A trailing \texttt{/} indicates a directory,
while a trailing \texttt{*} tell us that something is a runnable program.
Depending on our setup,
the shell might also use colors to indicate whether each entry is a file or directory.

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
Applications/ Documents/    Library/      Music/        Public/        todo.txt
Desktop/      Downloads/    Movies/       Pictures/     climate-data/
\end{verbatim}

Here,
we can see that almost everything in our home directory is a \href{glossary.html\#subdirectory}{subdirectory};
the only thing that isn't is a file called \texttt{todo.tt}.

\begin{quote}
\textbf{Spaces Matter}

\texttt{1+2} and \texttt{1~+~2} mean the same thing in mathematics,
but \texttt{ls~-F} and \texttt{ls-F} are very different things in the shell.
The shell splits whatever we type into pieces based on spaces,
so if we forget to separate \texttt{ls} and \texttt{-F} with at least one space,
the shell will try to find a program called \texttt{ls-F} and (quite sensibly)
give an error message like \texttt{ls-F:\ command\ not\ found}.
\end{quote}

Some options tell a command how to behave,
but others tell it what to act on.
For example,
if we want to see what's in the \texttt{/Users} directory,
we can type:

\begin{verbatim}
$ ls /Users
\end{verbatim}

\begin{verbatim}
Amira   Jun     Sami
\end{verbatim}

We often call the file and directory names that we give to commands \href{glossary.html\#command-argument}{arguments}
to distinguish them from the built-in options.
We can combine options and arguments:

\begin{verbatim}
$ ls -F /Users
\end{verbatim}

\begin{verbatim}
Amira/  Jun/    Sami/
\end{verbatim}

but we must put the command's options (like \texttt{-F})
before the names of any files or directories we want to work on
because once the command encounters one of the latter
it assumes that we are done giving it built-in options:

\begin{verbatim}
$ ls /Users -F
\end{verbatim}

\begin{verbatim}
ls: -F: No such file or directory
Amira   Jun     Sami
\end{verbatim}

FIXME: check if -F behaves differently on Linux.

\hypertarget{rse-bash-basics-navigate}{%
\section{How can I move around in the shell?}\label{rse-bash-basics-navigate}}

Let's run \texttt{ls} again to look at what's in our current working directory,
which is what the command shows if we don't ask it to show us anything else:

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
Applications/ Documents/    Library/      Music/        Public/        todo.txt
Desktop/      Downloads/    Movies/       Pictures/     climate-data/
\end{verbatim}

If we want to see what's in \texttt{climate-data},
we can ask \texttt{ls} to list its contents:

\begin{verbatim}
$ ls -F climate-data
\end{verbatim}

\begin{verbatim}
NOTES           bin/            docs/           thesis.md       tofu.config
README.md       cleaned/        raw/            thesis.pdf
\end{verbatim}

Notice that \texttt{climate-data} doesn't have a leading slash before its name.
This absence tells the shell that it is a \href{glossary.html\#relative-path}{relative path},
i.e.,
that it should be interpreted starting from our current working directory.
In contrast,
a path like \texttt{/Users/amira} is an \href{glossary.html\#absolute-path}{absolute path}:
it is always interpreted from the root directory down,
so it always refers to the same thing.
Using a relative path is like telling someone to go two kilometers north and then half a kilometer east;
using an absolute path is like giving them the latitude and longitude of their destination.

We can use whichever kind of path is easiest to type,
but if we are going to do a lot of work with our climate data,
the easiest thing would be to change our current working directory
so that we don't have to type \texttt{climate-data} at all.
The command to do this is \texttt{cd},
which stands for change directory.
This name is a bit misleading:
the command doesn't change the directory,
it changes the shell's idea of what directory we are in.
Let's try it out:

\begin{verbatim}
$ cd climate-data
\end{verbatim}

\texttt{cd} doesn't print anything.
This is normal:
many shell commands run silently unless something goes wrong,
on the theory that they should only ask for our attention when they need it.
To confirm that \texttt{cd} has done what we asked,
we can use \texttt{pwd}:

\begin{verbatim}
$ pwd
\end{verbatim}

\begin{verbatim}
/Users/amira/climate-data
\end{verbatim}

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
NOTES           bin/            docs/           thesis.md       tofu.config
README.md       cleaned/        raw/            thesis.pdf
\end{verbatim}

\begin{quote}
\textbf{Missing Directories and Unknown Options}

If we give a command an option that it doesn't understand,
it will usually print an error message and (if we're lucky)
tersely remind us of what we should have done:

\begin{verbatim}
$ cd -j
\end{verbatim}

\begin{verbatim}
-bash: cd: -j: invalid option
cd: usage: cd [-L|-P] [dir]
\end{verbatim}

If, on the other hand,
we get the syntax right but make a mistake in the name of a file or directory,
it will tell us that:

\begin{verbatim}
$ cd whoops
\end{verbatim}

\begin{verbatim}
-bash: cd: whoops: No such file or directory
\end{verbatim}
\end{quote}

We now know how to go down the directory tree,
but how do we go up?
This doesn't work:

\begin{verbatim}
$ cd amira
\end{verbatim}

\begin{verbatim}
cd: amira: No such file or directory
\end{verbatim}

because \texttt{amira} on its own is a relative path meaning
``a file or directory called \texttt{amira} \emph{below our current working directory}''.
To get back home,
we can either use an absolute path:

\begin{verbatim}
$ cd /Users/amira
\end{verbatim}

or a special relative path called \texttt{..} (two periods in a row with no spaces),
which always means ``the directory that contains the current one''.
The directory one level above the one we are in is called the \href{glossary.html\#parent-directory}{parent directory},
and sure enough,
\texttt{..} gets us there:

\begin{verbatim}
$ cd ..
$ pwd
\end{verbatim}

\begin{verbatim}
/Users/amira
\end{verbatim}

\texttt{ls} usually doesn't show us this special directory---since it's always there,
displaying it every time would be a distraction.
We can ask \texttt{ls} to include it using the \texttt{-a} option,
which stands for ``all'':

\begin{verbatim}
$ ls -F -a
\end{verbatim}

\begin{verbatim}
./              Applications/   Documents/      Library/        Music/          Public/         todo.txt
../             Desktop/        Downloads/      Movies/         Pictures/       climate-data/
\end{verbatim}

(Remember,
we are now in \texttt{/Users/amira}.)
The output also shows another special directory called \texttt{.} (a single period),
which refers to the current working directory.
It may seem redundant to have a name for it,
but we'll see some uses for it soon.

The special names \texttt{.} and \texttt{..} don't belong to \texttt{cd}:
they are interpreted the same way by every program.
For example,
if we are in \texttt{/Users/amira/climate-data},
the command \texttt{ls~..} will display a listing of \texttt{/Users/amira}.
When the meanings of the parts are the same no matter how they're combined,
programmers say they are \href{glossary.html\#orthogonality}{orthogonal}.
Orthogonal systems tend to be easier for people to learn
because there are fewer interactions and exceptions to keep track of.

\begin{quote}
\textbf{Other Hidden Files}

In addition to the hidden directories \texttt{..} and \texttt{.},
we may also comes across files with names like \texttt{.jupyter} or \texttt{.Rhistory}.
These usually contain settings or other data for particular programs;
the prefix \texttt{.} is used to prevent \texttt{ls} from cluttering up the output
when we run \texttt{ls}.
We can always use the \texttt{-a} option to display them.
\end{quote}

\texttt{cd} is a simple command,
but it allows us to explore several new ideas.
First,
several \texttt{..} can be joined by the path separator
to move higher than the parent directory in a single step.
For example, \texttt{cd~../..} will move us up two directories
(e.g., from \texttt{/Users/amira/climate-data} to \texttt{/Users}),
while \texttt{cd~../Movies} will move us up from \texttt{climate-data} and back down into \texttt{Movies}.

What happens if we type \texttt{cd} on its own without giving a directory?

\begin{verbatim}
$ pwd
\end{verbatim}

\begin{verbatim}
/Users/amira/Movies
\end{verbatim}

\begin{verbatim}
$ cd
$ pwd
\end{verbatim}

\begin{verbatim}
/Users/amira
\end{verbatim}

This works no matter where we are:
\texttt{cd} on its own always returns us to our home directory.
We can achieve the same thing using the special directory name \texttt{\textasciitilde{}},
which is a shortcut for our home directory:

\begin{verbatim}
$ ls ~
\end{verbatim}

\begin{verbatim}
Applications    Documents       Library         Music           Public          todo.txt
Desktop         Downloads       Movies          Pictures        climate-data
\end{verbatim}

(\texttt{ls} doesn't show any trailing slashes here because we haven't used \texttt{-F}.)
We can use \texttt{\textasciitilde{}} in paths,
so that (for example) \texttt{\textasciitilde{}/Downloads} always refers to our download directory.

Finally,
\texttt{cd} interprets the shortcut \texttt{-} (a single dash) to mean the last directory we were in.
Using this is usually faster and more reliable than trying to remember and type the path,
but unlike \texttt{\textasciitilde{}},
it only works with \texttt{cd}:
\texttt{ls~-} tries to print a listing of a directory called \texttt{-}
rather than showing us the contents of our previous directory.

\hypertarget{rse-bash-basics-filedir}{%
\section{How can I create new files and directories?}\label{rse-bash-basics-filedir}}

We now know how to explore files and directories,
but how do we create them?
To find out,
let's go back to our \texttt{climate-data} directory:

\begin{verbatim}
$ cd ~/climate-data
$ ls -F
\end{verbatim}

\begin{verbatim}
NOTES           bin/            docs/           thesis.md       tofu.config
README.md       cleaned/        raw/            thesis.pdf
\end{verbatim}

To create a new directory,
we use the command \texttt{mkdir} (short for make dirirectory):

\begin{verbatim}
$ mkdir analysis
\end{verbatim}

Since \texttt{analysis} is a relative path
(i.e., does not have a leading slash)
the new directory is created in the current working directory:

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
NOTES           analysis/       cleaned/        raw/            thesis.pdf
README.md       bin/            docs/           thesis.md       tofu.config
\end{verbatim}

Using the shell to create a directory is no different than using a graphical tool.
If we look at the current directory with our computer's file browser
we will see the \texttt{analysis} directory there too.
The shell and the file explorer are two different ways of interacting with the files;
the files and directories themselves are the same.

\begin{quote}
\textbf{Naming Files and Directories}

Complicated names of files and directories can make our life painful.
Following a few simple rules can save a lot of headaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Don't use spaces.}
  Spaces can make a name easier to read,
  but since they are used to separate arguments on the command line,
  most shell commands interpret a name like \texttt{My\ Thesis} as two names \texttt{My} and \texttt{Thesis}.
  Use \texttt{-} or \texttt{\_} instead,
  e.g, \texttt{climate-data} or \texttt{climate\_data}.
\item
  \textbf{Don't begin the name with \texttt{-} (dash)}
  to avoid confusion with command options like \texttt{-F}.
\item
  \textbf{Stick with letters, digits, \texttt{.} (period or `full stop'), \texttt{-} (dash) and \texttt{\_} (underscore).}
  Many other characters mean special things in the shell.
  We will learn about some of these during this lesson,
  but these are always safe.
\end{enumerate}

If we need to refer to files or directories that have spaces or other special characters in their names,
we can surround the name in quotes (\texttt{""}).
For example, \texttt{ls\ "My\ Thesis"} will work where \texttt{ls\ My\ Thesis} does not.
\end{quote}

Since we just created the \texttt{thesis} directory,
it doesn't contain anything:

\begin{verbatim}
$ ls -F analysis
\end{verbatim}

Let's change our working directory to \texttt{analysis} using \texttt{cd},
then use a very simple text editor called \href{glossary.html\#nano}{Nano} to create a file called \texttt{draft.txt}
(Figure~\ref{fig:rse-bash-basics-nano}):

\begin{verbatim}
$ cd analysis
$ nano draft.txt
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-nano}The Nano Editor}
\end{figure}

We could just as easily have run \texttt{nano\ analysis/draft.txt} to edit the file.
What's more important is that when we say ``Nano is a text editor'' we really do mean ``text'':
it can only work with plain character data,
not spreadsheets, images, Microsoft Word files, or anything else invented after 1970.
We use it in this lesson because it runs everywhere
and because it is as simple as something can be and still be called an editor.
However,
that last trait means that we \emph{shouldn't} use it for larger tasks,
such as writing a program or a paper.
Chapter~\ref{tools} discusses some alternatives.

\begin{quote}
\textbf{Recycling Pixels}

Unlike most modern editors,
Nano runs \emph{inside} the shell window instead of opening a new window of its own.
This is a holdover from an era when graphical terminals were a rarity
and different applications had to share a single character-only screen.
\end{quote}

Once Nano is open we can type in a few lines of text,
then press Ctrl+O
(the Control key and the letter `O' at the same time)
to save our work.
Nano will ask us what file we want to save it to;
press Return to accept the suggested default of \texttt{draft.txt}.
Once our file is saved,
we can use Ctrl+X to exit the editor and return to the shell.

\begin{quote}
\textbf{Control, Ctrl, or \^{} Key}

The Control key,
also called the ``Ctrl'' key,
can be described in a bewildering variety of ways.
For example,
holding down Control and X at the same time
may be written as any of:

\begin{itemize}
\tightlist
\item
  \texttt{Control-X}
\item
  \texttt{Control+X}
\item
  \texttt{Ctrl-X}
\item
  \texttt{Ctrl+X}
\item
  \texttt{C-x}
\item
  \texttt{\^{}X}
\end{itemize}

When Nano runs
it displays some help in the bottom two lines of the screen
using the last of these notations:
for example,
\texttt{\^{}G\ Get\ Help} means ``use Control+G to get help''
and \texttt{\^{}O\ WriteOut} means ``use Control+O to write out the current file''.
\end{quote}

Nano doesn't leave any output on the screen after it exits,
but we can use \texttt{ls} to show that we have indeed created a new file \texttt{draft.txt}:
but \texttt{ls} now shows that we have created a file called \texttt{draft.txt}:

\begin{verbatim}
$ ls
\end{verbatim}

\begin{verbatim}
draft.txt
\end{verbatim}

\begin{quote}
\textbf{What's In A Name?}

All of Amira's files are named ``something dot something''.
This is just a convention:
we can call a file \texttt{mythesis} or almost anything else.
However,
most people use two-part names to help them (and their programs)
tell different kinds of files apart.
The second part of such a name is called the \href{glossary.html\#filename-extension}{filename extension}
and indicates what type of data the file holds:
\texttt{.txt} for plain text,
\texttt{.pdf} for a PDF document,
\texttt{.png} for a PNG image, and so on.
This is just a convention:
saving a PNG image of a whale as \texttt{whale.mp3}
doesn't somehow magically turn it into a recording of whalesong,
though it \emph{might} cause the operating system to try to open it with a music player
when someone double-clicks it.
\end{quote}

\hypertarget{rse-bash-basics-move}{%
\section{How can move files and directories around?}\label{rse-bash-basics-move}}

Let's go back to our \texttt{climate-data} directory:

\begin{verbatim}
cd ~/climate-data
\end{verbatim}

The \texttt{analysis} directory contains a file called \texttt{draft.txt}.
That isn't a particularly informative name,
so let's change it using \texttt{mv} (short for move):

\begin{verbatim}
$ mv analysis/draft.txt analysis/prior-work.txt
\end{verbatim}

The first argument tells \texttt{mv} what we are ``moving'',
while the second is where it's to go:
here,
moving \texttt{analysis/draft.txt} to \texttt{analysis/prior-work.txt}
has the same effect as renaming the file:

\begin{verbatim}
$ ls analysis
\end{verbatim}

\begin{verbatim}
prior-work.txt
\end{verbatim}

We must be careful when specifying the destination
because \texttt{mv} will silently overwrite any existing file with the same name.
An option \texttt{-i} (for ``interactive'') makes \texttt{mv} ask us for confirmation before overwriting.
\texttt{mv} also works on directories,
so \texttt{mv\ analysis\ first-paper} would rename the directory without changing its contents.

Now suppose we want to move \texttt{prior-work.txt} into the current working directory.
If we don't want to change the file's name,
just its location,
we can provide \texttt{mv} with a directory as a destination
and it will move the file there.
In this case,
the directory we want is the special name \texttt{.} that we mentioned earlier:

\begin{verbatim}
$ mv analysis/prior-work.txt .
\end{verbatim}

\texttt{ls} now shows us that \texttt{analysis} is empty:

\begin{verbatim}
$ ls analysis
\end{verbatim}

and that our current directory now contains our file:

\begin{verbatim}
$ ls
\end{verbatim}

\begin{verbatim}
NOTES           analysis/       cleaned/        prior-work.txt  thesis.md       tofu.config
README.md       bin/            docs/           raw/            thesis.pdf
\end{verbatim}

If we only want to check that the file exists,
we can give its name to \texttt{ls}
just like we can give the command the name of a directory:

\begin{verbatim}
$ ls prior-work.txt
\end{verbatim}

\begin{verbatim}
prior-work.txt
\end{verbatim}

\hypertarget{rse-bash-basics-copy}{%
\section{How can I copy files and directories?}\label{rse-bash-basics-copy}}

The \texttt{cp} command
(for copy,
and no, we don't know why the creators of Unix seemed to be allergic to vowels)
works like \texttt{mv} except it copies a file instead of moving it.
We can check that it did the right thing using \texttt{ls} with two arguments:

\begin{verbatim}
$ cp prior-work.txt analysis/section-1.txt
$ ls prior-work.txt analysis/section-1.txt
\end{verbatim}

\begin{verbatim}
analysis/section-1.txt  prior-work.txt
\end{verbatim}

Notice that \texttt{ls} shows the output in alphabetical order.
If we leave off the second filename and ask it to show us a file and a directory
(or multiple directories)
it lists them one by one:

\begin{verbatim}
$ ls prior-work.txt analysis
\end{verbatim}

\begin{verbatim}
prior-work.txt

analysis:
section-1.txt
\end{verbatim}

Copying a directory and its content is a little more complicated.
If we use \texttt{cp} on its own,
we get an error message:

\begin{verbatim}
$ cp analysis backup
\end{verbatim}

\begin{verbatim}
cp: analysis is a directory (not copied).
\end{verbatim}

If we really want to copy everything,
we must give \texttt{cp} the \texttt{-r} option (meaning \href{glossary.html\#recursion}{recursive}:

\begin{verbatim}
$ cp -r analysis backup
\end{verbatim}

Once again we can check the result with \texttt{ls}:

\begin{verbatim}
$ ls analysis backup
\end{verbatim}

\begin{verbatim}
analysis/:
section-1.txt

backup/:
section-1.txt
\end{verbatim}

\hypertarget{rse-bash-basics-rm}{%
\section{How can I delete files and directories?}\label{rse-bash-basics-rm}}

Let's tidy up by removing the \texttt{prior-work.txt} file we created in our \texttt{climate-data} directory.
The command to do this is \texttt{rm} (for remove):

\begin{verbatim}
$ rm prior-work.txt
\end{verbatim}

We can confirm the file has gone using \texttt{ls}:

\begin{verbatim}
$ ls prior-work.txt
\end{verbatim}

\begin{verbatim}
ls: prior-work.txt: No such file or directory
\end{verbatim}

Deleting is forever:
the Unix shell doesn't have a trash bin that we can recover deleted files from
(though most graphical interfaces for Unix do),
so when we delete a file,
it really is gone---or at least gone-ish.
Tools for finding and recovering deleted files do exist,
but there is no guarantee they will work,
since the computer may recycle the file's disk space at any time.

In a half-hearted attempt to stop us from erasing things accidentally,
\texttt{rm} refuses to delete directories:

\begin{verbatim}
$ rm analysis
\end{verbatim}

\begin{verbatim}
rm: analysis: is a directory
\end{verbatim}

\texttt{rm} can remove a directory and everything it contains
if we use the recursive option \texttt{-r}:

\begin{verbatim}
$ rm -r analysis
\end{verbatim}

\texttt{rm\ -r} should be used with great caution:
in most cases,
it's safest to add the \texttt{-i} option (for interactive)
to get \texttt{rm} to ask us to confirm each deletion.
As a halfway measure,
we can use \texttt{-v} (for verbose)
to get \texttt{rm} to print a message for each file it deletes.
This options works the same way with \texttt{mv} and \texttt{cp}.

\hypertarget{rse-bash-basicswildcard}{%
\section{How can I run commands on lots of files at once?}\label{rse-bash-basicswildcard}}

The \texttt{cleaned} directory in our climate data project contains
precipitation and temperature records from four weather stations in Australia:

\begin{verbatim}
$ ls cleaned
\end{verbatim}

\begin{verbatim}
andamooka_prec.csv      badingarra_prec.csv     bellambi_prec.csv       tuggeranong_prec.csv
andamooka_temp.csv      badingarra_temp.csv     bellambi_temp.csv       tuggeranong_temp.csv
\end{verbatim}

The \texttt{wc} command (short for word count)
can tell us how many lines, words, and letters there are in one of these files:

\begin{verbatim}
$ wc cleaned/andamooka_prec.csv
\end{verbatim}

\begin{verbatim}
20003   20004  401858 cleaned/andamooka_prec.csv
\end{verbatim}

\begin{quote}
\textbf{What's in a Word?}

The number of lines and number of words in this file are almost the same
because \texttt{wc} only considers spaces to be word breaks, not commas,
and the only space in this file occurs in the name of one of the columns.
\end{quote}

We could run \texttt{wc} seven more times to count find out how many lines there are in the other files,
but that would be a lot of typing
and we would probably make at least one mistake.
We can't just give \texttt{wc} the name of the directory as we do with \texttt{ls}:

\begin{verbatim}
$ wc cleaned
\end{verbatim}

\begin{verbatim}
wc: cleaned: read: Is a directory
\end{verbatim}

Instead,
we can use \href{glossary.html\#wildcard}{wildcards} to select a set of filenames at once.
The most commonly-used wildcard is \texttt{*};
it matches zero or more characters,
so \texttt{cleaned/*.csv} matches all of the files in the \texttt{cleaned} directory:

\begin{verbatim}
$ ls cleaned/*.csv
\end{verbatim}

\begin{verbatim}
cleaned/andamooka_prec.csv       cleaned/badingarra_prec.csv      cleaned/bellambi_prec.csv        cleaned/tuggeranong_prec.csv
cleaned/andamooka_temp.csv       cleaned/badingarra_temp.csv      cleaned/bellambi_temp.csv        cleaned/tuggeranong_temp.csv
\end{verbatim}

while \texttt{cleaned/b*.csv} only matches the four whose names begin with a `b':

\begin{verbatim}
$ ls cleaned/b*.csv
\end{verbatim}

\begin{verbatim}
cleaned/badingarra_prec.csv      cleaned/badingarra_temp.csv      cleaned/bellambi_prec.csv        cleaned/bellambi_temp.csv
\end{verbatim}

Wildcards are expanded to match filenames \emph{before} commands are run,
so they work exactly the same way for every command.
This means that we can use them with \texttt{wc} to (for example)
count the number of records in the precipitation files:

\begin{verbatim}
$ wc cleaned/*_prec.csv
\end{verbatim}

\begin{verbatim}
   20003   20004  401858 cleaned/andamooka_prec.csv
   21099   21100  406584 cleaned/badingarra_prec.csv
    8315    8316  169740 cleaned/bellambi_prec.csv
    8681    8682  175998 cleaned/tuggeranong_prec.csv
   58098   58102 1154180 total
\end{verbatim}

or the number of records in the files from Tuggeranong:

\begin{verbatim}
$ wc cleaned/tug*.csv
\end{verbatim}

\begin{verbatim}
    8681    8682  175998 cleaned/tuggeranong_prec.csv
    8680    8681  196212 cleaned/tuggeranong_temp.csv
   17361   17363  372210 total
\end{verbatim}

The exercises will introduce and explore other wildcards
(such as \texttt{?}, which matches exactly one character).
For now,
the only other thing we need to know is that
it's possible for a wildcard expression to \emph{not} match anything.
In this case,
the command will usually print an error message:

\begin{verbatim}
$ wc cleaned/*.txt
\end{verbatim}

\begin{verbatim}
wc: cleaned/*.txt: open: No such file or directory
\end{verbatim}

\hypertarget{rse-bash-basics-help}{%
\section{How can I find out what commands there are and how to use them?}\label{rse-bash-basics-help}}

By default,
\texttt{wc} displays lines, words, and characters,
but we can ask it to display only a count of lines:

\begin{verbatim}
$ wc -l cleaned/*_prec.csv
\end{verbatim}

\begin{verbatim}
   20003 cleaned/andamooka_prec.csv
   21099 cleaned/badingarra_prec.csv
    8315 cleaned/bellambi_prec.csv
    8681 cleaned/tuggeranong_prec.csv
   58098 total
\end{verbatim}

\texttt{wc} has other options as well.
We can use the \texttt{man} command (short for manual)
to find out what they are:

\begin{verbatim}
$ man wc
\end{verbatim}

FIXME: typeset the text below as a figure with callouts pointing at its parts.

\begin{verbatim}
WC(1)                     BSD General Commands Manual                    WC(1)

NAME
     wc -- word, line, character, and byte count

SYNOPSIS
     wc [-clmw] [file ...]

DESCRIPTION
     The wc utility displays the number of lines, words, and bytes contained
     in each input file, or standard input (if no file is specified) to the
     standard output.  A line is defined as a string of characters delimited
     by a <newline> character.  Characters beyond the final <newline> charac-
     ter will not be included in the line count.

     A word is defined as a string of characters delimited by white space
     characters.  White space characters are the set of characters for which
     the iswspace(3) function returns true.  If more than one input file is
     specified, a line of cumulative counts for all the files is displayed on
     a separate line after the output for the last file.

     The following options are available:

     -c      The number of bytes in each input file is written to the standard
             output.  This will cancel out any prior usage of the -m option.

     -l      The number of lines in each input file is written to the standard
             output.

     -m      The number of characters in each input file is written to the
             standard output.  If the current locale does not support multi-
             byte characters, this is equivalent to the -c option.  This will
             cancel out any prior usage of the -c option.

     -w      The number of words in each input file is written to the standard
             output.

     When an option is specified, wc only reports the information requested by
     that option.  The order of output always takes the form of line, word,
     byte, and file name.  The default action is equivalent to specifying the
     -c, -l and -w options.

     If no files are specified, the standard input is used and no file name is
     displayed.  The prompt will accept input until receiving EOF, or [^D] in
     most environments.

ENVIRONMENT
     The LANG, LC_ALL and LC_CTYPE environment variables affect the execution
     of wc as described in environ(7).

EXIT STATUS
     The wc utility exits 0 on success, and >0 if an error occurs.

EXAMPLES
     Count the number of characters, words and lines in each of the files
     report1 and report2 as well as the totals for both:

           wc -mlw report1 report2

COMPATIBILITY
     Historically, the wc utility was documented to define a word as a "maxi-
     mal string of characters delimited by <space>, <tab> or <newline> charac-
     ters".  The implementation, however, did not handle non-printing charac-
     ters correctly so that "  ^D^E  " counted as 6 spaces, while
     "foo^D^Ebar" counted as 8 characters.  4BSD systems after 4.3BSD modi-
     fied the implementation to be consistent with the documentation.  This
     implementation defines a "word" in terms of the iswspace(3) function,
     as required by IEEE Std 1003.2 ("POSIX.2").

SEE ALSO
     iswspace(3)

STANDARDS
     The wc utility conforms to IEEE Std 1003.1-2001 ("POSIX.1").

HISTORY
     A wc command appeared in Version 1 AT&T UNIX.

BSD                            February 23, 2005                           BSD
\end{verbatim}

\begin{quote}
\textbf{Navigating the Manual}

If our screen is too small to display an entire manual page at once,
the shell will use a \href{glossary.html\#pager}{pager} called \texttt{less} to show it piece by piece.
We can use  and  to move line-by-line
or Ctrl+Spacebar and Spacebar
to skip up and down one page at a time.
(B and F also work.)

To search for a character or word,
use / followed by the character or word to search for.
If the search produces multiple hits,
we can move between them using N (for ``next'').
To quit, press Q.
\end{quote}

This is a lot of information,
most of which isn't really useful.
Some commands have a \texttt{-\/-help} option to provide a succinct summary of possibilites,
but the best place to go for help these days is probably the \href{glossary.html\#tldr}{TLDR} website.
The acronym stands for ``too long, didn't read'',
and its help for \texttt{wc} displays this:

\begin{verbatim}
wc
Count words, bytes, or lines.

Count lines in file:
wc -l {{file}}

Count words in file:
wc -w {{file}}

Count characters (bytes) in file:
wc -c {{file}}

Count characters in file (taking multi-byte character sets into account):
wc -m {{file}}

edit this page on github
\end{verbatim}

As the last line suggests,
all of its examples are in a public GitHub repository
so that users like you can add the ones you wish it had.
For more information,
we can search on \href{https://stackoverflow.com/questions/tagged/bash}{Stack Overflow}
or browse the \href{http://www.gnu.org/manual/manual.html}{GNU manuals}
(particularly those for the \href{http://www.gnu.org/software/coreutils/manual/coreutils.html}{core GNU utilities},
which include many of the commands introduced in this lesson).
In all cases,
though,
we need to have some idea of what we're looking for in the first place:
someone who wants to know how many lines there are in a data file
is unlikely to think to look for \texttt{wc}.

\hypertarget{rse-bash-basics-pipe}{%
\section{How can I combine commands?}\label{rse-bash-basics-pipe}}

Now that we know a few basic commands,
we can introduce one of the shell's most powerful features:
the ease with which it lets us combine existing programs in new ways.
Let's go into the \texttt{climate-data/cleaned} directory
and count the number of lines in each file once again:

\begin{verbatim}
$ cd ~/climate-data/cleaned
$ wc -l *.csv
\end{verbatim}

\begin{verbatim}
   20003 andamooka_prec.csv
   18541 andamooka_temp.csv
   21099 badingarra_prec.csv
   20002 badingarra_temp.csv
    8315 bellambi_prec.csv
    8314 bellambi_temp.csv
    8681 tuggeranong_prec.csv
    8680 tuggeranong_temp.csv
  113635 total
\end{verbatim}

Which of these files is shortest?
It's easy to see when there are only eight files,
but what if there were eight thousand?
Our first step toward a solution is to run the command:

\begin{verbatim}
$ wc -l *.csv > lengths.txt
\end{verbatim}

The greater than symbol \texttt{\textgreater{}} tells the shell to \href{glossary.html\#redirection}{redirect} the command's output to a file
instead of printing it.
Nothing appears on the screen:
everything that would have appeared has gone into the file \texttt{lengths.txt} instead.
The shell creates this file if it doesn't exist,
or overwrites it if it already exists.
\texttt{ls\ lengths.txt} confirms that the file exists:

\begin{verbatim}
$ ls lengths.txt
\end{verbatim}

\begin{verbatim}
lengths.txt
\end{verbatim}

We can print the contents of \texttt{lengths.txt} using \texttt{cat}
(short for concatenate).
Its name comes from the fact that it will print as many files as we give it,
one after the other.
If we give it just one name,
all we get is that file:

\begin{verbatim}
$ cat lengths.txt
\end{verbatim}

\begin{verbatim}
   20003 andamooka_prec.csv
   18541 andamooka_temp.csv
   21099 badingarra_prec.csv
   20002 badingarra_temp.csv
    8315 bellambi_prec.csv
    8314 bellambi_temp.csv
    8681 tuggeranong_prec.csv
    8680 tuggeranong_temp.csv
  113635 total
\end{verbatim}

We can now use \texttt{sort} to sort the lines in this file:

\begin{verbatim}
$ sort lengths.txt
\end{verbatim}

\begin{verbatim}
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
    8681 tuggeranong_prec.csv
   18541 andamooka_temp.csv
   20002 badingarra_temp.csv
   20003 andamooka_prec.csv
   21099 badingarra_prec.csv
  113635 total
\end{verbatim}

Just to be safe,
we should use \texttt{sort}'s \texttt{-n} option to specify that we want to sort numerically;
without this,
\texttt{sort} would sort things alphabetically
and \texttt{10} would come before \texttt{2}.

\texttt{sort} does not change \texttt{lengths.txt},
but it sends its output to the screen just as \texttt{wc} did.
We can therefore put the sorted list of lines in another temporary file called \texttt{sorted-lengths.txt} using \texttt{\textgreater{}}:

\begin{verbatim}
$ sort lengths.txt > sorted-lengths.txt
\end{verbatim}

\begin{quote}
\textbf{Redirecting to the Same File}

It's tempting to send the output of \texttt{sort} back to the file it reads:

\begin{verbatim}
$ sort -n lengths.txt > lengths.txt
\end{verbatim}

However, all this does is wipe out the contents of \texttt{lengths.txt}.
The reason is that when the shell sees the redirection,
it opens the file on the right of the \texttt{\textgreater{}} for writing,
which erases anything that file contained.
It then runs \texttt{sort}, which finds itself reading from a newly-empty file.
\end{quote}

Creating intermediate files with names like \texttt{lengths.txt} and \texttt{sorted-lengths.txt} works,
but keeping track of those files and cleaning them up when they're no longer needed is a burden.
We can produce the same result more safely and with less typing using a \href{glossary.html\#pipe-unix}{pipe}:

\begin{verbatim}
$ wc -l *.csv | sort -n
\end{verbatim}

\begin{verbatim}
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
    8681 tuggeranong_prec.csv
   18541 andamooka_temp.csv
   20002 badingarra_temp.csv
   20003 andamooka_prec.csv
   21099 badingarra_prec.csv
  113635 total
\end{verbatim}

The vertical bar \texttt{\textbar{}} between the \texttt{wc} and \texttt{sort} commands
tells the shell that we want to use the output of the command on the left
as the input to the command on the right.
Putting it another way,
the downstream command doesn't read from a file;
instead,
it reads the output of the upstream command.

We can use \texttt{\textbar{}} to build pipes of any length.
For example,
we can use the command \texttt{head} to get just the first three lines of sorted data:

\begin{verbatim}
$ wc -l *.csv | sort -n | head -n 3
\end{verbatim}

\begin{verbatim}
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
\end{verbatim}

If we want the last line of this,
we can add a call to the \texttt{tail} command:

\begin{verbatim}
$ wc -l *.csv | sort -n | head -n 3 | tail -n 1
\end{verbatim}

\begin{verbatim}
    8680 tuggeranong_temp.csv
\end{verbatim}

and finally save our result:

\begin{verbatim}
$ wc -l *.csv | sort -n | head -n 3 | tail -n 1 > results.txt
\end{verbatim}

In practice,
most Unix users would create this pipeline exactly as we have,
by starting with a single command and adding others one by one,
checking the output after each change.
The shell makes this easy by letting us move up and down in our \href{glossary.html\#command-history-unix}{command history}
using the  and  keys
and editing old commands to create new ones.

\hypertarget{rse-bash-basics-stdio}{%
\section{How do pipes work (and why do I need to know)?}\label{rse-bash-basics-stdio}}

In order to use pipes and redirection effectively,
we need to know just a little about how they work.
When a computer runs a program---any program---it creates a \href{glossary.html\#process}{process} in memory
to hold the program's instructions and data.
Every process in Unix has an input channel called \href{glossary.html\#standard-input}{standard input}
and an output channel called \href{glossary.html\#standard-output}{standard output}.
(By this point you may be surprised that their names are so memorable, but don't worry:
most Unix programmers call them \href{glossary.html\#stdin}{stdin} and \href{glossary.html\#stdout}{stdout}).

The shell is a program like any other,
and like any other,
it runs inside a process.
Under normal circumstances its standard input is connected to our keyboard
and its standard output to our screen,
so it reads what we type and displays its output for us to see (Figure~\ref{fig:rse-bash-basics-stdio}a).
When we tell the shell to run a program
it creates a new process
and temporarily reconnects the keyboard and stream
to that process's standard input and output (Figure~\ref{fig:rse-bash-basics-stdio}b).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-stdio}Standard I/O}
\end{figure}

Redirection with \texttt{\textgreater{}} tells the shell to connect the program's standard output to a file
instead of to the screen (Figure~\ref{fig:rse-bash-basics-stdio}c).
If we provide one or more files for the command to read,
as with \texttt{sort\ lengths.txt},
the program reads data from those files.
If we don't provide any filenames,
though,
the Unix convention is for the program to read from standard input.
We can test this by running \texttt{sort} on its own,
typing in a few lines of text,
and then pressing Ctrl+D to signal the end of input.
\texttt{sort} will then sort and print whatever we typed:

\begin{verbatim}
$ sort
one
two
three
four
^D
\end{verbatim}

\begin{verbatim}
four
one
three
two
\end{verbatim}

When we create a pipe like \texttt{wc\ *.csv\ \textbar{}\ sort},
the shell creates one process for each command so that \texttt{wc} and \texttt{sort} will run simultaneously,
and then connects the standard output of \texttt{wc} directly to the standard input of \texttt{sort}
(Figure~\ref{fig:rse-bash-basics-misconnect}).
\texttt{wc} doesn't know whether its output is going to the screen,
another program,
or to a file via \texttt{\textgreater{}}.
Equally,
\texttt{sort} doesn't know if its input is coming from the keyboard or another process;
it just knows that it has to read, sort, and print.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-misconnect}Misconnection}
\end{figure}

\begin{quote}
\textbf{Why Isn't It Doing Anything?}

What happens if a command is supposed to process a file
but we don't give it a filename?
For example, what if we type:

\begin{verbatim}
$ wc -l
\end{verbatim}

but don't type \texttt{*.csv} (or anything else) after the command?
Since it doesn't have any filenames,
\texttt{wc} assumes it is supposed to read from the keyboard,
so it waits for us to type in some data.
It doesn't tell us this:
it just sits and waits.

This mistake can be hard to spot,
particularly if we put the filename at the end of the pipeline:

\begin{verbatim}
$ wc -l | sort bellambi_temp.csv
\end{verbatim}

In this case,
\texttt{sort} ignores standard input and reads the data in the file,
but \texttt{wc} still just sits there waiting for input.

If we make this mistake,
we can end the program by typing Ctrl+C.
We can also use this to interrupt programs that are taking a long time to run
or are trying to connect to a website that isn't responding.
\end{quote}

Just as we can redirect standard output with \texttt{\textgreater{}},
we can connect standard input to a file using \texttt{\textless{}}.
In the case of a single file,
this has the same effect as providing the file's name to the command:

\begin{verbatim}
$ wc < bellambi_temp.csv
\end{verbatim}

\begin{verbatim}
    8314    8316  184203
\end{verbatim}

If we try to use redirection with a wildcard,
though,
the shell \emph{doesn't} concatenate all of the matching files:

\begin{verbatim}
$ wc < *.csv
\end{verbatim}

\begin{verbatim}
-bash: *.csv: ambiguous redirect
\end{verbatim}

It also doesn't print the error message to standard output,
which we can prove by redirecting:

\begin{verbatim}
$ wc < *.csv > all.txt
\end{verbatim}

\begin{verbatim}
-bash: *.csv: ambiguous redirect
\end{verbatim}

\begin{verbatim}
$ cat all.txt
\end{verbatim}

\begin{verbatim}
cat: all.txt: No such file or directory
\end{verbatim}

Every process has a second output channel called \href{glossary.html\#standard-error}{standard error} (or \href{glossary.html\#stderr}{stderr}).
Programs use it for error messages
so that their attempts to tell us something has gone wrong don't vanish silently into an output file.
There are ways to redirect standard error,
but doing so is almost always a bad idea.

\hypertarget{rse-bash-basics-loops}{%
\section{How can I do the same operations on many files?}\label{rse-bash-basics-loops}}

A \href{glossary.html\#loop-unix}{loop} is a way to repeat a set of commands for each item in a list.
We can use them to build complex workflows out of simple pieces,
and,
like wildcards,
they reduce the typing we have to do and the number of mistakes we might make.

Let's suppose that we want to take a slice out of each temperature data file in the \texttt{cleaned} directory---
more specifically,
that we want to get the first three rows of each file
\emph{without} the header row.
If we only cared about one file,
we could use a simple pipeline to take the first four lines
and then take the last three of those:

\begin{verbatim}
$ head -n 4 andamooka_temp.csv | tail -n 3
\end{verbatim}

\begin{verbatim}
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,
\end{verbatim}

If we try to use a wildcard to select files,
we only get three lines of output,
not the 12 we expect:

\begin{verbatim}
$ head -n 4 *.csv | tail -n 3
\end{verbatim}

\begin{verbatim}
70339,1996,1,1,,
70339,1996,1,2,,
70339,1996,1,3,,
\end{verbatim}

The problem is that \texttt{head} is producing a single stream of output
containing four lines for each file
(along with a header telling us the file's name):

\begin{verbatim}
$ head -n 4 *.csv
\end{verbatim}

\begin{verbatim}
==> andamooka_temp.csv <==
Station,Year,Month,Day,Max Temp (C),Quality
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,

==> badingarra_temp.csv <==
Station,Year,Month,Day,Max Temp (C),Quality
9037,1965,1,1,40.6,Y
9037,1965,1,2,40,Y
9037,1965,1,3,41.1,Y

==> bellambi_temp.csv <==
Station,Year,Month,Day,Max Temp (C),Quality
68228,1997,1,1,,
68228,1997,1,2,,
68228,1997,1,3,,

==> tuggeranong_temp.csv <==
Station,Year,Month,Day,MaxTemp (C),Quality
70339,1996,1,1,,
70339,1996,1,2,,
70339,1996,1,3,,
\end{verbatim}

Let's try this instead:

\begin{verbatim}
$ for filename in andamooka_temp.csv badingarra_temp.csv
> do
>   head -n 4 $filename
> done
\end{verbatim}

\begin{verbatim}
Station,Year,Month,Day,Max Temp (C),Quality
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,
Station,Year,Month,Day,Max Temp (C),Quality
9037,1965,1,1,40.6,Y
9037,1965,1,2,40,Y
9037,1965,1,3,41.1,Y
\end{verbatim}

There is a lot going on here,
so we will break it down into pieces:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The keywords \texttt{for}\ldots{}\texttt{in}\ldots{}\texttt{do}\ldots{}\texttt{done} create the loop,
  and must always appear in that order.
\item
  \texttt{filename} is a \href{glossary.html\#variable}{variable}.
  At any moment it contains a value,
  which can change over time.
\item
  The loop runs once for each item in the list.
  Each time it runs,
  it assigns the next item to the variable,
  so in this case,
  \texttt{filename} will be \texttt{andamooka\_temp.csv} the first time around
  and \texttt{badingarra\_temp.csv} the second time.
\item
  The commands that the loop executes (which are called the \href{glossary.html\#loop-body}{body} of the loop)
  appear between \texttt{do} and \texttt{done}.
  Those commands use the current value of the variable \texttt{filename},
  but to get it,
  we must put a dollar sign \texttt{\$} in front of the variable's name.
  If we forget and use \texttt{filename} instead of \texttt{\$filename},
  the shell will think that we are actually referring to something called \texttt{filename}.
\item
  The shell prompt changes from \texttt{\$} to \texttt{\textgreater{}} and back again as we type in our loop.
  The second prompt is called a \href{glossary.html\#continuation-prompt}{continuation prompt};
  it is different to remind us that we haven't finished typing a complete command yet.
\end{enumerate}

So what does the loop actually do?
As the output shows,
it runs our little pipeline separately for each file.
Let's use a wildcard expression to select all four temperature files
and add another stage to the pipeline:

\begin{verbatim}
$ for filename in *_temp.csv
> do
>   head -n 4 $filename | tail -n 3
> done
\end{verbatim}

\begin{verbatim}
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,
9037,1965,1,1,40.6,Y
9037,1965,1,2,40,Y
9037,1965,1,3,41.1,Y
68228,1997,1,1,,
68228,1997,1,2,,
68228,1997,1,3,,
70339,1996,1,1,,
70339,1996,1,2,,
70339,1996,1,3,,
\end{verbatim}

The weather station IDs in the first column show that
we are in fact getting exactly three rows from each of four files.

\begin{quote}
\textbf{Same Symbols, Different Meanings}

The computer displays \texttt{\textgreater{}} as a shell continuation prompt when we are typing,
but when we type \texttt{\textgreater{}} ourselves,
it means ``redirect output''.
Similarly,
\texttt{\$} is a regular prompt when the computer prints it
but means ``get the value of a variable'' when we type.
This \href{glossary.html\#overloading}{overloading} is sometimes confusing,
but there are only so many symbols on a standard keyboard.
\end{quote}

\hypertarget{rse-bash-basics-meaningless}{%
\section{What do variable names mean?}\label{rse-bash-basics-meaningless}}

The short answer to this section's title is ``nothing''.
We have called the variable in this loop \texttt{filename}
to make its purpose clear to human readers,
but the shell doesn't care what the variable is called.
We could equally well write our loop as:

\begin{verbatim}
$ for f in *_temp.csv
> do
>   head -n 4 $f | tail -n 3
> done
\end{verbatim}

or:

\begin{verbatim}
$ for username in *_temp.csv
> do
>   head -n 4 $username | tail -n 3
> done
\end{verbatim}

\emph{Don't do this.}
Programs are only useful if people can understand them,
so meaningless names like \texttt{f} and misleading names like \texttt{username}
increase the odds of misunderstanding.

\hypertarget{rse-bash-basics-history}{%
\section{How can I re-do things I have done recently?}\label{rse-bash-basics-history}}

Loops are useful if we know in advance what we want to repeat,
but if we have already run commands,
we can still repeat.
One way is to use  and  to go up and down in our command history as described earlier.
Another is to use the \texttt{history} command to get a list of the last few hundred commands that have been executed:

\begin{verbatim}
$ history
\end{verbatim}

\begin{verbatim}
  ...
  571  wc -l *.csv | sort -n | head n 3
  572  wc -l *.csv | sort -n | head -n 3
  573  wc -l *.csv | sort -n | head -n 3 | tail -n 1
  ...
\end{verbatim}

and then use an exclamation mark \texttt{!} followed by a number to repeat one of those commands:

\begin{verbatim}
$ !572
\end{verbatim}

\begin{verbatim}
wc -l *.csv | sort -n | head -n 3
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
\end{verbatim}

The shell prints the command it is going to run to standard error rather than to standard output,
so that (for example) \texttt{!572~\textgreater{}~results.txt} puts the command's output into a file
\emph{without} also writing the command.

Having an accurate record of what we have done
and a simple way to repeat it
are yet another reason why people use the Unix shell.
In fact,
being able to repeat history is such a powerful idea
that the shell gives us several ways to do it.

\begin{itemize}
\tightlist
\item
  \texttt{!head} re-runs the most recent command starting with \texttt{head},
  while \texttt{!wc} re-runs the most recent starting with \texttt{wc}.
\item
  If we type Ctrl+R
  the shell does an interactive reverse search for whatever we type next.
  If we don't like the first thing it finds,
  we can type Ctrl+R again to go further back.
\end{itemize}

If we use \texttt{history}, , or Ctrl+R,
we will quickly notice that loops don't have to be broken across lines.
Instead,
their parts can be separated with semi-colons:

\begin{verbatim}
$ for filename in *_temp.csv; do head -n 4 $filename | tail -n 3; done
\end{verbatim}

This is fairly readable,
although even experienced users have a tendency to put the semi-colon after \texttt{do} instead of before it.
If our loop contains multiple commands,
though,
the multi-line format is much easier to read---compare this:

\begin{verbatim}
$ for filename in *_temp.csv
> do
>   echo $filename
>   head -n 4 $filename | tail -n 3
> done
\end{verbatim}

with this:

\begin{verbatim}
$ for filename in *_temp.csv; do echo $filename; head -n 4 $filename | tail -n 3; done
\end{verbatim}

\hypertarget{rse-bash-basics-autoname}{%
\section{How can I create new filenames automatically?}\label{rse-bash-basics-autoname}}

Suppose we want to create a backup copy of each precipitation data file.
If we don't want to change the files' names,
we can do this with \texttt{cp}:

\begin{verbatim}
$ cd ~/climate-data
$ mkdir backup
$ cp cleaned/*_prec.csv backup
\end{verbatim}

But what if we want the backup files to have the \href{glossary.html\#filename-extension}{extension} \texttt{.bak} instead of \texttt{.csv}?
\texttt{cp} can do this for a single file:

\begin{verbatim}
$ cp cleaned/andamooka_prec.csv backup/andamooka_prec.bak
\end{verbatim}

but can't do all the files at once:

\begin{verbatim}
$ cp cleaned/*_prec.csv backup/*_prec.bak
\end{verbatim}

\begin{verbatim}
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvXc] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvXc] source_file ... target_directory
\end{verbatim}

\texttt{backup/*\_prec.bak} doesn't match anything---those files don't yet exist---so
what we are actually asking \texttt{cp} to do is:

\begin{verbatim}
$ cp cleaned/andamooka_prec.csv cleaned/badingarra_prec.csv cleaned/bellambi_prec.csv cleaned/tuggeranong_prec.csv
\end{verbatim}

This doesn't work because \texttt{cp} only understands how to do two things:
copy a single file to create another file,
or copy a bunch of files into a directory.
If we give it more than two names as arguments,
it expects the last one to be a directory;
since it isn't,
it reports an error.

Instead,
let's copy all the files and then rename them:

\begin{verbatim}
$ cp cleaned/*_prec.csv backup
$ for filename in backup/*.csv
> do
>   mv $filename $filename.bak
> done
$ ls backup
\end{verbatim}

\begin{verbatim}
andamooka_prec.csv.bak badingarra_prec.csv.bak bellambi_prec.csv.bak tuggeranong_prec.csv.bak
\end{verbatim}

\hypertarget{rse-bash-basics-exercises}{%
\section{Exercises}\label{rse-bash-basics-exercises}}

\hypertarget{rse-bash-basics-ex-more-ls}{%
\subsection{\texorpdfstring{Exploring more \texttt{ls} flags}{Exploring more ls flags}}\label{rse-bash-basics-ex-more-ls}}

You can also use two options at the same time. What does the command \texttt{ls} do when used
with the \texttt{-l} option? What about if you use both the \texttt{-l} and the \texttt{-h} option?

Some of its output is about properties that we do not cover in this lesson (such
as file permissions and ownership), but the rest should be useful
nevertheless.

\hypertarget{rse-bash-basics-ex-ls-rt}{%
\subsection{Listing recursively and by time}\label{rse-bash-basics-ex-ls-rt}}

The command \texttt{ls\ -R} lists the contents of directories recursively, i.e., lists
their subdirectories, sub-subdirectories, and so on at each level. The command
\texttt{ls\ -t} lists things by time of last change, with most recently changed files or
directories first.

In what order does \texttt{ls\ -R\ -t} display things? Hint: \texttt{ls\ -l} uses a long listing
format to view timestamps.

\hypertarget{rse-bash-basics-ex-paths}{%
\subsection{Absolute and relative paths}\label{rse-bash-basics-ex-paths}}

Starting from \texttt{/Users/amanda/data},
which of the following commands could Amanda use to navigate to her home directory,
which is \texttt{/Users/amanda}?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{cd\ .}
\item
  \texttt{cd\ /}
\item
  \texttt{cd\ /home/amanda}
\item
  \texttt{cd\ ../..}
\item
  \texttt{cd\ \textasciitilde{}}
\item
  \texttt{cd\ home}
\item
  \texttt{cd\ \textasciitilde{}/data/..}
\item
  \texttt{cd}
\item
  \texttt{cd\ ..}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-resolve-rel-path}{%
\subsection{Relative path resolution}\label{rse-bash-basics-ex-resolve-rel-path}}

Using the filesystem shown in Figure~\ref{fig:rse-bash-basics-ex-rel-path},
if \texttt{pwd} displays \texttt{/Users/thing},
what will \texttt{ls\ -F\ ../backup} display?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{../backup:\ No\ such\ file\ or\ directory}
\item
  \texttt{2012-12-01\ 2013-01-08\ 2013-01-27}
\item
  \texttt{2012-12-01/\ 2013-01-08/\ 2013-01-27/}
\item
  \texttt{original/\ pnas\_final/\ pnas\_sub/}
\end{enumerate}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-ex-rel-path}Exercise Filesystem}
\end{figure}

\hypertarget{rse-bash-basics-ex-reading-ls}{%
\subsection{\texorpdfstring{\texttt{ls} reading comprehension}{ls reading comprehension}}\label{rse-bash-basics-ex-reading-ls}}

Using the filesystem shown in Figure~\ref{fig:rse-bash-basics-ex-reading-ls},
if \texttt{pwd} displays \texttt{/Users/backup},
and \texttt{-r} tells \texttt{ls} to display things in reverse order,
what command(s) will result in the following output:

\begin{verbatim}
pnas_sub/ pnas_final/ original/
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-bash-basics-ex-reading-ls}Filesystem for Exercise}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{ls\ pwd}
\item
  \texttt{ls\ -r\ -F}
\item
  \texttt{ls\ -r\ -F\ /Users/backup}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-touch}{%
\subsection{Creating files a different way}\label{rse-bash-basics-ex-touch}}

We have seen how to create text files using the \texttt{nano} editor.
Now, try the following command:

\begin{verbatim}
$ touch my_file.txt
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What did the \texttt{touch} command do?
  When you look at your current directory using the GUI file explorer,
  does the file show up?
\item
  Use \texttt{ls\ -l} to inspect the files. How large is \texttt{my\_file.txt}?
\item
  When might you want to create a file this way?
\end{enumerate}

\hypertarget{rse-bash-basics-ex-move-dot}{%
\subsection{Moving to the current folder}\label{rse-bash-basics-ex-move-dot}}

After running the following commands,
Jamie realizes that she put the files \texttt{sucrose.dat} and \texttt{maltose.dat} into the wrong folder:

\begin{verbatim}
$ ls -F
  analyzed/ raw/
$ ls -F analyzed
  fructose.dat glucose.dat maltose.dat sucrose.dat
$ cd raw/
\end{verbatim}

Fill in the blanks to move these files to the current folder
(i.e., the one she is currently in):

\begin{verbatim}
$ mv ___/sucrose.dat  ___/maltose.dat ___
\end{verbatim}

\hypertarget{rse-bash-basics-ex-renaming-files}{%
\subsection{Renaming files}\label{rse-bash-basics-ex-renaming-files}}

Suppose that you created a plain-text file in your current directory to contain a list of the
statistical tests you will need to do to analyze your data, and named it: \texttt{statstics.txt}

After creating and saving this file you realize you misspelled the filename! You want to
correct the mistake, which of the following commands could you use to do so?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{cp\ statstics.txt\ statistics.txt}
\item
  \texttt{mv\ statstics.txt\ statistics.txt}
\item
  \texttt{mv\ statstics.txt\ .}
\item
  \texttt{cp\ statstics.txt\ .}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-last-ls}{%
\subsection{Moving and copying}\label{rse-bash-basics-ex-last-ls}}

What is the output of the closing \texttt{ls} command in the sequence shown below?

\begin{verbatim}
$ pwd
\end{verbatim}

\begin{verbatim}
/Users/jamie/data
\end{verbatim}

\begin{verbatim}
$ ls
\end{verbatim}

\begin{verbatim}
proteins.dat
\end{verbatim}

\begin{verbatim}
$ mkdir recombine
$ mv proteins.dat recombine/
$ cp recombine/proteins.dat ../proteins-saved.dat
$ ls
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{proteins-saved.dat\ recombine}
\item
  \texttt{recombine}
\item
  \texttt{proteins.dat\ recombine}
\item
  \texttt{proteins-saved.dat}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-safe-rm}{%
\subsection{\texorpdfstring{Using \texttt{rm} safely}{Using rm safely}}\label{rse-bash-basics-ex-safe-rm}}

What happens when we execute \texttt{rm\ -i\ thesis\_backup/quotations.txt}?
Why would we want this protection when using \texttt{rm}?

\hypertarget{rse-bash-basics-ex-copy-multi}{%
\subsection{Copy with multiple filenames}\label{rse-bash-basics-ex-copy-multi}}

For this exercise, you can test the commands in the \texttt{data-shell/data} directory.

In the example below, what does \texttt{cp} do when given several filenames and a directory name?

\begin{verbatim}
$ mkdir backup
$ cp amino-acids.txt animals.txt backup/
\end{verbatim}

In the example below, what does \texttt{cp} do when given three or more file names?

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
amino-acids.txt  animals.txt  backup/  elements/  morse.txt  pdb/  planets.txt  salmon.txt  sunspot.txt
\end{verbatim}

\begin{verbatim}
$ cp amino-acids.txt animals.txt morse.txt
\end{verbatim}

\hypertarget{rse-bash-basics-ex-ls-match}{%
\subsection{List filenames matching a pattern}\label{rse-bash-basics-ex-ls-match}}

When run in the \texttt{molecules} directory, which \texttt{ls} command(s) will
produce this output?

\texttt{ethane.pdb\ \ \ methane.pdb}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{ls\ *t*ane.pdb}
\item
  \texttt{ls\ *t?ne.*}
\item
  \texttt{ls\ *t??ne.pdb}
\item
  \texttt{ls\ ethane.*}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-more-wildcards}{%
\subsection{More on wildcards}\label{rse-bash-basics-ex-more-wildcards}}

Sam has a directory containing calibration data, datasets, and descriptions of
the datasets:

\begin{verbatim}
.
 2015-10-23-calibration.txt
 2015-10-23-dataset1.txt
 2015-10-23-dataset2.txt
 2015-10-23-dataset_overview.txt
 2015-10-26-calibration.txt
 2015-10-26-dataset1.txt
 2015-10-26-dataset2.txt
 2015-10-26-dataset_overview.txt
 2015-11-23-calibration.txt
 2015-11-23-dataset1.txt
 2015-11-23-dataset2.txt
 2015-11-23-dataset_overview.txt
 backup
    calibration
    datasets
 send_to_bob
     all_datasets_created_on_a_23rd
     all_november_files
\end{verbatim}

Before heading off to another field trip, she wants to back up her data and
send some datasets to her colleague Bob. Sam uses the following commands
to get the job done:

\begin{verbatim}
$ cp *dataset* backup/datasets
$ cp ____calibration____ backup/calibration
$ cp 2015-____-____ send_to_bob/all_november_files/
$ cp ____ send_to_bob/all_datasets_created_on_a_23rd/
\end{verbatim}

Help Sam by filling in the blanks.

The resulting directory structure should look like this

\begin{verbatim}
.
 2015-10-23-calibration.txt
 2015-10-23-dataset1.txt
 2015-10-23-dataset2.txt
 2015-10-23-dataset_overview.txt
 2015-10-26-calibration.txt
 2015-10-26-dataset1.txt
 2015-10-26-dataset2.txt
 2015-10-26-dataset_overview.txt
 2015-11-23-calibration.txt
 2015-11-23-dataset1.txt
 2015-11-23-dataset2.txt
 2015-11-23-dataset_overview.txt
 backup
    calibration
       2015-10-23-calibration.txt
       2015-10-26-calibration.txt
       2015-11-23-calibration.txt
    datasets
        2015-10-23-dataset1.txt
        2015-10-23-dataset2.txt
        2015-10-23-dataset_overview.txt
        2015-10-26-dataset1.txt
        2015-10-26-dataset2.txt
        2015-10-26-dataset_overview.txt
        2015-11-23-dataset1.txt
        2015-11-23-dataset2.txt
        2015-11-23-dataset_overview.txt
 send_to_bob
     all_datasets_created_on_a_23rd
        2015-10-23-dataset1.txt
        2015-10-23-dataset2.txt
        2015-10-23-dataset_overview.txt
        2015-11-23-dataset1.txt
        2015-11-23-dataset2.txt
        2015-11-23-dataset_overview.txt
     all_november_files
         2015-11-23-calibration.txt
         2015-11-23-dataset1.txt
         2015-11-23-dataset2.txt
         2015-11-23-dataset_overview.txt
\end{verbatim}

\hypertarget{rse-bash-basics-ex-organizing}{%
\subsection{Organizing directories and files}\label{rse-bash-basics-ex-organizing}}

Jamie is working on a project and she sees that her files aren't very well
organized:

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
analyzed/  fructose.dat    raw/   sucrose.dat
\end{verbatim}

The \texttt{fructose.dat} and \texttt{sucrose.dat} files contain output from her data
analysis. What command(s) covered in this lesson does she need to run so that the commands below will
produce the output shown?

\begin{verbatim}
$ ls -F
\end{verbatim}

\begin{verbatim}
analyzed/   raw/
\end{verbatim}

\begin{verbatim}
$ ls analyzed
\end{verbatim}

\begin{verbatim}
fructose.dat    sucrose.dat
\end{verbatim}

\hypertarget{rse-bash-basics-ex-reproduce-structure}{%
\subsection{Reproduce a directory structure}\label{rse-bash-basics-ex-reproduce-structure}}

You're starting a new experiment, and would like to duplicate the directory
structure from your previous experiment so you can add new data.

Assume that the previous experiment is in a folder called `2016-05-18',
which contains a \texttt{data} folder that in turn contains folders named \texttt{raw} and
\texttt{processed} that contain data files. The goal is to copy the folder structure
of the \texttt{2016-05-18-data} folder into a folder called \texttt{2016-05-20}
so that your final directory structure looks like this:

\begin{verbatim}
2016-05-20/
 data
     processed
     raw
\end{verbatim}

Which of the following set of commands would achieve this objective?

What would the other commands do?

\begin{verbatim}
$ mkdir 2016-05-20
$ mkdir 2016-05-20/data
$ mkdir 2016-05-20/data/processed
$ mkdir 2016-05-20/data/raw
\end{verbatim}

\begin{verbatim}
$ mkdir 2016-05-20
$ cd 2016-05-20
$ mkdir data
$ cd data
$ mkdir raw processed
\end{verbatim}

\begin{verbatim}
$ mkdir 2016-05-20/data/raw
$ mkdir 2016-05-20/data/processed
\end{verbatim}

\begin{verbatim}
$ mkdir 2016-05-20
$ cd 2016-05-20
$ mkdir data
$ mkdir raw processed
\end{verbatim}

\hypertarget{rse-bash-basics-ex-sort-n}{%
\subsection{\texorpdfstring{What does \texttt{sort\ -n} do?}{What does sort -n do?}}\label{rse-bash-basics-ex-sort-n}}

If we run \texttt{sort} on a file containing the following lines:

\begin{verbatim}
10
2
19
22
6
\end{verbatim}

the output is:

\begin{verbatim}
10
19
2
22
6
\end{verbatim}

If we run \texttt{sort\ -n} on the same input, we get this instead:

\begin{verbatim}
2
6
10
19
22
\end{verbatim}

Explain why \texttt{-n} has this effect.

\hypertarget{rse-bash-basics-ex-redirect-append}{%
\subsection{\texorpdfstring{What does \texttt{\textgreater{}\textgreater{}} mean?}{What does \textgreater{}\textgreater{} mean?}}\label{rse-bash-basics-ex-redirect-append}}

We have seen the use of \texttt{\textgreater{}}, but there is a similar operator \texttt{\textgreater{}\textgreater{}} which works slightly differently.
We'll learn about the differences between these two operators by printing some strings.
We can use the \texttt{echo} command to print strings e.g.

\begin{verbatim}
$ echo The echo command prints text
\end{verbatim}

\begin{verbatim}
The echo command prints text
\end{verbatim}

Now test the commands below to reveal the difference between the two operators:

\begin{verbatim}
$ echo hello > testfile01.txt
\end{verbatim}

and:

\begin{verbatim}
$ echo hello >> testfile02.txt
\end{verbatim}

Hint: Try executing each command twice in a row and then examining the output files.

\hypertarget{rse-bash-basics-ex-append-data}{%
\subsection{Appending data}\label{rse-bash-basics-ex-append-data}}

We have already met the \texttt{head} command, which prints lines from the start of a file.
\texttt{tail} is similar, but prints lines from the end of a file instead.

Consider the file \texttt{data-shell/data/animals.txt}.
After these commands, select the answer that
corresponds to the file \texttt{animals-subset.txt}:

\begin{verbatim}
$ head -n 3 animals.txt > animals-subset.txt
$ tail -n 2 animals.txt >> animals-subset.txt
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first three lines of \texttt{animals.txt}
\item
  The last two lines of \texttt{animals.txt}
\item
  The first three lines and the last two lines of \texttt{animals.txt}
\item
  The second and third lines of \texttt{animals.txt}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-piping}{%
\subsection{Piping commands}\label{rse-bash-basics-ex-piping}}

In our current directory, we want to find the 3 files which have the least number of
lines. Which command listed below would work?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{wc\ -l\ *\ \textgreater{}\ sort\ -n\ \textgreater{}\ head\ -n\ 3}
\item
  \texttt{wc\ -l\ *\ \textbar{}\ sort\ -n\ \textbar{}\ head\ -n\ 1-3}
\item
  \texttt{wc\ -l\ *\ \textbar{}\ head\ -n\ 3\ \textbar{}\ sort\ -n}
\item
  \texttt{wc\ -l\ *\ \textbar{}\ sort\ -n\ \textbar{}\ head\ -n\ 3}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-uniq-adjacent}{%
\subsection{\texorpdfstring{Why does \texttt{uniq} only remove adjacent duplicates?}{Why does uniq only remove adjacent duplicates?}}\label{rse-bash-basics-ex-uniq-adjacent}}

The command \texttt{uniq} removes adjacent duplicated lines from its input.
For example, the file \texttt{data-shell/data/salmon.txt} contains:

\begin{verbatim}
coho
coho
steelhead
coho
steelhead
steelhead
\end{verbatim}

Running the command \texttt{uniq\ salmon.txt} from the \texttt{data-shell/data} directory produces:

\begin{verbatim}
coho
steelhead
coho
steelhead
\end{verbatim}

Why do you think \texttt{uniq} only removes \emph{adjacent} duplicated lines?
(Hint: think about very large data sets.) What other command could
you combine with it in a pipe to remove all duplicated lines?

\hypertarget{rse-bash-basics-ex-reading-pipes}{%
\subsection{Pipe reading comprehension}\label{rse-bash-basics-ex-reading-pipes}}

A file called \texttt{animals.txt} (in the \texttt{data-shell/data} folder) contains the following data:

\begin{verbatim}
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
2012-11-06,deer
2012-11-06,fox
2012-11-07,rabbit
2012-11-07,bear
\end{verbatim}

What text passes through each of the pipes and the final redirect in the pipeline below?

\begin{verbatim}
$ cat animals.txt | head -n 5 | tail -n 3 | sort -r > final.txt
\end{verbatim}

Hint: build the pipeline up one command at a time to test your understanding

\hypertarget{rse-bash-basics-ex-pipe-construction}{%
\subsection{Pipe construction}\label{rse-bash-basics-ex-pipe-construction}}

For the file \texttt{animals.txt} from the previous exercise, consider the following command:

\begin{verbatim}
$ cut -d , -f 2 animals.txt
\end{verbatim}

The \texttt{cut} command is used to remove or ``cut out'' certain sections of each line in the file.
The \texttt{-d} option is used to define the delimiter.
A \textbf{delimiter} is a character that is used to separate each line of text into columns.
The default delimiter is Tab,
meaning that the \texttt{cut} command will automatically assume that
values in different columns will be separated by a tab.
The \texttt{-f} option is used to specify the field (column) to cut out.
The command above uses the \texttt{-d} option to split each line by comma,
and the \texttt{-f} option to print the second field in each line, to give the following output:

\begin{verbatim}
deer
rabbit
raccoon
rabbit
deer
fox
rabbit
bear
\end{verbatim}

What other command(s) could be added to this in a pipeline to find
out what animals the file contains (without any duplicates in their
names)?

\hypertarget{rse-bash-basics-ex-which-pipe}{%
\subsection{Which pipe?}\label{rse-bash-basics-ex-which-pipe}}

The file \texttt{animals.txt} contains 8 lines of data formatted as follows:

\begin{verbatim}
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
...
\end{verbatim}

The \texttt{uniq} command has a \texttt{-c} option which gives a count of the
number of times a line occurs in its input. Assuming your current
directory is \texttt{data-shell/data/}, what command would you use to produce
a table that shows the total count of each type of animal in the file?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{sort\ animals.txt\ \textbar{}\ uniq\ -c}
\item
  \texttt{sort\ -t,\ -k2,2\ animals.txt\ \textbar{}\ uniq\ -c}
\item
  \texttt{cut\ -d,\ -f\ 2\ animals.txt\ \textbar{}\ uniq\ -c}
\item
  \texttt{cut\ -d,\ -f\ 2\ animals.txt\ \textbar{}\ sort\ \textbar{}\ uniq\ -c}
\item
  \texttt{cut\ -d,\ -f\ 2\ animals.txt\ \textbar{}\ sort\ \textbar{}\ uniq\ -c\ \textbar{}\ wc\ -l}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-wildcard-expressions}{%
\subsection{Wildcard expressions}\label{rse-bash-basics-ex-wildcard-expressions}}

Wildcard expressions can be very complex, but you can sometimes write
them in ways that only use simple syntax, at the expense of being a bit
more verbose.
Consider the directory \texttt{data-shell/north-pacific-gyre/2012-07-03} :
the wildcard expression \texttt{*{[}AB{]}.txt}
matches all files ending in \texttt{A.txt} or \texttt{B.txt}. Imagine you forgot about
this.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Can you match the same set of files with basic wildcard expressions
  that do not use the \texttt{{[}{]}} syntax? \emph{Hint}: You may need more than one
  expression.
\item
  The expression that you found and the expression from the lesson match the
  same set of files in this example. What is the small difference between the
  outputs?
\item
  Under what circumstances would your new expression produce an error message
  where the original one would not?
\end{enumerate}

\hypertarget{rse-bash-basics-ex-remove-unneeded}{%
\subsection{Removing unneeded files}\label{rse-bash-basics-ex-remove-unneeded}}

Suppose you want to delete your processed data files, and only keep
your raw files and processing script to save storage.
The raw files end in \texttt{.dat} and the processed files end in \texttt{.txt}.
Which of the following would remove all the processed data files,
and \emph{only} the processed data files?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{rm\ ?.txt}
\item
  \texttt{rm\ *.txt}
\item
  \texttt{rm\ *\ .txt}
\item
  \texttt{rm\ *.*}
\end{enumerate}

\hypertarget{rse-bash-basics-ex-loop-dry-run}{%
\subsection{Doing a dry run}\label{rse-bash-basics-ex-loop-dry-run}}

A loop is a way to do many things at once---or to make many mistakes at
once if it does the wrong thing. One way to check what a loop \emph{would} do
is to \texttt{echo} the commands it would run instead of actually running them.

Suppose we want to preview the commands the following loop will execute
without actually running those commands:

\begin{verbatim}
$ for file in *.pdb
> do
>   analyze $file > analyzed-$file
> done
\end{verbatim}

What is the difference between the two loops below, and which one would we
want to run?

\begin{verbatim}
# Version 1
$ for file in *.pdb
> do
>   echo analyze $file > analyzed-$file
> done
\end{verbatim}

\begin{verbatim}
# Version 2
$ for file in *.pdb
> do
>   echo "analyze $file > analyzed-$file"
> done
\end{verbatim}

\hypertarget{rse-bash-basics-ex-nested-loops}{%
\subsection{Nested loops}\label{rse-bash-basics-ex-nested-loops}}

Suppose we want to set up up a directory structure to organize
some experiments measuring reaction rate constants with different compounds
\emph{and} different temperatures. What would be the
result of the following code:

\begin{verbatim}
$ for species in cubane ethane methane
> do
>     for temperature in 25 30 37 40
>     do
>         mkdir $species-$temperature
>     done
> done
\end{verbatim}

\hypertarget{rse-bash-basics-ex-loop-variables}{%
\subsection{Variables in loops}\label{rse-bash-basics-ex-loop-variables}}

This exercise refers to the \texttt{data-shell/molecules} directory.
\texttt{ls} gives the following output:

\begin{verbatim}
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
\end{verbatim}

What is the output of the following code?

\begin{verbatim}
$ for datafile in *.pdb
> do
>    ls *.pdb
> done
\end{verbatim}

Now, what is the output of the following code?

\begin{verbatim}
$ for datafile in *.pdb
> do
>   ls $datafile
> done
\end{verbatim}

Why do these two loops give different outputs?

\hypertarget{rse-bash-basics-ex-limiting-file-sets}{%
\subsection{Limiting sets of files}\label{rse-bash-basics-ex-limiting-file-sets}}

What would be the output of running the following loop in the \texttt{data-shell/molecules} directory?

\begin{verbatim}
$ for filename in c*
> do
>    ls $filename
> done
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No files are listed.
\item
  All files are listed.
\item
  Only \texttt{cubane.pdb}, \texttt{octane.pdb} and \texttt{pentane.pdb} are listed.
\item
  Only \texttt{cubane.pdb} is listed.
\end{enumerate}

How would the output differ from using this command instead?

\begin{verbatim}
$ for filename in *c*
> do
>    ls $filename
> done
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The same files would be listed.
\item
  All the files are listed this time.
\item
  No files are listed this time.
\item
  The files \texttt{cubane.pdb} and \texttt{octane.pdb} will be listed.
\item
  Only the file \texttt{octane.pdb} will be listed.
\end{enumerate}

\hypertarget{rse-bash-basics-ex-loop-save}{%
\subsection{Saving to a file in a loop}\label{rse-bash-basics-ex-loop-save}}

In the \texttt{data-shell/molecules} directory, what is the effect of this loop?

\begin{verbatim}
for alkanes in *.pdb
> do
>     echo $alkanes
>     cat $alkanes > alkanes.pdb
> done
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prints \texttt{cubane.pdb}, \texttt{ethane.pdb}, \texttt{methane.pdb}, \texttt{octane.pdb}, \texttt{pentane.pdb} and \texttt{propane.pdb},
  and the text from \texttt{propane.pdb} will be saved to a file called \texttt{alkanes.pdb}.
\item
  Prints \texttt{cubane.pdb}, \texttt{ethane.pdb}, and \texttt{methane.pdb}, and the text from all three files would be
  concatenated and saved to a file called \texttt{alkanes.pdb}.
\item
  Prints \texttt{cubane.pdb}, \texttt{ethane.pdb}, \texttt{methane.pdb}, \texttt{octane.pdb}, and \texttt{pentane.pdb}, and the text
  from \texttt{propane.pdb} will be saved to a file called \texttt{alkanes.pdb}.
\item
  None of the above.
\end{enumerate}

Also in the \texttt{data-shell/molecules} directory, what would be the output of the following loop?

\begin{verbatim}
for datafile in *.pdb
> do
>     cat $datafile >> all.pdb
> done
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All of the text from \texttt{cubane.pdb}, \texttt{ethane.pdb}, \texttt{methane.pdb}, \texttt{octane.pdb}, and
  \texttt{pentane.pdb} would be concatenated and saved to a file called \texttt{all.pdb}.
\item
  The text from \texttt{ethane.pdb} will be saved to a file called \texttt{all.pdb}.
\item
  All of the text from \texttt{cubane.pdb}, \texttt{ethane.pdb}, \texttt{methane.pdb}, \texttt{octane.pdb}, \texttt{pentane.pdb}
  and \texttt{propane.pdb} would be concatenated and saved to a file called \texttt{all.pdb}.
\item
  All of the text from \texttt{cubane.pdb}, \texttt{ethane.pdb}, \texttt{methane.pdb}, \texttt{octane.pdb}, \texttt{pentane.pdb}
  and \texttt{propane.pdb} would be printed to the screen and saved to a file called \texttt{all.pdb}.
\end{enumerate}

\hypertarget{rse-bash-basics-ex-list-unique}{%
\subsection{List unique species}\label{rse-bash-basics-ex-list-unique}}

Leah has several hundred data files, each of which is formatted like this:

\begin{verbatim}
2013-11-05,deer,5
2013-11-05,rabbit,22
2013-11-05,raccoon,7
2013-11-06,rabbit,19
2013-11-06,deer,2
2013-11-06,fox,1
2013-11-07,rabbit,18
2013-11-07,bear,1
\end{verbatim}

An example of this type of file is given in \texttt{data-shell/data/animal-counts/animals.txt}.

We can use the command \texttt{cut\ -d\ ,\ -f\ 2\ animals.txt\ \textbar{}\ sort\ \textbar{}\ uniq}
to produce the unique species in \texttt{animals.txt}.
In order to avoid having to type out this series of commands every time,
a scientist may choose to write a shell script instead.

Write a shell script called \texttt{species.sh} that takes any number of
filenames as command-line arguments,
and uses a variation of the above command to print a list
of the unique species appearing in each of those files separately.

\hypertarget{rse-bash-basics-ex-history-order}{%
\subsection{\texorpdfstring{Why does \texttt{history} record commands before running them?}{Why does history record commands before running them?}}\label{rse-bash-basics-ex-history-order}}

If you run the command:

\begin{verbatim}
$ history | tail -n 5 > recent.sh
\end{verbatim}

the last command in the file is the \texttt{history} command itself, i.e.,
the shell has added \texttt{history} to the command log before actually
running it. In fact, the shell \emph{always} adds commands to the log
before running them. Why do you think it does this?

\hypertarget{rse-bash-basics-keypoints}{%
\section{Key Points}\label{rse-bash-basics-keypoints}}

\begin{itemize}
\tightlist
\item
  A shell is a program whose primary purpose is to read commands and run other programs.
\item
  The shell's main advantages are its high action-to-keystroke ratio,
  its support for automating repetitive tasks,
  and its capacity to access networked machines.
\item
  The shell's main disadvantages are its primarily textual nature and how cryptic its commands and operation can be.
\item
  The file system is responsible for managing information on the disk.
\item
  Information is stored in files, which are stored in directories (folders).
\item
  Directories can also store other directories, which forms a directory tree.
\item
  \texttt{cd\ path} changes the current working directory.
\item
  \texttt{ls\ path} prints a listing of a specific file or directory; \texttt{ls} on its own lists the current working directory.
\item
  \texttt{pwd} prints the user's current working directory.
\item
  \texttt{/} on its own is the root directory of the whole file system.
\item
  A relative path specifies a location starting from the current location.
\item
  An absolute path specifies a location from the root of the file system.
\item
  Directory names in a path are separated with \texttt{/} on Unix, but \texttt{\textbackslash{}\textbackslash{}} on Windows.
\item
  \texttt{..} means `the directory above the current one'; \texttt{.} on its own means `the current directory'.
\item
  \texttt{cp\ old\ new} copies a file.
\item
  \texttt{mkdir\ path} creates a new directory.
\item
  \texttt{mv\ old\ new} moves (renames) a file or directory.
\item
  \texttt{rm\ path} removes (deletes) a file.
\item
  \texttt{*} matches zero or more characters in a filename, so \texttt{*.txt} matches all files ending in \texttt{.txt}.
\item
  \texttt{?} matches any single character in a filename, so \texttt{?.txt} matches \texttt{a.txt} but not \texttt{any.txt}.
\item
  Use of the Control key may be described in many ways, including \texttt{Ctrl-X}, \texttt{Control-X}, and \texttt{\^{}X}.
\item
  The shell does not have a trash bin: once something is deleted, it's really gone.
\item
  Most files' names are \texttt{something.extension}.
  The extension isn't required and doesn't guarantee anything,
  but is normally used to indicate the type of data in the file.
\item
  Depending on the type of work you do, you may need a more powerful text editor than Nano.
\item
  \texttt{cat} displays the contents of its inputs.
\item
  \texttt{head} displays the first 10 lines of its input.
\item
  \texttt{tail} displays the last 10 lines of its input.
\item
  \texttt{sort} sorts its inputs.
\item
  \texttt{wc} counts lines, words, and characters in its inputs.
\item
  \texttt{command\ \textgreater{}\ file} redirects a command's output to a file (overwriting any existing content).
\item
  \texttt{command\ \textgreater{}\textgreater{}\ file} appends a command's output to a file.
\item
  \texttt{\textless{}} operator redirects input to a command
\item
  \texttt{first\ \textbar{}\ second} is a pipeline: the output of the first command is used as the input to the second.
\item
  The best way to use the shell is to use pipes to combine simple single-purpose programs (filters).
\item
  A \texttt{for} loop repeats commands once for every thing in a list.
\item
  Every \texttt{for} loop needs a variable to refer to the thing it is currently operating on.
\item
  Use \texttt{\$name} to expand a variable (i.e., get its value). \texttt{\$\{name\}} can also be used.
\item
  Do not use spaces, quotes, or wildcard characters such as '*`or'?' in filenames, as it complicates variable expansion.
\item
  Give files consistent names that are easy to match with wildcard patterns to make it easy to select them for looping.
\item
  Use the up-arrow key to scroll up through previous commands to edit and repeat them.
\item
  Use \texttt{Ctrl-R} to search through the previously entered commands.
\item
  Use \texttt{history} to display recent commands, and \texttt{!number} to repeat a command by number.
\end{itemize}

\hypertarget{rse-bash-advanced}{%
\chapter{Going Further with the Unix Shell}\label{rse-bash-advanced}}

Chapter~\ref{rse-bash-basics} explained how we can use the command line
to do all of the things we can do with a graphical file explorer,
and how to go beyond that to combine commands in powerful ways using pipes and redirection.
This chapter extends those ideas to show how we can save commands in files
to create new tools of our own,
and how to use a more powerful version of \href{glossary.html\#wildcard}{wildcards}
to extract data from files.

\hypertarget{rse-bash-advanced-script}{%
\section{How can I create new commands of my own?}\label{rse-bash-advanced-script}}

Loops and history let us do tasks repeatedly,
but we can go even further and save commands in files
so that we can re-run complex sequences of operations with a few keystrokes.
For historical reasons,
a file full of shell commands is usually called a \href{glossary.html\#shell-script}{shell script},
but it is really just another kind of program.

Let's start by going into \texttt{climate-data} and creating a new file called \texttt{years.sh}
to hold our shell script:

\begin{verbatim}
$ cd ~/climate-data
$ nano years.sh
\end{verbatim}

Insert this line:

\begin{verbatim}
cut -d , -f 2 cleaned/bellambi_temp.csv
\end{verbatim}

This uses the \texttt{cut} command to split the CSV file on commas
and select the second field from each line.
(The option \texttt{-d} stands for \href{glossary.html\#delimiter}{delimiter}:
we can provide any character we want to split lines on colons, spaces, and so on.)
Note that we do \emph{not} put a dollar sign \texttt{\$} at the front of the line:
we have been showing that for interactive commands,
but in this case we are putting the command in a file rather than running it immediately.

Once we have added this line to the file,
we can write it out with Ctrl+O
and exit with Ctrl+O.
\texttt{ls} shows that our file now exists:

\begin{verbatim}
$ ls
\end{verbatim}

\begin{verbatim}
NOTES           backup/         cleaned/        raw/            thesis.pdf      years.sh
README.md       bin/            docs/           thesis.md       tofu.config
\end{verbatim}

and we can check its contents using \texttt{cat\ years.sh}.
More importantly,
we can now ask the shell to run this file:

\begin{verbatim}
$ bash years.sh
\end{verbatim}

\begin{verbatim}
Year
1997
1997
...many more lines...
2019
2019
2019
\end{verbatim}

Sure enough,
our script's output is exactly what we would get if we ran the command directly.
For example,
we can count how many lines of output there are by putting our script in a pipeline:

\begin{verbatim}
$ bash years.sh | wc -l
\end{verbatim}

\begin{verbatim}
    8314
\end{verbatim}

What if we want to remove duplicates from the output?
The command \texttt{uniq} will do what we want,
so let's edit our script,
add it,
look at our changes,
and then run the modified script:

\begin{verbatim}
$ nano years.sh
$ cat years.sh
\end{verbatim}

\begin{verbatim}
cut -d , -f 2 cleaned/bellambi_temp.csv | uniq
\end{verbatim}

\begin{verbatim}
$ bash years.sh
\end{verbatim}

\begin{verbatim}
Year
1997
1998
1999
...one line per year...
2017
2018
2019
\end{verbatim}

Once again,
we can pipe the output of our script into other commands
just as we would pipe the output from any other program:

\begin{verbatim}
$ bash years.sh | wc -l
\end{verbatim}

\begin{verbatim}
      24
\end{verbatim}

\hypertarget{rse-bash-advanced-params}{%
\section{How can I make my scripts more versatile?}\label{rse-bash-advanced-params}}

Creating a list of distinct years in a single specific data file isn't all that useful.
What we really want is a way to get the years from any of our files.
Let's edit \texttt{years.sh} again and replace \texttt{cleaned/bellambi\_temp.csv}
with a special variable \texttt{\$1}.
Once our change is made,
\texttt{years.sh} should contain:

\begin{verbatim}
cut -d , -f 2 $1 | uniq
\end{verbatim}

Inside a shell script,
\texttt{\$1} means ``the first argument on the command line''.
We can now run our script like this:

\begin{verbatim}
$ bash years.sh cleaned/bellambi_temp.csv
\end{verbatim}

and get exactly the same output as before,
or give it a different filename:

\begin{verbatim}
$ bash years.sh cleaned/andamooka_prec.csv
\end{verbatim}

and get the years from that file instead.

Our little script is now doing something useful,
but it may take the next person who reads it a moment to figure out exactly what that is.
We can improve our script by adding \href{glossary.html\#comment}{comments} at the top:

\begin{verbatim}
# Select distinct years from column 2 of climate data file.
# Usage: bash years.sh /path/to/file.csv
cut -d , -f 2 $1 | uniq
\end{verbatim}

As in R and Python,
a comment starts with a \texttt{\#} character and runs to the end of the line.
The computer ignores comments,
but they help people (including your future self) understand and use scripts.

Let's make one more change to our script.
Instead of always selecting the second column,
let's have it select whatever column the user specified:

\begin{verbatim}
# Select distinct years from column 2 of climate data file.
# Usage: bash years.sh /path/to/file.csv
cut -d , -f $2 $1 | uniq
\end{verbatim}

The change is very small:
we have replaced the fixed column number \texttt{2} with a reference to the special variable \texttt{\$2},
which is assigned the value of the second command-line argument we give the script when we run it.
Let's check that it works by asking for column 1,
which is the weather station ID:

\begin{verbatim}
$ bash years.sh cleaned/bellambi_prec.csv 1
\end{verbatim}

\begin{verbatim}
Station
68228
\end{verbatim}

But we have made a common mistake:
we have changed the script without changing the comment.
A description that sends readers in the wrong direction is worse than none at all,
so we should go back and update it.
We should probably also change the script's name from \texttt{years.sh} to \texttt{column.sh},
since a program's name is the first piece of documentation anyone sees.

And finally,
we should add one more command to our pipeline.
If we run the script as-is for column 3,
which holds months,
we get this:

\begin{verbatim}
$ bash years.sh cleaned/bellambi_prec.csv 3
\end{verbatim}

Month
1
2
\ldots{}
11
12
1
2
\ldots{}
8
9
10
```

Duplicate months aren't removed because \texttt{uniq} only removes \emph{adjacent} duplicates.
If we want to get rid of them all,
we must sort the data so that redundant lines are next to one another.
Here's our final script:

\begin{verbatim}
# Select distinct values from a column of a climate data file.
# Usage: bash years.sh /path/to/file.csv column_number
cut -d , -f $2 $1 | uniq
\end{verbatim}

\hypertarget{rse-bash-advanced-capture}{%
\section{How can I turn interactive work into a script?}\label{rse-bash-advanced-capture}}

Suppose we have just run a series of commands that did something useful,
such as creating a plot for a paper.
Instead of typing those commands into a file in an editor
(and potentially getting them wrong)
we can run this:

\begin{verbatim}
$ history 6 > make-figure-3.sh
\end{verbatim}

to put the most recent five commands in \texttt{make-figure-3.sh}.

\begin{verbatim}
297 bash stats.sh cleaned/*_temp.csv > temperature_stats.csv
298 bash trim-outliers.sh temperature_stats.csv > plot_data.txt
299 date
300 ygraph --format scatter --color bw --borders none plot_data.txt figure-3.png
301 rm temperature_stats.csv plot_data.txt
302 history 6 > make-figure-3.sh
\end{verbatim}

It only takes a few moments in an editor to remove the serial numbers
and delete the use of \texttt{date}
(which prints the current time and date)
to create a script that accurately captures what we actually did.
This is how we usually develop shell scripts:
run commands interactively a few times to make sure they are doing the right thing,
then save our recent history to a file and turn that into a reusable script.

\hypertarget{rse-bash-advanced-grep}{%
\section{How can I find things in a file?}\label{rse-bash-advanced-grep}}

We can use \texttt{head} and \texttt{tail} to select lines from a file by position,
but we also often want to select lines that contain certain values.
This operation is called \href{glossary.html\#filter}{filtering} When we are working with database tables or dataframes,
and in the shell,
we usually do it using a command called \texttt{grep}.
The name comes from ``global regular expression print'',
which was a common sequence of operations in early Unix text editors.
To show how \texttt{grep} works,
we will use a file that contains three haikus
taken from a 1998 competition in \emph{Salon} magazine.

\begin{verbatim}
$ cat haiku.txt
\end{verbatim}

\begin{verbatim}
The Tao that is seen
Is not the true Tao, until
You bring fresh toner.

With searching comes loss
and the presence of absence:
"My Thesis" not found.

Yesterday it worked
Today it is not working
Software is like that.
\end{verbatim}

\begin{quote}
\textbf{Forever, or Five Years}

We haven't linked to the original haikus because they don't appear to be on \emph{Salon}'s site any longer.
As \href{https://www.clir.org/wp-content/uploads/sites/6/ensuring.pdf}{Jeff Rothenberg said},
``Digital information lasts forever---or five years, whichever comes first.''
Luckily, popular content often \href{http://wiki.c2.com/?ComputerErrorHaiku}{has backups}.
\end{quote}

Let's find lines that contain the word ``not'':

\begin{verbatim}
$ grep not haiku.txt
\end{verbatim}

\begin{verbatim}
Is not the true Tao, until
"My Thesis" not found
Today it is not working
\end{verbatim}

Here, \texttt{not} is our (very simple) pattern.
\texttt{grep} searches the file line by line
and shows those lines that contain matches.
Let's search for the pattern \texttt{The}:

\begin{verbatim}
$ grep The haiku.txt
\end{verbatim}

\begin{verbatim}
The Tao that is seen
"My Thesis" not found.
\end{verbatim}

Two lines match,
but in one of them,
our pattern is part of a larger word \texttt{Thesis}.
To restrict matching to lines containing \texttt{The} on its own,
we can give \texttt{grep} with the \texttt{-w} option:

\begin{verbatim}
$ grep -w The haiku.txt
\end{verbatim}

\begin{verbatim}
The Tao that is seen
\end{verbatim}

What if we want to search for a phrase rather than a single word?

\begin{verbatim}
$ grep is not haiku.txt
\end{verbatim}

\begin{verbatim}
grep: not: No such file or directory
haiku.txt:The Tao that is seen
haiku.txt:"My Thesis" not found.
haiku.txt:Today it is not working
haiku.txt:Software is like that.
\end{verbatim}

In this case,
\texttt{grep} uses \texttt{is} as the pattern
and tries to find it in the files \texttt{not} and \texttt{haiku.txt}.
It then tells us that the file \texttt{not} cannot be found,
but prints \texttt{haiku.txt} as a prefix to each other line of output
to tell us which file those lines came from.

If we want to give \texttt{grep} both words as a single argument,
we must wrap them in quotation marks:

\begin{verbatim}
$ grep "is not" haiku.txt
\end{verbatim}

\begin{verbatim}
Today it is not working
\end{verbatim}

\begin{quote}
\textbf{Quoting}

Quotation marks aren't specific to \texttt{grep}:
the shell interprets them before running the command,
just as it expands wildcards to create actual filenames
no matter what we're asking it to do.
This allows us to do things like \texttt{head\ -n\ 5\ "My\ Thesis.txt"}
if we want to edit a file that has a space in its name.
It is also why many programmers write \texttt{"\$variable"} instead of just \texttt{\$variable}
when creating loops or shell scripts:
if there's any chance at all that the variable's value will contain spaces,
it's safest to quote it.
\end{quote}

\texttt{grep} has many options---so many,
in fact,
that almost every letter of the alphabet means something to it:

\begin{verbatim}
$ man grep
\end{verbatim}

\begin{verbatim}
GREP(1)                   BSD General Commands Manual                  GREP(1)

NAME
     grep, egrep, fgrep, zgrep, zegrep, zfgrep -- file pattern searcher

SYNOPSIS
     grep [-abcdDEFGHhIiJLlmnOopqRSsUVvwxZ] [-A num] [-B num] [-C[num]]
          [-e pattern] [-f file] [--binary-files=value] [--color[=when]]
          [--colour[=when]] [--context[=num]] [--label] [--line-buffered]
          [--null] [pattern] [file ...]
...more...
\end{verbatim}

One of the most useful options is \texttt{-n},
which numbers the lines that match:

\begin{verbatim}
$ grep -n it haiku.txt
\end{verbatim}

\begin{verbatim}
5:With searching comes loss
9:Yesterday it worked
10:Today it is not working
\end{verbatim}

Another is \texttt{-i},
which does case-insensitive matching:

We can combine options (i.e.~flags) as we do with other Unix commands.
For example, let's find the lines that contain the word ``the''. We can combine
the option \texttt{-w} to find the lines that contain the word ``the'' and \texttt{-n} to number the lines that match:

\begin{verbatim}
$ grep -i to haiku.txt
\end{verbatim}

\begin{verbatim}
You bring fresh toner.
Today it is not working
\end{verbatim}

We can combine options as with other commands:

\begin{verbatim}
$ grep -i -n haiku.txt
\end{verbatim}

\begin{verbatim}
3:You bring fresh toner.
10:Today it is not working
\end{verbatim}

We can also invert the match---i.e., print lines that \emph{don't} match the pattern---using \texttt{-v}:

\begin{verbatim}
$ grep -i -n -v to haiku.txt
\end{verbatim}

\begin{verbatim}
1:The Tao that is seen
2:Is not the true Tao, until
4:
5:With searching comes loss
6:and the presence of absence:
7:"My Thesis" not found.
8:
9:Yesterday it worked
11:Software is like that.
\end{verbatim}

If we want to search several files at once,
all we have to do is give \texttt{grep} all of their names.
We will frequently use wildcards to do this,
so if we want to count how many records in our climate data come from the year 2001,
we can do this:

\begin{verbatim}
$ grep 2001 cleaned/*.csv | wc -l
\end{verbatim}

\begin{verbatim}
    2920
\end{verbatim}

Finally,
the \texttt{-r} option (for ``recursive'') tells \texttt{grep} to search all of the files
in or below a directory:

\begin{verbatim}
$ grep -r . FIXME | wc -l
\end{verbatim}

\begin{verbatim}
      28
\end{verbatim}

\texttt{grep}`s real power comes from the fact that its patterns can include
a powerful kind of wildcards called \href{glossary.html\#regular-expression}{regular expressions}.
For example,
this command finds lines that start with the letter 'T':

\begin{verbatim}
$ grep -E "^T" haiku.txt
\end{verbatim}

\begin{verbatim}
The Tao that is seen
Today it is not working
\end{verbatim}

The \texttt{-E} option tells \texttt{grep} to interpret the pattern as a regular expression
rather than taking it literally.
The quotes prevent the shell from treating any special characters in the pattern as wildcards,
and the \texttt{\^{}} in front of the \texttt{T} means, ``Only match at the start of the line.''

Many tools support regular expressions:
we can use them in programming languages,
database queries,
online search engines,
and most text editors (though not Nano---its creators wanted to keep it as small as possible).
Chapter~\ref{FIXME-regexp-pointers} has pointers to a few tutorials if you would like to explore them further.

\hypertarget{rse-bash-advanced-find}{%
\section{How can I find files?}\label{rse-bash-advanced-find}}

While \texttt{grep} finds things in files,
the \texttt{find} command finds files themselves.
It also has a lot of options,
but unlike most Unix commands these are written as full words rather than abbreviations.
To show how it works,
we will explore the \texttt{docs} directory within \texttt{climate-data}:

\begin{verbatim}
$ cd docs
$ tree .
\end{verbatim}

\begin{verbatim}
.
 bibliography.bib
 chapter-1
  index.html
 chapter-2
  index.html
 chapter-3
  index.html
 figures
  figure-1.png
  figure-2.png
  figure-3.png
  figure-4.png
 index.html

4 directories, 9 files
\end{verbatim}

This directory contains \texttt{index.html}, \texttt{bibliography.bib}, and four subdirectories:
three for chapters and one for figures.
For our first command,
let's run \texttt{find\ .} to find and list everything in this directory.
(As always,
\texttt{.} on its own means the current working directory,
which is where we want our search to start.)

\begin{verbatim}
$ find .
\end{verbatim}

\begin{verbatim}
.
./index.html
./chapter-1
./chapter-1/index.html
./figures
./figures/figure-4.png
./figures/figure-1.png
./figures/figure-3.png
./figures/figure-2.png
./chapter-3
./chapter-3/index.html
./chapter-2
./chapter-2/index.html
./bibliography.bib
\end{verbatim}

If we only want to find directories,
we can tell \texttt{find} to show us things of type \texttt{d}:

\begin{verbatim}
$ find . -type d
\end{verbatim}

\begin{verbatim}
.
./chapter-1
./figures
./chapter-3
./chapter-2
\end{verbatim}

If we change \texttt{-type\ d} to \texttt{-type\ f}
we get a listing of all the files instead:

\begin{verbatim}
$ find . -type f
\end{verbatim}

\begin{verbatim}
./index.html
./chapter-1/index.html
./figures/figure-4.png
./figures/figure-1.png
./figures/figure-3.png
./figures/figure-2.png
./chapter-3/index.html
./chapter-2/index.html
./bibliography.bib
\end{verbatim}

Now let's try matching by name:

\begin{verbatim}
$ find . -name "*.png"
\end{verbatim}

\begin{verbatim}
./figures/figure-4.png
./figures/figure-1.png
./figures/figure-3.png
./figures/figure-2.png
\end{verbatim}

As we said earlier,
the command line's power lies in combining tools.
We have seen how to do that with pipes;
let's use another technique to see how large our HTML files are:

\begin{verbatim}
$ wc -l $(find . -name "*.html")
\end{verbatim}

\begin{verbatim}
      16 ./index.html
      14 ./chapter-1/index.html
      15 ./chapter-3/index.html
      11 ./chapter-2/index.html
      56 total
\end{verbatim}

When the shell executes this command,
it runs whatever is inside the \texttt{\$()}
and then replaces \texttt{\$()} with that command's output.
Since the output of \texttt{find} is the paths of four HTML files,
the shell constructs the command:

\begin{verbatim}
$ wc -l ./index.html ./chapter-1/index.html ./chapter-2/index.html ./chapter-3/index.html
\end{verbatim}

which is what we wanted.
It is exactly like expanding the wildcard in \texttt{*.html},
but more flexible.

We will often use \texttt{find} and \texttt{grep} together.
The first finds files whose names match a pattern,
while the second looks for lines inside those files that match another pattern.
For example,
we can look for the titles of all our HTML files:

\begin{verbatim}
$ grep "<title>" $(find . -name "*.html")
\end{verbatim}

\begin{verbatim}
./index.html:    <title>Climate Change in the Australian Subcontinent</title>
./chapter-1/index.html:    <title>Climate Change in the Australian Subcontinent: Introduction</title>
./chapter-3/index.html:    <title>Climate Change in the Australian Subcontinent: Methods</title>
./chapter-2/index.html:    <title>Climate Change in the Australian Subcontinent: Background</title>
\end{verbatim}

We can also use \texttt{\$()} expansion to create a list of filenames to use in a loop:

\begin{verbatim}
$ for page in $(find . -name "*.html")
> do
> cp $page $page.bak
> done
$ find . -name "*.bak"
\end{verbatim}

\begin{verbatim}
./chapter-1/index.html.bak
./index.html.bak
./chapter-3/index.html.bak
./chapter-2/index.html.bak
\end{verbatim}

\hypertarget{rse-bash-advanced-summary}{%
\section{Summary}\label{rse-bash-advanced-summary}}

FIXME: create concept map of advanced shell

The original Unix shell was created in 1971,
and will soon celebrate its fiftieth anniversary.
Its syntax may be cryptic and inconsistent,
but few programs have lasted as long,
and fewer still remain in daily use.
The secret to its success was and is its generality:
any program that reads text from standard input
and prints text to standard output
can work with any other.

FIXME: write a better summary for advanced shell

\hypertarget{rse-bash-advanced-exercises}{%
\section{Exercises}\label{rse-bash-advanced-exercises}}

\hypertarget{rse-bash-advanced-ex-script-variables}{%
\subsection{Variables in shell scripts}\label{rse-bash-advanced-ex-script-variables}}

In the \texttt{molecules} directory, imagine you have a shell script called \texttt{script.sh} containing the
following commands:

\begin{verbatim}
head -n $2 $1
tail -n $3 $1
\end{verbatim}

While you are in the \texttt{molecules} directory, you type the following command:

\begin{verbatim}
bash script.sh '*.pdb' 1 1
\end{verbatim}

Which of the following outputs would you expect to see?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All of the lines between the first and the last lines of each file ending in \texttt{.pdb}
  in the \texttt{molecules} directory
\item
  The first and the last line of each file ending in \texttt{.pdb} in the \texttt{molecules} directory
\item
  The first and the last line of each file in the \texttt{molecules} directory
\item
  An error because of the quotes around \texttt{*.pdb}
\end{enumerate}

\hypertarget{rse-bash-advanced-ex-longest-with-extension}{%
\subsection{Find the longest file with a given extension}\label{rse-bash-advanced-ex-longest-with-extension}}

Write a shell script called \texttt{longest.sh} that takes the name of a
directory and a filename extension as its arguments, and prints
out the name of the file with the most lines in that directory
with that extension. For example:

\begin{verbatim}
$ bash longest.sh /tmp/data pdb
\end{verbatim}

would print the name of the \texttt{.pdb} file in \texttt{/tmp/data} that has
the most lines.

\hypertarget{rse-bash-advanced-ex-reading-scripts}{%
\subsection{Script reading comprehension}\label{rse-bash-advanced-ex-reading-scripts}}

For this question, consider the \texttt{data-shell/molecules} directory once again.
This contains a number of \texttt{.pdb} files in addition to any other files you
may have created.
Explain what each of the following three scripts would do when run as
\texttt{bash\ script1.sh\ *.pdb}, \texttt{bash\ script2.sh\ *.pdb}, and \texttt{bash\ script3.sh\ *.pdb} respectively.

\begin{verbatim}
# Script 1
echo *.*
\end{verbatim}

\begin{verbatim}
# Script 2
for filename in $1 $2 $3
> do
>     cat $filename
> done
\end{verbatim}

\begin{verbatim}
# Script 3
echo $@.pdb
\end{verbatim}

\hypertarget{rse-bash-advanced-ex-using-grep}{%
\subsection{\texorpdfstring{Using \texttt{grep}}{Using grep}}\label{rse-bash-advanced-ex-using-grep}}

Which command would result in the following output:

\begin{verbatim}
and the presence of absence:
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{grep\ "of"\ haiku.txt}
\item
  \texttt{grep\ -E\ "of"\ haiku.txt}
\item
  \texttt{grep\ -w\ "of"\ haiku.txt}
\item
  \texttt{grep\ -i\ "of"\ haiku.txt}
\end{enumerate}

\hypertarget{rse-bash-advanced-ex-tracking-species}{%
\subsection{Tracking a species}\label{rse-bash-advanced-ex-tracking-species}}

Leah has several hundred
data files saved in one directory, each of which is formatted like this:

\begin{verbatim}
2013-11-05,deer,5
2013-11-05,rabbit,22
2013-11-05,raccoon,7
2013-11-06,rabbit,19
2013-11-06,deer,2
\end{verbatim}

She wants to write a shell script that takes a species as the first command-line argument
and a directory as the second argument. The script should return one file called \texttt{species.txt}
containing a list of dates and the number of that species seen on each date.
For example using the data shown above, \texttt{rabbit.txt} would contain:

\begin{verbatim}
2013-11-05,22
2013-11-06,19
\end{verbatim}

Put these commands and pipes in the right order to achieve this:

\begin{verbatim}
cut -d : -f 2
>
|
grep -w $1 -r $2
|
$1.txt
cut -d , -f 1,3
\end{verbatim}

Hint: use \texttt{man\ grep} to look for how to grep text recursively in a directory
and \texttt{man\ cut} to select more than one field in a line.

An example of such a file is provided in \texttt{data-shell/data/animal-counts/animals.txt}

\hypertarget{rse-bash-advanced-ex-little-women}{%
\subsection{Counting names}\label{rse-bash-advanced-ex-little-women}}

You and your friend, having just finished reading \emph{Little Women} by
Louisa May Alcott, are in an argument. Of the four sisters in the
book, Jo, Meg, Beth, and Amy, your friend thinks that Jo was the
most mentioned. You, however, are certain it was Amy. Luckily, you
have a file \texttt{LittleWomen.txt} containing the full text of the novel
(\texttt{data-shell/writing/data/LittleWomen.txt}).
Using a \texttt{for} loop, how would you tabulate the number of times each
of the four sisters is mentioned?

Hint: one solution might employ
the commands \texttt{grep} and \texttt{wc} and a \texttt{\textbar{}}, while another might utilize
\texttt{grep} options.
There is often more than one way to solve a programming task, so a
particular solution is usually chosen based on a combination of
yielding the correct result, elegance, readability, and speed.

\hypertarget{rse-bash-advanced-ex-match-subtract}{%
\subsection{Matching and subtracting}\label{rse-bash-advanced-ex-match-subtract}}

The \texttt{-v} option to \texttt{grep} inverts pattern matching, so that only lines
which do \emph{not} match the pattern are printed. Given that, which of
the following commands will find all files in \texttt{/data} whose names
end in \texttt{s.txt} (e.g., \texttt{animals.txt} or \texttt{planets.txt}), but do
\emph{not} contain the word \texttt{net}?
Once you have thought about your answer, you can test the commands in the \texttt{data-shell}
directory.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{find\ data\ -name\ \textquotesingle{}*s.txt\textquotesingle{}\ \textbar{}\ grep\ -v\ net}
\item
  \texttt{find\ data\ -name\ *s.txt\ \textbar{}\ grep\ -v\ net}
\item
  \texttt{grep\ -v\ "temp"\ \$(find\ data\ -name\ \textquotesingle{}*s.txt\textquotesingle{})}
\item
  None of the above.
\end{enumerate}

\hypertarget{rse-bash-advanced-ex-reading-find}{%
\subsection{\texorpdfstring{\texttt{find} pipeline reading comprehension}{find pipeline reading comprehension}}\label{rse-bash-advanced-ex-reading-find}}

Write a short explanatory comment for the following shell script:

\begin{verbatim}
wc -l $(find . -name '*.dat') | sort -n
\end{verbatim}

\hypertarget{rse-bash-advanced-ex-find-properties}{%
\subsection{Finding files with different properties}\label{rse-bash-advanced-ex-find-properties}}

The \texttt{find} command can be given several other criteria known as ``tests''
to locate files with specific attributes, such as creation time, size,
permissions, or ownership. Use \texttt{man\ find} to explore these, and then
write a single command to find all files in or below the current directory
that are owned by the user \texttt{ahmed} and were modified in the last 24 hours.

Hint 1: you will need to use three tests: \texttt{-type}, \texttt{-mtime}, and \texttt{-user}.

Hint 2: The value for \texttt{-mtime} will need to be negative---why?

\hypertarget{rse-bash-advanced-ex-combining-options}{%
\subsection{Combining options}\label{rse-bash-advanced-ex-combining-options}}

In most command line tools,
multiple options can be combined with a single \texttt{-} and no spaces between the options:
\texttt{ls\ -F\ -a} is equivalent to \texttt{ls\ -Fa}.
FIXME: finish this exercise

\hypertarget{rse-bash-advanced-ex-other-wildcards}{%
\subsection{Other wildcards}\label{rse-bash-advanced-ex-other-wildcards}}

FIXME: exercise to introduce ? and other wildcards.

\hypertarget{rse-bash-advanced-keypoints}{%
\section{Key Points}\label{rse-bash-advanced-keypoints}}

\begin{itemize}
\tightlist
\item
  Save commands in files (usually called shell scripts) for re-use.
\item
  \texttt{bash\ filename} runs the commands saved in a file.
\item
  \texttt{\$@} refers to all of a shell script's command-line arguments.
\item
  \texttt{\$1}, \texttt{\$2}, etc., refer to the first command-line argument, the second command-line argument, etc.
\item
  Place variables in quotes if the values might have spaces in them.
\item
  Letting users decide what files to process is more flexible and more consistent with built-in Unix commands.
\item
  \texttt{find} finds files with specific properties that match patterns.
\item
  \texttt{grep} selects lines in files that match patterns.
\item
  \texttt{-\/-help} is an option supported by many bash commands, and programs that can be run from within Bash, to display more information on how to use these commands or programs.
\item
  \texttt{man\ command} displays the manual page for a given command.
\item
  \texttt{\$(command)} inserts a command's output in place.
\end{itemize}

\hypertarget{rse-py-scripting}{%
\chapter{Command Line Programs in Python}\label{rse-py-scripting}}

The Jupyter Notebook, PyCharm and other interactive tools
are great for prototyping code and exploring data,
but in many cases you will ultimately want to take the code you've developed
using those tools and apply it to thousands of data files,
run it with many different parameters,
and/or combine it with other programs in a data analysis pipeline.
To effectively perform these tasks,
it is often necessary to turn the code you've developed into a standalone program
that can be run from the Unix shell,
just like other command-line tools.

In this chapter we take the code we developed in Chapter~\ref{FIXME}
and turn it into a Python script (i.e.~a command line program written in Python).

\hypertarget{rse-py-scripting-template}{%
\section{How are command-line scripts structured?}\label{rse-py-scripting-template}}

In general,
the first thing that gets added to any Python script is the following:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}
\NormalTok{    main()}
\end{Highlighting}
\end{Shaded}

The reason we need these two lines of code
is to differentiate between running a Python file as a standalone program
and importing it as a module.
When you import a Python file as a module,
the \texttt{\_\_name\_\_} variable is automatically set to the name of the file
(e.g.~when importing script.py, \texttt{\_\_name\_\_} is \texttt{script}).
When running a Python file as a standalone program,
\texttt{\_\_name\_\_} is always set to \texttt{\_\_main\_\_}.
This means that we can separate the two cases above by checking the value of \texttt{\_\_name\_\_}.
When the Python file is running as a standalone program,
we want to execute its main functionality.
Conventionally, this functionality is defined inside a function named \texttt{main()},
but you could call this function whatever you prefer.

The next thing you'll need is a library to parse the command line for input arguments.
The most widely used option is
\href{https://docs.python.org/3/library/argparse.html}{argparse}.

Putting those together,
let's create a basic python script called \texttt{script\_template.py}
by opening our favourite text editor and entering the following:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ argparse}

\CommentTok{#}
\CommentTok{# All your functions (that will be called by main()) go here.}
\CommentTok{#}

\KeywordTok{def}\NormalTok{ main(inargs):}
    \CommentTok{"""Run the program."""}

    \BuiltInTok{print}\NormalTok{(}\StringTok{'Input file: '}\NormalTok{, inargs.infile)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{'Output file: '}\NormalTok{, inargs.outfile)}


\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}

\NormalTok{    description}\OperatorTok{=}\StringTok{'Print the input arguments to the screen.'}
\NormalTok{    parser }\OperatorTok{=}\NormalTok{ argparse.ArgumentParser(description}\OperatorTok{=}\NormalTok{description)}

\NormalTok{    parser.add_argument(}\StringTok{"infile"}\NormalTok{, }\BuiltInTok{type}\OperatorTok{=}\BuiltInTok{str}\NormalTok{, }\BuiltInTok{help}\OperatorTok{=}\StringTok{"Input file name"}\NormalTok{)}
\NormalTok{    parser.add_argument(}\StringTok{"outfile"}\NormalTok{, }\BuiltInTok{type}\OperatorTok{=}\BuiltInTok{str}\NormalTok{, }\BuiltInTok{help}\OperatorTok{=}\StringTok{"Output file name"}\NormalTok{)}

\NormalTok{    args }\OperatorTok{=}\NormalTok{ parser.parse_args()}
\NormalTok{    main(args)}
\end{Highlighting}
\end{Shaded}

By running \texttt{script\_template.py} at the command line
we'll see that \texttt{argparse} handles all the input arguments:

\begin{verbatim}
$ python script_template.py in.csv out.png
\end{verbatim}

\begin{verbatim}
Input file:  in.csv
Output file:  out.png
\end{verbatim}

It also generates help information for the user:

\begin{verbatim}
$ python script_template.py -h
\end{verbatim}

\begin{verbatim}
usage: script_template.py [-h] infile outfile

Print the input arguments to the screen.

positional arguments:
  infile      Input file name
  outfile     Output file name

optional arguments:
  -h, --help  show this help message and exit
\end{verbatim}

and issues errors when users give the program invalid arguments:

\begin{verbatim}
$ python script_template.py in.csv
\end{verbatim}

\begin{verbatim}
usage: script_template.py [-h] infile outfile
script_template.py: error: the following arguments are required: outfile
\end{verbatim}

\hypertarget{rse-py-scripting-code}{%
\section{Where should I put my code?}\label{rse-py-scripting-code}}

Using this template as a starting point,
we can add the functions we developed previously to a script called\ldots{}

FIXME: In this section we will take the code developed in the novice book
and turn it into a Python script that can be executed at the command line.
See \href{https://carpentrieslab.github.io/python-aos-lesson/04-cmdline/index.html}{this lesson}
for an example of this approach in action.

\begin{quote}
\textbf{Code Reuse}

In the novice lessons,
we learned how to reuse (rather than cut and paste)
code by defining functions (Section~\ref{FIXME}).
In order to use those functions in other python notebooks/scripts,
we saw that we can save them in a file (called a module)
that can be imported (Section~\ref{FIXME}).
In this chapter, we've seen that we can go one step further
and run our Python code outside of a Python environment,
by writing Python scripts that can be executed at the command line.
\end{quote}

\hypertarget{rse-py-scripting-error-messages}{%
\section{How can I write useful error messages?}\label{rse-py-scripting-error-messages}}

The error message shown in Figure~\ref{fig:rse-py-scripting-error-message}(\#FIG) is not helpful:

\begin{figure}
\centering
\includegraphics{figures/rse-py-scripting/error-message.png}
\caption{\label{fig:rse-py-scripting-error-message}An Unhelpful Error Message}
\end{figure}

\{\% include figure.html id=``f:docs-error-message'' src=``error-message.png'' caption=``Error Message'' \%\}

Neither is this:

\begin{verbatim}
System.InvalidOperationException: Nullable object must have a value.
\end{verbatim}

or this:

\begin{verbatim}
I tried really hard but was unable to complete your request.
You probably need to talk to a human - have you tried calling Dave?
\end{verbatim}

Error messages are often the first thing people actually read about a piece of software
(or possibly the second if they had to install it themselves),
so they should therefore be the most carefully written documentation for that software.
A quick web search for ``writing good error messages'' turns up hundreds of hits,
but recommendations are often more like gripes than solid guidelines
and are usually not backed up by evidence.
What research there is gives us the following rules Becker et al. (\protect\hyperlink{ref-Beck2016}{2016}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Do not tell the user what the program did that caused the problem,
  but what the user did.
  Putting it another way,
  the message shouldn't state the effect of the error,
  it should state the cause.
\item
  Be spatially correct,
  i.e.,
  point at the actual location of the error.
  Few things are as frustrating as being pointed at line 28
  when the problem is really on line 35.
\item
  Do not provide tips or potential solutions.
  In most languages it is not possible to determine what the actual error is from the message with 100\% certainty.
  Therefore it is better to give an as-specific-as-possible message on what went wrong without offering guidance on fixing it.
  Tips and hints could be provided by a different tool,
  but they should be based on the error message and not part of it.
\item
  Be as specific as possible without ever being (or seeming) wrong:
  from a user's point of view,
  ``file not found'' is very different from ``don't have permissions to open file'' or ``file is empty''.
\item
  Write for your audience's level of understanding.
  For example, error messages should never use programming terms more advanced than
  those you would use to describe the code the user wrote.
\item
  Do not blame the user, and do not use words like fatal, illegal, etc.
  The former can frustrate---in many cases, ``user error'' actually isn't---and
  the latter can make people worry that the program has damaged their data,
  their computer,
  or their reputation.
\item
  Do not try to make the computer sound like a human being.
  In particular, avoid humor:
  very few jokes are funny on the dozenth re-telling,
  and most users are going to see error messages at least that often.
\item
  Use a consistent vocabulary.
  This rule can be hard to enforce when error messages are written by several different people,
  but putting them all in one module makes review easier.
\end{enumerate}

That last suggestion deserves a little elaboration.
Most people write error messages directly in their code:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{try}\NormalTok{:}
    \CommentTok{# ...do something complicated...}
\ControlFlowTok{except} \PreprocessorTok{OSError} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{'Unable to find or read file }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(filename))}
\NormalTok{    sys.exit(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A better approach for large projects is to put all of the error messages in a catalog:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ERROR_MESSAGES }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{'cannot_read_file'}\NormalTok{ : }\StringTok{'Unable to find or read file }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{,}
    \StringTok{'config_corrupted'}\NormalTok{ : }\StringTok{'Configuration file }\SpecialCharTok{\{\}}\StringTok{ corrupted'}\NormalTok{,}
    \CommentTok{# ...more error messages...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

and then only use messages from that catalog:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ error_messages }\ImportTok{import}\NormalTok{ ERROR_MESSAGES}

\ControlFlowTok{try}\NormalTok{:}
    \CommentTok{# ...do something complicated...}
\ControlFlowTok{except} \PreprocessorTok{OSError} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(ERROR_MESSAGES[}\StringTok{'cannot_read_file'}\NormalTok{].}\BuiltInTok{format}\NormalTok{(filename))}
\NormalTok{    sys.exit(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Doing this makes it much easier to ensure that messages are consistent.
It also makes it much easier to give messages in the user's preferred language:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ERROR_MESSAGES }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{'en'}\NormalTok{ : \{}
        \StringTok{'cannot_read_file'}\NormalTok{ : }\StringTok{'Unable to find or read file }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{,}
        \StringTok{'config_corrupted'}\NormalTok{ : }\StringTok{'Configuration file }\SpecialCharTok{\{\}}\StringTok{ corrupted'}\NormalTok{,}
        \CommentTok{# ...more error messages in English...}
\NormalTok{    \},}
    \StringTok{'fr'}\NormalTok{ : \{}
        \StringTok{'cannot_read_file'}\NormalTok{ : }\StringTok{'Impossible d'}\NormalTok{acceder au fichier \{\}}\StringTok{',}
\StringTok{        '}\NormalTok{config_corrupted}\StringTok{' : '}\NormalTok{Fichier de configuration \{\} corrompu}\StringTok{',}
\StringTok{        # ...more error messages in French...}
\StringTok{    \}}
\StringTok{    # ...other languages...}
\StringTok{\}}
\end{Highlighting}
\end{Shaded}

The error report is then looked up as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ERROR_MESSAGES[user_language][}\StringTok{'cannot_read_file'}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

where \texttt{user\_language} is a two-letter code for the user's preferred language.

\hypertarget{rse-pyscripts-keypoints}{%
\section{Key Points}\label{rse-pyscripts-keypoints}}

\begin{itemize}
\tightlist
\item
  Libraries such as argparse can be used the efficiently handle command line arguments.
\item
  Most Python scripts have a similar structure that can be used as a template.
\end{itemize}

\hypertarget{rse-git-cmdline}{%
\chapter{Git at the Command Line}\label{rse-git-cmdline}}

The first two volumes in this series introduced version control with Git
using a graphical interface like \href{https://www.gitkraken.com/}{GitKraken} or \href{https://www.rstudio.com/products/rstudio/}{the RStudio IDE}.
These interfaces are actually wrappers around Git's original command-line interface,
which allows us to access all of Git's features.
This lesson describes how the basic cycle of add, commit, push, and pull works at the command line.

\hypertarget{rse-git-cmdline-setup}{%
\section{How can I set up Git for command-line use?}\label{rse-git-cmdline-setup}}

On a command line,
we write Git commands as git verb options,
where the \href{glossary.html\#subcommand}{subcommand} verb tells Git what we want to do
and options provide any additional optional information that subcommand needs.
Using this syntax,
the first thing we need to do is configure Git:

\begin{verbatim}
$ git config --global user.name "Frances Bilas"
$ git config --global user.email "frances@eniac.org"
\end{verbatim}

(Please use your own name and email address instead of the one shown.)
Here,
\texttt{config} is the verb
and the rest of the command are options.
We put the name in quotation marks because it contains a space;
we don't actually need to quote the email address,
but do so for consistency.
Since we are going to be using \href{http://github.com}{GitHub},
the email address should be the same as the one we used when setting up our GitHub account.

These two commands only need to be run once:
the flag \texttt{-\/-global} tells Git to use the settings for all of our projects on this computer.
We can re-run these commands any time if you want to change our details.
We can also check our settings:

\begin{verbatim}
$ git config --list
\end{verbatim}

\begin{verbatim}
user.name=Frances Bilas
user.email=frances@eniac.org
core.autocrlf=input
core.editor=nano
core.repositoryformatversion=0
core.filemode=true
core.bare=false
core.ignorecase=true
\end{verbatim}

\begin{quote}
\textbf{Git Help and Manual}

If you forget a Git command,
we can get a list of the ones available using \texttt{-\/-help}.
This option will also give us more information about specific commands.

\begin{verbatim}
$ git --help
$ git config --help
\end{verbatim}
\end{quote}

\hypertarget{rse-git-cmdline-repos}{%
\section{How can I create a new repository?}\label{rse-git-cmdline-repos}}

Once Git is configured, we can start using it.
This section will cover how to create a Git repository associated with a project folder.
As a running example,
we will watch as Frances and her colleague Jean Jennings write a history of the \href{glossary.html\#eniac}{ENIAC project}.
First,
let's create a directory for our work beneath our home directory and go into it:

\begin{verbatim}
$ cd ~
$ mkdir eniac
$ cd eniac
\end{verbatim}

We then tell Git to make this directory a \href{glossary.html\#repository}{repository},
i.e.,
a place where Git can store versions of our files:

\begin{verbatim}
$ git init .
\end{verbatim}

\begin{verbatim}
Initialized empty Git repository in /Users/frances/eniac/.git/
\end{verbatim}

\texttt{ls} seems to show that the directory is still empty:

\begin{verbatim}
$ ls
\end{verbatim}

But if we add the \texttt{-a} flag to show everything,
we can see that Git has created a hidden directory within \texttt{eniac} called \texttt{.git}:

\begin{verbatim}
$ ls -a
\end{verbatim}

\begin{verbatim}
.   ..  .git
\end{verbatim}

Git stores information about the project in this special subdirectory.
If we ever delete it,
we will lose that history.

We can check that everything is set up correctly
by asking Git to tell us the status of our project:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
# On branch master
#
# Initial commit
#
nothing to commit (create/copy files and use "git add" to track)
\end{verbatim}

\hypertarget{rse-git-cmdline-changes}{%
\section{How can I save and track changes?}\label{rse-git-cmdline-changes}}

Now that we have a repository for our work,
we can add files to it and track their history.
Most of the commands we will use (such as \texttt{git\ add} and \texttt{git\ commit}) will sound familiar,
as the same terminology is used in graphical interfaces like \href{https://www.gitkraken.com/}{GitKraken} and \href{https://www.rstudio.com/products/rstudio/}{the RStudio IDE}.

To start, let's make sure we're still in the right directory:

\begin{verbatim}
$ cd ~/eniac
\end{verbatim}

Let's use our favorite text editor
to create a file called \texttt{names.txt} that contains
the names of the original six ENIAC programmers:

\begin{verbatim}
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay Mcnulty
Betty Synder
\end{verbatim}

(There are some typos and omissions here that we will fix later.)
If we check the status of our project,
Git tells us that we have a new file:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)

    names.txt

nothing added to commit but untracked files present (use "git add" to track)
\end{verbatim}

``Untracked files'' means there is a file in the directory whose history Git isn't tracking.
We can tell Git to start keeping track of of it using \texttt{git\ add}:

\begin{verbatim}
$ git add names.txt
\end{verbatim}

and then check that it did what we wanted:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master

No commits yet

Changes to be committed:
  (use "git rm --cached <file>..." to unstage)

    new file:   names.txt
\end{verbatim}

Git now knows that it's supposed to keep track of \texttt{names.txt},
but it hasn't actually recorded any changes yet.
To do taht,
we need to tell it to \href{glossary.html\#commit}{commit} our work:

\begin{verbatim}
$ git commit -m "Starting notes on ENIAC personnel"
\end{verbatim}

\begin{verbatim}
[master (root-commit) 8e966b5] Starting notes on ENIAC personnel
 1 file changed, 6 insertions(+)
 create mode 100644 names.txt
\end{verbatim}

When we run \texttt{git\ commit},
Git takes everything we have told it to save by using \texttt{git\ add}
and stores a copy permanently inside the special \texttt{.git} directory.
This permanent copy is called a \href{glossary.html\#commit}{commit} or a \href{glossary.html\#revision}{revision}.
In this case,
its \href{glossary.html\#short-identifier-git}{short identifier} is \texttt{8e966b5};
your commit may have another identifier.

We use the \texttt{-m} option (short for message)
to record a short descriptive that will remind us later what we did and why.
If we just run \texttt{git\ commit} without the \texttt{-m} option,
Git will launch \texttt{nano}
(or whatever other editor is configured using \texttt{git\ config\ -\/-global\ core.editor})
so that we can write a longer message.
Once again,
we put the message in double quotes because it contains spaces.

If we run \texttt{git\ status} now,
Git tells us that everything is up to date:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
nothing to commit, working directory clean
\end{verbatim}

If we want to know what we've done recently,
we can display the project's history using \texttt{git\ log}:

\begin{verbatim}
$ git log
\end{verbatim}

\begin{verbatim}
commit 8e966b5f0703d347e4f84817f12b48f456a58ae1
Author: Frances Bilas <frances@eniac.org>
Date:   Thu Oct 10 09:24:04 2019 -0400

    Starting notes on ENIAC personnel
\end{verbatim}

\texttt{git\ log} lists all commits made to a repository in reverse chronological order.
The listing for each commit includes
the commit's \href{glossary.html\#full-identifier-git}{full identifier}
(which starts with the same characters as the short identifier printed by \texttt{git\ commit} earlier),
the commit's author,
when it was created,
and the log message Git was given when the commit was created.

Now suppose Frances goes and corrects the two typos
(``Mcnulty'' should be ``McNulty'' and ``Synder'' should be ``Snyder'')
so that the file contains this:

\begin{verbatim}
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay McNulty
Betty Snyder
\end{verbatim}

When we run \texttt{git\ status} now,
it tells us that a file it already knows about has been modified:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

    modified:   names.txt

no changes added to commit (use "git add" and/or "git commit -a")
\end{verbatim}

The last line is the key phrase:
``no changes added to commit''.
We have changed this file,
but we haven't told Git we want to save those changes
(which we do with \texttt{git\ add})
nor have we saved them
(which we do with \texttt{git\ commit}).

Let's do that now.
It is good practice to review our changes before saving them,
which we can do with \texttt{git\ diff}.
This command shows us the differences between the current state of our repository
and the most recently saved version:

\begin{verbatim}
$ git diff
\end{verbatim}

\begin{verbatim}
diff --git a/names.txt b/names.txt
index deca855..46a1948 100644
--- a/names.txt
+++ b/names.txt
@@ -1,5 +1,5 @@
 Frances Bilas
 Jean Jennings
 Ruth Lichterman
-Kay Mcnulty
-Betty Synder
+Kay McNulty
+Betty Snyder
\end{verbatim}

The output is cryptic (even by the standards of the Unix command line)
because it is actually a series of commands telling editors and other tools
how to turn the file we \emph{had} into the file we \emph{have}.
If we break it down into pieces:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first line tells us that Git is producing output
  in the format of the Unix \texttt{diff} command.
\item
  The second line tells exactly which versions of the file Git is comparing:
  \texttt{deca855} and \texttt{46a1948} are unique computer-generated labels for those versions.
\item
  The third and fourth lines once again show the name of the file being changed;
  the name appears twice in case we are renaming a file as well as modifying it.
\item
  The remaining lines show us the changes and the lines on which they occur.
  A minus sign \texttt{-} in the first column indicates a line that is being removed,
  while a plus sign \texttt{+} shows a line that is being added.
\end{enumerate}

After reviewing our change
we can commit it just as we did before:

\begin{verbatim}
$ git commit -m "Correcting typos in names"
\end{verbatim}

\begin{verbatim}
On branch master
Changes not staged for commit:
    modified:   names.txt

no changes added to commit
\end{verbatim}

Whoops:
we forgot to add the file to the set of things we want to commit.
Let's fix that:

\begin{verbatim}
$ git add names.txt
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

    modified:   names.txt
\end{verbatim}

\begin{verbatim}
$ git commit -m "Correcting typos in names"
\end{verbatim}

\begin{verbatim}
[master 61d3964] Correcting typos in names
 1 file changed, 2 insertions(+), 2 deletions(-)
\end{verbatim}

If we are sure we want to commit all of our changes,
we can combine \texttt{git\ add} and \texttt{git\ commit} into a single command.
Let's add a missing name to the bottom of our file:

\begin{verbatim}
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay McNulty
Betty Snyder
Marlyn Wescoff
\end{verbatim}

and then save our work in one step by giving \texttt{git\ commit} the \texttt{-a} option:

\begin{verbatim}
$ git commit -a -m "Adding missing name"
\end{verbatim}

\begin{verbatim}
[master f865209] Adding missing name
 1 file changed, 1 insertion(+)
\end{verbatim}

\hypertarget{rse-git-cmdline-remotes}{%
\section{How can I synchronize with other repositories?}\label{rse-git-cmdline-remotes}}

Sooner or later our computer will experience a hardware failure,
be stolen,
or be thrown in the lake by someone who thinks we shouldn't spend the \emph{entire} vacation working on our thesis.
Even before that happens
we will probably want to collaborate with others,
which we can do by linking our local repository to one stored on a hosting service such as \href{http://github.com}{GitHub}.

The first step in doing that is to create an account on GitHub if we don't already have one,
and then to create a new repository to synchronize with.
The remote repository doesn't have to have the same name as the local one,
but we will probably get confused if they are different,
so the repository we create on GitHub will also be called \texttt{eniac}.

The next step is to connect the two repositories.
We do this by making the GitHub repository a \href{glossary.html\#remote-repository}{remote} for the local repository.
The home page of the repository on GitHub includes the string we need to identify it
(Figure~\ref{fig:rse-git-cmdline-repo-link}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-cmdline-repo-link}Where to Find the Repository Link}
\end{figure}

We can click on ``HTTPS'' to change the URL from SSH to HTTPS
and then copy that URL.

\begin{quote}
\textbf{HTTPS vs.~SSH}

We use HTTPS here because it does not require additional configuration.
You may want to set up SSH access,
which is a bit more secure,
by following one of the tutorials from \href{https://help.github.com/articles/generating-ssh-keys}{GitHub},
\href{https://confluence.atlassian.com/bitbucket/set-up-ssh-for-git-728138079.html}{Bitbucket},
or \href{https://about.gitlab.com/2014/03/04/add-ssh-key-screencast/}{GitLab}
(the last of which includes a screencast).
\end{quote}

Next,
let's go into the local \texttt{eniac} repository and run this command:

\begin{verbatim}
$ git remote add origin https://github.com/frances/eniac.git
\end{verbatim}

Make sure to use the URL for your repository instead of the one shown:
the only difference should be that it includes your username instead of \texttt{frances}.

A Git remote is like a bookmark:
it gives a short name to a URL.
In this case,
the remote's name is \texttt{origin};
we could use anything we want,
but \texttt{origin} is Git's default,
so we will stick with it.
We can check that the command has worked by running \texttt{git\ remote\ -v}
(where the \texttt{-v} option is short for verbose):

\begin{verbatim}
$ git remote -v
\end{verbatim}

\begin{verbatim}
origin  https://github.com/frances/eniac.git (fetch)
origin  https://github.com/frances/eniac.git (push)
\end{verbatim}

Git displays two lines because it's actually possible to set up a remote
to download from one URL but upload to another.
Sensible people don't do this,
so we won't explore this possibility any further.

Once we have set up the remote,
we can \href{glossary.html\#git-push}{push} the work we have done so far to the repository on GitHub:

\begin{verbatim}
$ git push origin master
\end{verbatim}

This may prompt us to enter our username and password;
once we do that,
we see a few lines of administrative information:

\begin{verbatim}
Counting objects: 9, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (6/6), done.
Writing objects: 100% (9/9), 833 bytes | 277.00 KiB/s, done.
Total 9 (delta 1), reused 0 (delta 0)
remote: Resolving deltas: 100% (1/1), done.
To github.com:frances/eniac.git
 * [new branch]      master -> master
\end{verbatim}

If we view our GitHub repository in the browser,
we will now see that it includes \texttt{names.txt}
along with all of the commits we made to create it (Figure~\ref{fig:rse-git-cmdline-history}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-cmdline-history}Repository History}
\end{figure}

We can now \href{glossary.html\#git-pull}{pull} changes from the remote repository to the local one as well:

\begin{verbatim}
$ git pull origin master
\end{verbatim}

\begin{verbatim}
From https://github.com/frances/eniac
 * branch            master     -> FETCH_HEAD
Already up-to-date.
\end{verbatim}

Pulling has no effect in this case
because the two repositories are already synchronized.

\hypertarget{rse-git-cmdline-history}{%
\section{How can I explore a project's history?}\label{rse-git-cmdline-history}}

We have now made three changes to our project.
Git lets us look at the previous versions of the project files
and restore specific files to earlier states if we want to.
To do this,
we need to use an identifier that indicates the versions we want.

There are two ways to do this,
which are similar in spirit to \href{glossary.html\#absolute-path}{absolute} and \href{glossary.html\#relative-path}{relative} paths.
The ``absolute'' version is the unique identifier that Git gives to each commit.
These identifiers are 40 characters long,
but in most situations Git will let us use just the first half dozen characters or so.
For example,
if we run \texttt{git\ log} right now,
it shows us something like this:

\begin{verbatim}
commit f86520952f1f4ed3751ff407673203a57dab633c
Author: Frances Bilas <frances@eniac.org>
Date:   Thu Oct 10 09:41:24 2019 -0400

    Adding missing name

commit 61d3964f81a081d80dc49d9610d5bcbe454b2ad6
Author: Frances Bilas <frances@eniac.org>
Date:   Thu Oct 10 09:38:12 2019 -0400

    Correcting typos in names

commit 8e966b5f0703d347e4f84817f12b48f456a58ae1
Author: Frances Bilas <frances@eniac.org>
Date:   Thu Oct 10 09:30:03 2019 -0400

    Starting notes on ENIAC personnel
\end{verbatim}

If we add a title to the top of \texttt{names.txt} so that it contains:

\begin{verbatim}
Personnel:
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay McNulty
Betty Snyder
Marlyn Wescoff
\end{verbatim}

then \texttt{git\ diff} on its own will show the difference between the file as it is
and the most recent version:

\begin{verbatim}
diff --git a/names.txt b/names.txt
index 53b863e..fddc85e 100644
--- a/names.txt
+++ b/names.txt
@@ -1,3 +1,4 @@
+Personnel:
 Frances Bilas
 Jean Jennings
 Ruth Lichterman
\end{verbatim}

but \texttt{git\ diff\ -r\ 61d3964} shows the difference between the current state
and the file as it was after that commit:

\begin{verbatim}
diff --git a/names.txt b/names.txt
index 46a1948..fddc85e 100644
--- a/names.txt
+++ b/names.txt
@@ -1,5 +1,7 @@
+Personnel:
 Frances Bilas
 Jean Jennings
 Ruth Lichterman
 Kay McNulty
 Betty Snyder
+Marlyn Wescoff
\end{verbatim}

Note that you may need to use a different identifier than \texttt{61d3964}.
Note also that we have \emph{not} committed this change:
we will look at ways of undoing it in the next section.

The ``relative'' version of history relies on a special identifier called \texttt{HEAD},
which always refers to the most recent version in the repository.
\texttt{git\ diff\ -r\ HEAD} (where the option \texttt{-r} is short for revision)
shows the same thing as \texttt{git\ diff}.
Instead of typing in a version identifier to back up one commit,
though,
we can simply use \texttt{HEAD\textasciitilde{}1} (where \texttt{\textasciitilde{}} is the tilde symbol).
This shorthand is read ``HEAD minus one'',
and gives us the difference to the \emph{previous} saved version.
\texttt{git\ diff\ -r\ HEAD\textasciitilde{}2} goes back two revisions and so on.

We can also look at the differences between two saved versions
by separating their identifiers with two dots \texttt{..} like this:

\begin{verbatim}
$ git diff -r HEAD~1..HEAD~2
\end{verbatim}

\begin{verbatim}
diff --git a/names.txt b/names.txt
index 46a1948..deca855 100644
--- a/names.txt
+++ b/names.txt
@@ -1,5 +1,5 @@
 Frances Bilas
 Jean Jennings
 Ruth Lichterman
-Kay McNulty
-Betty Snyder
+Kay Mcnulty
+Betty Synder
\end{verbatim}

If we want to see the changes made in a particular commit,
we can use \texttt{git\ show};
we do not need the \texttt{-r} option in this case:

\begin{verbatim}
$ git show HEAD~2 names.txt
\end{verbatim}

\begin{verbatim}
commit 8e966b5f0703d347e4f84817f12b48f456a58ae1
Author: Frances Bilas <frances@eniac.org>
Date:   Thu Oct 10 09:30:03 2019 -0400

    Starting notes on ENIAC personnel

diff --git a/names.txt b/names.txt
new file mode 100644
index 0000000..deca855
--- /dev/null
+++ b/names.txt
@@ -0,0 +1,5 @@
+Frances Bilas
+Jean Jennings
+Ruth Lichterman
+Kay Mcnulty
+Betty Synder
\end{verbatim}

\hypertarget{rse-git-cmdline-restore}{%
\section{How can I restore old versions of files?}\label{rse-git-cmdline-restore}}

We can see what we changed,
but how can we restore it?
Let's suppose we change our mind about the last update to \texttt{names.txt}
before we add it or commit it.
\texttt{git\ status} tells us that the file has been changed,
but those changes haven't been \href{glossary.html\#git-stage}{staged}:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

    modified:   names.txt

no changes added to commit (use "git add" and/or "git commit -a")
\end{verbatim}

We can put things back the way they were in the last saved revision using \texttt{git\ checkout}:

\begin{verbatim}
$ git checkout HEAD names.txt
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
nothing to commit, working tree clean
\end{verbatim}

\begin{verbatim}
$ cat names.txt
\end{verbatim}

\begin{verbatim}
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay McNulty
Betty Snyder
Marlyn Wescoff
\end{verbatim}

As its name suggests,
\texttt{git\ checkout} checks out (i.e., restores) an old version of a file.
In this case,
we told Git to recover the version of the file saved in \texttt{HEAD},
i.e.,
in the most recent commit.
We can use a commit identifier to go back as far as we want:

\begin{verbatim}
$ git checkout 8e966b5 names.txt
$ cat names.txt
\end{verbatim}

\begin{verbatim}
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay Mcnulty
Betty Synder
\end{verbatim}

Doing this does \emph{not} change the history:
\texttt{git\ log} still shows our three commits.
Instead,
it replaces the content of the file with the old content:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

    modified:   names.txt
\end{verbatim}

Notice that the changes have already been added to the staging area for new commits.
We can bring the file back in sync with the most recent commit using \texttt{git\ checkout}:

\begin{verbatim}
$ git checkout HEAD names.txt
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
nothing to commit, working tree clean
\end{verbatim}

\begin{verbatim}
$ cat names.txt
\end{verbatim}

\begin{verbatim}
Frances Bilas
Jean Jennings
Ruth Lichterman
Kay McNulty
Betty Snyder
Marlyn Wescoff
\end{verbatim}

Since we didn't commit the change in which we added the line \texttt{Personnel},
that work is now lost:
Git can only go back and forth between committed versions of files.

\hypertarget{rse-git-cmdline-ignore}{%
\section{How can I tell Git to ignore some files?}\label{rse-git-cmdline-ignore}}

We have only modified one file in this tutorial,
but everything we have done works just as well with multiple files.
Sometimes,
though,
we don't want Git to track files' history.
For example,
we might want to track text files with names ending in \texttt{.txt}
but not data files with names ending in \texttt{.dat}.

To stop Git from telling us about these files every time we call \texttt{git\ status},
we can create a file in the root directory of our project called \texttt{.gitignore}.
This file can contain filenames like \texttt{thesis.pdf}
or \href{glossary.html\#wildcard}{wildcard} patterns like \texttt{*.dat}.
Each must be on a line of its own,
and Git will ignore anything that matches any of these lines.

\begin{quote}
\textbf{Remember to Ignore}

Don't forget to commit \texttt{.gitignore} to your repository
so that Git knows to use it.
\end{quote}

\hypertarget{rse-git-cmdline-exercises}{%
\section{Exercises}\label{rse-git-cmdline-exercises}}

\hypertarget{rse-git-cmdline-ex-places}{%
\subsection{Places to create Git repositories}\label{rse-git-cmdline-ex-places}}

Along with information about the ENIAC project,
Frances would also like to keep some notes on the UNIVAC.
Despite her colleagues' concerns,
Frances creates a \texttt{univac} project inside her \texttt{eniac} project as follows:

\begin{verbatim}
$ cd ~/eniac     # go into eniac directory, which is already a Git repository
$ mkdir univac   # make a subdirectory eniac/univac
$ cd univac      # go into univac subdirectory
$ git init       # make univac a Git repository
\end{verbatim}

Is the \texttt{git\ init} command that she runs inside the \texttt{univac} subdirectory
required for tracking files stored there?

\hypertarget{rse-git-cmdline-ex-commit}{%
\subsection{Committing changes}\label{rse-git-cmdline-ex-commit}}

Which command(s) below would save changes to \texttt{myfile.txt} to a local Git repository?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
\begin{verbatim}
$ git commit -m "my recent changes"
\end{verbatim}
\item
\begin{verbatim}
$ git init myfile.txt
$ git commit -m "my recent changes"
\end{verbatim}
\item
\begin{verbatim}
$ git add myfile.txt
$ git commit -m "my recent changes"
\end{verbatim}
\item
\begin{verbatim}
$ git commit -m myfile.txt "my recent changes"
\end{verbatim}
\end{enumerate}

\hypertarget{rse-git-cmdline-ex-multiple}{%
\subsection{Committing multiple files}\label{rse-git-cmdline-ex-multiple}}

The staging area can hold changes from any number of files
that you want to commit as a single snapshot.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Add some commas to \texttt{names.txt} to separate first and last names.
\item
  Create a new file \texttt{old-computers.txt}
  and write a few lines about the first computer you remember seeing or using.
\item
  Add changes from both files to the staging area
  and commit those changes.
\end{enumerate}

\hypertarget{rse-git-cmdline-ex-bio}{%
\subsection{Write a biography}\label{rse-git-cmdline-ex-bio}}

\begin{itemize}
\tightlist
\item
  Create a new Git repository on your computer called \texttt{bio}.
\item
  Write a three-line biography for yourself in a file called \texttt{me.txt} and commit your changes.
\item
  Modify one line and add a fourth line.
\item
  Display the differences between the file's original state and its updated state.
\end{itemize}

\hypertarget{rse-git-cmdline-ex-ignore-nested}{%
\subsection{Ignoring nested files}\label{rse-git-cmdline-ex-ignore-nested}}

Suppose our project has a directory \texttt{results} with two subdirectories called \texttt{data} and \texttt{plots}.
How would we ignore all of the files in \texttt{results/plots}
but not ignore files in \texttt{results/data}?

\hypertarget{rse-git-cmdline-ex-include}{%
\subsection{Including specific files}\label{rse-git-cmdline-ex-include}}

How would you ignore all \texttt{.dat} files in your root directory except for \texttt{final.dat}?
(Hint: find out what the exclamation mark \texttt{!} means in a \texttt{.gitignore} file.)

\hypertarget{rse-git-cmdline-ex-github-interface}{%
\subsection{Exploring the GitHub interface}\label{rse-git-cmdline-ex-github-interface}}

Browse to your \texttt{eniac} repository on GitHub.
Under the \texttt{Code} tab,
find and click on the text that says ``NN commits'' (where ``NN'' is some number).
Hover over and click on the three buttons to the right of each commit.
What information can you gather/explore from these buttons?
How would you get that same information in the shell?

\hypertarget{rse-git-cmdline-ex-timestamp}{%
\subsection{GitHub timestamps}\label{rse-git-cmdline-ex-timestamp}}

\begin{itemize}
\tightlist
\item
  Create a remote repository on GitHub.
\item
  Push the contents of your local repository to the remote.
\item
  Make changes to your local repository and push these changes as well.
\item
  Go to the repo you just created on GitHub and check the timestamps of the files.
\end{itemize}

How does GitHub record times, and why?

\hypertarget{rse-git-cmdline-ex-push-commit}{%
\subsection{Push versus commit}\label{rse-git-cmdline-ex-push-commit}}

How is \texttt{git\ push} different from \texttt{git\ commit}?

\hypertarget{rse-git-cmdline-ex-boilerplate}{%
\subsection{License and README files}\label{rse-git-cmdline-ex-boilerplate}}

When we initialized our GitHub repo,
we didn't add a \texttt{README.md} or license file.
If we had,
what would have happened when we tried to link our local and remote repositories?

\hypertarget{rse-git-cmdline-ex-recover}{%
\subsection{Recovering older versions of a file}\label{rse-git-cmdline-ex-recover}}

Jennifer made changes this morning to a shell script called \texttt{data\_cruncher.sh}
that she has been working on for weeks.
Her changes broke the script,
and she has now spent an hour trying to get it back in working order.
Luckily,
she has been keeping track of her project's versions using Git.
Which of the commands below can she use
torecover the last committed version of her script?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{\$\ git\ checkout\ HEAD}
\item
  \texttt{\$\ git\ checkout\ HEAD\ data\_cruncher.sh}
\item
  \texttt{\$\ git\ checkout\ HEAD\textasciitilde{}1\ data\_cruncher.sh}
\item
  \texttt{\$\ git\ checkout\ \textless{}unique\ ID\ of\ last\ commit\textgreater{}\ data\_cruncher.sh}
\item
  Both 2 and 4
\end{enumerate}

\hypertarget{rse-git-cmdline-ex-history}{%
\subsection{Workflow and history}\label{rse-git-cmdline-ex-history}}

What is the output of the last command in the sequence below?

\begin{verbatim}
$ cd eniac
$ echo "ENIAC was the world's first computer." > history.txt
$ git add history.txt
$ echo "ENIAC was the world's first general-purpose electronic computer." > history.txt
$ git commit -m "Origins of ENIAC"
$ git checkout HEAD history.txt
$ cat history.txt
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
\begin{verbatim}
ENIAC was the world's first computer.
\end{verbatim}
\item
\begin{verbatim}
ENIAC was the world's first general-purpose electronic computer.
\end{verbatim}
\item
\begin{verbatim}
ENIAC was the world's first computer.
ENIAC was the world's first general-purpose electronic computer.
\end{verbatim}
\item
  An error message because we have changed \texttt{history.txt} without committing first.
\end{enumerate}

\hypertarget{rse-git-cmdline-ex-diff}{%
\subsection{\texorpdfstring{Understanding \texttt{git\ diff}}{Understanding git diff}}\label{rse-git-cmdline-ex-diff}}

\begin{itemize}
\tightlist
\item
  What will the command \texttt{git\ diff\ HEAD\textasciitilde{}9\ names.txt} do if we run it?
\item
  What does it actually do?
\item
  What does \texttt{git\ diff\ HEAD\ names.txt} do?
\end{itemize}

\hypertarget{rse-git-cmdline-ex-unstage}{%
\subsection{Getting rid of staged changes}\label{rse-git-cmdline-ex-unstage}}

\texttt{git\ checkout} can be used to restore a previous commit when unstaged changes have been made,
but will it also work for changes that have been staged but not committed?
To find out:

\begin{itemize}
\tightlist
\item
  Change \texttt{names.txt}.
\item
  \texttt{git\ add} that change.
\item
  Use \texttt{git\ checkout} to see if you can remove your change.
\end{itemize}

Does it work?

\hypertarget{rse-git-cmdline-ex-blame}{%
\subsection{Figuring out who did what}\label{rse-git-cmdline-ex-blame}}

Run the command \texttt{git\ blame\ names.txt}.

\begin{itemize}
\tightlist
\item
  What does each line of the output show?
\item
  Why do some lines start with a circumflex \texttt{\^{}}?
\end{itemize}

\hypertarget{rse-git-cmdline-keypoints}{%
\section{Key Points}\label{rse-git-cmdline-keypoints}}

\begin{itemize}
\tightlist
\item
  Use git config with the \texttt{-\/-global} option to configure a user name, email address, and other preferences once per machine.
\item
  \texttt{git\ init} initializes a repository.
\item
  Git stores all of its repository data in the \texttt{.git} directory.
\item
  \texttt{git\ status} shows the status of a repository.
\item
  \texttt{git\ add} puts files in the staging area.
\item
  \texttt{git\ commit} saves the staged content as a new commit in the local repository.
\item
  The \texttt{.gitignore} file tells Git what files to ignore.
\item
  \texttt{git\ push} copies changes from a local repository to a remote repository.
\item
  \texttt{git\ pull} copies changes from a remote repository to a local repository.
\item
  \texttt{git\ diff} displays differences between commits.
\item
  \texttt{git\ checkout} recovers old versions of files.
\end{itemize}

\hypertarget{rse-git-advanced}{%
\chapter{Advanced Git}\label{rse-git-advanced}}

Now that we are comfortable using Git at the command line,
we can look at two tools for organizing our development and working with others:
branching and pull requests.
\href{glossary.html\#git-branch}{Branches} let us work on multiple things simultaneously in a single repository;
\href{glossary.html\#pull-request}{pull requests} (PRs) let us submit our work for review,
get feedback,
and make updates.
Used together,
they allow us to go through the write-review-revise cycle
familiar to anyone who has ever written a journal paper
in hours rather than weeks.

\begin{quote}
This lesson is derived in part from one created at
\href{https://uw-madison-datascience.github.io/git-novice-custom/}{the University of Wisconsin-Madison}.
We are grateful to its authors for using an open license
so that we could repurpose their work.
\end{quote}

\hypertarget{rse-git-advanced-create-branch}{%
\section{What is a branch and how do I create one?}\label{rse-git-advanced-create-branch}}

The Git lessons so far have all used a sequential timeline:
each change builds on the one before,
and \emph{only} on the one before.
However,
there are times when we want to keep our main work safe from experimental changes.
To do this, we can use \href{glossary.html\#git-branch}{branches} to work on separate tasks in parallel.

We can see what branches a repository using this command:

\begin{verbatim}
$ git branch
\end{verbatim}

\begin{verbatim}
* master
\end{verbatim}

When we initialize a repository,
Git automatically creates a branch called {[}\texttt{master}{]}{[}git-branch-master{]}.
It is often considered the ``official'' version of the repository.
The asterisk '*' indicates that it is currently active,
i.e.,
that all changes we make will take place in this branch by default.
(The active branch is like the \href{glossary.html\#current-working-directory}{current working directory} in the shell.)

Suppose that we are working on an analysis
and don't know whether a \href{glossary.html\#violin-plot}{violin plot} or \href{glossary.html\#beeswarm-plot}{beeswarm plot}
will make our results clearer.
We decide to create a new branch from \texttt{master} for each
and then merge whichever we prefers back to \texttt{master} (Figure~\ref{fig:rse-git-advanced-creating-branches}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-creating-branches}Creating Branches}
\end{figure}

After tossing a coin,
we decide to experiment with violin plots first.
To create the branch,
we run:

\begin{verbatim}
$ git branch violin
\end{verbatim}

We can check that the branch exists by running \texttt{git\ branch} again:

\begin{verbatim}
$ git branch
\end{verbatim}

\begin{verbatim}
* master
  violin
\end{verbatim}

Our branch is there,
but the asterisk \texttt{*} shows that we are still in the \texttt{master} branch.
(By analogy,
creating a new directory doesn't automatically move us into that directory.)
As a further check,
let's see what our repository's status is:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
nothing to commit, working directory clean
\end{verbatim}

To switch to our new branch we can use the \texttt{checkout} command that we first saw in Section~\ref{rse-git-cmdline-restore}:

\begin{verbatim}
$ git checkout violin
$ git branch
\end{verbatim}

\begin{verbatim}
  master
* violin
\end{verbatim}

\texttt{git\ checkout} doesn't just check out a file from a specific commit:
it can also check out the whole repository,
i.e.,
switch it from one saved state to another
by updating all files in the repository to match the desired state.
A branch is a human-readable name for one such state.
We can (and should) choose the name to signal the purpose of the branch,
just as we choose the names of files and variables to indicate what they are for.

At this point,
\texttt{master} and \texttt{violin} are two names for the same repository state (Figure~\ref{fig:rse-git-advanced-repo-state}),
so commands like \texttt{ls} and \texttt{git\ log} shows that the files and history haven't changed
as we switch from \texttt{master} to \texttt{violin}.
This will be true until some changes are committed to our new branch.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-repo-state}Repository State}
\end{figure}

Let's make such a change by editing our analysis script,
adding a lines to create the violin plot,
and committing our changes:

\begin{verbatim}
$ nano bin/analysis.sh
$ git add bin/analysis.sh
$ git commit -m "Created violin plots"
\end{verbatim}

\begin{verbatim}
[violin 3ff8195] Created violin plots
 1 file changed, 1 insertion(+)
\end{verbatim}

If we look at the last couple of commits using \texttt{git\ log},
we see our most recent change:

\begin{verbatim}
$ git log --oneline -n 2
\end{verbatim}

\begin{verbatim}
3ff8195 Created violin plots
64b802f Adding overdue .gitignore file
\end{verbatim}

(We use \texttt{-\/-oneline} and \texttt{-n~2} to shorten the log display.)
But let's switch back to the \texttt{master} branch:

\begin{verbatim}
$ git checkout master
$ git branch
\end{verbatim}

\begin{verbatim}
* master
  violin
\end{verbatim}

If we look at the log,
our latest change is not there:

\begin{verbatim}
$ git log --oneline -n 2
\end{verbatim}

\begin{verbatim}
64b802f (HEAD -> master) Adding overdue .gitignore file
2fe5f6f Updating analysis script
\end{verbatim}

We have not lost our work:
it just isn't included in this branch (Figure~\ref{fig:rse-git-advanced-not-lost}).
We can prove this by switching back to the \texttt{violin} branch and checking the log again:

\begin{verbatim}
$ git checkout violin
$ git log --oneline -n 2
\end{verbatim}

\begin{verbatim}
3ff8195 Created violin plots
64b802f Adding overdue .gitignore file
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-not-lost}Work Is Not Lost}
\end{figure}

We can also look inside \texttt{bin/analysis.sh} and see our changes.
If we make another change and commit it,
that change will also go into the \texttt{violin} branch (Figure~\ref{fig:rse-git-advanced-change-into-branch}):

\begin{verbatim}
$ nano bin/analysis.sh
$ git add .
$ git commit -m "Adding facets to the violin plot"
\end{verbatim}

\begin{verbatim}
[violin db1d03f] Adding facets to the violin plot
 1 file changed, 4 insertions(+), 2 deletions(-)
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-change-into-branch}Change Goes Into Branch}
\end{figure}

And if we want to see the differences between two branches,
we can use \texttt{git\ diff} with the same double-dot \texttt{..} syntax we use
to view differences between two revisions:

\begin{verbatim}
$ git diff master..violin
\end{verbatim}

\begin{verbatim}
diff --git a/bin/analysis.sh b/bin/analysis.sh
index 8de12fa..bbe78f7 100644
--- a/bin/analysis.sh
+++ b/bin/analysis.sh
@@ -1,5 +1,5 @@
 #!/usr/bin/env bash
 cat $1 | \
   species --all | \
-  plot --default > \
+  plot --violin --facet=species_name > \
   plots/violin-by-species.png
\end{verbatim}

We can repeat this process to experiment with beeswarm plots.
Since we don't want to include any of the violin plot work in this branch,
we make our new branch from \texttt{master}.
To speed things up a bit,
we use the \texttt{-b} option to \texttt{git\ checkout} to create the branch and switch to it
in a single step:

\begin{verbatim}
$ git checkout master           # make sure of our starting point
$ git checkout -b beeswarm      # make branch and switch to it
$ nano bin/analysis.sh          # make our changes
$ git add bin/analysis.sh       # add our changes
$ git commit -m "Beeswarms"     # we really should use a more informative message
$ git log --oneline -n 2        # show history
\end{verbatim}

\begin{verbatim}
917823d (HEAD -> beeswarm) Beeswarms
64b802f (master) Adding overdue .gitignore file
\end{verbatim}

The log shows that the changes in the \texttt{beeswarm} branch are
on top of the changes in the \texttt{master} branch,
and do \emph{not} include the changes in the \texttt{violin} branch (Figure~\ref{fig:rse-git-advanced-where-changes-are}).
If we're curious,
we can use \texttt{git\ diff} to show the differences between the two plotting branches:

\begin{verbatim}
$ git diff beeswarm..violin
\end{verbatim}

\begin{verbatim}
diff --git a/bin/analysis.sh b/bin/analysis.sh
index 2ebd322..bbe78f7 100644
--- a/bin/analysis.sh
+++ b/bin/analysis.sh
@@ -1,5 +1,5 @@
 #!/usr/bin/env bash
 cat $1 | \
   species --all | \
-  plot --beeswarm --jitter=0.1 --facet=species_name > \
-  plots/beeswarm-by-species.png
+  plot --violin --facet=species_name > \
+  plots/violin-by-species.png
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-where-changes-are}Where Changes Are}
\end{figure}

\hypertarget{rse-git-advanced-merge}{%
\section{How do I merge work from separate branches?}\label{rse-git-advanced-merge}}

After a bit of experimentation,
we decide that we prefer violin plots to beeswarm plots.
We now have three options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add our changes to \texttt{bin/analysis.sh} once again in the \texttt{master} branch.
\item
  Stop working in \texttt{master} and start using the \texttt{violin} branch for future development.
\item
  \href{glossary.html\#git-merge}{Merge} the \texttt{violin} and \texttt{master} branches.
\end{enumerate}

The first option is tedious and error-prone,
while the second option will lead to confusion
and doesn't give us a way to combine changes made in two or more branches.
The third option the simplest, fastest, and most reliable.
To start,
let's make sure we're in the \texttt{master} branch:

\begin{verbatim}
$ git checkout master
$ git branch
\end{verbatim}

\begin{verbatim}
  beeswarm
* master
  violin
\end{verbatim}

We can now merge the changes in \texttt{violin} branch into our current branch
with a single command:

\begin{verbatim}
$ git merge violin
\end{verbatim}

\begin{verbatim}
Updating 64b802f..db1d03f
Fast-forward
 bin/analysis.sh | 5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)
\end{verbatim}

Merging doesn't change the source branch \texttt{violin} (Figure~\ref{fig:rse-git-advanced-merging-doesnt-change}).
Once the merge is done,
though,
all of the changes made in \texttt{violin} are also in the history of \texttt{master}:

\begin{verbatim}
$ git log --oneline -n 4
\end{verbatim}

\begin{verbatim}
db1d03f (HEAD -> master, violin) Adding facets to the violin plot
3ff8195 Created violin plots
64b802f Adding overdue .gitignore file
2fe5f6f Updating analysis script
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-merging-doesnt-change}Merging Doesn't Change Things}
\end{figure}

Note that Git automatically creates a new commit (in this case, \texttt{db1d03f}) to represent the merge.
If we now run \texttt{git\ diff\ master..violin},
Git doesn't print anything
because there aren't any differences to show.

Now that we have merged all of the changes from \texttt{violin} into \texttt{master}
there is no need to keep the \texttt{violin} branch,
so we can delete it:

\begin{verbatim}
$ git branch -d violin
\end{verbatim}

\begin{verbatim}
Deleted branch violin (was db1d03f).
\end{verbatim}

And since we don't want to keep the changes in the \texttt{beeswarm} branch,
we can delete it as well:

\begin{verbatim}
$ git branch -d beeswarm
\end{verbatim}

\begin{verbatim}
error: The branch 'beeswarm' is not fully merged.
If you are sure you want to delete it, run 'git branch -D beeswarm'.
\end{verbatim}

Git refuses to do what we have asked
because we have \emph{not} merged the changes from \texttt{beeswarm} into \texttt{master}
(or any other branch),
so deleting it actually would erase work.
If we are sure we want to do this,
we would have to follow Git's instructions and use the \texttt{-D} option.
We won't do this yet because we can use the changes in the \texttt{beeswarm} branch
to illustrate another key feature of Git:
handling conflicts.

\begin{quote}
\textbf{Not Just the Command Line}

We have been creating, merging, and deleting branches on the command line,
but we can do all of these things using \href{https://www.gitkraken.com/}{GitKraken},
\href{https://www.rstudio.com/products/rstudio/}{the RStudio IDE},
and other GUIs.
The operations stay the same;
all that changes is how we tell the computer what we want to do.
\end{quote}

\hypertarget{rse-git-advanced-conflict}{%
\section{How do I handle conflicting changes?}\label{rse-git-advanced-conflict}}

We now have two branches,
\texttt{master} and \texttt{beeswarm},
in which we have changes the same lines of \texttt{bin/analysis.sh} in different ways:

\begin{verbatim}
$ git diff beeswarm..master
\end{verbatim}

\begin{verbatim}
diff --git a/bin/analysis.sh b/bin/analysis.sh
index 2ebd322..bbe78f7 100644
--- a/bin/analysis.sh
+++ b/bin/analysis.sh
@@ -1,5 +1,5 @@
 #!/usr/bin/env bash
 cat $1 | \
   species --all | \
-  plot --beeswarm --jitter=0.1 --facet=species_name > \
-  plots/beeswarm-by-species.png
+  plot --violin --facet=species_name > \
+  plots/violin-by-species.png
\end{verbatim}

When we try to merge \texttt{beeswarm} into \texttt{master},
Git doesn't know which of these changes we want to keep.
This is called a \href{glossary.html\#git-conflict}{conflict}:

\begin{verbatim}
$ git merge beeswarm master
\end{verbatim}

\begin{verbatim}
Auto-merging bin/analysis.sh
CONFLICT (content): Merge conflict in bin/analysis.sh
Automatic merge failed; fix conflicts and then commit the result.
\end{verbatim}

If we look in \texttt{bin/analysis.sh},
we see that Git has kept both sets of changes,
but has marked which came from where:

\begin{verbatim}
$ cat bin/analysis.sh
\end{verbatim}

\begin{verbatim}
#!/usr/bin/env bash
cat $1 | \
  species --all | \
<<<<<<< HEAD
  plot --violin --facet=species_name > \
  plots/violin-by-species.png
=======
  plot --beeswarm --jitter=0.1 --facet=species_name > \
  plots/beeswarm-by-species.png
>>>>>>> beeswarm
\end{verbatim}

The lines from \texttt{\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}~HEAD} to \texttt{=======} are what was in \texttt{master},
while the lines from there to \texttt{\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}~beeswarm} show what was in \texttt{beeswarm}.
If there were several conflicting regions in the same file,
Git would mark each one this way.

We have to decide what to do next:
keep the \texttt{master} changes,
keep those from \texttt{beeswarm},
edit this part of the file to combine them,
or write something new.
Whatever we do,
we must remove the \texttt{\textgreater{}\textgreater{}\textgreater{}}, \texttt{===}, and \texttt{\textless{}\textless{}\textless{}} markers.
Let's combine the two sets of changes to create side-by-side plots:

\begin{verbatim}
$ nano bin/analysis.sh
$ cat bin/analysis.sh
\end{verbatim}

\begin{verbatim}
#!/usr/bin/env bash
cat $1 | \
  species --all | \
  plot -n 2 --violin --beeswarm --jitter=0.1 --facet=species_name > \
  plots/by-species.png
\end{verbatim}

This counts as yet another change to the file:

\begin{verbatim}
$ git status
\end{verbatim}

\begin{verbatim}
On branch master
All conflicts fixed but you are still merging.
  (use "git commit" to conclude merge)

Changes to be committed:

        modified:   bin/analysis.sh

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

        modified:   bin/analysis.sh
\end{verbatim}

Here,
Git thinks that there are changes to be committed because of the merge,
and that \texttt{bin/analysis.sh} has been modified again because of our edit.
Let's add the file and commit the change:

\begin{verbatim}
$ git add bin/analysis.sh
$ git commit -m "Merging beeswarm plots"
\end{verbatim}

\begin{verbatim}
[master b0c3fc6] Merging beeswarm plots
\end{verbatim}

The project's history now shows a single sequence of commits,
with the \texttt{beeswarm} changes on top of the earlier \texttt{violin} changes:

\begin{verbatim}
$ git log --oneline
\end{verbatim}

\begin{verbatim}
b0c3fc6 (HEAD -> master) Merging beeswarm plots
917823d (beeswarm) Beeswarms
db1d03f Adding facets to the violin plot
3ff8195 Created violin plots
64b802f Adding overdue .gitignore file
2fe5f6f Updating analysis script
c89bf68 Northern data
dc4adfb Initial commit
\end{verbatim}

If we want to see what really happened,
we can add the \texttt{-\/-graph} option to \texttt{git\ log}:

\begin{verbatim}
$ git log --oneline --graph
\end{verbatim}

\begin{verbatim}
*   b0c3fc6 (HEAD -> master) Merging beeswarm plots
|\
| * 917823d (beeswarm) Beeswarms
* | db1d03f Adding facets to the violin plot
* | 3ff8195 Created violin plots
|/
* 64b802f Adding overdue .gitignore file
* 2fe5f6f Updating analysis script
* c89bf68 Northern data
* dc4adfb Initial commit
\end{verbatim}

\hypertarget{rse-git-advanced-workflow}{%
\section{How should I use branches?}\label{rse-git-advanced-workflow}}

People use Git in different ways,
but all of those ways rely heavily on branches.
If we are working on our own computer,
this workflow will help us keep track of what we are doing:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{git\ checkout\ master} to make sure we are in the \texttt{master} branch.
\item
  \texttt{git\ checkout\ -b\ name-of-feature} to create a new branch.
  We \emph{always} create a branch when making changes,
  since we never know what else might come up.
  The branch name should be as descriptive as a variable name or filename would be.
\item
  Make our changes.
  If something occurs to us along the way---for example,
  if we are writing a new function and realize that
  the documentation for some other function should be updated---we do \emph{not}
  do that work in this branch just because we happen to be there.
  Instead,
  we commit our changes,
  switch back to \texttt{master},
  and create a new branch for the other work.
\item
  When the new feature is complete,
  we \texttt{git\ merge\ master\ name-of-feature}
  to get any changes we merged into \texttt{master} after creating \texttt{name-of-feature}
  and resolve any conflicts.
  This is an important step:
  we want to do the merge and test that everything still works in our feature branch,
  not in \texttt{master}.
\item
  Finally,
  we switch back to \texttt{master} and \texttt{git\ merge\ name-of-feature\ master}
  to merge our changes into \texttt{master}.
  We should not have any conflicts,
  and all of our tests should pass.
\end{enumerate}

Most experienced developers use this ``branch-per-feature'' workflow,
but what exactly is a ``feature''?
These rules make sense for small projects with or without collaborators:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Anything cosmetic that is only one or two lines long can be done in \texttt{master} and committed right away.
  ``Cosmetic'' means changes to comments or documentation:
  nothing that affects how code runs, not even a simple variable renaming.
\item
  A pure addition that doesn't change anything else is a feature and goes into a branch.
  For example,
  if we run a new analysis and save the results,
  that should be done on its own branch
  because it might take several tries to get the analysis to run,
  and we might interrupt ourselves to fix things that we discover aren't working.
\item
  Every change to code that someone might want to undo later in one step gets is a feature.
  For example,
  if a new parameter is added to a function,
  then every call to the function has to be updated.
  Since neither alteration makes sense without the other,
  those changes are considered a single feature and should be done in one branch.
\end{enumerate}

The hardest thing about using a branch-per-feature workflow is sticking to it for small changes.
As the first point in the list above suggests,
most people are pragmatic about this on small projects;
on large ones,
where dozens of people might be committing,
even the smallest and most innocuous change needs to be in its own branch
so that it can be reviewed (which we discuss below).

\hypertarget{rse-git-advanced-fork}{%
\section{How can I use someone else's work?}\label{rse-git-advanced-fork}}

So far we have used Git to manage individual work,
but it really comes into its own when we are working with other people.
We can do this in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Everyone has read and write access to a single shared repository.
\item
  Everyone can read from the project's main repository,
  but only a few people can commit changes to it.
  The project's other contributors \href{glossary.html\#git-fork}{fork} the main repository to create one that they own,
  do their work in that,
  and then submit their changes to the main repository.
\end{enumerate}

The first approach works well for teams of up to half a dozen people
who are all comfortable using Git,
but if the project is larger,
or if contributors are worried that they might make a mess in the \texttt{master} branch,
the second approach is safer.

Git itself doesn't have any notion of a ``main repository'',
but \href{glossary.html\#forge}{forges} like GitHub, GitLab, and BitBucket all encourage people
to use Git in ways that effectively create one.
Suppose,
for example,
that Frances Bilas has put her plotting project online
and that the repository's URL is \texttt{https://github.com/francesbilas/plotting}.
Jean Jennings (another of the original ENIAC programmers) would like to contribute to the project,
so she goes to that URL and clicks on the ``Fork'' button in the upper right corner
(Figure~\ref{fig:rse-git-advanced-fork-button}).
GitHub immediately creates a copy of Frances's repository within Jean's account on GitHub's own servers.
When the command completes,
the setup on GitHub now looks like this:

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-fork-button}Forking}
\end{figure}

Nothing has happened yet on Jean's own machine:
the new repository exists only on GitHub.
When Jean explores its history,
she sees that it contains all of the changes Frances made.

A copy of a repository is called a \href{glossary.html\#git-clone}{clone}.
In order to start working on the project,
Jean needs to create a clone of \emph{her} repository (not Frances's) on her own computer.
We will modify Jean's prompt to include her desktop user ID (\texttt{jj})
and her working directory (initially \texttt{\textasciitilde{}})
to make it easier to follow what's happening:

\begin{verbatim}
jj:~ $ git clone https://github.com/jeanjennings/plotting.git
\end{verbatim}

\begin{verbatim}
Cloning into 'plotting'...
remote: Enumerating objects: 32, done.
remote: Counting objects: 100% (32/32), done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 32 (delta 5), reused 32 (delta 5), pack-reused 0
Unpacking objects: 100% (32/32), done.
\end{verbatim}

This command creates a new directory with the same name as the project,
i.e., \texttt{plotting}.
When Jean goes into this directory and runs \texttt{ls} and \texttt{git\ log},
she sees that all of the project's files and history are there:

\begin{verbatim}
jj:~ $ cd plotting
jj:~/plotting $ ls
\end{verbatim}

\begin{verbatim}
README.md       bin             data
\end{verbatim}

\begin{verbatim}
jj:~/plotting $ git log --oneline
\end{verbatim}

\begin{verbatim}
b0c3fc6 Merging beeswarm plots
917823d Beeswarms
db1d03f Adding facets to the violin plot
3ff8195 Created violin plots
64b802f Adding overdue .gitignore file
2fe5f6f Updating analysis script
c89bf68 Northern data
dc4adfb Initial commit
\end{verbatim}

She also sees that Git has automatically created a remote for her repository
that points back at her repository on GitHub:

\begin{verbatim}
jj:~/plotting $ git remote -v
\end{verbatim}

\begin{verbatim}
origin  https://github.com/jeanjennings/plotting.git (fetch)
origin  https://github.com/jeanjennings/plotting.git (push)
\end{verbatim}

Frances's original repository,
Jean's fork on GitHub,
and Jean's new clone on her desktop
are now arranged as shown in Figure~\ref{fig:rse-git-advanced-forked-repos}.
Jean can pull changes from her fork and push work back there,
but she needs to do one more thing before she can get changes from Frances's repository:

\begin{verbatim}
jj:~/plotting $ git remote add upstream https://github.com/francesbilas/plotting.git
jj:~/plotting $ git remote -v
\end{verbatim}

\begin{verbatim}
origin      https://github.com/jeanjennings/plotting.git (fetch)
origin      https://github.com/jeanjennings/plotting.git (push)
upstream    https://github.com/francesbilas/plotting.git (fetch)
upstream    https://github.com/francesbilas/plotting.git (push)
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-forked-repos}Forked Repositories}
\end{figure}

Jean has called her new remote \texttt{upstream} because it points at the repository hers are derived from.
She could use any name,
but \texttt{upstream} is a nearly universal convention.

With this remote in place,
Jean is finally set up (Figure~\ref{fig:rse-git-advanced-finished-forking}).
Suppose,
for example,
that Frances has modified the project's \texttt{README.md} file to add Jean as a contributor.
(Again, we show Frances's user ID and working directory in her prompt to make it clear who's doing what).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-finished-forking}Finished Forking}
\end{figure}

\begin{verbatim}
frances:~/plotting $ pwd
\end{verbatim}

\begin{verbatim}
/Users/frances/plotting
\end{verbatim}

\begin{verbatim}
frances:~/plotting $ nano README.md
frances:~/plotting $ cat README.md
\end{verbatim}

\begin{verbatim}
# Plotting Species Distribution

Contributors:

-   Frances Bilas
-   Jean Jennings
\end{verbatim}

Frances commits her changes and pushes them to \emph{her} repository on GitHub:

\begin{verbatim}
frances:~/plotting $ git commit -a -m "Adding Jean as a contributor"
\end{verbatim}

\begin{verbatim}
[master 766c2cd] Adding Jean as a contributor
 1 file changed, 6 insertions(+)
\end{verbatim}

\begin{verbatim}
frances:~/plotting $ git push origin master
\end{verbatim}

\begin{verbatim}
Counting objects: 3, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 340 bytes | 340.00 KiB/s, done.
Total 3 (delta 1), reused 0 (delta 0)
remote: Resolving deltas: 100% (1/1), completed with 1 local object.
To https://github.com/francesbilas/plotting.git
   b0c3fc6..766c2cd  master -> master
\end{verbatim}

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-pull-request}Pull Request}
\end{figure}

The situation is now as shown in (Figure~\ref{fig:rse-git-advanced-pull-request}a),
with Frances's changes on her desktop and in her GitHub repository
but not in either of Jean's repositories.
Since Jean has create a remote that points at Frances's GitHub repository,
though,
she can easily pull those changes to her desktop (Figure~\ref{fig:rse-git-advanced-pull-request}b):

\begin{verbatim}
jj:~/plotting $ git pull upstream master
\end{verbatim}

\begin{verbatim}
remote: Enumerating objects: 5, done.
remote: Counting objects: 100% (5/5), done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0
Unpacking objects: 100% (3/3), done.
From https://github.com/francesbilas/plotting
 * branch            master     -> FETCH_HEAD
 * [new branch]      master     -> upstream/master
Updating b0c3fc6..766c2cd
Fast-forward
 README.md | 6 ++++++
 1 file changed, 6 insertions(+)
\end{verbatim}

Pulling from a repository owned by someone else
is no different than pulling from a repository we own.
In either case,
Git merges the changes and asks us to resolve any conflicts that arise.
The only significant difference is that,
as with \texttt{git\ push} and \texttt{git\ pull},
we have to specify both a remote and a branch:
in this case,
\texttt{upstream} and \texttt{master}.

\hypertarget{rse-git-advanced-pull-requests}{%
\section{What is a pull request and how do I create one?}\label{rse-git-advanced-pull-requests}}

Jean can now get Frances's work,
but how can Frances get Jean's?
One way would be for Frances to create a remote that pointed at Jean's repository on GitHub
and then pull in Jean's changes periodically,
but that would lead to chaos,
since we could never be sure that everyone's work was in any one place at the same time.
Instead,
almost everyone uses \href{glossary.html\#pull-request}{pull requests} instead.

Pull requests aren't part of Git itself,
but a great way to collaborate with others using \href{glossary.html\#forge}{forges} like GitHub.
A pull request is essentially a note saying,
``Someone would like to merge branch A of repository B into branch X of repository Y''.
The pull request does not contain the changes,
but instead points at two particular branches.
If either branch changes,
the difference displayed is always up to date (Figure~\ref{fig:rse-git-advanced-how-pr-works}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-how-pr-works}How Pull Requests Work}
\end{figure}

A pull request can store more than just the source and destination branches:
it can also store comments people have made about the proposed merge.
GitHub and other \href{glossary.html\#forge}{forges} allow users to comment on the pull request as a whole,
or on particular lines,
and mark comments as out of date
if the author of the pull request updates the code that the comment is attached to
(Figure~\ref{fig:rse-git-advanced-comment-out-of-date}).
It is common for complex changes to go through several rounds of review and revision
before being merged.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-git-advanced-comment-out-of-date}Comment Out of Date}
\end{figure}

To see this in action,
suppose that Jean wants to add her email address to \texttt{README.md} in the plotting project.
She creates a new branch and switches to it:

\begin{verbatim}
jj:~/plotting $ git checkout -b adding-email
\end{verbatim}

\begin{verbatim}
Switched to a new branch 'adding-email'
\end{verbatim}

then makes her change and commits it:

\begin{verbatim}
jj:~/plotting $ nano README.md
jj:~/plotting $ git commit -a -m "Adding my email address"
\end{verbatim}

\begin{verbatim}
[master b8938eb] Adding my email address
 1 file changed, 1 insertion(+), 1 deletion(-)
\end{verbatim}

\begin{verbatim}
jj:~/plotting $ git diff -r HEAD~1
\end{verbatim}

\begin{verbatim}
diff --git a/README.md b/README.md
index a55a9bb..eb24a3f 100644
--- a/README.md
+++ b/README.md
@@ -3,4 +3,4 @@
 Contributors:

 -   Frances Bilas
--   Jean Jennings
+-   Jean Jennings <jj@eniac.org>
\end{verbatim}

Her changes are in her desktop repository, and \emph{only} in her desktop repository.
She cannot create a pull request until those changes are on GitHub,
so she pushes her new branch to her repository on GitHub:

\begin{verbatim}
jj:~/plotting $ git push origin adding-email
\end{verbatim}

\begin{verbatim}
Counting objects: 3, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 307 bytes | 307.00 KiB/s, done.
Total 3 (delta 2), reused 0 (delta 0)
remote: Resolving deltas: 100% (2/2), completed with 2 local objects.
remote:
remote: Create a pull request for 'adding-email' on GitHub by visiting:
remote:      https://github.com/jeanjennings/plotting/pull/new/adding-email
remote:
To https://github.com/jeanjennings/plotting.git
 * [new branch]      adding-email -> adding-email
\end{verbatim}

When Jean goes to her GitHub repository in the browser,
GitHub notices that she has just pushed a new branch
and asks her if she wants to create a pull request:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/after-jean-pushes.png}
\caption{\label{fig:image-after-jean-pushes}After Jean Pushes}
\end{figure}

When Jean clicks on the button,
GitHub displays a page showing the default source and destination of the pull request
and a pair of editable boxes for the pull request's title and a longer comment:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pull-request-a.png}
\caption{\label{fig:image-pull-request-a}Starting Pull Request}
\end{figure}

If she scrolls down,
she can see a summary of the changes that will be in the pull request:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pull-request-b.png}
\caption{\label{fig:image-pull-request-b}Summary of Pull Request}
\end{figure}

She fills in the top two boxes:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/fill-in-pull-request.png}
\caption{\label{fig:image-fill-in-pull-request}Filling In Pull Request}
\end{figure}

and clicks on ``Create Pull Request''.
GitHub does that
and then displays a page showing the new pull request:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/new-pull-request.png}
\caption{\label{fig:image-new-pull-request}New Pull Request}
\end{figure}

Note that this pull request is displayed in Frances's repository rather than Jean's
since it is Frances's repository that will be affected if the pull request is merged.

Some time later,
Frances checks her repository and sees that there is a pull request:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/viewing-new-pull-request.png}
\caption{\label{fig:image-viewing-new-pull-request}Viewing Pull Request}
\end{figure}

Clicking on the ``Pull requests'' tab brings up a list of PRs:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-list.png}
\caption{\label{fig:image-pr-list}Listing Pull Requests}
\end{figure}

and clicking on the pull link itself displays more information about it:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-details.png}
\caption{\label{fig:image-pr-details}Pull Request Details}
\end{figure}

Since there are no conflicts,
GitHub will allow Frances to merge the PR immediately using the ``Merge pull request'' button.
She could also discard or reject it \emph{without} merging using the ``Close pull request'' button.
Instead,
she clicks on the ``Files changed'' tab to see what Jean has changed:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-changes.png}
\caption{\label{fig:image-pr-changes}Files Changed}
\end{figure}

If she moves her mouse over particular lines,
a white-on-blue cross appears near the numbers to indicate that she can add comments:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-comment-marker.png}
\caption{\label{fig:image-pr-comment-marker}Comment Marker}
\end{figure}

She clicks on the marker beside her own name and writes a comment:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-writing-comment.png}
\caption{\label{fig:image-pr-writing-comment}Writing Comment}
\end{figure}

She only wants to make one comment rather than write a lengthier multi-comment review,
so she clicks on ``Add single comment''.
GitHub redisplays the page with her comment inserted inline:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-with-comment.png}
\caption{\label{fig:image-pr-with-comment}Pull Request With Comment}
\end{figure}

While all of this has been doing on,
GitHub has been emailing notifications to both Jean and Frances.
When Jean clicks on the link in hers,
it takes her to the PR and shows her Frances's comment.
She changes \texttt{README.md},
commits,
and pushes,
but does \emph{not} create a new pull request or do anything to the existing one.
As explained above,
a PR is a note asking that two branches be merged,
so if either end of the merge changes,
the PR updates automatically.

Sure enough,
when Frances looks at the PR again a few moments later she sees this:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-with-fix.png}
\caption{\label{fig:image-pr-with-fix}Pull Request With Fix}
\end{figure}

Satisfied,
she goes back to the ``Conversation'' tab and clicks on ``Merge''.
The icon at the top of the PR's page changes text and color to show that the merge was successful:

\begin{figure}
\centering
\includegraphics{figures/rse-git-advanced/pr-successful-merge.png}
\caption{\label{fig:image-pr-successful-merge}Successful Merge}
\end{figure}

To get those changes from GitHub to her desktop repository,
Frances uses \texttt{git\ pull}:

\begin{verbatim}
frances:~/plotting $ git pull origin master
\end{verbatim}

\begin{verbatim}
remote: Enumerating objects: 9, done.
remote: Counting objects: 100% (9/9), done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 7 (delta 3), reused 5 (delta 2), pack-reused 0
Unpacking objects: 100% (7/7), done.
From https://github.com/francesbilas/plotting
 * branch            master     -> FETCH_HEAD
   766c2cd..984b116  master     -> origin/master
Updating 766c2cd..984b116
Fast-forward
 README.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)
\end{verbatim}

To get the change she just made from her \texttt{adding-email} branch into her \texttt{master} branch,
Jean could use \texttt{git\ merge} on the command line.
It's a little clearer,
though,
if she also uses \texttt{git\ pull} from her \texttt{upstream} repository (i.e., Frances's repository)
so that she's sure to get any other changes that Frances may have merged:

\begin{verbatim}
jj:~/plotting $ git checkout master
\end{verbatim}

\begin{verbatim}
Switched to branch 'master'
Your branch is ahead of 'origin/master' by 1 commit.
  (use "git push" to publish your local commits)
\end{verbatim}

\begin{verbatim}
jj:~/plotting $ git pull upstream master
\end{verbatim}

\begin{verbatim}
remote: Enumerating objects: 1, done.
remote: Counting objects: 100% (1/1), done.
remote: Total 1 (delta 0), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (1/1), done.
From https://github.com/francesbilas/plotting
 * branch            master     -> FETCH_HEAD
   766c2cd..984b116  master     -> upstream/master
Updating 766c2cd..984b116
Fast-forward
 README.md | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)
\end{verbatim}

All four repositories are now synchronized.

This process may seem overly complicated when it is described step by step,
but it quickly becomes second nature.
Everyone involved in the project can work at their own pace,
picking up others' changes and submitting their own whenever they want.
More importantly,
everyone has a chance to review work before it lands in the main repository.
As we discuss in Chapter~\ref{FIXME},
doing reviews doesn't just prevent errors from creeping in:
it is also an effective way to spread understanding and skills.

\hypertarget{rse-git-advanced-tag}{%
\section{How can I label a particular version of my project?}\label{rse-git-advanced-tag}}

FIXME: explain tagging

\hypertarget{rse-git-advanced-exercises}{%
\section{Exercises}\label{rse-git-advanced-exercises}}

\hypertarget{rse-git-advanced-ex-additional-pr}{%
\subsection{Add new country file and make additional PR}\label{rse-git-advanced-ex-additional-pr}}

\begin{itemize}
\tightlist
\item
  Starting in the master branch make a new branch
\item
  Copy other country file into a new country
\item
  Edit the file to include info on the new country
\item
  Add and commit this new file
\item
  Push the new changes to GitHub
\end{itemize}

\hypertarget{rse-git-advanced-keypoints}{%
\section{Key Points}\label{rse-git-advanced-keypoints}}

\begin{itemize}
\tightlist
\item
  \texttt{git\ branch} creates a new branch where new features can be developed while leaving the master branch untouched.
\item
  \texttt{git\ clone} copies a remote repository to create a local repository with a remote called origin automatically set up.
\item
  Pull requests suggest changes to repos where you don't have write privileges.
\item
  Reorganizing code in consistent ways makes errors less likely.
\item
  Replace a value with a name to make code more readable and to forestall typing errors.
\item
  Replace a repeated test with a flag to ensure consistency.
\item
  Turn small pieces of large functions into functions in their own right, even if they are only used once.
\item
  Combine functions if they are always used together on the same inputs.
\item
  Use lookup tables to make decision rules easier to follow.
\item
  Use comprehensions instead of loops.
\end{itemize}

\hypertarget{rse-style}{%
\chapter{Code Style, Review, and Refactoring}\label{rse-style}}

To paraphrase Dobzhansky (\protect\hyperlink{ref-Dobz1973}{1973}),
nothing in software development makes sense except in light of human psychology.
This is particularly true when we look at programming style.
Computers don't need to understand programs in order to execute them,
but people do in order to create them, maintain them, and fix them.
The more clearly those programs are laid out,
the easier it is to find things and make sense of them.
This lesson therefore describes some widely-used rules of Python programming style,
and some features of the language that you can use to make your programs more flexible and more readable.

The biggest benefit of having a second person work on a programming project
is therefore not getting twice as much code written,
but having code reviewed.
Study after study over more than 40 years has shown that code review is the most effective way to find bugs in software
Fagan (\protect\hyperlink{ref-Faga1976}{1976}),Fagan (\protect\hyperlink{ref-Faga1986}{1986}),Cohen (\protect\hyperlink{ref-Cohe2010}{2010})Bacchelli and Bird (\protect\hyperlink{ref-Bacc2013}{2013}).
Despite this,
it still isn't common in research software development,
in part because it isn't part of the culture Segal (\protect\hyperlink{ref-Sega2005}{2005}),
but also because code review is mostly useful
when the reviewers understand the problem domain well enough to comment on algorithms and design choices
rather than indentation and variable naming,
and the number of people who can do that for a research project is typically very small---sometimes
as small as one Petre and Wilson (\protect\hyperlink{ref-Petr2014}{2014}).

This lesson will look at the mechanics of code review
and present some short examples of the kinds of things reviewers should look for.

\hypertarget{rse-style-github}{%
\section{How should I review someone else's code?}\label{rse-style-github}}

How you review is just as important as what you look for:
being dismissive or combative are good ways to ensure that people don't pay attention to your reviews,
or avoid having you review their work.
Equally,
being defensive when someone offers suggestions politely and sincerely is very human,
but can stunt your development as a programmer.

Lots of people have written guidelines for doing reviews,
which are also useful when reviewing written work Quenneville (\protect\hyperlink{ref-Quen2018}{2018}),Sankarram (\protect\hyperlink{ref-Sank2018}{2018}).
A few key points are:

\begin{description}
\tightlist
\item[Work in small increments.]
As Cohen (\protect\hyperlink{ref-Cohe2010}{2010}) and others have found,
code review is most effective when done in short bursts.
That means that change requests should also be short:
anything that's more than a couple of screens long
should be broken into smaller pieces.
\item[Look for algorithmic problems first.]
Code review isn't just (or even primarily) about style:
its real purpose is to find bugs before they can affect anyone.
The first pass over any change should therefore look for algorithmic problems.
Are the calculations right?
Are any rare cases going to be missed?
Are errors being caught and handled Chapter~\ref{logging}?
\item[Use a rubric.]
Linters are great,
but can't decide when someone should have used a lookup table instead of conditionals.
A list of things to check for can make review faster and more comprehensible,
especially when you can copy-and-paste or drag-and-drop specific comments
onto specific lines
(something that GitHub unfortunately doesn't yet support).
\item[Ask for clarification.]
If you don't understand something,
or don't understand why the author did it,
ask.
(And when the author explains it,
think about suggesting that the explanation ought to be documented somewhere.)
\item[Offer alternatives.]
Telling authors that something is wrong is helpful;
telling them what they might do instead is more so.
\item[Don't be sarcastic or disparaging.]
``Did you maybe think about \emph{testing} this garbage?''
is a Code of Conduct violation in any well-run project.
\item[Don't present opinions as facts.]
``Nobody uses X any more'' might be true.
If it is,
the person making the claim ought to be able to point at download statistics
or a Google Trends search;
if they can't,
they should say,
``I don't think anybody uses X any more'' and explain why they think that.
\item[Don't feign surprise or pass judgment.]
``Gosh, didn't you know {[}some obscure fact{]}?'' isn't helpful;
neither is, ``Geez, why don't you {[}some clever trick{]} here?''
\item[Don't overwhelm people with details.]
If someone has used the letter \texttt{x} as a variable name in several places,
and they shouldn't have,
comment on the first two or three and simply put a check beside the others---the reader
won't need the comment repeated.
\item[Don't ask people to do extra work.]
Nobody enjoys fixing bugs and style violations.
Asking them to add a few features while they're at it is rude.
\item[Don't let people break these rules just because they're frequent contributors or in positions of power.]
The culture of any organization is shaped by the worst behavior it is willing to tolerate Gruenert and Whitaker (\protect\hyperlink{ref-Grue2015}{2015}).
If you let people be rude to one another,
\emph{that} is your culture.
\item[Be specific in replies to reviewers.]
If someone has suggested a better variable name,
you can probably simply fix it.
If someone has suggested a major overhaul to an algorithm,
you should reply to their comment to point at the commit that includes the fix.
\item[Thank your reviewers.]
If someone has taken the time to read your code carefully,
thank them for doing it.
\end{description}

\hypertarget{rse-style-pep8}{%
\section{What are the standard rules of good style for Python programs?}\label{rse-style-pep8}}

The single most important rule of style is to be consistent,
both internally and with other programs.
Python's standard style is called \href{https://www.python.org/dev/peps/pep-0008/}{PEP-8};
the term ``PEP'' is short for ``Python Enhancement Proposal'',
and PEP-8 has become the most widely used standard for Python coding.
Some of its rules are listed below,
along with others borrowed from ``\href{https://github.com/jennybc/code-smells-and-feels}{Code Smells and Feels}'':

FIXME: add before-and-after examples for each of these.

FIXME: move this to the novice material.

\hypertarget{always-indent-code-blocks-using-4-spaces-and-use-spaces-instead-of-tabs.}{%
\subsection{Always indent code blocks using 4 spaces, and use spaces instead of tabs.}\label{always-indent-code-blocks-using-4-spaces-and-use-spaces-instead-of-tabs.}}

\hypertarget{do-not-put-spaces-inside-parentheses}{%
\subsection{Do not put spaces inside parentheses}\label{do-not-put-spaces-inside-parentheses}}

I.e., don't write (~1+2~).
This applies to function calls as well:
do not write max(~a,~b~).

\hypertarget{always-use-spaces-around-comparisons-like-and-.}{%
\subsection{\texorpdfstring{Always use spaces around comparisons like \texttt{\textgreater{}} and \texttt{\textless{}=}.}{Always use spaces around comparisons like \textgreater{} and \textless{}=.}}\label{always-use-spaces-around-comparisons-like-and-.}}

Use your own judgment for spacing around common arithmetic operators like \texttt{+} and \texttt{/}.

\hypertarget{use-all_caps_with_underscores-for-constants-also-called-global-variables.}{%
\subsection{\texorpdfstring{Use \texttt{ALL\_CAPS\_WITH\_UNDERSCORES} for constants (also called global variables).}{Use ALL\_CAPS\_WITH\_UNDERSCORES for constants (also called global variables).}}\label{use-all_caps_with_underscores-for-constants-also-called-global-variables.}}

\hypertarget{use-lower_case_with_underscores-for-the-names-of-functions-and-variables.}{%
\subsection{\texorpdfstring{Use \texttt{lower\_case\_with\_underscores} for the names of functions and variables.}{Use lower\_case\_with\_underscores for the names of functions and variables.}}\label{use-lower_case_with_underscores-for-the-names-of-functions-and-variables.}}

This naming convention is called \href{glossary.html\#snake-case}{snake case} or \href{glossary.html\#pothole-case}{pothole case}.
You should only use {[}\texttt{CamelCase}{]}{[}camel-case{]} for classes,
which are outside the scope of this lesson.

\hypertarget{put-two-blank-links-between-each-function-definition.}{%
\subsection{Put two blank links between each function definition.}\label{put-two-blank-links-between-each-function-definition.}}

This helps them stand out.

\hypertarget{avoid-abbreviations-in-function-and-variable-names.}{%
\subsection{Avoid abbreviations in function and variable names.}\label{avoid-abbreviations-in-function-and-variable-names.}}

They can be ambiguous,
and can be be hard for non-native speakers to understand.
This rule doesn't necessarily you will have to do more typing:
a good programming editor will \href{glossary.html\#auto-completion}{auto-complete} names for you.

\hypertarget{use-short-names-for-short-lived-local-variables-but-longer-names-for-things-with-wider-scope.}{%
\subsection{Use short names for short-lived local variables but longer names for things with wider scope.}\label{use-short-names-for-short-lived-local-variables-but-longer-names-for-things-with-wider-scope.}}

Loop indices Beniamini et al. (\protect\hyperlink{ref-Beni2017}{2017}) can be \texttt{i} and \texttt{j} (provided the loop is only a few lines long).
Anything that is used at a greater distance
or whose purpose isn't immediately clear
(such as a function) should have a longer name.

\hypertarget{put-everything-in-a-file-in-order.}{%
\subsection{Put everything in a file in order.}\label{put-everything-in-a-file-in-order.}}

The order of items in each file should be:

\begin{itemize}
\tightlist
\item
  The \href{glossary.html\#shebang}{shebang} line (because it has to come first to work).
\item
  The file's documentation string (Chapter~\ref{style}).
\item
  All of the \texttt{import} statements, one per line.
\item
  Constant definitions.
\item
  Function definitions.
\item
  If the file can be run as a program,
  the \texttt{if\ \_\_name\_\_\ ==\ \textquotesingle{}\_\_main\_\_\textquotesingle{}} statement discussed in
  Section~\ref{rse-package-py-import}.
\end{itemize}

\hypertarget{do-not-comment-and-uncomment-sections-of-code-to-change-behavior.}{%
\subsection{Do not comment and uncomment sections of code to change behavior.}\label{do-not-comment-and-uncomment-sections-of-code-to-change-behavior.}}

If you need to do something in some runs of the program and not do it in others,
use an \texttt{if} statement to enable or disable that block of code.
It's much more reliable---you're far less likely to accidentally comment out one too many lines---and
you may find that you want to leave those conditional sections in the finished program
for logging purposes (Chapter~\ref{logging}).

\hypertarget{keep-functions-short.}{%
\subsection{Keep functions short.}\label{keep-functions-short.}}

Nothing should be more than a page long
or have more than three levels of indentation because of nested loops and conditionals.
Anything longer or more deeply nested will be hard for readers to fit into \href{glossary.html\#working-memory}{working memory};
when you find yourself needing to break these limits,
extract a function.

\hypertarget{handle-special-cases-at-the-start-of-the-function.}{%
\subsection{Handle special cases at the start of the function.}\label{handle-special-cases-at-the-start-of-the-function.}}

This helps readers mentally get them out of the way
and focus on the ``normal'' case.

\hypertarget{rse-style-check}{%
\section{How can I check that code follows style guidelines?}\label{rse-style-check}}

Checking that code conforms to guidelines like the ones above can be time consuming,
but luckily it doesn't have to be done by hand.
Most languages have tools that will check code style rules and report violations.
These are often called \href{glossary.html\#linter}{linters},
after an early tool called \texttt{{[}lint{]}{[}lint{]}} that found lint (or fluff) in C code.
Python has a tool that used to be called \texttt{pep8}
and is now called \texttt{pycodestyle}
that will do this.
For example,
this program is supposed to count the number of stop words in a document:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stops }\OperatorTok{=}\NormalTok{ [}\StringTok{'a'}\NormalTok{, }\StringTok{'A'}\NormalTok{, }\StringTok{'the'}\NormalTok{, }\StringTok{'The'}\NormalTok{, }\StringTok{'and'}\NormalTok{]}

\KeywordTok{def}\NormalTok{ count(ln):}
\NormalTok{    n }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(ln)):}
\NormalTok{        line }\OperatorTok{=}\NormalTok{ ln[i]}
\NormalTok{        stuff }\OperatorTok{=}\NormalTok{ line.split()}
        \ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ stuff:}
            \CommentTok{# print(word)}
\NormalTok{            j }\OperatorTok{=}\NormalTok{ stops.count(word)}
            \ControlFlowTok{if}\NormalTok{ (j }\OperatorTok{>} \DecValTok{0}\NormalTok{) }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{                n }\OperatorTok{=}\NormalTok{ n }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ n}

\ImportTok{import}\NormalTok{ sys}

\NormalTok{lines }\OperatorTok{=}\NormalTok{ sys.stdin.readlines()}
\CommentTok{# print('number of lines', len(lines))}
\NormalTok{n }\OperatorTok{=}\NormalTok{ count(lines)}
\BuiltInTok{print}\NormalTok{(}\StringTok{'number'}\NormalTok{, n)}
\end{Highlighting}
\end{Shaded}

When we run this command:

\begin{verbatim}
$ pycodestyle count_stops.py
\end{verbatim}

it prints this report:

\begin{verbatim}
src/style/count_stops_before.py:3:1: E302 expected 2 blank lines, found 1
src/style/count_stops_before.py:11:24: E712 comparison to True should be 'if cond is True:' or 'if cond:'
src/style/count_stops_before.py:12:13: E101 indentation contains mixed spaces and tabs
src/style/count_stops_before.py:12:13: W191 indentation contains tabs
src/style/count_stops_before.py:15:1: E305 expected 2 blank lines after class or function definition, found 1
src/style/count_stops_before.py:15:1: E402 module level import not at top of file
\end{verbatim}

Fixing these issues gives us this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}


\NormalTok{stops }\OperatorTok{=}\NormalTok{ [}\StringTok{'a'}\NormalTok{, }\StringTok{'A'}\NormalTok{, }\StringTok{'the'}\NormalTok{, }\StringTok{'The'}\NormalTok{, }\StringTok{'and'}\NormalTok{]}


\KeywordTok{def}\NormalTok{ count(ln):}
\NormalTok{    n }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(ln)):}
\NormalTok{        line }\OperatorTok{=}\NormalTok{ ln[i]}
\NormalTok{        stuff }\OperatorTok{=}\NormalTok{ line.split()}
        \ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ stuff:}
            \CommentTok{# print(word)}
\NormalTok{            j }\OperatorTok{=}\NormalTok{ stops.count(word)}
            \ControlFlowTok{if}\NormalTok{ j }\OperatorTok{>} \DecValTok{0}\NormalTok{:}
\NormalTok{                n }\OperatorTok{=}\NormalTok{ n }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ n}


\NormalTok{lines }\OperatorTok{=}\NormalTok{ sys.stdin.readlines()}
\CommentTok{# print('number of lines', len(lines))}
\NormalTok{n }\OperatorTok{=}\NormalTok{ count(lines)}
\BuiltInTok{print}\NormalTok{(}\StringTok{'number'}\NormalTok{, n)}
\end{Highlighting}
\end{Shaded}

This program gets a clean bill of health,
so it's worth looking at in more detail.
Here are things that should be changed:

\begin{itemize}
\tightlist
\item
  The commented-out \texttt{print} statements should either be removed
  or turned into proper logging statements (Chapter~\ref{logging}).
\item
  The variables \texttt{ln}, \texttt{i}, and \texttt{j} should be given clearer names.
\item
  The outer loop in \texttt{count} loops over the indices of the line list
  rather than over the lines.
  It should do the latter
  (which will allow us to get rid of the variable \texttt{i}).
\item
  There's no reason to store the result of \texttt{line.split} in a temporary variable:
  the inner loop of \texttt{count} can use it directly.
\item
  Rather than counting how often a word occurs in the list of stop words
  and then adding 1 to \texttt{n},
  we can create a set of stop words and use a simple membership test.
  This will be more readable \emph{and} more efficient.
\item
  Since the set of stop words is a global variable,
  it should be written in upper case.
\item
  We should use \texttt{+=} to increment the counter \texttt{n}.
\item
  Rather than reading the input into a list of lines and then looping over that,
  we can give \texttt{count} a stream and have it process the lines one by one.
\item
  Since we might want to use \texttt{count} in other programs some day,
  we should put the two lines at the bottom that handle input
  into a conditional
  so that they aren't executed when this script is imported.
\end{itemize}

After making all these changes,
our little program looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}


\NormalTok{STOPS }\OperatorTok{=}\NormalTok{ \{}\StringTok{'a'}\NormalTok{, }\StringTok{'A'}\NormalTok{, }\StringTok{'the'}\NormalTok{, }\StringTok{'The'}\NormalTok{, }\StringTok{'and'}\NormalTok{\}}


\KeywordTok{def}\NormalTok{ count(reader):}
\NormalTok{    n }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in}\NormalTok{ reader:}
        \ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ line.split():}
            \ControlFlowTok{if}\NormalTok{ word }\KeywordTok{in}\NormalTok{ STOPS:}
\NormalTok{                n }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ n}


\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ count(sys.stdin)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{'number'}\NormalTok{, n)}
\end{Highlighting}
\end{Shaded}

\hypertarget{rse-style-how}{%
\section{How should I collaborate with people when doing code reviews?}\label{rse-style-how}}

FIXME: how to review on GitHub.

\hypertarget{rse-style-order}{%
\section{In what order should functions be defined?}\label{rse-style-order}}

When encountering code for the first time,
most people scan it from top to bottom.
If that code is a program or script,
rather than a library,
its main function should be put first,
and should probably be called \texttt{main}.

After reading that function,
someone should have a good idea of what the program does in what order.
Three common patterns that people might match against are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Figure out what the user has asked it to do (Chapter~\ref{configure}).
\item
  Read all input data.
\item
  Process it.
\item
  Write output.
\end{enumerate}

or:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Figure out what the user has asked for.
\item
  For each input file:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Read.
  \item
    Process.
  \item
    Write file-specific output (if any).
  \end{enumerate}
\item
  Write summary output (if any).
\end{enumerate}

or:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Figure out what the user has asked for.
\item
  Repeatedly:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Wait for user input.
  \item
    Do what the user has asked.
  \end{enumerate}
\item
  Exit when a ``stop'' command of some sort is received.
\end{enumerate}

Each step in each of the outlines above usually becomes a function.
Those functions depend on others,
some of which are written to break code into comprehensible chunks and are then called just once,
others of which are utilities that may be called many times from many different places.

FIXME: figure

Different people order these differently;
our preferred order is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Put all of the single-use functions in the first half of the file
  in the order in which they are likely to be called.
\item
  Put all of the multi-use utility functions in the bottom of the file in alphabetical order.
\end{enumerate}

If any of those utility functions are used by other scripts or programs,
they should go in a file of their own.
In fact,
this is a good practice even if they're only used by one program,
since it signals even more clearly which functions are in the ``structural'' layer
and which are in the ``utility'' layer.

\hypertarget{rse-style-defaults}{%
\section{How can I specify default values for functions' parameters?}\label{rse-style-defaults}}

\href{glossary.html\#working-memory}{Working memory} can only hold a few items at once:
initial estimates in the 1950s put the number at 7 plus or minus 2 Miller (\protect\hyperlink{ref-Mill1956}{1956}),
and more recent estimates put it as low as 4 or 5.
If your function requires two dozen parameters,
the odds are very good that users will frequently forget them
or put them in the wrong order.
One solution is to give parameters default values (Chapter~\ref{style});
another is to bundle them together so that (for example)
people pass three \texttt{point} objects instead of nine separate \texttt{x}, \texttt{y}, and \texttt{z} values.

A third approach (which can be combined with the preceding two)
is to specify default values for some of the parameters.
Doing this gives users control over everything
while also allowing them to ignore details;
it also codifies what you consider ``normal'' for the function.

For example,
suppose we are comparing images to see if they are the same or different.
We can specify two kinds of tolerance:
how large a difference in color value to notice,
and how many differences above that threshold to tolerate
(as a percentage of the total number of pixels).
By default,
any color difference is considered significant,
and only 1\% of pixels are allowed to differ:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ image_diff(left, right, per_pixel}\OperatorTok{=}\DecValTok{0}\NormalTok{, fraction}\OperatorTok{=}\FloatTok{0.01}\NormalTok{):}
    \CommentTok{# ...implementation...}
\end{Highlighting}
\end{Shaded}

When this function is called using \texttt{image\_diff(old,\ new)},
those default values apply.
However,
it can also be called like this:

\begin{itemize}
\tightlist
\item
  \texttt{image\_diff(old,\ new,\ per\_pixel=2)}
  allows pixels to differ slightly without those differences being significant.
\item
  \texttt{image\_diff(old,\ new,\ fraction=0.05)} allows more pixels to differ.
\item
  \texttt{image\_diff(old,\ new,\ per\_pixel=1,\ fraction=0.005)}
  raises the per-pixel threshold but decrease number of allowed differences.
\end{itemize}

Default parameter values make code easier to understand and use,
but there is a subtle trap.
When Python executes a function definition like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ collect(new_value, accumulator}\OperatorTok{=}\BuiltInTok{set}\NormalTok{()):}
\NormalTok{    accumulator.add(new_value)}
    \ControlFlowTok{return}\NormalTok{ accumulator}
\end{Highlighting}
\end{Shaded}

it calls \texttt{set()} to create a new empty set,
and then uses that set as the default value for \texttt{accumulator} every time the function is called.
It does \emph{not} call \texttt{set()} anew for each call,
so all calls using the default will share the same set:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>>>}\NormalTok{ collect(}\StringTok{'first'}\NormalTok{)}
\NormalTok{\{}\StringTok{'first'}\NormalTok{\}}
\OperatorTok{>>>}\NormalTok{ collect(}\StringTok{'second'}\NormalTok{)}
\NormalTok{\{}\StringTok{'first'}\NormalTok{, }\StringTok{'second'}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

A common way to avoid this is to pass \texttt{None} to the function
to signal that the user didn't provide a value:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ collect(new_value, accumulator}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ accumulator }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{        accumulator }\OperatorTok{=} \BuiltInTok{set}\NormalTok{()}
\NormalTok{    accumulator.add(new_value)}
    \ControlFlowTok{return}\NormalTok{ accumulator}
\end{Highlighting}
\end{Shaded}

\hypertarget{rse-style-varargs}{%
\section{How can I write functions to handle a variable number of arguments?}\label{rse-style-varargs}}

We can often make programs simpler by writing functions that take a variable number of arguments,
just like \texttt{print} and \texttt{max}.
One way to to require user to stuff those arguments into a list,
e.g.,
to write \texttt{find\_limits({[}a,\ b,\ c,\ d{]})}.
However,
Python can do this for us.
If we declare a single argument whose name starts with a single \texttt{*},
Python will put all ``extra'' arguments into a \href{glossary.html\#tuple}{tuple}
and pass that as the argument.
By convention,
this argument is called \texttt{args}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ find_limits(}\OperatorTok{*}\NormalTok{args):}
    \BuiltInTok{print}\NormalTok{(args)}

\NormalTok{find_limits(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(1, 3, 5, 2, 4)
\end{verbatim}

This catch-all parameter can be used with regular parameters,
but must come last in the parameter list to avoid ambiguity:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ select_outside(low, high, }\OperatorTok{*}\NormalTok{values):}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ v }\KeywordTok{in}\NormalTok{ values:}
        \ControlFlowTok{if}\NormalTok{ (v }\OperatorTok{<}\NormalTok{ low) }\KeywordTok{or}\NormalTok{ (v }\OperatorTok{>}\NormalTok{ high):}
\NormalTok{            result.add(v)}
    \ControlFlowTok{return}\NormalTok{ result}

\BuiltInTok{print}\NormalTok{(select_outside(}\DecValTok{0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{-0.2}\NormalTok{, }\FloatTok{-0.5}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{1.7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[-0.2, -0.5, 1.7]
\end{verbatim}

An equivalent special form exists for named arguments:
the catch-all variable is conventionally called \texttt{kwargs}
(for ``keyword arguments'')
and its name is prefixed with \texttt{**} (i.e., two asterisks instead of one).
When this is used,
the function is given a \href{glossary.html\#dictionary}{dictionary} of names and values
rather than a list:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ set_options(tag, }\OperatorTok{**}\NormalTok{kwargs):}
\NormalTok{    result }\OperatorTok{=} \StringTok{'<}\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(tag)}
    \ControlFlowTok{for}\NormalTok{ key }\KeywordTok{in}\NormalTok{ kwargs:}
\NormalTok{        result }\OperatorTok{+=} \StringTok{' }\SpecialCharTok{\{\}}\StringTok{="}\SpecialCharTok{\{\}}\StringTok{"'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(key, kwargs[key])}
\NormalTok{    result }\OperatorTok{+=} \StringTok{'/>'}
    \ControlFlowTok{return}\NormalTok{ result}

\BuiltInTok{print}\NormalTok{(set_options(}\StringTok{'h1'}\NormalTok{, color}\OperatorTok{=}\StringTok{'blue'}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(set_options(}\StringTok{'p'}\NormalTok{, align}\OperatorTok{=}\StringTok{'center'}\NormalTok{, size}\OperatorTok{=}\StringTok{'150%'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<h1 color="blue"/>
<p align="center" size="150%"/>
\end{verbatim}

Notice that the names of parameters are not quoted:
the call is \texttt{color=\textquotesingle{}blue\textquotesingle{}} and not \texttt{\textquotesingle{}color\textquotesingle{}=\textquotesingle{}blue\textquotesingle{}}.

\hypertarget{rse-style-starargs}{%
\section{How can I pass an unknown number of parameters?}\label{rse-style-starargs}}

We can use the inverse of \texttt{*args} and \texttt{**kwargs} to match a list of values to arguments.
In this case,
we put the \texttt{*} in front of a list and \texttt{**} in front of a dictionary when \emph{calling} the function,
rather than in front of the parameter when \emph{defining} it:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ trim_value(data, low, high):}
    \BuiltInTok{print}\NormalTok{(data, }\StringTok{"with"}\NormalTok{, low, }\StringTok{"and"}\NormalTok{, high)}

\NormalTok{parameters }\OperatorTok{=}\NormalTok{ [}\StringTok{'some matrix'}\NormalTok{, }\StringTok{'lower bound'}\NormalTok{]}
\NormalTok{named_parameters }\OperatorTok{=}\NormalTok{ \{}\StringTok{'high'}\NormalTok{: }\StringTok{'upper bound'}\NormalTok{\}}
\NormalTok{trim_value(}\OperatorTok{*}\NormalTok{parameters, }\OperatorTok{**}\NormalTok{named_parameters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
some matrix with lower bound and upper bound
\end{verbatim}

\hypertarget{rse-style-destructure}{%
\section{How can I get values out of structured data more easily?}\label{rse-style-destructure}}

Modern programming languages have lots of other tools to make life more convenient for programmers.
One that's particularly useful is \href{glossary.html\#destructuring}{destructuring}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[first, [second, third]] }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, [}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]]}
\BuiltInTok{print}\NormalTok{(first)}
\BuiltInTok{print}\NormalTok{(second)}
\BuiltInTok{print}\NormalTok{(third)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1
2
3
\end{verbatim}

As this example shows,
if the variables on the left are arranged in the same way as the values on the right,
Python will automatically unpack the values and assign them correctly.
This is particularly useful when looping over lists of structured values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [[}\StringTok{'Kay'}\NormalTok{, }\StringTok{'McNulty'}\NormalTok{], }\StringTok{'mcnulty@eniac.org'}\NormalTok{],}
\NormalTok{    [[}\StringTok{'Betty'}\NormalTok{, }\StringTok{'Jennings'}\NormalTok{], }\StringTok{'jennings@eniac.org'}\NormalTok{],}
\NormalTok{    [[}\StringTok{'Marlyn'}\NormalTok{, }\StringTok{'Wescoff'}\NormalTok{], }\StringTok{'mwescoff@eniac.org'}\NormalTok{]}
\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ [[first, last], email] }\KeywordTok{in}\NormalTok{ people:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{\}}\StringTok{ }\SpecialCharTok{\{\}}\StringTok{ <}\SpecialCharTok{\{\}}\StringTok{>'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(first, last, email))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kay McNulty <mcnulty@eniac.org>
Betty Jennings <jennings@eniac.org>
Marlyn Wescoff <mwescoff@eniac.org>
\end{verbatim}

\hypertarget{rse-style-refactor}{%
\section{What should I look for when reviewing code?}\label{rse-style-refactor}}

The best way to answer the question in this section's title is
to describe some of the things people should change in their software.
\href{glossary.html\#refactoring}{Refactoring} means changing the structure of code without changing what it does,
like refactoring an equation to simplify it.
It is just as much a part of programming as writing code in the first place:
nobody gets things right the first time Brand (\protect\hyperlink{ref-Bran1995}{1995}),
and needs or insights can change over time.

Most discussions of refactoring focus on \href{glossary.html\#oop}{object-oriented programming},
but many patterns can and should be used to clean up \href{glossary.html\#procedural-programming}{procedural} code.
This lesson describes and motivates some of the most useful patterns;
These rules are examples of \href{glossary.html\#design-pattern}{design patterns}:
general solutions to commonly occurring problems in software design.
Knowing them and their names will help you create better software,
and also make it easier for you to communicate with your peers.

\hypertarget{do-not-repeat-values}{%
\subsection{Do not repeat values}\label{do-not-repeat-values}}

Our first and simplest refactoring is called ``replace value with name''.
It tells us to replace magic numbers with names,
i.e., to define constants.
This can seem ridiculous in simple cases
(why define and use \texttt{inches\_per\_foot} instead of just writing 12?).
However,
what may be obvious to you when you're writing code won't be obvious to the next person,
particularly if they're working in a different context
(most of the world uses the metric system and doesn't know how many inches are in a foot).
It's also a matter of habit:
if you write numbers without explanation in your code for simple cases,
you're more likely to do so for complex cases,
and more likely to regret it afterward.

Using names instead of raw values also makes it easier to understand code when you read it aloud,
which is always a good test of its style.
Finally,
a single value defined in one place is much easier to change
than a bunch of numbers scattered throughout your program.
You may not think you will have to change it,
but then people want to use your software on Mars and you discover that constants aren't \protect\hyperlink{BIB}{Mak2006}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\NormalTok{seconds_elapsed }\OperatorTok{=}\NormalTok{ num_days }\OperatorTok{*} \DecValTok{24} \OperatorTok{*} \DecValTok{60} \OperatorTok{*} \DecValTok{60}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\NormalTok{SECONDS_PER_DAY }\OperatorTok{=} \DecValTok{24} \OperatorTok{*} \DecValTok{60} \OperatorTok{*} \DecValTok{60}
\CommentTok{# ...other code...}
\NormalTok{seconds_elapsed }\OperatorTok{=}\NormalTok{ num_days }\OperatorTok{*}\NormalTok{ SECONDS_PER_DAY}
\end{Highlighting}
\end{Shaded}

\hypertarget{do-not-repeat-calculations-in-loops}{%
\subsection{Do not repeat calculations in loops}\label{do-not-repeat-calculations-in-loops}}

It's inefficient to calculate the same value over and over again.
It also makes code less readable:
if a calculation is inside a loop or a function,
readers will assume that it might change each time the code is executed.

Our second refactoring,
``hoist repeated calculation out of loop'',
tells us to move the repeated calculation out of the loop or function.
Doing this signals that its value is always the same.
And by naming that common value,
you help readers understand what its purpose is.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\ControlFlowTok{for}\NormalTok{ sample }\KeywordTok{in}\NormalTok{ signals:}
\NormalTok{    output.append(}\DecValTok{2} \OperatorTok{*}\NormalTok{ pi }\OperatorTok{*}\NormalTok{ sample }\OperatorTok{/}\NormalTok{ weight)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\NormalTok{scaling }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ pi }\OperatorTok{/}\NormalTok{ weight}
\ControlFlowTok{for}\NormalTok{ sample }\KeywordTok{in}\NormalTok{ signals:}
\NormalTok{    output.append(sample }\OperatorTok{*}\NormalTok{ scaling)}
\end{Highlighting}
\end{Shaded}

\hypertarget{replace-tests-with-flags-to-clarify-repeated-tests}{%
\subsection{Replace tests with flags to clarify repeated tests}\label{replace-tests-with-flags-to-clarify-repeated-tests}}

Novice programmers frequently write conditional tests like this:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (a }\OperatorTok{>}\NormalTok{ b) }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
    \CommentTok{# ...do something...}
\end{Highlighting}
\end{Shaded}

The comparison to \texttt{True} is unnecessary because \texttt{a\ \textgreater{}\ b} is a Boolean value
that is itself either \texttt{True} or \texttt{False}.
Like any other value,
Booleans can be assigned to variables,
and those variables can then be used directly in tests:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{was_greater }\OperatorTok{=}\NormalTok{ estimate }\OperatorTok{>} \FloatTok{0.0}
\CommentTok{# ...other code that might change estimate...}
\ControlFlowTok{if}\NormalTok{ was_greater:}
    \CommentTok{# ...do something...}
\end{Highlighting}
\end{Shaded}

This refactoring is called ``replace repeated test with flag''.
When it is used,
there is no need to write \texttt{if\ was\_greater\ ==\ True}:
that always produces the same result as \texttt{if\ was\_greater}.
Similarly, the equality tests in \texttt{if\ was\_greater\ ==\ False} is redundant:
the expression can simply be written \texttt{if\ not\ was\_greater}.
Creating and using a \href{glossary.html\#flag-variable}{flag} instead of repeating the test
is therefore like moving a calculation out of a loop:
even if that value is only used once,
it makes our intention clearer---these really are the same test.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\KeywordTok{def}\NormalTok{ process_data(data, scaling):}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(data) }\OperatorTok{>}\NormalTok{ THRESHOLD:}
\NormalTok{        scaling }\OperatorTok{=}\NormalTok{ sqrt(scaling)}
    \CommentTok{# ...process data to create score...}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(data) }\OperatorTok{>}\NormalTok{ THRESHOLD:}
\NormalTok{        score }\OperatorTok{=}\NormalTok{ score }\OperatorTok{**} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\KeywordTok{def}\NormalTok{ process_data(data, scaling):}
\NormalTok{    is_large_data }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(data) }\OperatorTok{>}\NormalTok{ THRESHOLD}
    \ControlFlowTok{if}\NormalTok{ is_large_data:}
\NormalTok{        scaling }\OperatorTok{=}\NormalTok{ sqrt(scaling)}
    \CommentTok{# ...process data to create score...}
    \ControlFlowTok{if}\NormalTok{ is_large_data:}
\NormalTok{        score }\OperatorTok{=}\NormalTok{ score }\OperatorTok{**} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

If it takes many lines of code to process data and create a score,
and the test then needs to change from \texttt{\textgreater{}} to \texttt{\textgreater{}=},
we're more likely to get the refactored version right the first time,
since the test only appears in one place and its result is given a name.

\hypertarget{use-in-place-operators-to-avoid-duplicating-expression}{%
\subsection{Use in-place operators to avoid duplicating expression}\label{use-in-place-operators-to-avoid-duplicating-expression}}

An \href{glossary.html\#in-place-operator}{in-place operator},
sometimes called an \href{glossary.html\#update-operator}{update operator},
does a calculation with two values
and overwrites one of the values.
For example,
instead of writing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{step }\OperatorTok{=}\NormalTok{ step }\OperatorTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

we can write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{step }\OperatorTok{+=} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

In-place operators save us some typing.
They also make the intention clearer,
and most importantly,
they make it harder to get complex assignments wrong.
For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples[least_factor_index, }\BuiltInTok{max}\NormalTok{(current_offset, offset_limit)] }\OperatorTok{*=}\NormalTok{ scaling_factor}
\end{Highlighting}
\end{Shaded}

is much easier to read than the equivalent expression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples[least_factor_index, }\BuiltInTok{max}\NormalTok{(current_offset, offset_limit)] }\OperatorTok{=} \OperatorTok{\textbackslash{}}
\NormalTok{    scaling_factor }\OperatorTok{*}\NormalTok{ samples[least_factor_index, }\BuiltInTok{max}\NormalTok{(current_limit, offset_limit)]}
\end{Highlighting}
\end{Shaded}

(The proof of this claim is that you probably didn't notice on first reading
that the long form uses different expressions to index \texttt{samples}
on the left and right of the assignment.)
The refactoring ``use in-place operator'' does what its name suggests:
converts normal assignments into their briefer equivalents.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\ControlFlowTok{for}\NormalTok{ least_factor }\KeywordTok{in}\NormalTok{ all_factors:}
\NormalTok{    samples[least_factor] }\OperatorTok{=} \OperatorTok{\textbackslash{}}
\NormalTok{        samples[least_factor] }\OperatorTok{*}\NormalTok{ bayesian_scaling}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\ControlFlowTok{for}\NormalTok{ least_factor }\KeywordTok{in}\NormalTok{ all_factors:}
\NormalTok{    samples[least_factor] }\OperatorTok{*=}\NormalTok{ bayesian_scaling}
\end{Highlighting}
\end{Shaded}

\hypertarget{handle-special-cases-first}{%
\subsection{Handle special cases first}\label{handle-special-cases-first}}

A \href{glossary.html\#short-circuit-test}{short circuit test} is a quick check to handle a special case,
such as checking the length of a list of values
and returning \texttt{math.nan} for the average if the list is empty.
``Place short circuits early'' tells us to put short-circuit tests near the start of functions
so that readers can mentally remove special cases from their thinking
while reading the code that handles the usual case.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\KeywordTok{def}\NormalTok{ rescale_by_average(values, factors, weights):}
\NormalTok{    a }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ (f, w) }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(factors, weights):}
\NormalTok{        a }\OperatorTok{+=}\NormalTok{ f }\OperatorTok{*}\NormalTok{ w}
    \ControlFlowTok{if}\NormalTok{ a }\OperatorTok{==} \FloatTok{0.0}\NormalTok{:}
        \ControlFlowTok{return}
\NormalTok{    a }\OperatorTok{/=} \BuiltInTok{len}\NormalTok{(f)}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ values:}
        \ControlFlowTok{return}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{for}\NormalTok{ (i, v) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(values):}
\NormalTok{            values[i] }\OperatorTok{=}\NormalTok{ v }\OperatorTok{/}\NormalTok{ a}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\KeywordTok{def}\NormalTok{ rescale_by_average(values, factors, weights):}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{not}\NormalTok{ values) }\KeywordTok{or}\NormalTok{ (}\KeywordTok{not}\NormalTok{ factors) }\KeywordTok{or}\NormalTok{ (}\KeywordTok{not}\NormalTok{ weights):}
        \ControlFlowTok{return}
\NormalTok{    a }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ (f, w) }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(factors, weights):}
\NormalTok{        a }\OperatorTok{+=}\NormalTok{ f }\OperatorTok{*}\NormalTok{ w}
\NormalTok{    a }\OperatorTok{/=} \BuiltInTok{len}\NormalTok{(f)}
    \ControlFlowTok{for}\NormalTok{ (i, v) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(values):}
\NormalTok{        values[i] }\OperatorTok{=}\NormalTok{ v }\OperatorTok{/}\NormalTok{ a}
\end{Highlighting}
\end{Shaded}

A related refactoring pattern is called ``default and override''.
To use it,
find cases where a value is set conditionally;
assign the default or most common value unconditionally,
and then override it in a special case.
The result is fewer lines of code and clearer control flow;
however,
it does mean executing two assignments instead of one,
so it shouldn't be used if the common case is expensive
(e.g., involves a database lookup or a web request).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before..}
\ControlFlowTok{if}\NormalTok{ configuration[}\StringTok{'threshold'}\NormalTok{] }\OperatorTok{>}\NormalTok{ UPPER_BOUND:}
\NormalTok{    scale }\OperatorTok{=} \FloatTok{0.8}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    scale }\OperatorTok{=} \FloatTok{1.0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\NormalTok{scale }\OperatorTok{=} \FloatTok{1.0}
\ControlFlowTok{if}\NormalTok{ configuration[}\StringTok{'threshold'}\NormalTok{] }\OperatorTok{>}\NormalTok{ UPPER_BOUND:}
\NormalTok{    scale }\OperatorTok{=} \FloatTok{0.8}
\end{Highlighting}
\end{Shaded}

In simple cases,
people will sometimes put the test and assignment on a single line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scale }\OperatorTok{=} \FloatTok{1.0}
\ControlFlowTok{if}\NormalTok{ configuration[}\StringTok{'threshold'}\NormalTok{] }\OperatorTok{>}\NormalTok{ UPPER_BOUND: scale }\OperatorTok{=} \FloatTok{0.8}
\end{Highlighting}
\end{Shaded}

Some programmers take this even further
and use a \href{glossary.html\#conditional-expression}{conditional expression}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scale }\OperatorTok{=} \FloatTok{0.8} \ControlFlowTok{if}\NormalTok{ configuration[}\StringTok{'threshold'}\NormalTok{] }\OperatorTok{>}\NormalTok{ UPPER_BOUND }\ControlFlowTok{else} \FloatTok{1.0}
\end{Highlighting}
\end{Shaded}

However,
this puts the default last instead of first,
which is less clear.

\hypertarget{use-functions-to-make-code-more-comprehensible}{%
\subsection{Use functions to make code more comprehensible}\label{use-functions-to-make-code-more-comprehensible}}

Functions were created so that programmers could write common operations and re-use them
in order to reduce the amount of code that needed to be compiled.
Moving complex operations into functions also reduces \href{glossary.html\#cognitive-load}{cognitive load}
by reducing the number of things that have to be understood simultaneously.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\KeywordTok{def}\NormalTok{ check_neighbors(grid, point):}
    \ControlFlowTok{if}\NormalTok{ (}\DecValTok{0} \OperatorTok{<}\NormalTok{ point.x) }\KeywordTok{and}\NormalTok{ (point.x }\OperatorTok{<}\NormalTok{ grid.width}\DecValTok{-1}\NormalTok{) }\KeywordTok{and} \OperatorTok{\textbackslash{}}
\NormalTok{       (}\DecValTok{0} \OperatorTok{<}\NormalTok{ point.y) }\KeywordTok{and}\NormalTok{ (point.y }\OperatorTok{<}\NormalTok{ grid.height}\DecValTok{-1}\NormalTok{):}
        \CommentTok{# ...look at all four neighbors}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after..}
\KeywordTok{def}\NormalTok{ check_neighbors(grid, point):}
    \ControlFlowTok{if}\NormalTok{ in_interior(grid, point):}
        \CommentTok{# ...look at all four neighbors...}

\KeywordTok{def}\NormalTok{ in_interior(grid, point):}
    \ControlFlowTok{return} \OperatorTok{\textbackslash{}}
\NormalTok{    (}\DecValTok{0} \OperatorTok{<}\NormalTok{ point.x) }\KeywordTok{and}\NormalTok{ (point.x }\OperatorTok{<}\NormalTok{ grid.width}\DecValTok{-1}\NormalTok{) }\KeywordTok{and} \OperatorTok{\textbackslash{}}
\NormalTok{    (}\DecValTok{0} \OperatorTok{<}\NormalTok{ point.y) }\KeywordTok{and}\NormalTok{ (point.y }\OperatorTok{<}\NormalTok{ grid.height}\DecValTok{-1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You should always extract functions when code can be used in other contests.
Even if it can't,
you should extract functions whenever it makes the function clearer
when it is read aloud.
Multi-part conditionals,
parts of long equations,
and the bodies of loops are good candidates for extraction;
if you can't think of a plausible name,
or if a lot of data has to be passed into the function after it's extracted,
the code should probably be left where it is.
Finally,
it's often helpful to keep using the original variable names as parameter names during refactoring
to reduce typing.

\hypertarget{combine-operations-in-functions}{%
\subsection{Combine operations in functions}\label{combine-operations-in-functions}}

``Combine functions'' is the opposite of ``extract function''.
If operations are always done together,
it can sometimes be be more efficient to do them together,
and \emph{might} be easier to understand.
However,
combining functions often reduces their reusability and readability;
one sign that functions shouldn't have been combined is
how often people use the combination and throw some results away.

The fragment below shows how two functions can be combined:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before...}
\KeywordTok{def}\NormalTok{ count_vowels(text):}
\NormalTok{    num }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{if}\NormalTok{ char }\KeywordTok{in}\NormalTok{ VOWELS:}
\NormalTok{            num }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ num}

\KeywordTok{def}\NormalTok{ count_consonants(text):}
\NormalTok{    num }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{if}\NormalTok{ char }\KeywordTok{in}\NormalTok{ CONSONANTS:}
\NormalTok{            num }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ num}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\KeywordTok{def}\NormalTok{ count_vowels_and_consonants(text):}
\NormalTok{    num_vowels }\OperatorTok{=} \DecValTok{0}
\NormalTok{    num_consonants }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{if}\NormalTok{ char }\KeywordTok{in}\NormalTok{ VOWELS:}
\NormalTok{            num_vowels }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{elif}\NormalTok{ char }\KeywordTok{in}\NormalTok{ CONSONANTS:}
\NormalTok{            num_consonants }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ num_vowels, num_consonants}
\end{Highlighting}
\end{Shaded}

One thing you may not notice about the combination is that
it assumes characters are either vowels or consonants,
which means it might work differently than separate calls to the two original functions.
Issues like this are why experienced developers write unit tests (Chapter~\ref{rse-correct})
\emph{before} starting to refactor.

\hypertarget{replace-code-with-data}{%
\subsection{Replace code with data}\label{replace-code-with-data}}

It is sometimes easier to understand and maintain lookup tables than complicated conditionals,
so the ``create lookup table'' refactoring tells us to turn the latter into the former:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...before..}
\KeywordTok{def}\NormalTok{ count_vowels_and_consonants(text):}
\NormalTok{    num_vowels }\OperatorTok{=} \DecValTok{0}
\NormalTok{    num_consonants }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ text:}
        \ControlFlowTok{if}\NormalTok{ char }\KeywordTok{in}\NormalTok{ VOWELS:}
\NormalTok{            num_vowels }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{elif}\NormalTok{ char }\KeywordTok{in}\NormalTok{ CONSONANTS:}
\NormalTok{            num_consonants }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ num_vowels, num_consonants}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ...after...}
\NormalTok{IS_VOWEL }\OperatorTok{=}\NormalTok{ \{}\StringTok{'a'}\NormalTok{ : }\DecValTok{1}\NormalTok{, }\StringTok{'b'}\NormalTok{ : }\DecValTok{0}\NormalTok{, }\StringTok{'c'}\NormalTok{ : }\DecValTok{0}\NormalTok{, ... \}}
\NormalTok{IS_CONSONANT }\OperatorTok{=}\NormalTok{ \{}\StringTok{'a'}\NormalTok{ : }\DecValTok{0}\NormalTok{, }\StringTok{'b'}\NormalTok{ : }\DecValTok{1}\NormalTok{, }\StringTok{'c'}\NormalTok{ : }\DecValTok{1}\NormalTok{, ... \}}

\KeywordTok{def}\NormalTok{ count_vowels_and_consonants(text):}
\NormalTok{    num_vowels }\OperatorTok{=}\NormalTok{ num_consonants }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ text:}
\NormalTok{        num_vowels }\OperatorTok{+=}\NormalTok{ IS_VOWEL[char]}
\NormalTok{        num_consonants }\OperatorTok{+=}\NormalTok{ IS_CONSONANT[char]}
    \ControlFlowTok{return}\NormalTok{ num_vowels, num_consonants}
\end{Highlighting}
\end{Shaded}

The more distinct cases there are,
the greater the advantage lookup tables have over multi-branch conditionals.
Those advantages multiply when items can belong to more than one category,
in which case the table is often best written as a dictionary with items as keys
and sets of categories as values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LETTERS }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{'A'}\NormalTok{ : \{}\StringTok{'vowel'}\NormalTok{, }\StringTok{'upper_case'}\NormalTok{\},}
    \StringTok{'B'}\NormalTok{ : \{}\StringTok{'consonant'}\NormalTok{, }\StringTok{'upper_case'}\NormalTok{\},}
    \CommentTok{# ...other upper-case letters...}
    \StringTok{'a'}\NormalTok{ : \{}\StringTok{'vowel'}\NormalTok{, }\StringTok{'lower_case'}\NormalTok{\},}
    \StringTok{'b'}\NormalTok{ : \{}\StringTok{'consonant'}\NormalTok{, }\StringTok{'lower_case'}\NormalTok{\},}
    \CommentTok{# ...other lower-case letters...}
    \StringTok{'+'}\NormalTok{ : \{}\StringTok{'punctuation'}\NormalTok{\},}
    \StringTok{'@'}\NormalTok{ : \{}\StringTok{'punctuation'}\NormalTok{\},}
    \CommentTok{# ...other punctuation...}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ count_vowels_and_consonants(text):}
\NormalTok{    num_vowels }\OperatorTok{=}\NormalTok{ num_consonants }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ char }\KeywordTok{in}\NormalTok{ text:}
\NormalTok{        num_vowels }\OperatorTok{+=} \BuiltInTok{int}\NormalTok{(}\StringTok{'vowel'} \KeywordTok{in}\NormalTok{ LETTERS[char])}
\NormalTok{        num_consonants }\OperatorTok{+=} \BuiltInTok{int}\NormalTok{(}\StringTok{'consonant'} \KeywordTok{in}\NormalTok{ LETTERS[char])}
    \ControlFlowTok{return}\NormalTok{ num_vowels, num_consonants}
\end{Highlighting}
\end{Shaded}

The expressions used to update \texttt{num\_vowels} and \texttt{num\_consonants} make use of the fact that
\texttt{in} produces either \texttt{True} or \texttt{False},
which the function \texttt{int} converts to either 1 or 0.
We will explore ways of making this code more readable in the exercises.

\hypertarget{rse-style-summary}{%
\section{Summary}\label{rse-style-summary}}

George Orwell laid out \href{https://en.wikipedia.org/wiki/Politics_and_the_English_Language\#Remedy_of_Six_Rules}{six rules for good writing},
the last and most important of which is,
``Break any of these rules sooner than say anything outright barbarous.''
PEP8 conveys the same message in the section \href{https://www.python.org/dev/peps/pep-0008/\#a-foolish-consistency-is-the-hobgoblin-of-little-minds}{Foolish consistency is the
hobgoblin of little minds}.
There will always be cases where your code will be easier to understand
if you \emph{don't} do the things described in this lesson,
but there are probably fewer of them than you think.

FIXME: create concept map for style and review.

\hypertarget{rse-style-exercises}{%
\section{Exercises}\label{rse-style-exercises}}

FIXME: create exercises for review.

\hypertarget{rse-style-keypoints}{%
\section{Key Points}\label{rse-style-keypoints}}

\begin{itemize}
\tightlist
\item
  The brain thinks every difference is significant, so removing unnecessary differences in formatting reduces cognitive load.
\item
  Python software should always conform to the formatting the rules in PEP 8.
\item
  Use \texttt{name=value} to define a default value for a function parameter.
\item
  Use \texttt{*args} to define a catch-all parameter for functions taking a variable number of unnamed arguments.
\item
  Use \texttt{**kwargs} to define a catch-all parameter for functions taking a variable number of named arguments.
\item
  Use destructuring to unpack data structures as needed.
\end{itemize}

\hypertarget{rse-automate}{%
\chapter{Automating Analyses}\label{rse-automate}}

It's easy to run one program to process a single data file,
but what happens when our analysis depends on many files,
or when we need to re-do the analysis every time new data arrives?
What should we do if the analysis has several steps
that we have to do in a particular order?

If we try to keep track of this ourselves,
we will inevitably forget some crucial steps,
and it will be hard for other people to pick up our work.
Instead,
we should use a \href{glossary.html\#build-tool}{build tool}
to keep track of what depends on what
and run our analysis programs automatically.
These tools were invented to help programmers rebuild complex software,
but can be used to automate any workflow.

As a running example,
we will look at the distribution of word frequencies in classic English novels.
Zipf's Law states that
the second most common word in a body of text appears half as often as the most common,
the third most common appears a third as often,
and so on.
To test it,
we will use these works from \href{https://www.gutenberg.org/}{Project Gutenberg}:

\begin{longtable}[]{@{}lr@{}}
\toprule
Book & Length (words)\tabularnewline
\midrule
\endhead
anne\_of\_green\_gables.txt & 105642\tabularnewline
common\_sense.txt & 24999\tabularnewline
count\_of\_monte\_cristo.txt & 464226\tabularnewline
dracula.txt & 164424\tabularnewline
emma.txt & 160458\tabularnewline
ethan\_frome.txt & 37732\tabularnewline
frankenstein.txt & 78098\tabularnewline
jane\_eyre.txt & 188455\tabularnewline
life\_of\_frederick\_douglass.txt & 43789\tabularnewline
moby\_dick.txt & 215830\tabularnewline
mysterious\_affair\_at\_styles.txt & 59604\tabularnewline
pride\_and\_prejudice.txt & 124974\tabularnewline
sense\_and\_sensibility.txt & 121590\tabularnewline
sherlock\_holmes.txt & 107533\tabularnewline
time\_machine.txt & 35524\tabularnewline
treasure\_island.txt & 71616\tabularnewline
\bottomrule
\end{longtable}

The most common words in this \href{glossary.html\#corpus}{corpus} appear this many times:

\begin{longtable}[]{@{}lr@{}}
\toprule
Word & Count\tabularnewline
\midrule
\endhead
the & 97278\tabularnewline
and & 59385\tabularnewline
to & 56028\tabularnewline
of & 55190\tabularnewline
I & 45680\tabularnewline
a & 40483\tabularnewline
in & 30030\tabularnewline
was & 24512\tabularnewline
that & 24386\tabularnewline
you & 22123\tabularnewline
it & 21420\tabularnewline
\bottomrule
\end{longtable}

The frequencies don't match Zipf's predictions exactly---for example,
we would expect about 48,600 occurrences of ``and''---but
there certainly seems to be a decay curve of some kind.

Our goals are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Analyze one input file to see how well it conforms to Zipf's Law.
\item
  Analyze multiple input files to see how well they conform in aggregate.
\item
  Plot individual and aggregate word frequency distributions and their expected values.
\end{enumerate}

Our starting point is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The books are text files in the \texttt{data/} directory.
\item
  \texttt{bin/countwords.py} reads a text file and creates a CSV file with two columns:
  a word and how many times the word occurs.
  We can analyze several files at once by \href{glossary.html\#pipe-unix}{piping} them into the program
  using something like \texttt{cat\ data/*.txt\ \textbar{}\ bin/countwords.py}.
  (If you don't know what this means,
  now would be a good time to review the material on the Unix shell
  in Chapters~\ref{rse-bash-basics} and~\ref{rse-bash-advanced}).
\item
  \texttt{bin/collate.py} takes one or more of these two-column CSV files as input
  and sums the counts for all occurrences of each word.
\item
  \texttt{bin/plotcounts.py} creates a plot that shows word rankings on the X axis
  and word counts on the Y axis.
\item
  \texttt{bin/testfit.py} compares actual distributions against theory
  and give a fitting score.
\end{enumerate}

We will use a program called \href{https://www.gnu.org/software/make/}{Make} to automate our analysis
so that every time we add a new book to our data,
we can create new plots and update our fits with a single command.
Make works as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Every time the \href{glossary.html\#operating-system}{operating system} creates, reads, or changes a file,
  it updates a \href{glossary.html\#timestamp}{timestamp} on the file to show when the operation took place.
  Make can compare these timestamps to figure out whether files are newer or older than one another.
\item
  A user can describe which files depend on each other
  by writing \href{glossary.html\#rule-make}{rules} in a \href{glossary.html\#makefile}{Makefile}.
  For example,
  one rule could say that \texttt{results/moby\_dick.csv} depends on \texttt{data/moby\_dick.txt},
  while another could say that the plot \texttt{results/comparison.png}
  depends on all of the CSV files in the \texttt{results} directory.
\item
  Each rule also tells Make how to update an out-of-date file.
  For example,
  the rule for \emph{Moby Dick} could tell Make to run \texttt{bin/countwords.py}
  if the result file is older than either the raw data file or the program.
\item
  When the user runs Make,
  the program checks all of the rules in the Makefile
  and runs the commands needed to update any that are out of date.
  If there are \href{glossary.html\#transitive-dependency}{transitive dependencies}---i.e.,
  if A depends on B and B depends on C---then Make will trace them through
  and run all of the commands it needs to in the right order.
\end{enumerate}

\begin{quote}
\textbf{Alternatives to Make}

The first version of Make was written in 1976.
Programmers have created many replacements for it in the decades since then---so many,
in fact,
that none have attracted enough users to displace it.
If you would like to explore them,
check out \href{https://snakemake.readthedocs.io/}{Snakemake} (for Python)
and \href{https://ropenscilabs.github.io/drake-manual/}{drake} (for R).
If you want to go deeper,
(Smith, \protect\hyperlink{ref-Smit2011}{2011}) describes the design and implementation of several build tools.
\end{quote}

\hypertarget{setting-up}{%
\subsection{Setting Up}\label{setting-up}}

This chapter uses a version of Make called \href{http://www.gnu.org/software/make/}{GNU Make}.
It comes with MacOS and Linux,
and you can install it on Windows using \href{https://chocolatey.org/}{Chocolatey}:

\begin{verbatim}
$ choco install make
\end{verbatim}

\hypertarget{acknowledgements}{%
\subsection{Acknowledgements}\label{acknowledgements}}

This chapter is based on the \href{https://github.com/swcarpentry/make-novice}{Software Carpentry lesson on Make}
maintained by \href{https://github.com/gcapes}{Gerard Capes}
and on \href{https://www.dursi.ca/}{Jonathan Dursi}'s
\href{https://github.com/ljdursi/make_pattern_rules}{introduction to pattern rules}.

\hypertarget{rse-automate-single-file}{%
\section{How can I update a single file using Make?}\label{rse-automate-single-file}}

To start,
let's create a file called \texttt{Makefile} in the root of our project:

\begin{verbatim}
# Regenerate results for "Moby Dick"
results/moby_dick.csv : data/moby_dick.txt
        python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv
\end{verbatim}

As in the shell and many other programming languages,
\texttt{\#} indicates that the first line is a comment.
The second and third lines form a \href{glossary.html\#rule-make}{rule}:
the \href{glossary.html\#target-make}{target} of the rule is \texttt{results/moby\_dick.csv},
its single \href{glossary.html\#prerequisite-make}{prerequisite} is the file \texttt{data/moby\_dick.txt},
and the two are separated by a single colon \texttt{:}.

The target and prerequisite tell Make what depends on what;
the indented line below them describes the \href{glossary.html\#action-make}{action} needed
to update the target if it is out of date.
The action can be one or more shell commands,
but each command \emph{must} be indented by a single tab character:
we can't use spaces or a mix of spaces and tabs.
In this rule,
the action is ``run \texttt{bin/countwords.py} on the raw data file
and put the output in a CSV file in the \texttt{results} directory''.

To test our rule, run this command in the shell:

\begin{verbatim}
$ make
\end{verbatim}

Make automatically looks for a file called \texttt{Makefile}
and follows the rules it contains.
In this case,
one of three things will happen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \texttt{results/moby\_dick.csv} doesn't exist,
  Make runs the action to create it.
\item
  If \texttt{data/moby\_dick.txt} is newer than \texttt{results/moby\_dick.csv},
  Make runs the action to update the results.
\item
  If \texttt{results/moby\_dick.csv} is newer than its prerequisite,
  Make does nothing.
\end{enumerate}

In the first two cases,
Make prints the commands it runs
along with anything those command prints to the screen
via \href{glossary.html\#stdout}{standard output} or \href{glossary.html\#stderr}{standard error}.
There is no screen output in this case,
so we only see the command.

\begin{quote}
\textbf{Indentation Errors}

If a \texttt{Makefile} indents a rule with spaces rather than tabs,
Make produces an error message like this:

\begin{verbatim}
Makefile:3: *** missing separator.  Stop.
\end{verbatim}
\end{quote}

No matter what happened the first time we ran \texttt{make},
if we run it again right away it does nothing
because our rule's target is now up to date.
It tells us this by displaying the message:

\begin{verbatim}
make: `results/moby_dick.csv' is up to date.
\end{verbatim}

We can check that it is telling the truth by listing the files with their timestamps,
ordered by how recently they have been updated:

\begin{verbatim}
$ ls -l -t data/moby_dick.txt results/moby_dick.csv
\end{verbatim}

\begin{verbatim}
-rw-r--r--  1 hamilton  staff   219107 31 Dec 08:58 results/moby_dick.csv
-rw-r--r--  1 hamilton  staff  1276201 31 Dec 08:58 data/moby_dick.txt
\end{verbatim}

As a further test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Delete \texttt{results/moby\_dick.csv} and run \texttt{make} again (case \#1);
  Make runs the action.
\item
  Use \texttt{touch\ data/moby\_dick.txt} to update the timestamp on the data file,
  then run \texttt{make} (case \#2).
  Again,
  Make runs the action.
\end{enumerate}

\hypertarget{rse-automate-multiple}{%
\section{How can I manage multiple files?}\label{rse-automate-multiple}}

Our Makefile isn't particularly helpful so far,
though it \emph{does} already document exactly how to reproduce one specific result.
Let's add another rule to it:

\begin{verbatim}
# Regenerate results for "Moby Dick"
results/moby_dick.csv : data/moby_dick.txt
        python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv

# Regenerate results for "Jane Eyre"
results/jane_eyre.csv : data/jane_eyre.txt
        python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
\end{verbatim}

When we run \texttt{make} it tells us:

\begin{verbatim}
make: `results/moby_dick.csv' is up to date.
\end{verbatim}

By default Make only attempts to update the first target it finds in the Makefile,
which is called the \href{glossary.html\#default-target-make}{default target}.
In this case,
the first target is \texttt{results/moby\_dick.csv},
which is already up to date.
To update something else,
we need to tell Make specifically what we want:

\begin{verbatim}
$ make results/jane_eyre.csv
\end{verbatim}

\begin{verbatim}
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
\end{verbatim}

\hypertarget{rse-automate-phony}{%
\section{How can I update several files at once?}\label{rse-automate-phony}}

If we have to run \texttt{make} once for each result
we're right back where we started.
However,
we can add a rule to our Makefile to update all of our results at once.
The key is to create a \href{glossary.html\#phony-target-make}{phony target}
that doesn't correspond to an actual file.
Let's add this line to the top of our Makefile:

\begin{verbatim}
all : results/moby_dick.csv results/jane_eyre.csv
\end{verbatim}

There is no file called \texttt{all},
and this rule doesn't have any actions of its own,
but when we run \texttt{make\ all},
it creates a list of the things \texttt{all} depends on,
then brings each of those prerequisites up to date (Figure~\ref{fig:automate-all}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:automate-all}Making Everything}
\end{figure}

As this diagram shows,
the order in which rules appear in the Makefile
does not necessarily determine the order in which actions are run.
Make is free to run commands in any order
so long as nothing is updated before its prerequisites are up to date.
This is called \href{glossary.html\#declarative-programming}{declarative programming}:
we declare what outcome we want
and the program figures out how to achieve it.

We can use phony targets to automate and document all of the tasks in our work.
For example,
let's add another target to our Makefile to delete all of the result files we have generated
so that we can start afresh.
By convention this target is called \texttt{clean},
and ours looks like this:

\begin{verbatim}
# Remove all generated files.
clean :
        rm -f results/*
\end{verbatim}

The \texttt{-f} flag to \texttt{rm} means ``force removal'':
when we use it,
\texttt{rm} won't complain if the files we have told it to remove are already gone.
If we now run:

\begin{verbatim}
$ make clean
\end{verbatim}

Make will delete any results files we have.
This is a lot safer than typing \texttt{rm\ -f\ results/*} at the command-line,
because if we mistakenly put a space after the \texttt{/}
and delete all of the files in the project's root directory,
we'll only make the mistake once.

Phony targets are very useful,
but there is a catch.
Try doing this:

\begin{verbatim}
$ mkdir clean
$ make clean
\end{verbatim}

\begin{verbatim}
make: `clean' is up to date.
\end{verbatim}

Since there is a directory called \texttt{clean},
Make thinks the target \texttt{clean} in the Makefile refers to this directory.
Since the rule has no prerequisites,
it can't be out of date,
so no actions are executed.

We can unconfuse Make by putting this line at the top of Makefile
to tell it explicitly which targets are phony:

\begin{verbatim}
.PHONY : all clean
\end{verbatim}

\hypertarget{rse-automate-depend-programs}{%
\section{How can I update files when programs change?}\label{rse-automate-depend-programs}}

Right now,
our Makefile says that each result file depends only on the corresponding data file.
But that's not true:
each result also depends on the program used to generate it,
and if we change our program,
we should regenerate our results.
To do that,
we can add the program to the prerequisites for each result:

\begin{verbatim}
# ...phony targets...

# Regenerate results for "Moby Dick"
results/moby_dick.csv : data/moby_dick.txt bin/countwords.py
        python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv

# Regenerate results for "Jane Eyre"
results/jane_eyre.csv : data/jane_eyre.txt bin/countwords.py
        python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
\end{verbatim}

\begin{verbatim}
$ touch bin/countwords.py
$ make all
\end{verbatim}

\begin{verbatim}
python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
\end{verbatim}

The exercises will explore how we can write a rule
to tell us whether our results will be different
after a change to a program
without actually updating them.
Rules like this can help us test our programs:
if we don't think an addition or modification ought to affect the results,
but it would,
we may have some debugging to do.

\hypertarget{rse-automate-variables}{%
\section{How can I reduce repetition in a Makefile?}\label{rse-automate-variables}}

Our Makefile now mentions \texttt{bin/countwords.py} four times.
If we ever change the name of the program or move it to a different location,
we will have to find and replace each of those occurrences.
More importantly,
this redundancy makes our Makefile harder to understand,
just as scattering \href{glossary.html\#magic-number}{magic numbers} through programs
makes them harder to understand.

The solution is the same one we use in programs:
define and use \href{glossary.html\#variable-make}{variables}.
Let's create names for the word-counting script and the command used to run it:

\begin{verbatim}
# ...phony targets...

COUNT=bin/countwords.py
RUN_COUNT=python $(COUNT)

# Regenerate results for "Moby Dick"
results/moby_dick.csv : data/moby_dick.txt $(COUNT)
        $(RUN_COUNT) data/moby_dick.txt > results/moby_dick.csv

# Regenerate results for "Jane Eyre"
results/jane_eyre.csv : data/jane_eyre.txt $(COUNT)
        $(RUN_COUNT) data/jane_eyre.txt > results/jane_eyre.csv
\end{verbatim}

Each definition takes the form \texttt{NAME=value}.
Variables are written in upper case by convention
so that they'll stand out from filenames
(which are usually in lower case),
but Make doesn't require this.
What \emph{is} required is using parentheses to refer to the variable,
i.e.,
to use \texttt{\$(NAME)} and not \texttt{\$NAME}.
For historical reasons,
Make interprets \texttt{\$NAME} to be a ``variable called \texttt{N} followed by the three characers `AME'\,'',
If no variable called \texttt{N} exists,
\texttt{\$NAME} becomes \texttt{AME},
which is almost certainly not what we want.

As in programs,
variables don't just cut down on typing.
They also signal to readers that several things are always and exactly the same,
which reduces \href{glossary.html\#cognitive-load}{cognitive load}.

\hypertarget{rse-automate-autovar}{%
\section{How can I take specific filenames out of my rules?}\label{rse-automate-autovar}}

We could add a third rule to analyze a third novel and a fourth to analyze a fourth,
but that clearly wouldn't scale to analyzing hundreds or thousands of files.
Instead,
we should write a generic rule that will produce a CSV file for any book.
To do this,
we need to understand Make's \href{glossary.html\#automatic-variable-make}{automatic variables}.
The first step is to use the very cryptic expression \texttt{\$@} in the rule's action
to mean ``the target of the rule''.
With it,
we can replace this:

\begin{verbatim}
# Regenerate results for "Moby Dick"
results/moby_dick.csv : data/moby_dick.txt $(COUNT)
        $(RUN_COUNT) data/moby_dick.txt > results/moby_dick.csv
\end{verbatim}

with this:

\begin{verbatim}
# Regenerate results for "Moby Dick"
results/moby_dick.csv : data/moby_dick.txt $(COUNT)
        $(RUN_COUNT) data/moby_dick.txt > $@
\end{verbatim}

Make defines a value of \texttt{\$@} separately for each rule,
so it always refers to that rule's target.
And yes,
\texttt{\$@} is an unfortunate name:
something like \texttt{\$TARGET} would have been easier to understand,
but we're stuck with it now.

The next step is to replace the explicit list of prerequisites in the action
with the automatic variable \texttt{\$\^{}}:

\begin{verbatim}
# Regenerate results for "Jane Eyre"
results/moby_dick.csv : data/moby_dick.txt $(COUNT)
        $(RUN_COUNT) $^ > $@
\end{verbatim}

However,
this doesn't work.
The rule's prerequisites are the novel and the word-counting program.
When Make expands the action,
the resulting command tries to process the program as if it were a data file:

\begin{verbatim}
python bin/countwords.py data/moby_dick.txt bin/countwords.py > results/moby_dick.csv
\end{verbatim}

Make solves this problem with another automatic variable \texttt{\$\textless{}},
which mean ``the first prerequisite''.
Using it lets us rewrite our rule as:

\begin{verbatim}
# Regenerate results for "Jane Eyre"
results/moby_dick.csv : data/moby_dick.txt $(COUNT)
        $(RUN_COUNT) $< > $@
\end{verbatim}

\texttt{\$\textless{}\ \textgreater{}\ \$@} is hard to read, even with practice.
Using an editor with \href{glossary.html\#syntax-highlighting}{syntax highlighting} (Chapter~\ref{tools}) only helps a little,
so please don't ever create something this cryptic yourself.

\hypertarget{rse-automate-pattern}{%
\section{How can I write a generic rule to update many files?}\label{rse-automate-pattern}}

We can now replace all the rules for generating results files with one \href{glossary.html\#pattern-rule-make}{pattern rule}
using the \href{glossary.html\#wildcard}{wildcard} \texttt{\%},
which matches zero or more characters in a filename.
Whatever matches \texttt{\%} in the target also matches in the prerequisites,
so the rule:

\begin{verbatim}
results/%.csv : data/%.txt $(COUNT)
        $(RUN_COUNT) $< > $@
\end{verbatim}

will handle \emph{Jane Eyre}, \emph{Moby Dick}, \emph{The Time Machine}, and every other novel in the \texttt{data} directory.
(Unfortunately,
\texttt{\%} cannot be used in rules' actions,
which is why \texttt{\$\textless{}} and \texttt{\$@} are needed.)
With this rule in place, our entire Makefile is reduced to:

\begin{verbatim}
.PHONY: all clean

COUNT=bin/countwords.py
RUN_COUNT=python $(COUNT)

# Regenerate all results.
all : results/moby_dick.csv results/jane_eyre.csv results/time_machine.csv

# Regenerate result for any book.
results/%.csv : data/%.txt $(COUNT)
        $(RUN_COUNT) $< > $@

# Remove all generated files.
clean :
        rm -f results/*
\end{verbatim}

To test our shortened Makefile,
let's delete all of the results files:

\begin{verbatim}
$ make clean
\end{verbatim}

\begin{verbatim}
rm -f results/*
\end{verbatim}

and then recreate them:

\begin{verbatim}
$ make all
\end{verbatim}

\begin{verbatim}
python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
python bin/countwords.py data/time_machine.txt > results/time_machine.csv
\end{verbatim}

We can still rebuild individual files if we want,
since Make will take the target filename we give on the command line
and see if a pattern rule matches it:

\begin{verbatim}
$ touch data/jane_eyre.txt
$ make results/jane_eyre.csv
\end{verbatim}

\begin{verbatim}
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
\end{verbatim}

\hypertarget{rse-automate-functions}{%
\section{How can I automatically define a set of files?}\label{rse-automate-functions}}

Our analysis is still not fully automated:
if we add another book to \texttt{data},
we have to remember to add its name to the \texttt{all} target in the Makefile as well.
Once again we will fix this in steps.

To start,
imagine that all the results files already exist
and we just want to update them.
We can define a variable called \texttt{RESULTS}
to be a list of all the results files
using the same wildcards we would use in the shell:

\begin{verbatim}
RESULTS=results/*.csv
\end{verbatim}

We can then rewrite \texttt{all} to depend on that list:

\begin{verbatim}
# Regenerate all results.
all : $(RESULTS)
\end{verbatim}

However,
this only works if the results files already exist:
if one doesn't,
its name won't be included in \texttt{RESULTS}
and Make won't realize that we want to generate it.

What we really want is generate the list of results files
from the list of books in the \texttt{data/} directory.
We can use a \href{glossary.html\#function-make}{function} to do this.
The syntax is odd because functions were added to Make long after it was first written,
but at least they have readable names.
Let's create a variable \texttt{DATA} that holds the names of all of our data files:

\begin{verbatim}
DATA=$(wildcard data/*.txt)
\end{verbatim}

This calls the function \texttt{wildcard} with the argument \texttt{data/*.txt}.
The result is a list of all the text files in the \texttt{data} directory,
just as we would get with \texttt{data/*.txt} in the shell.

To check that this did the right thing,
we can add another phony target called \texttt{settings}
that uses the shell command \texttt{echo} to print the names and values of our variables:

\begin{verbatim}
.PHONY: all clean settings

# ...everything else...

# Show variables' values.
settings :
        echo COUNT: $(COUNT)
        echo DATA: $(DATA)
\end{verbatim}

Let's run this:

\begin{verbatim}
$ make settings
\end{verbatim}

\begin{verbatim}
echo COUNT: bin/countwords.py
COUNT: bin/countwords.py
echo DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt
DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt
\end{verbatim}

The output appears twice
because Make shows us the command it's going to run before running it.
If we put \texttt{@} before the command,
Make doesn't display it,
which makes the output easier to read:

\begin{verbatim}
settings :
    @echo COUNT: $(COUNT)
    @echo DATA: $(DATA)
\end{verbatim}

\begin{verbatim}
$ make settings
\end{verbatim}

\begin{verbatim}
COUNT: bin/countwords.py
DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt
\end{verbatim}

We now have the names of our input files,
but what we need is the names of the corresponding output files.
Make's \texttt{patsubst} function
(short for pattern substitution)
does exactly this:

\begin{verbatim}
RESULTS=$(patsubst data/%.txt,results/%.csv,$(DATA))
\end{verbatim}

The first argument to \texttt{patsubst} is the pattern to look,
which in this case is a text file in the \texttt{data} directory.
We use \texttt{\%} to match the \href{glossary.html\#filename-stem}{stem} of the file's name,
which is the part we want to keep.

The second argument is the replacement we want.
As in a pattern rule,
Make replaces \texttt{\%} in this argument with whatever matched \texttt{\%} in the pattern,
which creates the name of the result file we want.
Finally,
the third argument is what to do the substitution in,
which is our list of books' names.

Let's check our worke by adding another command to the \texttt{settings} target:

\begin{verbatim}
settings :
        @echo COUNT: $(COUNT)
        @echo DATA: $(DATA)
        @echo RESULTS: $(RESULTS)
\end{verbatim}

\begin{verbatim}
$ make settings
\end{verbatim}

\begin{verbatim}
COUNT: bin/countwords.py
DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt
RESULTS: results/common_sense.csv results/jane_eyre.csv results/life_of_frederick_douglass.csv results/moby_dick.csv results/sense_and_sensibility.csv results/time_machine.csv
\end{verbatim}

Excellent:
\texttt{DATA} has the names of the files we want to process
and \texttt{RESULTS} automatically has the names of the corresponding result files.
Let's recreate all of the latter:

\begin{verbatim}
$ make clean
\end{verbatim}

\begin{verbatim}
rm -f results/*.csv
\end{verbatim}

\begin{verbatim}
$ make all
\end{verbatim}

\begin{verbatim}
python bin/countwords.py data/common_sense.txt > results/common_sense.csv
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
python bin/countwords.py data/life_of_frederick_douglass.txt > results/life_of_frederick_douglass.csv
python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv
python bin/countwords.py data/sense_and_sensibility.txt > results/sense_and_sensibility.csv
python bin/countwords.py data/time_machine.txt > results/time_machine.csv
\end{verbatim}

Our workflow is now just two steps:
add a data file and run Make.
This is a big improvement over running things manually,
particularly as we start to add more steps like merging data files and generating plots.

\hypertarget{rse-automate-doc}{%
\section{How can I document my Makefile?}\label{rse-automate-doc}}

Every well-behaved program tells people how to use it (Taschuk and Wilson, \protect\hyperlink{ref-Tasc2017}{2017}).
If we run \texttt{make\ -\/-help},
for example,
we get a long list of options that Make understands.

How can we document the workflow embodied in a specific Makefile?
We could create another phony target called \texttt{help} that prints a list of available commands:

\begin{verbatim}
.PHONY: all clean help settings

# ...other definitions...

# Show help.
help :
        @echo "all : regenerate all out-of-date results files."
        @echo "results/*.csv : regenerate a particular results file."
        @echo "clean : remove all generated files."
        @echo "settings : show the values of all variables."
        @echo "help : show this message."
\end{verbatim}

Sooner or later,
though,
we will add a target or rule and forget to update this list.
A better approach is to format some comments in a special way
and then extract and display those comments when asked to.
We'll use \texttt{\#\#} (a double comment marker) to indicate the lines we want displayed
and \texttt{grep} (Section~\ref{rse-bash-advanced-find}) to pull these lines out of the file:

\begin{verbatim}
.PHONY: all clean help settings

COUNT=bin/countwords.py
RUN_COUNT=python $(COUNT)
DATA=$(wildcard data/*.txt)
RESULTS=$(patsubst data/%.txt,results/%.csv,$(DATA))

## all : regenerate all results.
all : $(RESULTS)

## results/%.csv : regenerate result for any book.
results/%.csv : data/%.txt $(COUNT)
        $(RUN_COUNT) $< > $@

## clean : remove all generated files.
clean :
        rm -f results/*.csv

## settings : show variables' values.
settings :
        @echo COUNT: $(COUNT)
        @echo DATA: $(DATA)
        @echo RESULTS: $(RESULTS)

## help : show this message.
help :
        @grep '^##' ./Makefile
\end{verbatim}

Let's test:

\begin{verbatim}
$ make help
\end{verbatim}

\begin{verbatim}
## all : regenerate all results.
## results/%.csv : regenerate result for any book.
## clean : remove all generated files.
## settings : show variables' values.
## help : show this message.
\end{verbatim}

With a bit more work we ccan remove the \texttt{\#\#} markers,
but this is a good start.

\hypertarget{rse-automate-pipeline}{%
\section{How can I create entire analysis pipelines?}\label{rse-automate-pipeline}}

To finish our example,
we will automatically generate a collated list of word frequencies.
The target is a file called \texttt{results/collated.csv}
that depends on the results generated by \texttt{countwords.py} (Figure~\ref{fig:automate-pipelines}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:automate-pipelines}Creating Pipelines}
\end{figure}

To create it,
we add or change these lines in our Makefile:

\begin{verbatim}
# ...phony targets and previous variable definitions...

COLLATE=bin/collate.py
RUN_COLLATE=python $(COLLATE)

## all : regenerate all results.
all : results/collated.csv

## results/collated.csv : collate all results.
results/collated.csv : $(RESULTS) $(COLLATE)
    $(RUN_COLLATE) $(RESULTS) > $@
\end{verbatim}

The first two lines tell Make about the collation program,
while the change to \texttt{all} tells it what the final target of our pipeline is.
Since this target depends on the single-novel results files,
\texttt{make\ all} will regenerate all of those automatically.

The rule to regenerate \texttt{results/collated.csv} should look familiar by now:
it tells Make that all of the individual results have to be up-to-date
and that the final result should be regenerated if the program used to create it has changed.
One difference between the action in this rule and the actions we've seen before
is that this action uses \texttt{\$(RESULTS)} directly instead of an automatic variable.
We have written the rule this way because
there isn't an automatic variable that means ``all but the last prerequisite'',
so there's no way to use automatic variables that wouldn't result in us trying to process our program.

\hypertarget{rse-automate-summary}{%
\section{Summary}\label{rse-automate-summary}}

Make's reliance on shell commands instead of direct calls to functions in Python or R
sometimes makes it clumsy to use.
However,
that also makes it very flexible:
a single Makefile can run shell commands and programs written in a variety of languages,
which makes it a great way to assemble pipelines out of whatever is lying around.

\begin{figure}
\centering
\includegraphics{figures/rse-automate/concept.pdf}
\caption{\label{fig:automate-concept}Automation Concept Map}
\end{figure}

\begin{itemize}
\tightlist
\item
  A build tool re-runs commands so all files and their dependencies are up-to-date with each other.
\item
  Make is a widely-used build tool that uses files' timestamps to find out-of-date prerequisites.
\item
  A Make rule has targets, prerequisites, and actions.
\item
  A target can correspond to a file or be a phony target (used simply to trigger actions).
\item
  When a target is out of date with respect to its prerequisites, Make executes the actions associated with its rule.
\item
  Make executes as many rules as it needs to when updating files, but always respect prerequisite order.
\item
  Make defines the automatic variables \texttt{\$@} (target), \texttt{\$\^{}} (all prerequisites), and \texttt{\$\textless{}} (first prerequisite).
\item
  Pattern rules can use \texttt{\%} as a placeholder for parts of filenames.
\item
  Makefiles can define variables using \texttt{NAME=value}.
\item
  Makefiles can also use functions such as \texttt{\$(wildcard\ ...)} and \texttt{\$(patsubst\ ...)}.
\item
  Specially-formatted comments can be used to make Makefiles self-documenting.
\end{itemize}

\hypertarget{rse-automate-exercises}{%
\section{Exercises}\label{rse-automate-exercises}}

\hypertarget{rse-automate-ex-create-summary-results}{%
\subsection{Create a summary results file}\label{rse-automate-ex-create-summary-results}}

\begin{itemize}
\tightlist
\item
  Add a rule to Makefile to create a summary CSV file from all of the book CSV files.
\item
  Be careful about writing the prerequisites so that it doesn't depend on itself.
\end{itemize}

\hypertarget{rse-automate-ex-plot-top-n}{%
\subsection{Generate a plot for the top N words}\label{rse-automate-ex-plot-top-n}}

\begin{itemize}
\tightlist
\item
  Make it depend on the summary.
\end{itemize}

\hypertarget{rse-automate-ex-mkdir}{%
\subsection{Make sure the output directory exists}\label{rse-automate-ex-mkdir}}

\begin{itemize}
\tightlist
\item
  Why is \texttt{mkdir\ -p} useful?
\end{itemize}

\hypertarget{rse-automate-ex-report-change}{%
\subsection{Report results that would change}\label{rse-automate-ex-report-change}}

\begin{itemize}
\tightlist
\item
  Write a rule to report which result files would actually change.
\item
  Hint: use \texttt{diff}.
\end{itemize}

\hypertarget{rse-automate-ex-readable-help}{%
\subsection{Create more readable help}\label{rse-automate-ex-readable-help}}

\begin{itemize}
\tightlist
\item
  Modify the command in the \texttt{help} action to remove the leading `\#\#' markers from the output.
\end{itemize}

\hypertarget{rse-automate-ex-wildcard-perils}{%
\subsection{The perils of shell wildcards}\label{rse-automate-ex-wildcard-perils}}

What is wrong with writing the rule for \texttt{results/collated.csv} like this:

\begin{verbatim}
results/collated.csv : results/*.csv
    $(RUN_COLLATE) $^ > $@
\end{verbatim}

Hint: the fact that the result no longer depends on the program used to create it isn't the only problem.

\hypertarget{rse-teams}{%
\chapter{Working in Teams}\label{rse-teams}}

The previous lesson talked about the physical organization of projects.
This one talks about the social structure,
which is more important to the project's success.
A project can survive badly-organized code;
none will survive for long if people are confused,
pulling in different directions,
or hostile.
This lesson therefore talks about what projects can do to make newcomers feel welcome
and to make things run smoothly after that.
It draws on Fogel (\protect\hyperlink{ref-Foge2005}{2005}),
which describes how good open source software projects are run,
and on Bollier (\protect\hyperlink{ref-Boll2014}{2014}),
which explains what a \href{glossary.html\#commons}{commons} is and when it's the right model to use.

\hypertarget{rse-teams-software-license}{%
\section{How should we license our software?}\label{rse-teams-software-license}}

It might seem strange to open a discussion of teams with a discussion of licensing,
but if the law or a publication agreement prevents people from reading our work or using our software,
they can't contribute.
We may occasionally need to keep things closed to respect personal or commercial confidentiality,
but the first and most important rule is to be open by default.

However,
that is easier said than done,
not least because the law hasn't kept up with everyday practice.
Morin et al. (\protect\hyperlink{ref-Mori2012}{2012}) and \href{http://www.astrobetter.com/blog/2014/03/10/the-whys-and-hows-of-licensing-scientific-code/}{this blog post} are good starting points
to understand licensing and intellectual property from a researcher's point of view,
while Lindberg (\protect\hyperlink{ref-Lind2008}{2008}) is a deeper dive for those who want details.
In brief,
creative works are automatically eligible for intellectual property (and thus copyright) protection.
This means that every creative work has some sort of license:
the only question is whether authors and users know what it is.

Every project should therefore include an explicit license.
This license should be chosen early:
if we don't set it up right at the start,
then each collaborator will hold copyright on their work
and will need to be asked for approval when a license \emph{is} chosen.
As explained in Section~\ref{rse-project-boilerplate},
the license is usually put in a file called \texttt{LICENSE} in the project's root directory.
This file should clearly state the license(s) under which the content is being made available;
the plural is used because code, data, and text may be covered by different licenses.

\begin{quote}
\textbf{Leave It To The Professionals}

Don't write your own license.
Legalese is a highly technical language,
and words don't mean what you think they do.
\end{quote}

To make license selection as easy as possible,
GitHub allows us to select one of the most common licenses when creating a repository.
The Open Source Initiative maintains \href{http://opensource.org/licenses}{a list of licenses},
and \href{http://choosealicense.com/}{choosealicense.com} will help us find a license that suits our needs.
Some of the things we need to think about are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Do we want to license the code at all?
\item
  Is the content we are licensing source code?
\item
  Do we require people distributing derivative works to also distribute their code?
\item
  Do we want to address patent rights?
\item
  Is our license compatible with the licenses of the software we depend on?
\end{enumerate}

To answer the last of these questions for software,
we need to understand the difference between two kinds of open licenses.
The two most popular are
the \href{glossary.html\#mit-license}{MIT License} and the \href{glossary.html\#gpl}{GNU Public License} (GPL).
The MIT License (and its close sibling the BSD License)
say that people can do whatever they want to with the software as long as they cite the original source,
and that the authors accept no responsibility if things go wrong.
The GPL gives people similar rights,
but requires them to share their own work on the same terms:

\begin{quote}
You may copy, distribute and modify the software as long as you track changes/dates in source files.
Any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL
along with build \& install instructions.

--- \href{https://tldrlegal.com/license/gnu-general-public-license-v3-(gpl-3)}{tl;dr}
\end{quote}

We recommend the MIT license:
it places the fewest restrictions on future action,
it can be made stricter later on,
and the last thirty years shows that it's good enough to keep work open.

\hypertarget{rse-teams-other-license}{%
\section{How should we license our data and reports?}\label{rse-teams-other-license}}

The MIT license and the GPL apply to software.
When it comes to data and reports,
the most widely used family of licenses are those produced by \href{https://creativecommons.org/}{Creative Commons},
which have been written and checked by lawyers and are well understood by the community.

The most liberal license is referred to as \href{glossary.html\#cc-0}{CC-0},
where the ``0'' stands for ``zero restrictions''.
CC-0 puts work in the public domain,
i.e.,
allows anyone who wants to use it to do so however they want with no restrictions.
This is usually the best choice for data,
since it simplifies aggregate analysis.
For example,
if we choose a license for data that requires people to cite their source,
then anyone who uses that data in an analysis must cite us;
so must anyone who cites \emph{their} results,
and so on,
which quickly becomes unwieldy.

The next most common license is the Creative Commons---Attribution license,
usually referred to as \href{glossary.html\#cc-by}{CC-BY}.
This allows people to do whatever they want to with the work
as long as they cite the original source.
This is the best license to use for manuscripts,
since we \emph{want} people to share them widely
but also want to get credit for our work.

Other Creative Commons licenses incorporate various restrictions on specific use cases:

\begin{itemize}
\tightlist
\item
  ND (no derivative works) prevents people from creating modified versions of our work.
  Unfortunately, this also inhibits translation and reformatting.
\item
  SA (share-alike) requires people to share work that incorporates ours
  on the same terms that we used.
  Again,
  it is fine in principle but in practice makes aggregation a headache.
\item
  Finally,
  NC (no commercial use) does \emph{not} mean that people cannot charge money for something that includes our work,
  though some publishers still try to imply that in order to scare people away from open licensing.
  Instead,
  the NC clause means that people cannot charge for something that uses our work without our explicit permission,
  which we can give under whatever terms we want.
\end{itemize}

\hypertarget{rse-teams-issues}{%
\section{How can we keep track of the work we still have to do?}\label{rse-teams-issues}}

Version control tells us where we've been;
\href{glossary.html\#issue}{issues} tells us where we're going.
An \href{glossary.html\#issue-tracking-system}{issue tracking system} is just a glorified to-do list for a project.
Issues are sometimes called \href{glossary.html\#ticket}{tickets},
so issue tracking systems are sometimes called \href{glossary.html\#ticketing-system}{ticketing systems}.
They are also often called \href{glossary.html\#bug-tracker}{bug trackers}
because they were created to keep track of bugs that need fixing.
However,
they can be used to manage any kind of work
and are often a convenient way to manage discussions as well.

Like other \href{glossary.html\#forge}{forges},
GitHub allows members of a project to create issues for a project,
modify existing ones,
and search both.
Every issue has:

\begin{itemize}
\item
  A unique ID, such as \texttt{\#123}, which is also part of its link.
  This makes issues easy to find and refer to:
  in particular, GitHub automatically turns the expression \texttt{\#123} in a \href{glossary.html\#commit-message}{commit message}
  into a link to that issue.
\item
  A one-line title to aid browsing and search.
\item
  The issue's current status.
  In simple systems (like GitHub's) each issue is either open or closed,
  and by default,
  only open issues are displayed.
\item
  The user ID of the issue's creator, which is written as \texttt{@name}.
  The IDs of people who have commented on it or modified it are also embedded in the issue's history,
  which helps when using issues to manage discussions.
\item
  A full description that may include screenshots,
  error messages,
  and anything else that can be put in a web page.
\item
  Replies, counter-replies, and so on from people who are interested in the issue.
\end{itemize}

Broadly speaking,
people create three kinds of issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \href{glossary.html\#bug-report}{Bug reports} to describe problems they have encountered.
  A good bug report is a work of art,
  so we discuss them in detail in the next section.
\item
  \href{glossary.html\#feature-request}{Feature requests},
  such as ``add this function to this package'' or ``add a menu to the website''.
  A feature is what do we want to do next rather than something that needs to be fixed.
\item
  \emph{Questions}.
  Many projects encourage people to ask questions on a mailing list or in a chat channel,
  but answers given there can be hard to find later,
  which leads to the same questions coming up over and over again.
  If people can be persuaded to ask questions by filing issues,
  and to respond to issues of this kind,
  then the project's old issues become a customized \href{https://stackoverflow.com/}{Stack Overflow} for the project.
  (Many projects eventually create a page of links to old questions and answers that are particularly helpful.)
\end{enumerate}

\hypertarget{rse-teams-bugs}{%
\section{How can we write good bug reports?}\label{rse-teams-bugs}}

The better the bug report,
the faster the response,
and the more likely the response will actually address the issue \protect\hyperlink{BIB}{Bett2008}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Make sure the problem actually \emph{is} a bug.
  It's always possible that we have called a function the wrong way
  or done an analysis using the wrong configuration file;
  if we take a minute to double-check,
  we could well fix the problem ourselves.
\item
  Try to come up with a \href{glossary.html\#reprex}{reproducible example} or ``reprex''.
  A reprex includes only the steps or lines of code needed to make the problem happen;
  again,
  it's surprising how often we can solve the problem ourselves
  as we trim down the steps to create one.
\item
  Write a one-line title for the issue
  and a longer (but still brief) description that includes relevant details).
\item
  Attach any screenshots that show the problem or (slimmed-down) input files needed to re-create it.
\item
  Describe the version of the software we were using,
  the operating system we were running on,
  and anything else that might affect behavior.
\item
  Describe each problem separately so that each one can be tackled on its own.
  (This parallels the rule about creating a branch in version control for each bug fix or feature
  discussed in Chapter~\ref{rse-git-advanced}.
\end{enumerate}

Here is an example of a well-written bug report with all of the fields mentioned above:

\begin{verbatim}
ID: 1278
Creator: standage
Owner: malvika
Labels: Bug, Assigned
Summary: wordbase.py fails on accented characters
Description:

1.  Create a text file called 'accent.txt' containing the word "Pumpernickel"
    with an umlaut over the 'u'.

2.  Run 'python wordbase.py --all --message accent.txt'

Program should print "Pumpernickel" on a line by itself with the umlaut,
but fails instead with:

    No encoding for [] on line 1 of 'accent.txt'.

(`[]` shows where a solid black box appears in the output instead of a printable character.)

Versions:
-   `python wordbase.py --version` reports 0.13.1
-   Using on Windows 10.
\end{verbatim}

\hypertarget{rse-teams-labels}{%
\section{How can we use labels to organize work?}\label{rse-teams-labels}}

There is always more work to do than there is time to do it.
Issue trackers let project members add \href{glossary.html\#issue-label}{labels} to issue to manage this.
Labels are also often called \href{glossary.html\#tag}{tags};
whatever term is used,
they are just a word or two to help users sort and search.

GitHub allows project owners to create any labels they want
that users can then add to their issues.
A small project should always use these four labels:

\begin{itemize}
\item
  \emph{Bug}: something should work but doesn't.
\item
  \emph{Enhancement}: something that someone wants added.
\item
  \emph{Task}: something needs to be done, but won't show up in code
  (e.g., organizing the next team meeting).
\end{itemize}

Projects also often use:

\begin{itemize}
\item
  \emph{Question}: how is something supposed to work?
\item
  \emph{Discussion} or \emph{Proposal}: something the team needs to make a decision about
  or a concrete proposal to resolve such a discussion.
  All issues can have discussion---this category is for issues that start that way---but
  as we'll discuss in Section~\ref{rse-teams-martha},
  every project should have clear rules for making decisions.
\item
  \emph{Suitable for Newcomer} or \emph{Beginner-Friendly}:
  to identify an easy starting point for someone who has just joined the project.
  If we help potential new contributors find places to start,
  they are more likely to do so Steinmacher et al. (\protect\hyperlink{ref-Stei2014}{2014}).
\end{itemize}

The labels listed above described what kind of work an issue describes;
a separate set of labels can be used to indicate how important that work is.
These labels typically have names like ``High Priority'' or ``Low Priority''.
Their purpose is to help \href{glossary.html\#triage}{triage},
which is the process of deciding what is going to be worked on in what order.

In a large project,
triage is the responsibility of a \href{glossary.html\#product-manager}{product manager}.
In a small one,
the project's lead usually makes triage decisions,
or project members add \href{glossary.html\#upvote}{upvotes} and \href{glossary.html\#downvote}{downvotes} to issues
to indicate how important they think things are.
However decisions are made,
it's helpful to use six more labels to record the outcomes:

\begin{itemize}
\tightlist
\item
  \emph{Urgent}: this needs to be done \emph{now}
  (and is typically reserved for security fixes).
\item
  \emph{Current}: this issue is included in the current round of work.
\item
  \emph{Next}: this issue is (probably) going to be included in the next round.
\item
  \emph{Eventually}: someone has looked at the issue and believes it needs to be tackled,
  but there's no immediate plan to do it.
\item
  \emph{Won't Fix}: someone has decided that the issue isn't going to be addressed,
  either because it's out of scope or because it's not actually a bug.
  Once an issue is marked this way,
  it is usually then closed.
\item
  \emph{Duplicate}: this issue is a duplicate of one that's already in the system.
  Again,
  issues marked this way are usually then closed.
\end{itemize}

Larger projects will replaced Current, Next, and Eventually
with labels corresponding to upcoming software releases, journal issues, or conferences.
Unfortunately,
this doesn't work well on GitHub's issue tracking system
because there's no way to retire a label without deleting it.
After a while,
a project that uses labels for specific events can have a \emph{lot} of old labels\ldots{}

\hypertarget{rse-teams-workflow}{%
\section{How can we enforce a workflow for a project?}\label{rse-teams-workflow}}

If we are using GitHub,
the short answer to this section's title question is,
``We can't,''
but we can create conventions,
and more sophisticated issue tracking systems will let us enforce those conventions.
The basic idea is to only allow issues to move from some states to others,
and to only allow some people to move them.
For example,
Figure~\ref{fig:teams-lifecycle} how issues can move from state to state in a small project:

\begin{figure}
\centering
\includegraphics{figures/rse-teams/lifecycle.pdf}
\caption{\label{fig:teams-lifecycle}Issue Lifecycle}
\end{figure}

\begin{itemize}
\item
  An Open issue becomes Assigned when someone is made responsible for it.
\item
  An Assigned issue becomes Active when that person starts to work on it.
\item
  If they stop work for any length of time, it becomes Suspended.
  (That way, people who are waiting for it know not to hold their breath.)
\item
  An Active bug can either be Fixed or Cancelled,
  where the latter state means that the person working on it has decided it's not really a bug.
\item
  Once a bug is Fixed,
  it can either be Closed or, if the fix doesn't work properly,
  moved back into the Open state and go around again.
\end{itemize}

Workflows like this are more complex than small projects need,
but when the team is distributed
it is essential to be able to find out what's going on
without having to wait for someone to respond to email.

\hypertarget{rse-teams-martha}{%
\section{How can we make decisions?}\label{rse-teams-martha}}

Every team has a power structure:
the only question is
whether it's formal and accountable or informal and therefore unaccountable Freeman (\protect\hyperlink{ref-Free1972}{1972}).
The latter can work for groups of up to half a dozen people
in which everyone knows everyone else.
Beyond that,
groups need to spell out
who has the authority to make which decisions
and how to achieve consensus.

\href{glossary.html\#marthas-rules}{Martha's Rules} are a practical lightweight to do this
in groups with up to a hundred or so members Minahan (\protect\hyperlink{ref-Mina1986}{1986}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Before each meeting, anyone who wishes may sponsor a proposal
  by filing an issue in the GitHub repository tagged ``proposal''.
  Proposals must be filed at least 24 hours before a meeting
  in order to be considered at that meeting, and must include:

  \begin{itemize}
  \tightlist
  \item
    a one-line summary (the subject line of the issue)
  \item
    the full text of the proposal
  \item
    any required background information
  \item
    pros and cons
  \item
    possible alternatives
  \end{itemize}
\item
  A quorum is established in a meeting if half or more of voting members are present.
\item
  Once a person has sponsored a proposal, they are responsible for it.
  The group may not discuss or vote on the issue unless the sponsor or their delegate is present.
  The sponsor is also responsible for presenting the item to the group.
\item
  After the sponsor presents the proposal,
  a \href{glossary.html\#sense-vote}{sense vote} is cast for the proposal prior to any discussion:

  \begin{itemize}
  \tightlist
  \item
    Who likes the proposal?
  \item
    Who can live with the proposal?
  \item
    Who is uncomfortable with the proposal?
  \end{itemize}
\item
  If all or most of the group likes or can live with the proposal,
  it is immediately moved to a formal vote with no further discussion.
\item
  If most of the group is uncomfortable with the proposal,
  it is postponed for further rework by the sponsor.
\item
  If some members are uncomfortable they can briefly state their objections.
  A timer is then set for a brief discussion moderated by the facilitator.
  After 10 minutes or when no one has anything further to add (whichever comes first),
  the facilitator calls for a yes-or-no vote on the question:
  ``Should we implement this decision over the stated objections?''
  If a majority votes ``yes'' the proposal is implemented.
  Otherwise, the proposal is returned to the sponsor for further work.
\end{enumerate}

\hypertarget{rse-teams-meetings}{%
\section{How can we run meetings more efficiently?}\label{rse-teams-meetings}}

Working with other people means meetings,
and most people do meetings poorly:
they don't have an agenda going in,
they don't take minutes,
they waffle on or wander off into irrelevancies,
they repeat what others have said or recite banalities simply so that they'll have said something,
and they hold side conversations
(which pretty much guarantees that the meeting will be a waste of time).
Knowing how to run a meeting efficiently is a core skill for anyone who wants to get things done.
Knowing how to take part in someone else's meeting is just as important,
but gets far less attention:
everyone offers leadership training,
nobody offers followership training.

The rules for running meetings quickly and smoothly are well known but rarely followed:

\begin{description}
\tightlist
\item[Decide if there actually needs to be a meeting.]
If the only purpose is to share information,
have everyone send a brief email instead.
Remember, people can read faster than they can speak:
if someone has facts for the rest of the team to absorb,
the most polite way to communicate them is to type them in.
\item[Write an agenda.]
If nobody cares enough about the meeting to prepare a point-form list of what's to be discussed,
the meeting itself probably doesn't need to happen.
Note that ``the agenda is all the open issues in our GitHub repo'' doesn't count.
\item[Include timings in the agenda.]
Timings help prevent early items stealing time from later ones.
The first estimates with any new group are inevitably optimistic,
so we should revise them upward for subsequent meetings.
However,
we shouldn't have a second or third meeting just because the first one ran over-time:
instead, we should try to figure out \emph{why} we're running over and fix the underlying problem.
\item[Prioritize.]
Tackle issues that will have high impact but take little time first,
and things that will take more time but have less impact later.
That way, if the first things run over time,
the meeting will still have accomplished something.
\item[Make one person responsible for keeping things moving.]
One person should be made moderator
and be responsible for keeping items to time,
chiding people who are having side conversations or checking email,
and asking people who are talking too much to get to the point.
The moderator should \emph{not} do all the talking;
in fact,
whoever is in charge will talk less in a well-run meeting than most other participants.
\item[Require politeness.]
No one gets to be rude,
no one gets to ramble,
and if someone goes off topic,
it's the moderator's job to say,
``Let's discuss that elsewhere.''
\item[No interruptions.]
Participants should raise a finger,
put up a sticky note,
or make one of the other gestures people use at high-priced auctions
when they want to speak.
The moderator should keep track of who wants to speak and give them time in turn.
\item[No side conversations.]
This makes meetings more efficient because
Nobody can actually pay attention to two things at once.
If distractions are tolerated,
people will miss things or they'll have to be repeated.
More importantly,
not paying attention is insulting:
chatting with a friend while someone explains what they did last week
is a really effective way to say,
``I don't think you or your work is important.''
\item[No technology.]
Insist that everyone put their phones, tablets, and laptops into politeness mode
(i.e., close them).
If this is too stressful,
let participants hang on to their electronic pacifiers but turn off the network
so that they really \emph{are} using them just to take notes or check the agenda.
The one exception is accessibility needs:
if someone needs their phone, their laptop, or some other aid
in order to take part in the meeting,
nobody has a right to tell them ``no''.
\item[Take minutes.]
Someone other than the moderator should take point-form notes
about the most important information that was shared,
and about every decision that was made or every task that was assigned to someone.
\item[Take notes.]
While other people are talking,
participants should take notes of questions they want to ask
or points they want to make.
(It's surprising how smart it makes us look when it's our turn to speak.)
\item[End early.]
If the meeting is scheduled for 10:00-11:00,
aim to end at 10:55 to give people time to get where they need to go next.
\end{description}

As soon as the meeting is over, circulate the minutes
(i.e., emailed them to everyone or post them to a wiki):

\begin{description}
\tightlist
\item[People who weren't at the meeting can keep track of what's going on.]
We all have to juggle tasks from several projects or courses,
which means that sometimes we can't make it to meetings.
A wiki page,
email message,
or blog entry is a much more efficient way to catch up than asking a team mate,
``What did I miss?''
\item[Everyone can check what was actually said or promised.]
More than once,
one of us has looked over the minutes of a meeting and thought,
``Did I say that?'' or,
``I didn't promise to have it ready then!''
Accidentally or not,
people will often remember things differently;
writing them down gives everyone a chance to correct mis-recollection,
mis-interpretation,
or mis-representation,
which can save a lot of anguish later on.
\item[People can be held accountable at subsequent meetings.]
There's no point making lists of questions and action items
if we don't follow up on them later.
If we are using a \href{glossary.html\#ticketing-system}{ticketing system},
we should create a ticket for each new question or task right after the meeting
and update those that are being carried forward.
This helps a lot when the time comes to draw up the agenda for the next meeting.
\end{description}

\hypertarget{rse-teams-fair}{%
\section{How can we keep people from talking too much or too little?}\label{rse-teams-fair}}

Some people are so used to the sound of their own voice that
they will talk half the time no matter how many other people are in the room.
Other are so used to being trampled that they don't speak up
even when they have valuable points to make
(which is an example of \href{glossary.html\#learned-helplessness}{learned helplessness}).

One way to combat this is to give everyone \href{glossary.html\#three-stickies}{three sticky notes} at the start of the meeting.
Every time they speak,
they have to give up one sticky note.
When they're out of stickies,
they aren't allowed to speak until everyone has used at least one,
at which point everyone gets all of their sticky notes back.
This ensures that nobody talks more than three times as often as
the quietest person in the meeting,
which completely changes group dynamics:
people who have given up trying to be heard because they always get trampled
suddenly have space to contribute,
and the overly-frequent speakers quickly realize just how unfair they have been.

Another useful technique is called \href{glossary.html\#interruption-bingo}{interruption bingo}.
Draw a grid and label the rows and columns with the participants' names.
Each time one person interrupts another,
add a tally mark to the appropriate cell;
halfway through the meeting,
take a moment to look at the results.
In most cases it will be clear that
one or two people are doing all of the interrupting.
After that, saying, ``All right, I'm adding another tally to the bingo card,''
is often enough to get them to throttle back.

Note that this technique is for managing interruptions,
not speaking time.
It may be completely appropriate for people with more knowledge of a subject
to speak about it more often in a meeting,
but it is never appropriate to repeatedly cut people off.

\hypertarget{rse-teams-online}{%
\section{How should we run online meetings?}\label{rse-teams-online}}

\href{https://chelseatroy.com/2018/03/29/why-do-remote-meetings-suck-so-much/}{This discussion} of why online meetings are often frustrating and unproductive
points out that in most online meetings,
the first person to speak during a pause gets the floor.
As a result,
``If you have something you want to say,
you have to stop listening to the person currently speaking
and instead focus on when they're gonna pause or finish
so you can leap into that nanosecond of silence and be the first to utter something.
The format\ldots{}encourages participants who want to contribute
to say more and listen less.''

The solution is to run a text chat beside the video conference
where people can signal that they want to speak.
The moderator can then select people from the waiting list.
This practice can be reinforced by having everyone mute themselves,
and only allowing the moderator to unmute people.
Brookfield and Preskill (\protect\hyperlink{ref-Broo2016}{2016}) has many other useful suggestions for managing meetings.

\hypertarget{rse-teams-conflict}{%
\section{How should we handle conflict within the team?}\label{rse-teams-conflict}}

You just missed an important deadline,
and people are unhappy.
The sick feeling in the pit of your stomach has turned to anger:
you did \emph{your} part,
but Marta didn't finish her stuff until the very last minute,
which meant that no one else had time to spot the two big mistakes she'd made.
As for Cho,
well, he didn't deliver at all---again.
If something doesn't change,
this project is going to pull down your performance review so far
that you might have to start looking for a new job.

Situations like this come up all the time.
Broadly speaking, there are four ways we can deal with them:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Cross our fingers and hope that things will get better on their own,
  even though they didn't the last three times.
\item
  Do extra work to make up for others' shortcomings.
  This saves us the mental anguish of confronting others in the short run,
  but the time for that ``extra'' has to come from somewhere.
  Sooner or alter,
  our personal lives or other parts of the project will suffer.
\item
  Lose our temper.
  People often wind up displacing anger into other parts of their life:
  they may yell at someone for taking an extra thirty seconds to make change
  when what they really need to do is tell their boss
  that they won't work through another holiday weekend
  to make up for management's decision to short-staff the project.
\item
  Take constructive steps to fix the underlying problem.
\end{enumerate}

Most of us find number four hard because we don't like confrontation.
If we manage it properly,
though,
it is a lot less bruising,
which means that we don't have to be as afraid of initiating it.
Also,
if people believe that we will take steps when they bully, lie, procrastinate, or do a half-assed job,
they will usually avoid making it necessary.

\begin{description}
\tightlist
\item[Make sure we are not guilty of the same sin.]
We won't get very far complaining about someone else interrupting in meetings
if we do it just as frequently.
\item[Check expectations.]
Are we sure the offender knows what standards they are supposed to be meeting?
This is where things like job descriptions
or up-front discussion of who's responsible for what
come in handy.
\item[Check the situation.]
Is someone dealing with an ailing parent or immigration woes?
Have they been put to work on three other projects that we don't know about?
\item[Document the offense.]
Write down what the offender has actually done and why it's not good enough.
Doing this helps us clarify what we're upset about
and is absolutely necessary if we have to escalate.
\item[Check with other team members.]
Are we alone in feeling that the offender is letting the team down?
If so, we aren't necessarily wrong,
but it'll be a lot easier to fix things if we have the support of the rest of the team.
Finding out who else on the team is unhappy can be the hardest part of the whole process,
since we can't even ask the question without letting on that we are upset
and word will almost certainly get back to whoever we are asking about,
who might then accuse us of stirring up trouble.
\item[Talk with the offender.]
This should be a team effort:
put it on the agenda for a team meeting,
present the complaint,
and make sure that the offender understands it.
This is often enough:
if someone realizes that they're going to be called on their hitchhiking or bad manners,
they will usually change their ways.
\item[Escalate as soon as there's a second offense.]
People who don't have good intentions
count on us giving them one last chance after another
until the project is finished and they can go suck the life out of their next victim.
\emph{Don't fall into this trap.}
If someone stole a laptop, we would report it right away.
If someone steals time,
we are being foolishly generous to give them a chance to do it again and again.
\end{description}

In the context of a research project,
``escalation'' often means ``taking the issue to our supervisor''.
Of course,
our supervisor has probably had dozens of students complain to her over the years
about teammates not doing their share,
and it isn't uncommon to have both halves of a pair tell the supervisor that they're doing all the work.
(This is yet another reason to use version control:
it makes it easy to check who's actually written what.)
In order to get her to take us seriously and help us fix our problem,
we should send her an email signed by several people
that describes the problem and the steps we have already taken to resolve it.
Make sure the offender gets a copy as well,
and ask the supervisor to arrange a meeting to resolve the issue.

This is where documentation is crucial.
\href{glossary.html\#hitchhiker}{Hitchhikers} are usually very good at appearing reasonable:
they will nod as we present our case,
then say, ``Well, yes, but\ldots{}'' and list a bunch of minor exceptions
or cases where others on the team have also fallen short of expectations.
If we can't back up our complaint,
our supervisor will likely be left with the impression that the whole team is dysfunctional,
and nothing will improve.

\hypertarget{rse-teams-conduct}{%
\section{Why should we establish a code of conduct for a project?}\label{rse-teams-conduct}}

Having a Code of Conduct for our project makes it easier for people to contribute
by reducing uncertainty about what is acceptable and unacceptable behavior.
This is particularly important when people from very different cultural backgrounds are trying to collaborate,
and for newcomers who don't know what the local rules are.

Having a Code of Conduct is particularly important for people from marginalized or under-represented groups,
who have probably experienced harassment or unwelcoming behavior before.
By adopting one,
we signal that our project is trying to be a better place than YouTube,
Twitter,
and other online cesspools.
Some people may push back claiming that it's unnecessary,
or that it infringes freedom of speech,
but what they really mean is that thinking about how they might have benefited from past inequity makes them feel uncomfortable,
or that they like to argue for the sake of arguing.
If having a Code of Conduct leads to them going elsewhere,
that will probably make our project run more smoothly.

Just as we shouldn't write our own license for a project,
we shouldn't write our own Code of Conduct.
We recommend using the \href{https://www.contributor-covenant.org}{Contributor Covenant} for development projects
and the \href{http://geekfeminism.wikia.com/wiki/Conference_anti-harassment/Policy}{model code of conduct} from the \href{http://geekfeminism.wikia.com/}{Geek Feminism Wiki} for in-person events.
Both have been thought through carefully and revised in the light of experience,
and both are now used widely enough that
many potential participants in our project will not need to have them explained.

Rules are meaningless if they aren't enforced.
If we adopt a Code of Conduct,
we must be clear about how to report issues and who will handle them.
Aurora et al. (\protect\hyperlink{ref-Auro2018}{2018}) is a short, practical guide to handling incidents;
like the Contributor Covenant and the model code of conduct,
it's better to start with something that other people have thought through and refined
than to try to create something from scratch.

\hypertarget{rse-teams-allyship}{%
\section{How can we be good allies for members of marginalized groups?}\label{rse-teams-allyship}}

Setting out rules and handling incidents when they arise is what projects can do;
if we have power
(even or especially the power that comes from being a member of the majority group),
we can also be a good ally for members of marginalized groups Aurora and Gardiner (\protect\hyperlink{ref-Auro2019}{2019}).

First,
\href{glossary.html\#privilege}{privilege} is an unearned advantage given to some people but not all,
while \href{glossary.html\#oppression}{oppression} is systemic, pervasive inequality that benefits the privileged
and harms those without privilege.
A straight, white, physically able, economically secure male
is less likely to be interrupted when speaking,
more likely to be called on in class,
and more likely to get a job interview based on an identical CV
than someone who is perceived as being outside these categories.
The unearned advantage may be small in any individual case,
but compound interest quickly amplifies these differences:
someone who is called on more often in class is more likely to be remembered by a professor,
who in turn is therefore more likely to recommend them to a potential employer,
who is more likely to excuse the poor grades on their transcripts,
and on and on it goes.
People who are privileged are often not aware of it
for the same reason that most fish don't know what water tastes like.

A \href{glossary.html\#target}{target} is someone who suffers from oppression.
Targets are often called ``members of a marginalized group'',
but that phrasing is deliberately passive.
Targets don't choose to be marginalized:
those with privilege marginalize them.
An \href{glossary.html\#ally}{ally} is a member of a privileged group
who is working to understand their own privilege and end oppression.
For example,
privilege is being able to walk into a store and have the owner assume we are there to buy things,
not to steal them.
Oppression is the stories told about (for example) indigenous people being thieves,
and the actions people take as a result of them.
A target is an indigenous person who wants to buy milk,
and an ally is a white person who pays attention to a lesson like this one (raising their own awareness),
calls out peers who spread racist stories (a \href{glossary.html\#peer-action}{peer action}),
or asks the shopkeeper whether they should leave too (a \href{glossary.html\#situational-action}{situational action}).

So what should we actually do if we're able to?
A few simple rules will go a long way:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Be short, simple, and firm.
\item
  Don't try to be funny:
  it almost always backfires, or will later be used against us.
\item
  Play for the audience:
  we probably won't change the mind of the oppressor we are calling out,
  but we might change the minds or give heart to people who are observing.
\item
  Pick our battles.
  We can't challenge everyone, every time,
  without exhausting ourselves and deafening our audience.
  An occasional sharp retort will be much more effective than constant criticism.
\item
  Don't shame or insult one group when trying to help another.
  For example,
  don't call someone stupid when what we really mean is that they're racist or homophobic.
\item
  Change the terms of the debate.
\end{enumerate}

The last rule is best explained by example.
Suppose someone says,
``Why should we take diversity into account when hiring?
Why don't we just hire the best candidate?''
Our response could be,
``Because taking diversity into account \emph{is} hiring the best candidate.
If you can run a mile in four minutes and someone else can do it in 4:15 with a ball and chain on their leg,
who the better athlete?
Who will perform better \emph{if the impediment is removed}?
If you intend to preserve an exclusionary culture in this lab,
considering how much someone has achieved despite systemic unfairness might not make sense,
but you're not arguing for that,
are you?''
If they then say,
``But it's not fair to take anything other than technical skill into account when hiring for a technical job,''
you can say,
``You're right,
which means that what you're \emph{really} upset about is the thought that
you might be treated the way targets have been treated their whole lives.''

\href{https://captainawkward.com/}{Captain Awkward} has useful advice for discussions like these,
and \href{http://geekfeminism.wikia.com/wiki/Charles\%27_Rules_of_Argument}{Charles' Rules of Argument} are very useful online:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Don't go looking for an argument.
\item
  State your position once, speaking to the audience.
\item
  Wait for absurd replies.
\item
  Reply once more to correct any misunderstandings of your original statement.
\item
  Do not reply again---go do something fun instead.
\end{enumerate}

Finally,
it's important to recognize that good principles sometimes conflict.
For example,
consider this scenario:

\begin{quote}
A manager consistently uses male pronouns to refer to software and people of unknown gender.
When you tell them it makes you uncomfortable to treat maleness as the norm,
they say that male is the default gender in their first language
and you should be more considerate of people from other cultures.
\end{quote}

On the one hand,
we want to respect other people's cultures;
on the other hand,
we want to be inclusive of women.
In this case,
the manager's discomfort about changing pronouns
matters less than the career harm caused by them being exclusionary,
but many cases are not this clear cut.
Like any written rules,
a Code of Conduct requires constant interpretation,
and like everything else,
discussion about specific cases becomes easier with practice.

\hypertarget{rse-teams-summary}{%
\section{Summary}\label{rse-teams-summary}}

FIXME: summarize rse-teams

\hypertarget{rse-teams-exercises}{%
\section{Exercises}\label{rse-teams-exercises}}

FIXME: write exercises for rse-teams

\hypertarget{rse-project}{%
\chapter{Project Structure}\label{rse-project}}

Project organization is like a diet:
everyone has one,
it's just a question of whether it's healthy or not.
In the case of a project,
``healthy'' means that people can find what they need and do what they want without becoming frustrated.
This depends on two things:
how well organized the project is,
and how familiar people are with that style of organization.

As with coding style,
small pieces in predictable places with readable names are easier to find and use
than large chunks that vary from project to project
and have names like ``stuff''.
While we can be messy while we are working and then tidy up later,
experience teaches that we will be more productive if we make tidiness a habit
and put things in the right place right from the start.
This lesson therefore describes a widely-used template
for organizing small and medium-sized data analysis projects Noble (\protect\hyperlink{ref-Nobl2009}{2009}).

\begin{quote}
\textbf{Version First}

This chapter assumes that a project's history and development is managed with Git,
and that each project lives in a single repository.
If you are organizing something messy,
please start using version control before you make any changes
so that you don't accidentally lose valuable work.
\end{quote}

\hypertarget{rse-project-thinking}{%
\section{What is a project?}\label{rse-project-thinking}}

The first decision we have to make is what exactly constitutes a ``project'' Wilson et al. (\protect\hyperlink{ref-Wils2017}{2017}).
Some examples are:

\begin{itemize}
\item
  A dataset that is being used by several research projects.
  The project includes the raw data,
  the programs used to tidy that data,
  the tidied data,
  the extra files needed to make the dataset an R package (Chapter~\ref{rse-package-r}),
  and a few text files describing the data's authors, license, and \href{glossary.html\#provenance}{provenance}.
\item
  A set of annual reports written for an \href{glossary.html\#ngo}{NGO}.
  The project includes several Jupyter notebooks,
  some supporting Python libraries used by those notebooks,
  copies of the HTML and PDF versions of the reports,
  a text file containing links to the datasets used in the report
  (which can't be stored on GitHub since they contain personal identifying information),
  and a text file explaining details of the analysis that the authors didn't include in the reports themselves.
\item
  A software library that provides an interactive glossary of data science terms in both Python and R.
  The project contains the files needed to create a package in both languages,
  a Markdown file full of terms and definitions,
  and a Makefile with targets to check cross-references, compile packages, and so on.
\end{itemize}

More generally,
some common criteria for creating projects are one per publication,
one per deliverable piece of software,
or one per team.
The first tends to be too small:
a good dataset will result in several reports,
and the goal of some projects is to produce a steady stream of reports (such as monthly forecasts).
The second is a good fit for software engineering projects
whose primary aim is to produce tools rather than results,
but can be an awkward fit for data analysis work.
The third tends to be too large:
a team of half a dozen people may work on many different things at once,
and a repository that holds them all quickly looks like someone's basement.

The best rule of thumb for deciding what is and isn't a project
is to ask what people have meetings about.
If the same set of people need to get together on a regular basis to talk about something,
that ``something'' probably deserves its own repository.
And if the list of people changes slowly over time but the meetings continue,
that's an even stronger sign.

\hypertarget{rse-project-boilerplate}{%
\section{What files should every project contain?}\label{rse-project-boilerplate}}

Most projects' repositories contain four files.
Three of these are so widely used in open source software projects that GitHub provides support for them,
while the fourth is common in research work.
All of these files may be plain text or Markdown,
and may have a \texttt{.txt} or \texttt{.md} suffix (or no suffix at all),
but should use the principal names given in upper case
since a growing number of tools expect them.

\begin{itemize}
\item
  \texttt{README} includes the project's title and a one-paragraph description of its purpose or content.
  GitHub displays the content of this file on the project's home page.
\item
  \texttt{LICENSE} is the project's license (discussed in Chapter~\ref{rse-teams-software-license}).
\item
  \texttt{CONDUCT} is its code of conduct (also discussed in Chapter~\ref{rse-teams-conduct}).
\item
  \texttt{CITATION} explains how the work should be cited.
  This file should contains a plain text citation that can be copied and pasted into email,
  and may also include entries formatted for various bibliographic systems like \href{http://www.bibtex.org/}{BibTeX}.
\end{itemize}

Other information may be included as sections in these files or put into files of their own:

\begin{itemize}
\item
  \texttt{CONTRIBUTORS}
  lists everyone who has contributed to the project.
  Software projects often put this information in \texttt{README},
  while research projects make it a section in \texttt{CITATION}.
\item
  \texttt{CONTRIBUTING}
  explains how to contribute,
  i.e.,
  what naming conventions to use for functions,
  what tags to put on issues (Section~\ref{rse-git-advanced-tag}),
  or how to install and configure the software needed to start work on the project.
  These instructions can also be included as a section in \texttt{README};
  wherever they go,
  remember that the easier it is for people to get set up and contribute,
  the more likely they are to do so Steinmacher et al. (\protect\hyperlink{ref-Stei2014}{2014}).
\item
  \texttt{GOVERNANCE}
  explains how the project is run.
  It is still uncommon for this to be in a file of its own---it is more often included
  in \texttt{README} or \texttt{CONTRIBUTING}---but the open source and open science communities have learned the hard way
  that \emph{not} being explicit about who has a voice in which decisions
  and how contributors can tell when a decision has been made
  causes trouble sooner or later.
\end{itemize}

Even the four core files may seem like a lot,
but two of these (\texttt{LICENSE} and \texttt{CONDUCT}) are usually chosen rather than written (Chapter~\ref{rse-teams-conduct})
and the others (\texttt{README} and \texttt{CITATION}) can be quite short to start with.
Having these files helps new contributors orient themselves,
and also signals that the project is well run.

What's more,
much of this content is \href{glossary.html\#boilerplate}{boilerplate},
i.e.,
the files are copied from one project to another without any changes.
Once someone knows that a project is using the MIT License (Section~\ref{rse-teams-software-license})
and the Contributor Covenant (Section~\ref{rse-teams-conduct}),
they don't need to read the files to know what's there.

\hypertarget{rse-project-organize}{%
\section{How should I structure the rest of my project?}\label{rse-project-organize}}

Noble (\protect\hyperlink{ref-Nobl2009}{2009}) described a way to organize small bioinformatics projects
that is equally useful for other kinds of research computing.
Each project is put in a separate Git repository,
and the directories in the root of this repository are organized according to purpose.
It specifies five top-level directories:

\begin{itemize}
\item
  The \texttt{./src/} directory (short for ``source'') holds source code
  for programs written in languages like C or C++ that need to be compiled.
  Many projects don't have this directory
  because all of their code is written in languages that don't need compilation.
\item
  Runnable programs go in \texttt{./bin/} (an old Unix abbreviation for ``binary'', meaning ``not text'').
  This includes the compiled and runnable versions of C and C++ programs,
  and also shell scripts,
  Python or R programs,
  and everything else that can be executed.
\item
  Raw data goes in in \texttt{./data/} and is never modified after being stored.
\item
  Results are put in \texttt{./results/}.
  This includes cleaned-up data,
  figures,
  and everything else that can be rebuilt using what's in \texttt{./bin/} and \texttt{./data/}.
  If intermediate results can be re-created quickly and easily,
  they might not be stored in version control,
  but anything that is included in a manuscript should be here.
\item
  Finally,
  documentation and manuscripts go in \texttt{./doc/}.
\end{itemize}

\begin{figure}
\centering
\includegraphics{figures/rse-project/noble.pdf}
\caption{\label{fig:project-noble}Project Layout}
\end{figure}

Figure~\ref{fig:project-noble} below shows this layout for a project called \texttt{g-trans}.
A few things to notice are:

\begin{itemize}
\item
  The documentation for the \texttt{regulate} script appears in the root of \texttt{./doc/},
  while the paper for JCMB is stored in a subdirectory,
  since it contains several files.
\item
  The \texttt{./src/} directory contains a Makefile to re-build the \texttt{regulate} program (Chapter~\ref{rse-automate}).
  Some projects put the Makefile in the root directory,
  reasoning that since it affects both \texttt{./src/} and \texttt{./bin/},
  it belongs above them both rather than in either one.
\item
  There are several subdirectories underneath \texttt{./data/} and \texttt{./results/}.
  Each of the subdirectories in \texttt{./results/} has a Makefile
  to re-create the contents of that directory.
\end{itemize}

While the directories in the top level of each project are organized by purpose,
the directories within \texttt{./data/} and \texttt{./results/} are organized chronologically
to make it easy to see when data was gathered and when results were generated.
These directories all have names in \href{glossary.html\#iso-date-format}{ISO date format} like \texttt{YYYY-MM-DD}
to make it easy to sort them chronologically.
This naming is particularly helpful when data and results are used in several reports.

At all levels,
filenames are chosen so that they will be easy to match with simple shell wildcards.
For example,
a project might use species\emph{organ}treatment.csv
as a file-naming convention,
giving filenames like \texttt{gorilla\_kidney\_cm200.csv}.
This allows \texttt{gorilla\_*\_cm200.csv} to match all gorilla organs
or \texttt{*\_kidney\_*.csv} to match all kidney data.
It does produce long filenames,
but \href{glossary.html\#tab-completion}{tab completion} means that we only have to type the full name once.
Long filenames are just as easy to match in programs:
Python's \texttt{glob} and R's \texttt{Sys.glob} will both take a pattern and return a list of matching filenames.

Marwick et al. (\protect\hyperlink{ref-Marw2018}{2018}) describes a simple layout:

\begin{verbatim}
 DESCRIPTION
 README.md
 LICENSE
 data
    my_data.csv
 analysis
     my_report.Rmd
\end{verbatim}

The key differences are:

\begin{itemize}
\item
  The \texttt{DESCRIPTION} file in the root directory contains
  the information needed to make the project an R package (Chapter~\ref{rse-package-r}).
\item
  Instead of having a separate directory for tidied tata,
  this layout assumes that all work (including data tidying) is done with \href{glossary.html\#computational-notebook}{computational notebooks},
  so Noble's \texttt{bin}, \texttt{src}, and \texttt{results} directories are combined.
\end{itemize}

Going in the other direction,
\href{https://drivendata.github.io/cookiecutter-data-science/}{Cookiecutter Data Science} describes a more elaborate layout
with separate subdirectories for models, features, visualizations, and much more.
All of these share a few key advantages:

\begin{itemize}
\item
  People can find things easily,
  and can easily tell where to put new work.
\item
  Programs within the project can find things just as easily
  (e.g., by using simple filename patterns).
\item
  External tools such as GitHub can \href{glossary.html\#aggregate}{aggregate} information
  so that the project itself is easier to find.
\item
  We don't have to document the structure ourselves.
\end{itemize}

\hypertarget{rse-project-scripts}{%
\section{How should I manage a mix of compiled programs and scripts?}\label{rse-project-scripts}}

Programming languages come in two flavors: compiled and interpreted.
In order to run a program in a \href{glossary.html\#compiled-language}{compiled language} such as C++ or Java,
we give the source files to a \href{glossary.html\#compiler}{compiler}
that translates them into instructions a computer can actually execute
and saves those instructions in files (Figure~\ref{fig:rse-project-languages}a).
Those files full of instructions can then be re-used as often as we want.
If we are using an \href{glossary.html\#interpreted-language}{interpreted language} like R or Python,
on the other hand,
we give our source files to an \href{glossary.html\#interpeter}{interpreter}.
It also translates the code into instructions,
but puts those instructions in memory and executes them immediately (Figure~\ref{fig:rse-project-languages}b).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-project-languages}Language Types}
\end{figure}

Saving instructions in files versus executing them immediately may seem like a small difference,
but historically it led to very different styles of programming.
Compiled languages usually ran faster than interpreted languages,
but compilation took time,
so interpreted languages were better for \href{glossary.html\#exploratory-programming}{exploratory programming}.
The differences are much smaller these days than they were twenty years ago,
but we do still tend to use compiled languages for anything that has to interact directly with hardware
and then \href{glossary.html\#wrap-code}{wrap} those libraries for use in interpreted languages.

\begin{quote}
\textbf{Why \texttt{bin}?}

The name \texttt{bin} is short for ``binary'',
and comes from the fact that the source files of compiled programs are human-readable text,
but files containing the compiler's output are not.
Programmers often call files that aren't text ``binaries'',
even though text is itself stored in binary as well.
\end{quote}

All of this is preamble to deciding where to put things if a project contains compiled programs.
Most software engineers put source code in version control
and recompile it as needed to produce executables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It saves disk space.
\item
  Version control tools can't diff or merge a compiled program.
\item
  Compiled programs are much more sensitive to small differences
  between operating system versions and external dependencies
  than interpreted programs,
  so something compiled on one computer might not work on another anyway.
\end{enumerate}

The authors of this book handle all of this in different ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Put the source code for compiled languages in \texttt{./src/},
  the runnable programs produced from this code in \texttt{./bin/},
  and programs for interpreted languages in \texttt{./scripts/}.
  This makes version control easy---ignore what's in \texttt{./bin/} and save everything else---but
  means users have to look in two places to run things.
\item
  Put the source for compiled programs in \texttt{./src/}
  and both the compiler's output and interpreted programs in \texttt{./bin/}.
  This makes version control a little more complicated,
  since some of what's in \texttt{./bin/} needs to be saved and some doesn't,
  but means there's only one place to look for runnable programs.
  (It's also easy to delete the hand-written scripts in \texttt{./bin/} by accident
  when we only meant to delete the compiled programs,
  but since we have everything under version control,
  that's not a problem, right?)
\item
  Put all source files in \texttt{./src/} and the compiler's output in \texttt{./bin/}.
  This is slightly simpler than the first option,
  but still means users have to look in two places for things they can run.
\end{enumerate}

As always,
the approach matters less than being consistent
and including a note in \texttt{CONTRIBUTING} or elsewhere to document the decision.

\hypertarget{rse-project-software-docs}{%
\section{How should I document the software in a project?}\label{rse-project-software-docs}}

An old proverb says, ``Trust, but verify.''
The equivalent in programming is, ``Be clear, but document.''
No matter how well software is written,
it always embodies decisions that aren't explicit in the final code
or accommodates complications that aren't going to be obvious to the next reader.
Putting it another way,
the best function names in the world aren't going to answer the questions
``Why does the software do this?''
and
``Why doesn't it do this in a simpler way?''
This lesson will explore who we should write documentation for,
what we should write for them,
and where it should go.

Noble's layout places documentation and manuscripts in \texttt{./docs/}.
We recommend separating these into \texttt{./docs/} and \texttt{./reports/}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Most projects generate the documentation for their software directly from the source code
  (Chapters~\ref{rse-package-r} and~\ref{rse-package-py}).
  Putting these files in the same directory as handwritten files
  has the same problems as putting a compiler's output in the same directory as handwritten scripts.
  In fact,
  it's often worse,
  since \href{glossary.html\#documentation-generator}{documentation generators} often create many subdirectories and support files.
\item
  We often create several reports for a single project,
  which complicates file management even further.
\end{enumerate}

There are three kinds of people in any domain:
\href{glossary.html\#novice}{novices},
\href{glossary.html\#competent-practitioner}{competent practitioners},
and \href{glossary.html\#expert}{experts} Wilson (\protect\hyperlink{ref-Wils2018}{2019}\protect\hyperlink{ref-Wils2018}{a}).
A novice doesn't yet have a \href{glossary.html\#mental-model}{mental model} of the domain;
they don't know what the key terms are,
how they relate,
what the causes of their problems are,
or how to tell whether a solution to their problem is appropriate or not.

Competent practitioners know enough to accomplish routine tasks with routine effort:
they may need to check \href{https://stackoverflow.com/}{Stack Overflow} every few minutes,
but they know what to search for and what ``done'' looks like.
Finally,
experts have such a deep and broad understanding of the domain
that they can solve routine problems at a glance
and are able to handle the one-in-a-thousand cases
that would baffle the merely competent.

Each of these three groups needs a different kind of documentation.
A novice needs a tutorial that introduces her to key ideas one by one
and shows how they fit together.
A competent practitioner needs reference guides, cookbooks, and Q\&A sites;
these give her solutions close enough to what she needs
that she can tweak them the rest of the way.
Experts need this material as well---nobody's memory is perfect---but
they may also paradoxically want tutorials.
The difference between them and novices is that experts want tutorials on how things work
and why they were designed that way.

The first thing to decide when writing documentation
is therefore to decide which of these needs we are trying to meet.
Tutorials like this one should be long-form prose that contain code samples and diagrams.
They should use \href{glossary.html\#authentic-task}{authentic tasks} to motivate ideas,
i.e.,
show people things they actually want to do rather than printing the numbers from 1 to 10,
and should include regular check-ins
so that learners and instructors alike can tell if they're making progress.

Tutorials help novices build a mental model,
but competent practitioners and experts will be frustrated by their slow pace and low information density.
They will want single-point solutions to specific problems like
how to find cells in a spreadsheet that contain a certain string
or how to configure the web server to load an access control module.
They can make use of an alphabetical list of the functions in a library,
but are much happier if they can search by keyword to find what they need;
one of the signs that someone is no longer a novice is that
they're able to compose useful queries and tell if the results are on the right track or not.

That observation brings us to the notion of a \href{glossary.html\#false-beginner}{false beginner},
which is someone who appears not to know anything,
but who has enough prior experience in other domains
to be able to piece things together much more quickly than a genuine novice.
Someone who is proficient with MATLAB, for example,
will speed through a tutorial on Python's numerical libraries
much more quickly than someone who has never programmed before.

In an ideal world,
we would satisfy these needs with a \href{https://hapgood.us/2016/05/13/choral-explanations/}{chorus of explanations},
some long and detailed,
others short and to the point.
In our world, though,
time and resources are limited,
so all but the most popular packages must make do with single explanations.
The rest of this section will therefore look at
how to create reference guides and FAQs.

\hypertarget{rse-project-infer}{%
\section{What should I document?}\label{rse-project-infer}}

The answer to the question in this section's title depends on what stage of development we are in.
If we are doing \href{glossary.html\#exploratory-programming}{exploratory programming},
a short docstring to remind ourselves of each function's purpose is good enough.
(In fact, it's probably better than what most people do.)
That one- or two-liner should begin with an active verb and describe either
how inputs are turned into outputs,
or what side effects the function has;
as we discuss below,
if we need to describe both,
we should probably rewrite our function.

An active verb is something like ``extract'', ``normalize'', or ``find''.
For example,
these are all good one-line docstrings:

\begin{itemize}
\tightlist
\item
  ``Create a list of current ages from a list of birth dates.''
\item
  ``Clip signals to lie in {[}0\ldots{}1{]}.''
\item
  ``Reduce the red component of each pixel.''
\end{itemize}

We can tell our one-liners are useful if we can read them aloud in the order the functions are called
in place of the function's name and parameters.

Once we start writing code for other people---including ourselves three months from now---our
docstrings should describe:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The name and purpose of every public class, function, and constant in our code.
\item
  The name, purpose, and default value (if any) of every parameter to every function.
\item
  Any side effects the function has.
\item
  The type of value returned by every function.
\item
  What exceptions those functions can raise and when.
\end{enumerate}

The word ``public'' in the first rule is important.
We don't have to write full documentation for helper functions
that are only used inside our package and aren't meant to be called by users,
but these should still have at least a comment explaining their purpose.
We also don't have to document unit testing functions:
as discussed in Chapter~\ref{rse-correct},
these should have long names that describe what they're checking
so that failure reports are easy to scan.

\hypertarget{rse-project-faq}{%
\section{How can I create a useful FAQ?}\label{rse-project-faq}}

An \href{glossary.html\#faq}{FAQ} is a list of frequently-asked questions and corresponding answers.
A good FAQ uses the terms and concepts that people bring to the software
rather than the vocabulary of its authors;
putting it another way,
the questions should be things that people might search for online,
and the answers should give them enough information to solve their problem.

Creating and maintaining a FAQ is a lot of work,
and unless the community is large and active,
a lot of that effort may turn out to be wasted,
because it's hard for the authors or maintainers of a piece of software
to anticipate what newcomers will be mystified by.
A better approach is to leverage sites like \href{https://stackoverflow.com/}{Stack Overflow},
which is where most programmers are going to look for answers anyway:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Post every question that someone actually asks us,
  whether it's online, by email, or in person.
  Be sure to include the name of the software package in the question
  so that it's findable.
\item
  Answer the question,
  making sure to mention which version of the software we're talking about
  (so that people can easily spot and discard stale answers in the future).
\end{enumerate}

With a bit of work,
the \href{https://data.stackexchange.com/}{Stack Exchange Data Explorer}
can be used to download questions and answers about our software
if we want to put them all in an offline guide.
we can also use \href{http://www.stackprinter.com/}{Stack Printer} for this;
for example, the URL
\url{http://www.stackprinter.com/topvoted?service=stackoverflow\&tagged=rstudio}
will bring up a paged view of top-voted questions about RStudio.

\href{https://stackoverflow.com/}{Stack Overflow}'s guide to \href{https://stackoverflow.com/help/how-to-ask}{asking a good question}
has been refined over many years,
and is a good guide for any project:

\begin{description}
\tightlist
\item[Write the most specific title we can.]
``Why does division sometimes give a different result in Python 2.7 and Python 3.5?''
is much better than, ``Help! Math in Python!!''
\item[Give context before giving sample code.]
A few sentences to explain what are are trying to do and why
will help people determine if their question is a close match to ours or not.
\item[Provide a minimal reprex.]
Section~\ref{rse-teams-bugs} explains the value of a \href{glossary.html\#reprex}{reproducible example} (reprex),
and why reprexes should be as short as possible.
Readers will have a much easier time figuring out if this question and its answers are for them
if they can see \emph{and understand} a few lines of code.
\item[Tag, tag, tag.]
Keywords make everything more findable,
from scientific papers and left-handed musical instruments
to solutions for programming problems.
\item[Use ``I'' and question words (how/what/when/where/why).]
The section headings in these lessons follow this rule for the same reason that questions in a FAQ should:
writing this way forces us to think more clearly about
what someone might actually be thinking when they need help.
\item[Keep each item short.]
The ``minimal manual'' approach to instructional design Carroll (\protect\hyperlink{ref-Carr2014}{2014})
breaks everything down into single-page steps,
with half of that page devoted to troubleshooting.
This may feel like baby steps to the person doing the writing,
but is often as much as a person searching and reading can handle.
It also helps writers realize just how much implicit knowledge they are assuming.
\item[Allow for a \href{https://hapgood.us/2016/05/13/choral-explanations/}{chorus of explanations}.]
As discussed earlier,
users are all different from one another,
and are therefore best served by a chorus of explanations.
Do not be afraid of providing multiple explanations to a single question
that suggest different approaches
or are written for different prior levels of understanding.
\end{description}

\hypertarget{rse-project-data-vs-code}{%
\section{How does documentation for data differ from documentation for code?}\label{rse-project-data-vs-code}}

Documenting data is different from code
because there's rarely an element of time within code documentation:
we we describe what is there,
not how it came to be.
Data,
on the other hand,
always comes from somewhere and has (almost always) had something done to it.
Its documentation must therefore include details about process,
selection,
and transformation.

Often,
all we need to document data is short action statements to describe the actions taken.
Depending on the complexity,
the content description and the process information may be contained in the same statement,
such as,
``Contents of gorilla\_genome\_2020-010-12.bam after run through process.py.''
(This statement presumes that there is documentation on where these \texttt{.bam} files come from
and what \texttt{process.py} does is described somewhere. )

However,
a file that has recieved extensive manual editing and curation
should have much more detailed description of what was done.
Good tools,
like \href{http://openrefine.org/}{OpenRefine},
allow us to export a list of all the changes we have made to a file.
This is extremely detailed process information,
much like looking at a lenthy series of diffs in version control history,
but these change lists do not include \emph{why} these changes were made.
(Some of this may be in version control commit messages,
but there may be many changes within a single commit.)
For example,
documenting that we changed all values like ``{[}1980?{]}'', ``1980?'', and ``{[}{[}1980{]}'' to ``1980'' is not enough:
the question of ``What happened here?'' as been answered,
but now \emph{why}.
Both pieces must be present for full understanding.

\hypertarget{rse-project-data-docs}{%
\section{How should I document the data in a project?}\label{rse-project-data-docs}}

To answer the question in this section's title,
think about the questions we would have playing a text adventure game
in which we wake up in a room we don't recognize.
We would ask things like:
Where am I?
What's around me?
What can I do here?
Where can I go?
We need to know that things exist before we can go investigate them,
and the same goes for making our way around data.

This is why documentation starts with a README file and a short abstract for the project:
they are where most newcomers will start building their expectations around the data.\\
At a minimum,
we need to answer questions about content and time.
Documentation about content should start
with what is available and where things have gone.
Newcomers will generally drill down from:

\begin{itemize}
\tightlist
\item
  Project: what did we do?
\item
  Dataset: what did we make?
\item
  Data files: what's in this file or that column?
\item
  Datum: where did this value come from?
\end{itemize}

Discussions about what should go into documentation about data will often begin with the end game
and ask authors to report absolutely \emph{everything} that has happened to the data.
However,
the better place to start would be with \emph{anything}.\\
Anything is better than nothing and is a useful starting point:
improving from something is easier and less daunting than trying to write everything.
While a project's documentation will become more complex over time,
the most important requirement at every stage is sustainability:
we must design requirements that are still feasible,
and still collect useful information,
even during crunch times.

Multiple styles of material will help different people best (Section~\ref{rse-project-faq}),
but most projects do not have the resources to do this.
More details are great to have in documentation,
but asking for too much risks burnout and no documentation at all.
The rules to live by when deciding what should go in are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Documentation should have enough information,
\item
  about the project, methods, and materials
\item
  such that the information is maintainable over time,
\item
  in an accessible format,
\item
  and valuable for those who need it.
\end{enumerate}

Breaking this down:

\begin{itemize}
\item
  ``Enough information'' means that some information can always be safely omitted.
  For example,
  some methods or computations are best described in published papers;
  unless those papers are locked behind paywalls,
  we can link to them instead of duplicating their content.
\item
  ``About the project, methods, and materials''
  are the elements of most projects that we need to describe.
  Just like a personal introduction should have our name, pronouns, and position,
  we should introduce our data by explaining what the project was for,
  what methods we used in int,
  and what materials we applied those methods to.
\item
  ``Maintainable'' reflects the fact that active projects outgrow static documentation.
  Since an out-of-date description can be more misleading than no description at all,
  we should construct documentation in ways that ensure everyone can contribute
  without it being burdensome.
\item
  ``Accessible format'' means two things.
  First,
  proprietary formats create barriers to access,
  since some people may not have access to the software needed to read the documents.
  They also shorten those documents' useful lifespan,
  since manufacturers can change those formats over time.
  If we do use such a format because we have to integrate with particular systems
  or because formatting and image embedding are better,
  we should always make a plain-text copy available as well.

  The other meaning is accessible to all people.
  We should use proper markup so screen readers can interact with the structure of the text,
  provide transcripts for all screencasts or other videos,
  add alt-text descriptions to images,
  and ensure that all text is actually rendered so that it can be selected and interacted with.
  The UK Home Office has published a \href{https://ukhomeoffice.github.io/accessibility-posters/posters/accessibility-posters.pdf}{set of posters} that summarize accessibility guidelines
  and also serve as a great checklist for ensuring access.
\item
  Finally,
  ``for those who need it'' means taking your audience into account
  when prioritizing what to include and at what level of detail.
  For example,
  the documentation for a specialized bioinformatics package could reasonably expect readers to have
  a graduate-level understanding of the topic,
  so documentation aimed at high school students may simply not be worth creating.
\end{itemize}

\begin{quote}
\textbf{Your Future Self Will Thank You}

Documentation is often promoted for the good of people reusing data who were not part of creating the data in the first place.
Prioritizing their needs can be difficult:
how can we justify spending time for other people
when our current projects need work fo the good of the people working on them right now?

Instead of thinking about people who are unknown and unrelated,
we can think about newcomers to our team
and the time we will save \emph{ourselves} in onboarding them.
We can also think about the time we will save ourselves
when we come back to this project five months or five years from now.
Documentation that serves these two groups well
will almost certainly serve the needs of strangers as well.
\end{quote}

\hypertarget{rse-project-data-start}{%
\section{How can I get started documenting my data?}\label{rse-project-data-start}}

Writing anything can be difficult,
and documentation is no different.
Setting up places for information to be housed both formally and informally is the first step;
creating a checklist should be the second.

The data used within an active project is often rapidly changed, added, or removed.
Maintaining and collecting relevant documentation within this phase of constant change can be difficult.
One way to tackle this is to keep informal documentation.
Creating a \href{glossary.html\#parking-lot}{parking lot} file for each dataset
provides a place to jot down notes when things happen.
These documents should be the home for quick notes,
important links,
and any other piece of information we aren't yet sure what to do with.
This takes off the pressure of trying to write formally and well,
and avoids the decision fatigue of trying to figure out exactly where everything should go
while we're trying to do something else.

Screenshots can also be a quick way to grab a bunch of metadata at once.
Whenever we download data from a repository,
we can take a screenshot of the landing page we downloaded it from and save the URL (or suggested citation) to the notes file.
Any website where we have to fill in query fields,
ranges,
or other information to get a result
is a perfect candidate for a screenshot.

A second strategy for dealing with change in active projects is
to create a to-do list and assign responsibilities,
either as a file in the project's repository
or as a set of tasks in its issue tracking system (Section~\ref{rse-teams-issues}).
``Just write documentation'' is a large ask of anyone without boundaries or a clear end;
like any other element of a research project,
establishing a clear scope of work and responsibilities is essential.

\hypertarget{rse-project-data-first}{%
\section{What data documentation should I create first?}\label{rse-project-data-first}}

The first thing to produce when docmenting data is an inventory of what data is available.
This inventory collects important informaion for the team during the active collection and analysis process.

We start with a bulleted outline of our data collection points.
This is different for every domain,
but always describes the phases or points within the project that data is collected or produced.
For example,
a longitudinal study may collect data from subjects during intake and every three weeks following.

Using that list,
we can fill in the data products being collected in each interaction.
The data points being collected are less important than
the named instruments, inventories, samples, and so on being collected during each phase.
For example,
``participant demographic sheet version April 2019''
might be all that our team needs.

Some of this information may already be available in
the detailed descriptions of data collection in project proposals or human subjects review documentation,
so the data inventory can start as
a bulleted list of these data products with extra notes.
The bullet names can be short,
but should be meaningful to the team.
This can serve as a starting point for describing information already collected
and provide a placeholder for work yet to be completed.

The following questions should be answered for each data product,
but can be answered in any order:

\begin{itemize}
\tightlist
\item
  Where are any physical representations of this data stored?
  This is relevant for anything with surveys, samples, specimines, interview sheets, etc.
\item
  Where are any digital representations of this data stored
  and what are their file names?\\
\item
  Are there any caveats to data collection for this piece of data?
  Note the details and timeframe for any changes to data collection methods,
  formatting differences,
  or any other detail that may make this part of the different or surprising in comparison to the others.
\end{itemize}

The next task is to think about the possible sections of documentation
and create a list of relevant sections needed.\\
One problem with general guides is the number of possible sections needed across all domains.
Not every section is relevant or valuable for each project,
so each team or lab should create its own template for data documentation.
This should combine the elements commonly used within their domain and required by their institutions and funders.
Each project may then customize the field further
and change its customization over time,
but this way all the documentation will be similar.

This template should be a list of sections
with relevant boilerplate text when that makes sense.
For example,
a lab may have a standard license,
contributors' guide,
and code of conduct.
This template should be stored under version control so proposed changes can be discussed
and changes that are made can be tracked over time.

\hypertarget{rse-project-external}{%
\section{How should I manage data that can't be stored in version control?}\label{rse-project-external}}

Small datasets that don't contain sensitive information should be stored in version control:
as a rule of thumb,
anything you would send as an email attachment is probably small enough to be put into Git,
while anything that might reveal someone's identity should not be.
If data is large or sensitive,
there should still be something in \texttt{./data/} to show its existence,
and that ``something'' should be easy for programs to read.
One option is a CSV file whose columns are:

\begin{itemize}
\tightlist
\item
  the name of the dataset,
\item
  its URL or other unique identifier,
\item
  the date it was last checked, and
\item
  its size (so that users will have some idea of how much work is involved in processing it).
\end{itemize}

Another option is to have one file per dataset,
so that instead of reading \texttt{gorilla\_genome.bam},
the program reads \texttt{gorilla\_genome.yml} and then uses the \texttt{url} key in that file to find the data it actually wants.
Whatever you do,
you should always include a \texttt{README.md} file in \texttt{./data/}
that documents the \href{glossary.html\#provenance}{provenance} and organization of the data
(Section~\ref{rse-publish-data}).

\hypertarget{rse-project-summary}{%
\section{Summary}\label{rse-project-summary}}

FIXME: create concept map for project structure

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

The exercises explore the dataset from Meili (\protect\hyperlink{ref-Meil2015}{2016})
and try to understand the relationship between the study and the included files.
Go to the dataset's page (\url{http://doi.org/10.3886/E17507V2}) and download the files.
You will need to make an ICPSER account and agree to their data agreement before you can download.

See Wickes and Stein (\protect\hyperlink{ref-Wick2016}{2016}). FIXME where should this citation go?

\hypertarget{rse-project-ex-understand-project}{%
\subsection{Understand a project}\label{rse-project-ex-understand-project}}

Review the dataset's main page to get a sense of the study.
Review the spreadsheet file and the coded response file.
Answer the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Who are the participants of this study?
\item
  What types of data was collected and used for analysis?
\item
  Can you find information on the demographics of the interviewees?
\item
  This dataset is clearly in support of an article.
  What information can you find about it, and can you find a link to it?
\end{enumerate}

\hypertarget{rse-project-ex-audience}{%
\subsection{Determining the audience for documentation}\label{rse-project-ex-audience}}

The first step to creting documentation is to think about the audience over time.
Not every potential user of the documentation will need it in the short term,
so starting with the most immediate users provides some strategic planning for time.

Below is a grid of potential audiences and timeframes.
These groups are generic and should be adjusted as needed for your project's domain.
For example,
some geology and climate data are meant to be preserved for decades,
while other projects only have a useful lifespan of a few years.

For each potential audience group,
mark each box to indicate if that audience group would be in need of the data and in which timeframes.
You can repeat marks in each column and row as needed.
You may also want to use several types of marks to indicate certainty or uncertainty.

\begin{longtable}[]{@{}lllll@{}}
\toprule
Currently & Within the next 2 years & 2-5 years from now & 5+ years & Audience\tabularnewline
\midrule
\endhead
& & & & Just me\tabularnewline
& & & & My project advisor or PI\tabularnewline
& & & & Others in my project team\tabularnewline
& & & & My lab or department members\tabularnewline
& & & & Researchers in my field\tabularnewline
& & & & Researchers outside of my field\tabularnewline
& & & & Publication reviewers\tabularnewline
& & & & Federal or government agency\tabularnewline
& & & &\tabularnewline
& & & &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{rse-project-ex-create-doc-template}{%
\subsection{Creating documentation template}\label{rse-project-ex-create-doc-template}}

Below is a list of sections and subsections that are generally useful across many domains.
Start with this list and edit to remove, add, or edit sections that would be relevant to your team or project.
You may also wish to print this out and pass this around to your group members to fill out independently
and have a facilitator gather and synthesize the results.
A blank section is at the end to encourage you to add your own information.
Additional notes and other metadata could be added to each section to assign a specific person to write that section,
collect up the information for it, etc.

\begin{itemize}
\tightlist
\item
  Administrative and personnel details:

  \begin{itemize}
  \tightlist
  \item
    Authors, principle investigators, contributors, etc.,
    with associated institutions, contact information, and other identifiers
  \item
    Description of project team
  \item
    Associated papers, code, talks, datasets, etc.
  \item
    Funders and grant numbers
  \end{itemize}
\item
  Data licensing information

  \begin{itemize}
  \tightlist
  \item
    Suggested citation
  \item
    Who to contact if there are any questions
  \end{itemize}
\item
  Project information:

  \begin{itemize}
  \tightlist
  \item
    Brief description of the dataset and/or abstract, including relevant collection and processing dates
  \item
    Collection methods, including dates of collection, data processing, etc.
  \item
    Names, model numbers, and calibration information for any instruments used during data collection
  \item
    Description of scripts (e.g.~R, Python, MATLAB, etc.) and their purpose
  \item
    Data processing workflow and stages
  \item
    Data cleaning process
  \item
    De-identification or other data scrubbing steps that occurred
  \end{itemize}
\item
  Data file information (repeat for each file as needed):

  \begin{itemize}
  \tightlist
  \item
    List of files to be included, grouped in meaningful units
  \item
    Number of rows, fields, columns, etc.
  \item
    Description of folder contents and/or of large groups of similar files
  \item
    Explanation of formats and required software to read them
  \item
    Languages represented within your data
  \item
    Description of the values, units, etc. for each column or field (codebook)
  \item
    Description of columns and fields in the data files (data dictionary)
  \item
    Other domain specific descriptive information
  \end{itemize}
\item
  Other (add sections as desired)
\end{itemize}

\hypertarget{rse-project-ex-permanent-links}{%
\subsection{Making permanent links}\label{rse-project-ex-permanent-links}}

The link to the UK Home Office's \href{https://ukhomeoffice.github.io/accessibility-posters/posters/accessibility-posters.pdf}{accessibility guideline posters} might change in future.
Use the {[}Wayback Machine{]}{[}wayback-machine{]} to find a link that is more likely to be usable in the long run.

\hypertarget{rse-project-keypoints}{%
\section{Key Points}\label{rse-project-keypoints}}

\begin{itemize}
\tightlist
\item
  Put source code for compilation in \texttt{./src/}.
\item
  Put runnable code in \texttt{./bin/}.
\item
  Put raw data in \texttt{./data/}.
\item
  Put results in \texttt{./results/}.
\item
  Put documentation and manuscripts in \texttt{./doc/}.
\item
  Use file and directory names that are easy to match and include dates for the level under \texttt{./data/} and \texttt{./results/}.
\item
  Create README, LICENSE, CONDUCT, and CITATION files in the root directory of the project.
\end{itemize}

\hypertarget{rse-ci}{%
\chapter{Continuous Integration}\label{rse-ci}}

\href{glossary.html\#continuous-integration}{Continuous integration} (CI) a simple idea:
merge and test changes to a project automatically and frequently
so that developers get feedback about problems
while those changes are still fresh in their minds.
Automation tools like Make (Chapter~\ref{rse-automate}) can compile software,
run tests,
re-do analyses,
and re-create reports with a single command;
CI tools run those commands and give users feedback every time
something changes in the project's repository.
These tools can also be set up to run tests with several different configurations of the software
or on several different operating systems,
so that (for example) if a programmer makes a change on Windows
that would break something for Mac users or vice versa,
they find out before their friends' lives are disrupted.

This chapter introduces a CI tool called \href{https://travis-ci.org/}{Travis~CI}
that integrates well with \href{http://github.com}{GitHub}.
If Travis~CI has been set up,
then every time a change is committed to a GitHub repository,
Travis~CI creates a fresh environment,
checks out a copy of the project,
and runs whatever commands the project's managers have set up.
If the project uses a language like C++ or Java,
those commands usually start by compiling the source code to create a runnable program
(Figure~\ref{fig:rse-ci-conceptual}).
If we want to reassure ourselves that our software works,
we can create unit tests as described in Chapter~\ref{rse-correct}
and ask Travis~CI to run them as well
so that (for example) we know whether the code on a branch is working
before we merge it into the master branch (Section~\ref{rse-git-advanced-merge}).
And if we want to do more,
we just have to write the commands.
For example,
we have Travis~CI set up to re-create the PDF and HTML versions of this book
every time a pull request is merged.

\begin{quote}
\textbf{Virtual Machines}

A {[}virtual machine{]}{[}virtual-machine{]} (VM) is a program that pretends to be a computer.
This may seem a bit redundant,
but VMs are quick to create and start up,
and changes made inside the virtual machine are contained within that VM
so we can install new packages or run a completely different operating system
without affecting the underlying computer.

Every time Travis~CI runs,
it creates a new virtual machine to ensure that builds and tests run in a completely clean environment:
nothing left over from previous attempts to compile, run, or publish will affect the current attempt.
\end{quote}

\hypertarget{rse-ci-basic}{%
\section{How do I set up continuous integration?}\label{rse-ci-basic}}

We have to do five things to set up continuous integration:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an account with \href{https://travis-ci.org/}{Travis~CI}.
\item
  Link our Travis~CI account to our GitHub account.
\item
  Tell Travis~CI to watch the repository that contains our project.
\item
  Create a \texttt{.travis.yml} file in your repository
  to tell Travis~CI what language we are working in
  and what we want to do.
\item
  Commit that \texttt{.travis.yml} file to the repository.
\end{enumerate}

\begin{quote}
\textbf{There's More Than One Way to Do It}

This process is for general Travis~CI usage.
If we are working with R projects or packages,
we can simplify things considerably using \texttt{usethis::use\_travis}
(Section~\ref{rse-ci-setting-up-with-r}).
\end{quote}

Creating an account with an online service is a familiar process for most of us,
but linking our Travis~CI account to our GitHub account may be new.
When we do this,
we are not only telling Travis~CI who we are on GitHub:
we are also telling GitHub that it's OK for Travis~CI to get information about us
via GitHub's {[}API{]}{[}api{]}.
This permission is necessary---without it,
we would have to enter our password every time Travis~CI wanted to run,
which would hardly count as ``automated''---but
we should always check carefully when authorizing a service like this
to make sure we are comfortable with what it is planning to do.

\hypertarget{watching-a-repository}{%
\subsection{Watching a repository}\label{watching-a-repository}}

We tell Travis~CI about our repository through its website interface.
On the left,
besides the ``My Repositories'',
we can click the ``+'' to add a repository that is already on GitHub
(Figure~\ref{fig:rse-ci-add-repo}).
After finding our repository in the list that pops up,
we flick the switch button so that it is green
(Figure~\ref{fig:rse-ci-list-repos}).
If our repository doesn't show up,
we can re-synchronize the list using the green ``Sync account'' button on the left sidebar.
If it still doesn't appear,
the repository may belong to someone else or be private.

\begin{figure}
\centering
\includegraphics{figures/rse-ci/add-repo.png}
\caption{\label{fig:rse-ci-add-repo}ci-add-repo}
\end{figure}

\begin{figure}
\centering
\includegraphics{figures/rse-ci/list-repos.png}
\caption{\label{fig:rse-ci-list-repos}ci-list-repos}
\end{figure}

\hypertarget{configuring-a-project}{%
\subsection{Configuring a project}\label{configuring-a-project}}

The next step is to create a file called \texttt{.travis.yml}
in the root directory of the repository.
(The leading \texttt{.} in the name hides the file from casual listings on Mac or Linux,
but not on Windows.)
This file contains settings that control Travis~CI's operation,
and is written in a format called \href{https://bookdown.org/yihui/rmarkdown/html-document.html}{YAML} (Appendix~\ref{yaml}).
A simple template \texttt{.travis.yml} configuration file looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{language:}\AttributeTok{ python}

\FunctionTok{python:}
\KeywordTok{-} \StringTok{"3.6"}

\FunctionTok{install:}
\KeywordTok{-}\NormalTok{ pip install -r requirements.txt}

\FunctionTok{script:}
\CommentTok{# if an example build.py file was in the src folder}
\KeywordTok{-}\NormalTok{ python src/build.py}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{language} tells Travis~CI which programming language we are using
  so that it knows which of its standard {[}virtual machine{]}{[}virtual-machines{]}
  to use as a starting point for our project.
\item
  This example uses Python,
  so we tell Travis~CI which Python version we want under the \texttt{python} key.
  We can ask Travis~CI to test our project with several different versions
  by adding more versions to the list.
\item
  \texttt{install} tells Travis~CI how to install the software we need for our project.
  For Python projects,
  we can put a list of packages in a file called \texttt{requirements.txt}
  and then use \texttt{pip} to install them (Chapter~\ref{rse-package-py}).
\item
  Finally,
  the entries under the \texttt{script} key tell Travis~CI what to do.
  We can put almost anything here provided it doesn't need human interaction
  (i.e., doesn't ask questions).
  In this case,
  our only action is to run \texttt{src/build.py}.
\end{itemize}

After we have add this file to our repository,
Travis~CI will obey the instructions in this file
every time a change is pushed to GitHub.
More specifically,
it will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a new Linux virtual machine.
\item
  Install the desired version of Python.
\item
  Install the software described in \texttt{requirements.txt}.
\item
  Run the commands below the \texttt{script} key.
\item
  Report the results at \url{https://travis-ci.org/}user/repo,
  where user/repo
  identifies the repository.
\end{enumerate}

Travis~CI' summary report tells us whether the build passed,
which is shown in green Figure~\ref{fig:rse-ci-build-overview},
or whether it produced warnings or errors,
which are shown in red.

\begin{figure}
\centering
\includegraphics{figures/rse-ci/build-overview.png}
\caption{\label{fig:rse-ci-build-overview}Travis Build Overview}
\end{figure}

The log below this overview contains a wealth of information
that can help us debug when the build fails.

\hypertarget{rse-ci-display-github}{%
\section{How can I display my project's status on GitHub?}\label{rse-ci-display-github}}

Travis~CI's dashboard is very useful,
but is also convenient to display the status of the build on our project's GitHub home page
(which most people look at more often).
To add a status badge,
click the ``build icon'' shown in the top right corner of Figure~\ref{fig:image-github-icon}
to bring up a dialog box.
Select ``Markdown'' from the ``Format'' menu list,
then copy the Markdown text displayed in the ``Result'' box
and paste it into the project's \texttt{README.md} file.
(It's best to paste the text right below the page's title so that it will be instantly visible).
Once we commit and push thsi change to GitHub,
our project will now show the badge for its Travis~CI status Figure~\ref{fig:image-github-icon}.

\begin{figure}
\centering
\includegraphics{figures/rse-ci/github-icon.png}
\caption{\label{fig:rse-ci-github-icon}The Travis~CI Build Badge}
\end{figure}

\begin{quote}
\textbf{How Do Badges Work?}

Badges are much simpler than they first appear.
When someone visits our project,
GitHub sends them the HTML of the project's home page.
That page includes a link to an image on Travis~CI's site,
so our browser sends a request to Travis~CI.
The URL tells Travis~CI's web server what project we're interested in;
it then sends back a different image
depending on the most recently recorded build status for that project.
\end{quote}

\hypertarget{rse-ci-setting-up-with-r}{%
\section{How can I set up Travis~CI for an R project?}\label{rse-ci-setting-up-with-r}}

The simplest way to set up and use Travis~CI with R is to call:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_travis}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This command:

\begin{itemize}
\tightlist
\item
  Creates a \texttt{.travis.yml} configuration file with typical settings for an R package.
\item
  Adds a Travis~CI build badge to the project's \texttt{README.md}.
\item
  Opens the Travis~CI page for the package so that we can activate Travis~CI for the repository.
  (This assumes the package is already on GitHub and that we have a Travis~CI account.)
\end{itemize}

FIXME: add example here of generated YAML file

The \href{https://usethis.r-lib.org/reference/ci.html}{usethis} documentation has more details,
but it really is this simple.

\hypertarget{rse-ci-testing}{%
\section{How can I use Travis~CI to test my project?}\label{rse-ci-testing}}

The main reason to set up CI is to test the project every time a change is made.
We still have to write unit tests (Chapter~\ref{rse-correct}),
but CI makes those tests much more valuable.

For R packages,
running tests is built into Travis~CI so you don't need to modify anything.

FIXME: er, what? does this mean the commands are auto-generated by \texttt{usethis}?

FIXME: Include example here of what failing tests in R look like in Travis~CI.

For Python packages,
we need to add \texttt{pytest} to the \texttt{script} key in the \texttt{.travis.yml} file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{language:}\AttributeTok{ python}
\FunctionTok{python:}
\KeywordTok{-} \StringTok{"3.6"}
\FunctionTok{install:}
\KeywordTok{-}\NormalTok{ pip install -r requirements.txt}
\FunctionTok{script:}
\KeywordTok{-}\NormalTok{ pytest}
\end{Highlighting}
\end{Shaded}

With this in place,
the Travis~CI log located right below the build overview
will contain all of the information that our testing command produces when tests fail.

FIXME: Include example here of what failing tests in Python look like in Travis~CI.

We can also have Travis~CI report the \href{glossary.html\#code-coverage}{code coverage} of our tests (Section~\ref{rse-correct-coverage}).
If we are using R,
this command will set everything up for us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_coverage}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

FIXME: explain what this does to the YAML file.

To add code coverage for a Python project,
we add one line to \texttt{.travis.yml} to install the code coverage tool
and another to run it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{language:}\AttributeTok{ python}
\FunctionTok{python:}
\KeywordTok{-} \StringTok{"3.6"}
\FunctionTok{install:}
\KeywordTok{-}\NormalTok{ pip install -r requirements.txt}
\CommentTok{# Install code coverage for CI}
\KeywordTok{-}\NormalTok{ pip install codecov}
\FunctionTok{script:}
\KeywordTok{-}\NormalTok{ pytest}
\FunctionTok{after_success:}
\CommentTok{# Send to codecov services for report}
\KeywordTok{-}\NormalTok{ codecov}
\end{Highlighting}
\end{Shaded}

We use \href{https://codecov.io/}{\texttt{codecov}} rather than the \texttt{coverage} tool described in Section~\ref{rse-correct-coverage}
because it is a service like Travis~CI.

FIXME: fill in this explanation

\hypertarget{rse-ci-deploying}{%
\section{How can I use CI for other purposes?}\label{rse-ci-deploying}}

Continuous integration was invented for testing,
but it can be used to automate almost anything.
For example,
if we want to create a website for an R package,
we can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_pkgdown}\NormalTok{()}
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_pkgdown_travis}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

to add lines like this to our \texttt{.travis.yml} file:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{language:}\AttributeTok{ R}
\FunctionTok{sudo:}\AttributeTok{ false}
\FunctionTok{cache:}\AttributeTok{ packages}

\FunctionTok{before_cache:}\AttributeTok{ Rscript -e 'remotes::install_cran("pkgdown")'}
\FunctionTok{deploy:}
  \FunctionTok{provider:}\AttributeTok{ script}
  \FunctionTok{script:}\AttributeTok{ Rscript -e 'pkgdown::deploy_site_github()'}
  \FunctionTok{skip_cleanup:}\AttributeTok{ true}
\end{Highlighting}
\end{Shaded}

The \texttt{deploy} section tells Travis~CI to put the generated website files online
using \href{https://pages.github.com/}{GitHub Pages}.

FIXME: this section needs more detail.
As it is, readers will know something is possible
but not how it works or how to debug it when things go wrong.
At the least,
please explain what keys like \texttt{before\_cache} are about
and provide a diagram showing what files are produced and copied where.

FIXME: Add an example for deployment Python projects.

\hypertarget{rse-ci-summary}{%
\section{Summary}\label{rse-ci-summary}}

FIXME: create concept map for continuous integration.

\hypertarget{rse-ci-exercises}{%
\section{Exercises}\label{rse-ci-exercises}}

FIXME: Get feedback on this and fill it out more later.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Setup Travis~CI for the package you've been working on (for the course).
\item
  Get Travis~CI to run unit tests of your package (not applicable for R packages).
\item
  Write a new unit test in your package that you \emph{know} will fail. Push the changes
  up to GitHub.
\item
  Go through the Travis~CI log and see what the log says about the failure and
  how to fix it. Since you made it purposefully fail, you will already know how to
  fix it, but the point is to get comfortable looking through Travis~CI' logs.
\item
  Correct the test, push to GitHub, and get Travis~CI to build your package
  successfully.
\end{enumerate}

\hypertarget{rse-ci-keypoints}{%
\section{Key Points}\label{rse-ci-keypoints}}

\begin{itemize}
\tightlist
\item
  Continuous integration re-builds and/or re-tests software every time something changes.
\item
  Use continuous integration to check changes before they are inspected.
\item
  Check style as well as correctness.
\end{itemize}

\hypertarget{rse-package-r}{%
\chapter{R Packaging}\label{rse-package-r}}

\hypertarget{rse-package-r-intro}{%
\section{What's in an R package?}\label{rse-package-r-intro}}

\begin{quote}
Another response of the wizards,
when faced with a new and unique situation,
was to look through their libraries to see if it had ever happened before.
This was\ldots{}a good survival trait.
It meant that in times of danger you spent the day sitting very quietly
in a building with very thick walls.

--- Terry Pratchett
\end{quote}

The more software you write,
the more you realize that a programming language is mostly
a way to build and combine software packages.
Every widely-used language now has an online \href{glossary.html\#repository}{repository}
from which people can download and install packages,
and sharing yours is a great way to contribute to the community
that has helped you get where you are.
This lesson shows you how to use R's tools to do this.

\begin{quote}
\hypertarget{cran-and-alternatives}{%
\subsection{CRAN and Alternatives}\label{cran-and-alternatives}}

\href{https://cran.r-project.org/}{CRAN},
the Comprehensive R Archive Network,
is the best place to find the packages you need.
CRAN's famously strict rules ensure that packages run for everyone,
but also makes package development a little more onerous than it might be.
You can also share packages directly from GitHub,
which many people do while packages are still in development.
We will explore this in more detail below.
\end{quote}

\hypertarget{exercise-what-packages-do-you-have}{%
\subsection{Exercise: What packages do you have?}\label{exercise-what-packages-do-you-have}}

What R packages are currently installed on your computer?
How did you figure this out?

\hypertarget{exercise-can-you-build-a-package}{%
\subsection{Exercise: Can you build a package?}\label{exercise-can-you-build-a-package}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clone the GitHub repository for the \texttt{here} package at \url{https://github.com/r-lib/here}.
\item
  Open \texttt{here.Rproj} in RStudio.
\item
  Build the package.
\end{enumerate}

Does the package build successfully?
What messages do you see?
Do any of them worry you?

\hypertarget{rse-package-r-background}{%
\section{\texorpdfstring{What \emph{is} a package, exactly?}{What is a package, exactly?}}\label{rse-package-r-background}}

Suppose you have written a useful R script and want to share it with colleagues.
You could email it to them (or point them at the GitHub repository it's in),
but what should they do next?
They could copy the file onto their computer,
but then they would have to decide where to put it
and remember their decision later on when you updated the script to fix a few bugs.
And what about the documentation you so lovingly crafted---where should it go?
Oh,
and what if different people decide to organize their files differently---how
hard will it be to make the things they have built play nicely together?

\href{glossary.html\#package}{Packages} solve these problems.
While the details vary from language to language,
packages always require that information about the software
be stored in a specific format and in a specific location,
so that statements like \texttt{library(something)} know where to find what they need.
They are rather like the USB ports of the software world:
anything that conforms to a few simple rules can plug in to anything else.

\hypertarget{rse-package-r-create}{%
\section{How do I create a package?}\label{rse-package-r-create}}

An R package must contain the following files:

\begin{itemize}
\item
  The text file \texttt{DESCRIPTION} (with no suffix) describes what the package does,
  who wrote it,
  and what other packages it requires to run.
  We will edit its contents as we go along.
\item
  \href{glossary.html\#namespace}{\texttt{NAMESPACE}},
  (whose name also has no suffix)
  contains the names of everything exported from the package
  (i.e., everything that is visible to the outside world).
  As we will see,
  we should leave its management in the hands of RStudio
  and the \texttt{devtools} package we will meet below.
\item
  Just as \texttt{.gitignore} tells Git what files in a project to ignore,
  \texttt{.Rbuildignore} tells R which files to include or not include in the package.
\item
  All of the R source for our package must go in a directory called \texttt{R};
  subdirectories below this are not allowed.
\item
  As you would expect from its name,
  the optional \texttt{data} directory contains any data we have put in our package.
  In order for it to be loadable as part of the package,
  the data must be saved in R's custom \texttt{.rda} format.
  We will see how to do this below.
\item
  Manual pages go in the \texttt{man} directory.
  The bad news is that they have to be in a sort-of-LaTeX format
  that is only a bit less obscure than the runes inscribed on the ancient dagger
  your colleague brought back from her latest archeological dig.
  The good news is,
  we can embed Markdown comments in our source code
  and use a tool called \texttt{roxygen2}
  to extract them and translate them into the format that R packages require.
\item
  The \texttt{tests} directory holds the package's unit tests.
  It should contain files with names like test\_some\_feature.R,
  which should in turn contain functions named test\_something\_specific.
  We'll have a closer look at these in Chapter~\ref{rse-correct}.
\end{itemize}

You can type all of this in if you want,
but R has a very useful package called \texttt{usethis} that will help you create and maintain packages.
To show how it works,
we will create an R package called \texttt{zipffreq} (with no dashes or other special characters in its name)
to hold word frequencies in classic English novels.
The first step is to load \texttt{usethis} in the console with \texttt{library(usethis)}
and use \texttt{usethis::create\_package}
with the path to the new package directory as an argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{create_package}\NormalTok{(}\StringTok{'~/zipffreq'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Setting active project to '/Users/hamilton/zipffreq'
 Creating 'R/'
 Creating 'man/'
 Writing 'DESCRIPTION'
 Writing 'NAMESPACE'
 Writing 'zipffreq.Rproj'
 Adding '.Rproj.user' to '.gitignore'
 Adding '^zipffreq\\.Rproj$', '^\\.Rproj\\.user$' to '.Rbuildignore'
 Opening new project 'zipffreq' in RStudio
\end{verbatim}

Every well-behaved package should have a README file,
a license,
and a Code of Conduct,
so we will ask \texttt{usethis} to add those:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_readme_md}\NormalTok{()}
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_mit_license}\NormalTok{(}\DataTypeTok{name=}\StringTok{"Merely Useful"}\NormalTok{)}
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_code_of_conduct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

(Note that \texttt{use\_mit\_license} creates two files: \texttt{LICENSE} and \texttt{LICENSE.md}.
The rules for R packages require the former,
but GitHub expects the latter.)
We then edit \texttt{README.md} to be:

\begin{verbatim}
# zipffreq

An example package for Merely Useful that checks Zipf's Law for classic English novels.

## Installation

TBD

## Example

FIXME: add an example.
\end{verbatim}

and make a similar edit to \texttt{DESCRIPTION} so that it contains:

\begin{verbatim}
Package: zipffreq
Title: Checks Zipf's Law for classic English novels.
Version: 0.0.0.9000
Authors@R:
    person(given = "Merely",
           family = "Useful",
           role = c("aut", "cre"),
           email = "merely.useful@gmail.com")
Description: >
  Contains data and functions for checking Zipf's Law
  for a set of classic English novels.
License: MIT + file LICENSE
Encoding: UTF-8
LazyData: true
\end{verbatim}

We can now go to the \texttt{Build} tab in RStudio and run \texttt{Check}
to see if our empty package makes sense.
When we do,
the check warns us that there shouldn't be a period at the end of the package title.
Once we fix that,
we get a clean bill of health.

\begin{quote}
\hypertarget{leftovers}{%
\subsection{Leftovers}\label{leftovers}}

Running \texttt{Check} creates another directory called \texttt{zipffreq.Rcheck}
and a file called \texttt{zipffreq\_0.0.0.9000.tar.gz}.
These are created beside our project directory rather than in it
so as not to confuse version control---we don't want
the files we are building committed to our repository.
\end{quote}

We can now create a placeholder for one of the functions we want to write
in a file called \texttt{R/frequency.R}
either by using \texttt{File\ldots{}New} in RStudio
or by running \texttt{usethis::use\_r(\textquotesingle{}frequency.R\textquotesingle{})}
(which always creates the file in the \texttt{R} directory):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word_count <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(word, text) \{}
  \DecValTok{0}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\texttt{Build\ldots{}Check} runs a lot more checks now
because we have some actual code for it to look at.
It warns us that our function needs documentation,
so we will look at that next.

\hypertarget{exercise-packaging-the-co2-functions}{%
\subsection{Exercise: Packaging the CO2 functions}\label{exercise-packaging-the-co2-functions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a brand-new package to hold the CO2 data and functions for analyzing it.
\item
  Put the license file where R's packaging rules wants it.
\item
  Read the documentation for \texttt{usethis::use\_citation} and create a citation file.
\item
  Build the package: what warnings do you get and what do they mean?
\end{enumerate}

\hypertarget{exercise-exploring-a-package}{%
\subsection{Exercise: Exploring a package}\label{exercise-exploring-a-package}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clone a GitHub repository that contains an R package, such as \url{https://github.com/r-lib/usethis}.
\item
  Which files in this repository do you recognize, and what are they for?
\item
  Which files are there to satisfy something other than R's packaging system?
\end{enumerate}

\hypertarget{exercise-ignoring-files}{%
\subsection{Exercise: Ignoring Files}\label{exercise-ignoring-files}}

What does \texttt{usethis::use\_build\_ignore} do?
When would you use it?

\hypertarget{rse-package-r-use}{%
\section{How do I use a package while I'm creating it?}\label{rse-package-r-use}}

The \texttt{devtools} package includes functions to help you create and test packages.
After you install it,
\texttt{devtools::load\_all("path/to/package")}
will do the same thing that \texttt{library(package)} does,
but use the files you are developing.
(If you run \texttt{load\_all()} without a path,
it will re-load the package in the current working directory,
which is what you usually want to do during development.)

\texttt{devtools::load\_all} is different from \texttt{devtools::install},
which not only installs your package,
but also tries to install its dependencies from CRAN.
After you use \texttt{install},
the package will be available to other projects on your computer.
This is therefore usually one of the last things you do
as you come to the end of development.

\hypertarget{rse-package-r-document}{%
\section{How do I document a R package?}\label{rse-package-r-document}}

Our next task is to document our function.
To do this,
we turn to Hadley Wickham's \emph{\href{http://r-pkgs.had.co.nz/}{R Packages}}
and Karl Broman's ``\href{https://kbroman.org/pkg_primer/}{R package primer}''
for advice on writing roxygen2.
We then return to our source file and put a specially-formatted comment in front of our code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Count how often a word appears in a piece of text}
\CommentTok{#'}
\CommentTok{#' @param word the word to search for}
\CommentTok{#' @param text the text to search in}
\CommentTok{#'}
\CommentTok{#' @return number of times the word appears in the text}
\CommentTok{#'}
\CommentTok{#' @export}

\NormalTok{word_count <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(word, text) \{}
  \DecValTok{0}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This comment text,
is an example of \href{glossary.html\#embedded-documentation}{embedded documentation},
can be easily inserted by placing the cursor somewhere in the function
and clicking the button ``Code -\textgreater{} Insert Roxygen Skeleton'' in RStudio.
Over the years,
programmers have found that if they put code in one file and documentation in another,
the documentation quickly falls out of date with the code:
people will change a function's name or add a new parameter and forget to update the docs.
If the documentation lives right beside the code,
on the other hand,
the next person to modify the code is far more likely to remember to update it.
Tools like roxygen2 can read the code,
extract the documentation,
and format it as HTML or PDF.
They can also do things like create an index,
which would be even more painful to do by hand than writing the documentation itself.

roxygen2 processes comment lines that start with \texttt{\#\textquotesingle{}} (hash followed by single quote).
Putting a comment block right before a function associates that documentation with that function,
and \texttt{@something} indicates a roxygen2 command,
so what this file is saying is:

\begin{itemize}
\tightlist
\item
  the function has two parameters called \texttt{word} and \texttt{text}
\item
  it returns the number of times the word is found in the text; and
\item
  we want to export it (i.e., we want it to be visible outside the package).
\end{itemize}

These roxygen2 text can also be written with \href{https://cran.r-project.org/web/packages/roxygen2/vignettes/markdown.html}{Markdown formatting},
so that for instance using \texttt{**word**} will bold the word. Other common formatting
includes:

\begin{itemize}
\tightlist
\item
  \texttt{{[}function\_name(){]}} or \texttt{{[}object\_name{]}} to link to other function or object,
  respectively, documentation in the same package.
\item
  \texttt{{[}pkgname::function\_name(){]}} or \texttt{{[}pkgname::object\_name{]}} to link to function
  or object, respectively, documentation in another package.
\item
  \texttt{{[}link\ name{]}(website-link)} to link to a website.
\end{itemize}

To enable Markdown with roxygen2 we'll need to install the package roxygen2md package
and use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roxygen2md}\OperatorTok{::}\KeywordTok{roxygen2md}\NormalTok{(}\StringTok{"full"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which will output something like:

\begin{verbatim}
 Setting active project to '/Users/hamilton/zipffreq'
No files changed.
 0 source files changed
 Running `devtools::document()`
Updating zipffreq documentation
Writing NAMESPACE
Loading zipffreq
Writing NAMESPACE
 Review the changes carefully
 Commit the changes to version control
 Setting active project to '<no active project>'
\end{verbatim}

This converts all existing documentation to use Markdown version and will also
add this line to the \texttt{DESCRIPTION} file:

\begin{verbatim}
Roxygen: list(markdown = TRUE)
\end{verbatim}

Ok, our function is now documented,
but when we run \texttt{Check},
we still get a warning.
After a bit more searching and experimentation,
we discover that we need to run \texttt{devtools::document()} to regenerate documentation
because it isn't done automatically.
When we do this,
we get:

\begin{verbatim}
Updating zipffreq documentation
Updating roxygen version in /Users/hamilton/zipffreq/DESCRIPTION
Writing NAMESPACE
Loading zipffreq
Writing NAMESPACE
Writing word_count.Rd
\end{verbatim}

We now have two files to look at:
\texttt{NAMESPACE} and \texttt{word\_count.Rd}.
The first looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Generated by roxygen2: do not edit by hand}

\KeywordTok{export}\NormalTok{(word_count)}
\end{Highlighting}
\end{Shaded}

The comment at the start tells roxygen2 it can overwrite the file,
and reminds us that we shouldn't edit it by hand.
The \texttt{export(word\_count)} directive is what we really want:
as you might guess from the name \texttt{export},
it tells the package builder to make this function visible outside the package.

What about \texttt{word\_count.Rd}?
It lives in the \texttt{man} directory
and now contains:

\begin{verbatim}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/frequency.R
\name{word_count}
\alias{word_count}
\title{Count how often a word appears in a piece of text}
\usage{
word_count(word, text)
}
\arguments{
\item{word}{the word to search for}

\item{text}{the text to search in}
}
\value{
number of times the word appears in the text
}
\description{
Count how often a word appears in a piece of text
}
\end{verbatim}

Again,
there's a comment at the top to remind us that we shouldn't edit this by hand.
After doing this,
we go into ``Build\ldots{}More\ldots{}Configure build tools'' and check ``Generate documentation with Roxygen''.
Running \texttt{Check} again now gives us a clean bill of health.
If we use the \texttt{Install\ and\ Restart} button in RStudio's \texttt{Build} tab,
we can now use \texttt{?word\_count} in the console to view our help.

\hypertarget{exercise-document-a-function}{%
\subsection{Exercise: Document a function}\label{exercise-document-a-function}}

FIXME: write this exercise once the CO2 manipulation functions exist.

\hypertarget{exercise-use-markdown-documentation}{%
\subsection{Exercise: Use Markdown documentation}\label{exercise-use-markdown-documentation}}

Read the documentation for the \texttt{roxygen2md} package,
which allows you to write R documentation in Markdown,
and then convert \texttt{zipffreq} to use Markdown documentation.

\hypertarget{rse-package-r-docwhat}{%
\section{What should I document?}\label{rse-package-r-docwhat}}

The answer to the question in this section's title depends on what stage of development you are in.
If you are doing \href{glossary.html\#exploratory-programming}{exploratory programming},
a one-line comment to remind yourself of each function's purpose is good enough.
(In fact, it's probably better than what most people do.)
That comment should begin with an active verb and describe
how inputs are turned into outputs.
If the function has any \href{glossary.html\#side-effects}{side effects},
you should eliminate them.
If you can't,
you should describe them too.

An active verb is something like ``extract'', ``normalize'', or ``find''.
For example,
these are all good one-line comments:

\begin{itemize}
\tightlist
\item
  ``Create a list of current ages from a list of birth dates.''
\item
  ``Ensure training parameters lie in {[}0..1{]}.''
\item
  ``Reduce the red component of each pixel.''
\end{itemize}

You can tell your one-liners are useful
if you can read them aloud in the order the functions are called
in place of the function's name and parameters.

Once you start writing code for other people---including yourself three months from now---your
documentation should describe:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The name and purpose of every function and constant in your code.
\item
  The name, purpose, and default value (if any) of every parameter to every function.
\item
  Any side effects the function has.
\item
  The type of value returned by every function.
\item
  Why and how the function will deliberately fail.
  If a function uses something like \texttt{stopifnot} or \texttt{assert} to check that a condition holds,
  then that halt-and-catch-fire behavior is effectively part of its interface.
\end{enumerate}

\hypertarget{exercise-fixing-documentation}{%
\subsection{Exercise: Fixing documentation}\label{exercise-fixing-documentation}}

FIXME: provide poorly-documented function in CO2 package and ask learner to find and fix gaps.

\hypertarget{exercise-cross-references}{%
\subsection{Exercise: Cross-references}\label{exercise-cross-references}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What should you add to the roxygen2 comments for one function to link to the documentation for another function?
\item
  Add a cross-reference from the documentation for the function FIXME in the CO2 package to the documentation for the function FIXME.
\end{enumerate}

\hypertarget{exercise-documenting-error-conditions}{%
\subsection{Exercise: Documenting error conditions}\label{exercise-documenting-error-conditions}}

The guidelines above said that authors should document why and how their functions will deliberately fail.
Where and how should you do this using roxygen2 for R?

\hypertarget{rse-package-r-dependencies}{%
\section{How do I manage package dependencies?}\label{rse-package-r-dependencies}}

In order to understand the rest of what follows,
it's important to understand that R packages are distributed as compiled \href{glossary.html\#byte-code}{byte code},
\emph{not} as \href{glossary.html\#source-code}{source code} (which is how Python does it)
or as \href{glossary.html\#binary-code}{binary code} (which is how app stores distribute things).
When a package is built,
R loads and checks the code,
then saves the corresponding low-level instructions.
Our R files should therefore define functions,
not run commands immediately:
if they do the latter,
those commands will be executed every time the script loads,
which is probably not what users will want.

As a side effect,
this means that if a package uses \texttt{load(something)},
then that \texttt{load} command is executed \emph{while the package is being compiled},
and \emph{not} while the compiled package is being loaded by a user after distribution
(Figure~\ref{fig:package-distribution}).
If we have loaded the library by hand in our R session during development,
though,
we might not notice the problem.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:package-distribution}Package Distribution}
\end{figure}

How then can our packages use other packages?
The safest way is to use \href{glossary.html\#fully-qualified-name}{fully-qualified names}
such as \texttt{stringr::str\_replace}
every time we call a function from another package or that is defined somewhere outside our package.
Let's modify our word counter to use \texttt{stringr::str\_count}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word_count <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(word, text) \{}
\NormalTok{  stringr}\OperatorTok{::}\KeywordTok{str_count}\NormalTok{(text, word)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Since our compiled-and-distributable package will only contain the compiled code for its own functions,
direct calls to functions from other packages won't work after the package is installed.
To fix this,
we ask \texttt{usethis} to add a note about \texttt{stringr} to \texttt{DESCRIPTION}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_package}\NormalTok{(}\StringTok{'stringr'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The bottom of \texttt{DESCRIPTION} now has these two lines:

\begin{verbatim}
Imports:
    stringr
\end{verbatim}

The \texttt{Imports} field in \texttt{DESCRIPTION} tells R what else it has to install when it installs our package.
To be explicit about the version of the package, we can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_tidy_versions}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Which will add the version of the package to the \texttt{DESCRIPTION} file, which will
look like:

\begin{verbatim}
Imports:
    stringr (>= 1.4.0)
\end{verbatim}

(See Section~\ref{rse-git-advanced-tag} for a discussion of version numbering.)

All right: are we done now?
No, we are not:

\hypertarget{exercise-document-dependencies}{%
\subsection{Exercise: Document dependencies}\label{exercise-document-dependencies}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Modify the functions in the CO2 package to use \texttt{package::function} name for everything.
\item
  Modify the \texttt{DESCRIPTION} file to document the package's dependencies.
\end{enumerate}

\hypertarget{importing}{%
\subsection{Importing}\label{importing}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What does \texttt{@import} do in roxygen2 documentation?
\item
  When should or shouldn't you use it?
\end{enumerate}

\hypertarget{rse-package-r-github}{%
\section{How can I share my package via GitHub?}\label{rse-package-r-github}}

We said in Section~\ref{rse-package-r-intro} that R packages could be shared through GitHub
as well as through CRAN.
If someone has done this,
installing the package on your computer is as simple as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"username/reponame"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where \texttt{username} and \texttt{reponame} are the names of the user and the project respectively.
If you want to share your work,
all you have to do is create a repository whose contents are laid out
as described earlier in this chapter:
if it looks like an R project,
\texttt{install\_github} will treat it as one.

\hypertarget{rse-package-r-data}{%
\section{How can I add data to a package?}\label{rse-package-r-data}}

The last steps are to add some cleaned-up data to our package
and document the package as a whole. If we want to create fake data or clean up
raw data, we should first run the command:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's name the data "small_data", but it can be anything.}
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_data_raw}\NormalTok{(}\StringTok{"small_data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which outputs:

\begin{verbatim}
 Creating 'data-raw/'
 Adding '^data-raw$' to '.Rbuildignore'
 Writing 'data-raw/small_data.R'
 Modify 'data-raw/small_data.R'
 Finish the data preparation script in 'data-raw/small_data.R'
 Use `usethis::use_data()` to add prepared data to package
\end{verbatim}

that creates the \texttt{data-raw/} folder and the \texttt{small\_data.R} file in that folder.
Inside that new file is:

\begin{verbatim}
## code to prepare `small_data` dataset goes here

usethis::use_data("small_data")
\end{verbatim}

Let's replace the first comment with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small_data <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \OperatorTok{~}\NormalTok{word,  }\OperatorTok{~}\StringTok{ }\NormalTok{count,}
  \StringTok{"some"}\NormalTok{,   }\DecValTok{2}\NormalTok{,}
  \StringTok{"words"}\NormalTok{,  }\DecValTok{1}\NormalTok{,}
  \StringTok{"appear"}\NormalTok{, }\DecValTok{1}\NormalTok{,}
  \StringTok{"times"}\NormalTok{,  }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which looks like:

\begin{verbatim}
# A tibble: 4 x 2
  word   count
  <chr>  <dbl>
1 some       2
2 words      1
3 appear     1
4 times      1
\end{verbatim}

Remove the quotes within the \texttt{use\_data()}. The new file should contain the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small_data <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
  \OperatorTok{~}\NormalTok{word,  }\OperatorTok{~}\StringTok{ }\NormalTok{count,}
  \StringTok{"some"}\NormalTok{,   }\DecValTok{2}\NormalTok{,}
  \StringTok{"words"}\NormalTok{,  }\DecValTok{1}\NormalTok{,}
  \StringTok{"appear"}\NormalTok{, }\DecValTok{1}\NormalTok{,}
  \StringTok{"times"}\NormalTok{,  }\DecValTok{1}
\NormalTok{  )}

\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_data}\NormalTok{(small_data)}
\end{Highlighting}
\end{Shaded}

Run the script by hitting ``Source'' (or \texttt{source("data-raw/small\_data.R")}). This
will output:

\begin{verbatim}
 Creating 'data/'
 Saving 'small_data' to 'data/small_data.rda'
\end{verbatim}

Now the \texttt{small\_data} data.frame object is saved to \texttt{data/} and is now accessible
throughout the package!

When we run \texttt{Check},
we get a complaint about an undocumented data set,
so we create a file called \texttt{R/small\_data.R} to hold documentation about the dataset
and put this in it:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Sample word frequency data.}
\CommentTok{#'}
\CommentTok{#' This small dataset contains word frequencies for tutorial purposes.}
\CommentTok{#'}
\CommentTok{#' @docType data}
\CommentTok{#'}
\CommentTok{#' @format A data frame}
\CommentTok{#' \textbackslash{}describe\{}
\CommentTok{#'   \textbackslash{}item\{word\}\{The word being counted (chr)\}}
\CommentTok{#'   \textbackslash{}item\{n\}\{The number of occurrences (int)\}}
\CommentTok{#' \}}
\StringTok{"small_data"}
\end{Highlighting}
\end{Shaded}

Everything except the last line is a roxygen2 comment block
that describes the data in plain language,
then uses some tags and directives to document its format and fields.
The line \texttt{@docType} tells roxygen2 that this comment describes data rather than a function,
and the last line is the string \texttt{"small\_data"},
i.e.,
the name of the dataset.
We will create one placeholder R file like this for each of our datasets,
and each will have that dataset's name as the thing being documented.

Running \texttt{Check} now gives us a different warning:

\begin{verbatim}
    Warning: package needs dependence on R (>= 2.10)
\end{verbatim}

We do \emph{not} fix this by adding another line under \texttt{Imports} in \texttt{DESCRIPTION},
since R itself isn't a package.
Instead,
we add this line:

\begin{verbatim}
Depends: R (>= 2.10)
\end{verbatim}

and get a clean bill of health.

We use a similar trick to document the package as a whole:
we create a file \texttt{R/zipffreq.R}
(i.e., a file with exactly the same name as the package)
and put this in it:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Example of an R package.}
\CommentTok{#'}
\CommentTok{#' @author Merely Useful, \textbackslash{}email\{merely.useful@gmail.com\}}
\CommentTok{#' @docType package}
\CommentTok{#' @name zipffreq}
\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

That's right:
to document the entire package,
we document \texttt{NULL}.
One last build,
and our package is ready to deliver.

\begin{quote}
\hypertarget{the-virtues-of-laziness}{%
\subsection{The Virtues of Laziness}\label{the-virtues-of-laziness}}

We should always put \texttt{LazyData:\ TRUE} in \texttt{DESCRIPTION}
so that datasets are only loaded on demand.
\end{quote}

\hypertarget{exercise-letting-usethis-do-even-more-work}{%
\subsection{\texorpdfstring{Exercise: Letting \texttt{usethis} do even more work}{Exercise: Letting usethis do even more work}}\label{exercise-letting-usethis-do-even-more-work}}

What does \texttt{usethis::use\_package\_doc} do?

\hypertarget{exercise-scripting-data-creation}{%
\subsection{Exercise: Scripting data creation}\label{exercise-scripting-data-creation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Put the code two create \texttt{small\_data} in an R script in \texttt{data-raw} via \texttt{usethis::use\_data\_raw()}.
\item
  Include \texttt{usethis::use\_data(small\_data)} at the end of the script.
\item
  Describe what this script does step by step.
\end{enumerate}

\hypertarget{exercise-add-sample-data-to-the-co2-package}{%
\subsection{Exercise: Add sample data to the CO2 package}\label{exercise-add-sample-data-to-the-co2-package}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a small sample of the CO2 data in a tibble called \texttt{sample\_CO2\_data}.
\item
  Save it and document it.
\end{enumerate}

\hypertarget{exercise-reproducible-data-set-creation}{%
\subsection{Exercise: Reproducible data set creation}\label{exercise-reproducible-data-set-creation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a small script to create and save the sample dataset from the previous exercise.
\item
  Where should this script go in your package? How should you document its existence and usage?
\item
  Swap packages with a colleague. Can you regenerate their sample data using only what is (documented) in their package?
\end{enumerate}

\hypertarget{rse-package-r-summary}{%
\section{Summary}\label{rse-package-r-summary}}

\begin{figure}
\centering
\includegraphics{figures/rse-package-r/rse-package-r.pdf}
\caption{\label{fig:rse-package-r-concept}R Packaging Concept Map}
\end{figure}

\hypertarget{rse-package-r-keypoints}{%
\section{Key Points}\label{rse-package-r-keypoints}}

\begin{itemize}
\tightlist
\item
  Packages allow software to be shared in manageable ways.
\item
  R packages can be shared through CRAN or GitHub, or managed locally during development.
\item
  Packages can contain code and data.
\item
  A package must contain \texttt{DESCRIPTION} and \texttt{NAMESPACE} files.
\item
  Use \texttt{.Rbuildignore} to control what is and isn't included in a package.
\item
  Include a README, a license, and a citation file in every package.
\item
  Use \texttt{usethis} and \texttt{devtools} to manage package development.
\item
  Put documentation in the \texttt{man} directory and tests in the \texttt{tests} directory.
\item
  Use roxygen2 or Markdown to document the contents of a package.
\end{itemize}

\hypertarget{rse-package-py}{%
\chapter{Python Packaging}\label{rse-package-py}}

\begin{quote}
Another response of the wizards,
when faced with a new and unique situation,
was to look through their libraries to see if it had ever happened before.
This was\ldots{}a good survival trait.
It meant that in times of danger you spent the day sitting very quietly in a building with very thick walls.

--- Terry Pratchett
\end{quote}

The more software you write,
the more you realize that a programming language is a way to build and combine software libraries.
Every widely-used language now has an online repository
from which people can download and install those libraries.
This lesson shows you how to use Python's tools to create and share libraries of your own.

This material is based in part on \href{https://python-102.readthedocs.io/}{Python 102} by \href{https://ccit.clemson.edu/research/researcher-profiles/ashwin-srinath/}{Ashwin Srinath}.

\hypertarget{rse-package-py-modules}{%
\section{How can I turn a set of Python source files into a module?}\label{rse-package-py-modules}}

Any Python source file can be imported by any other.
(This is why Python files should be named using \href{glossary.html\#pothole-case}{\texttt{pothole\_case}}
instead of {[}\texttt{kebab-case}{]}{[}kebab-case{]}:
an expression like \texttt{import\ some-thing} isn't allowed
because \texttt{some-thing} isn't a legal variable name.)
When a file is imported,
the statements in it are executed as it loads.
Variables, functions, and items defined in the file are
then available as \texttt{module.name}, where \texttt{module} is the
filename (without the \texttt{.py} extention) and \texttt{name} is the
name of the item.

As an example,
we can put a constant and two functions used in our Zipf's Law study in a file called \texttt{zipf.py}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pytest }\ImportTok{import}\NormalTok{ approx}

\NormalTok{RELATIVE_ERROR }\OperatorTok{=} \FloatTok{0.05}

\KeywordTok{def}\NormalTok{ make_zipf(length):}
    \ControlFlowTok{assert}\NormalTok{ length }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Zipf distribution must have at least one element'}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\NormalTok{ i) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(length)]}
    \ControlFlowTok{return}\NormalTok{ result}


\KeywordTok{def}\NormalTok{ is_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(hist) }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Cannot test Zipfiness without data'}
\NormalTok{    scaled }\OperatorTok{=}\NormalTok{ [h}\OperatorTok{/}\NormalTok{hist[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ h }\KeywordTok{in}\NormalTok{ hist]}
\NormalTok{    perfect }\OperatorTok{=}\NormalTok{ make_zipf(}\BuiltInTok{len}\NormalTok{(hist))}
    \ControlFlowTok{return}\NormalTok{ scaled }\OperatorTok{==}\NormalTok{ approx(perfect, rel}\OperatorTok{=}\NormalTok{rel)}
\end{Highlighting}
\end{Shaded}

and then use \texttt{import\ zipf},
\texttt{from\ zipf\ import\ is\_zipf},
and so on:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ zipf }\ImportTok{import}\NormalTok{ make_zipf, is_zipf}

\NormalTok{generated }\OperatorTok{=}\NormalTok{ make_zipf(}\DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{'generated distribution: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(generated))}
\NormalTok{generated[}\OperatorTok{-}\DecValTok{1}\NormalTok{] }\OperatorTok{*=} \DecValTok{2}
\BuiltInTok{print}\NormalTok{(}\StringTok{'passes test with default tolerance: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(is_zipf(generated)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{'passes test with tolerance of 1.0: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(is_zipf(generated, rel}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Running this program produces the following output:

\begin{verbatim}
generated distribution: [1.0, 0.5, 0.3333333333333333, 0.25, 0.2]
passes test with default tolerance: False
passes test with tolerance of 1.0: True
\end{verbatim}

It also creates a subdirectory called \texttt{\_\_pycache\_\_}
that holds the compiled versions of the imported files.
The next time Python imports \texttt{zipf},
it checks the timestamp on \texttt{zipf.py} and the timestamp on the corresponding file in \texttt{\_\_pycache\_\_}.
If the latter is more recent,
Python doesn't bother to recompile the file:
it just loads the bytes in the cached version and uses those.
In the general spirit of \href{glossary.html\#dry}{``Don't Repeat Yourself (DRY)''}
we typically don't put both the original and processed versions of files in
version control (it can be convenient, but then you have to have processes
in place to make sure that they stay in sync) so we normally add the
\texttt{\_\_pycache\_\_} directory to in \texttt{.gitignore}.

\hypertarget{exercise-hello-zipf}{%
\subsection{Exercise: Hello, Zipf!}\label{exercise-hello-zipf}}

Put a statement

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hello, Zipf!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

at the end of \texttt{zipf.py}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What happens when you start Python interactively and import the
  module with the command \texttt{import\ zipf}?
\item
  Python caches modules that have been imported. What happens if you
  type \texttt{import\ zipf} again?
\end{enumerate}

\hypertarget{rse-package-py-import}{%
\section{How can I control what is executed during import and what isn't?}\label{rse-package-py-import}}

Sometimes it's handy to be able to import code and also run it as a program.
For example,
we may have a file full of useful functions for extracting keywords from text
that we want to be able to use in other programs,
but also want to be able to run \texttt{keywords\ somefile.txt} to get a listing.

To help us do this,
Python automatically creates a variable called \texttt{\_\_name\_\_} in each module.
If the module is the main program,
(e.g.~when the script is run from the command line)
that variable is assigned the string \texttt{\textquotesingle{}\_\_main\_\_\textquotesingle{}}.
Otherwise (e.g.~when the module is imported into a script), it is assigned the module's name.
Using this leads to modules like this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\ImportTok{from}\NormalTok{ pytest }\ImportTok{import}\NormalTok{ approx}

\NormalTok{USAGE }\OperatorTok{=} \StringTok{'''zipf num [num...]: are the given values Zipfy?'''}
\NormalTok{RELATIVE_ERROR }\OperatorTok{=} \FloatTok{0.05}

\KeywordTok{def}\NormalTok{ make_zipf(length):}
\NormalTok{    ...}\ImportTok{as}\NormalTok{ before...}

\KeywordTok{def}\NormalTok{ is_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
\NormalTok{    ...}\ImportTok{as}\NormalTok{ before...}

\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(sys.argv) }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(USAGE)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        values }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(a) }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ sys.argv[}\DecValTok{1}\NormalTok{:]]}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ is_zipf(values)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{\}}\StringTok{: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(result, values))}
\NormalTok{    sys.exit(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here,
the code guarded by \texttt{if\ \_\_name\_\_\ ==\ \textquotesingle{}\_\_main\_\_\textquotesingle{}} isn't executed when the file loaded by something else.
We can test this by re-running \texttt{use.py} as before:
the usage message doesn't appear,
which means the main block wasn't executed,
which is what we want.

\hypertarget{exercise-what-is-__name__-in-different-contexts}{%
\subsection{\texorpdfstring{Exercise: What is \texttt{\_\_name\_\_} in different contexts?}{Exercise: What is \_\_name\_\_ in different contexts?}}\label{exercise-what-is-__name__-in-different-contexts}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start an interactive Python interpreter, and run \texttt{print(\_\_name\_\_)}.
  What do you get?
\item
  Add the statement \texttt{print(\_\_name\_\_)} to the end of the file \texttt{zipf.py}
  and from the Python interpreter, run \texttt{import\ zipf}. What is the name
  printed?
\end{enumerate}

\hypertarget{exercise-turn-use.py-into-a-unit-test}{%
\subsection{\texorpdfstring{Exercise: Turn \texttt{use.py} into a unit test}{Exercise: Turn use.py into a unit test}}\label{exercise-turn-use.py-into-a-unit-test}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Turn the \texttt{use.py} script into a \texttt{pytest} unit test; name the file \texttt{test\_zipf.py}.
\end{enumerate}

\hypertarget{rse-package-py-creating}{%
\section{How do I create a package?}\label{rse-package-py-creating}}

Let's say we wanted to add some capabilities to what
we've built: we want to be able to generate noisy
Zipf distributions---noise within the same default \texttt{RELATIVE\_ERROR}---and
we want to add a Zipf-checker that can handle unsorted inputs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}\ImportTok{as}\NormalTok{ before...}

\KeywordTok{def}\NormalTok{ make_zipf(length):}
    \ControlFlowTok{assert}\NormalTok{ length }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Zipf distribution must have at least one element'}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\NormalTok{ i) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(length)]}
    \ControlFlowTok{return}\NormalTok{ result}

\KeywordTok{def}\NormalTok{ make_noisy_zipf(length, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
\NormalTok{    data }\OperatorTok{=}\NormalTok{ make_zipf(length)}
\NormalTok{    minnoise }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{-}\NormalTok{ rel}\OperatorTok{/}\DecValTok{2}
\NormalTok{    maxnoise }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{+}\NormalTok{ rel}\OperatorTok{/}\DecValTok{2}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(length):}
\NormalTok{        data[i] }\OperatorTok{=}\NormalTok{ data[i] }\OperatorTok{*}\NormalTok{ random_uniform(minnoise, maxnoise)}
    \ControlFlowTok{return}\NormalTok{ data}

\KeywordTok{def}\NormalTok{ is_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(hist) }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Cannot test Zipfiness without data'}
\NormalTok{    scaled }\OperatorTok{=}\NormalTok{ [h}\OperatorTok{/}\NormalTok{hist[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ h }\KeywordTok{in}\NormalTok{ hist]}
\NormalTok{    perfect }\OperatorTok{=}\NormalTok{ make_zipf(}\BuiltInTok{len}\NormalTok{(hist))}
    \ControlFlowTok{return}\NormalTok{ scaled }\OperatorTok{==}\NormalTok{ approx(perfect, rel}\OperatorTok{=}\NormalTok{rel)}

\KeywordTok{def}\NormalTok{ is_unsorted_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
\NormalTok{    sortedhist }\OperatorTok{=} \BuiltInTok{sorted}\NormalTok{(hist)}
    \ControlFlowTok{return}\NormalTok{ is_zipf(unsorted, rel)}

\NormalTok{...}\ImportTok{as}\NormalTok{ before...}
\end{Highlighting}
\end{Shaded}

As our package grows,
we should split its source code into multiple files,
and this is now starting to look like two broad types of
related functionality: generating a Zipf distribution, and
checking against a Zipf distribution. But they are related;
and what's more, there are constants that would be common
across the two pieces - the \texttt{RELATIVE\_ERROR} constant.

If we want multiple files, but they are related and
and have some common items, that's a good indication that
we are outgrowing a single-file module and we want a Python
package.

A package in Python is no more than a directory containing
Python code, and by convention a Python file with a special
name, \texttt{\_\_init\_\_.py} (this was a requirement before Python 3.3).
Just as importing a module file executes the code in the module,
importing a package executes the code in \texttt{\_\_init\_\_.py}.

\hypertarget{exercise-create-a-package-for-our-zipf-code.}{%
\subsection{Exercise: Create a package for our Zipf code.}\label{exercise-create-a-package-for-our-zipf-code.}}

We're going to create a package for the code we've written
for the Zipf distribution. In the long and honourable(?)
tradition of package names ending in ``py'', we'll call our
package zipfpy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a \texttt{zipfpy} directory and move the file \texttt{zipf.py} into that directory.
\item
  Create an empty file \texttt{zipfpy/\_\_init\_\_.py}.
\item
  Modify the test suite so that it runs; functions will now be found in the \href{glossary.html\#namespace}{namespace} \texttt{zipfpy.zipf} rather than \texttt{zipf}.
\item
  Run the test suite with \texttt{pytest}
\end{enumerate}

\hypertarget{exercise-hello-__init__.py}{%
\subsection{\texorpdfstring{Exercise: ``Hello \texttt{\_\_init\_\_.py}''}{Exercise: ``Hello \_\_init\_\_.py''}}\label{exercise-hello-__init__.py}}

Repeat the ``Hello, Zipf!'' exercise by temporarily adding the
line \texttt{print("Hello,\ Init!")} to the \texttt{\_\_init\_\_.py}. Then
from the directory containing the directory \texttt{zipfpy}, start
a Python interpreter and run:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ zipfpy}
\end{Highlighting}
\end{Shaded}

What happens?

When you're done, remove the \texttt{print()} line from \texttt{\_\_init\_\_.py}.

\hypertarget{referring-to-modules-within-the-same-package-multiple-files}{%
\section{Referring to modules within the same package: multiple files}\label{referring-to-modules-within-the-same-package-multiple-files}}

With that done, we're going to start splitting our module up into smaller components,
which will each be part of the package. We'll split our routines up into functions that
generate Zipf-distributed data and those that test for Zipf-y-ness. So we'll
have a \texttt{zipf/generate.py}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ random }\ImportTok{import}\NormalTok{ uniform}

\NormalTok{RELATIVE_ERROR }\OperatorTok{=} \FloatTok{0.05}

\KeywordTok{def}\NormalTok{ make_zipf(length):}
\NormalTok{    ...}


\KeywordTok{def}\NormalTok{ make_noisy_zipf(length, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
\NormalTok{    ...}
\end{Highlighting}
\end{Shaded}

and a \texttt{zipf/check.py}. Now here we have another
change to make; the the \texttt{is\_zipf} function refers
to the \texttt{generate} routine, which is in another file,
but within the same module.

In this case of within-package imports, we have to use {[}\texttt{relative\ imports}{]}{[}relative-import{]} to
refer to other files within the module; this is
a little bit like using relative paths vs absolutely
paths in the shell, right down to \texttt{.} referring
to the current directory. We could use:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ . }\ImportTok{import}\NormalTok{ generate}

\NormalTok{...}
\NormalTok{x }\OperatorTok{=}\NormalTok{ generate.make_zipf(...)}
\end{Highlighting}
\end{Shaded}

or

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ .generate }\ImportTok{import}\NormalTok{ make_zipf}

\NormalTok{...}
\NormalTok{x }\OperatorTok{=}\NormalTok{ make_zipf(...)}
\end{Highlighting}
\end{Shaded}

either would work. Let's use the second to write \texttt{zipf/check.py}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\ImportTok{from}\NormalTok{ pytest }\ImportTok{import}\NormalTok{ approx}
\ImportTok{from}\NormalTok{ .generate }\ImportTok{import}\NormalTok{ make_zipf}


\NormalTok{USAGE }\OperatorTok{=} \StringTok{'''zipf num [num...]: are the given values Zipfy?'''}
\NormalTok{RELATIVE_ERROR }\OperatorTok{=} \FloatTok{0.05}


\KeywordTok{def}\NormalTok{ is_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(hist) }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Cannot test Zipfiness without data'}
\NormalTok{    scaled }\OperatorTok{=}\NormalTok{ [h}\OperatorTok{/}\NormalTok{hist[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ h }\KeywordTok{in}\NormalTok{ hist]}
\NormalTok{    perfect }\OperatorTok{=}\NormalTok{ make_zipf(}\BuiltInTok{len}\NormalTok{(hist))}
    \ControlFlowTok{return}\NormalTok{ scaled }\OperatorTok{==}\NormalTok{ approx(perfect, rel}\OperatorTok{=}\NormalTok{rel)}


\KeywordTok{def}\NormalTok{ is_unsorted_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
\NormalTok{    ...}

\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(sys.argv) }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(USAGE)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        values }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(a) }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ sys.argv[}\DecValTok{1}\NormalTok{:]]}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ is_zipf(values)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{\}}\StringTok{: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(result, values))}
\NormalTok{    sys.exit(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we just need to update our tests; outside of the package,
we refer to \texttt{zipfpy.generate} and \texttt{zipfpy.check} as we did
before with \texttt{zipfpy.zipf}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ zipfpy.generate}
\ImportTok{import}\NormalTok{ zipfpy.check}

\KeywordTok{def}\NormalTok{ test_default_tolerance(length}\OperatorTok{=}\DecValTok{5}\NormalTok{):}
\NormalTok{    generated }\OperatorTok{=}\NormalTok{ zipf.generate.make_zipf(length)}
    \ControlFlowTok{assert}\NormalTok{ zipf.check.is_zipf(generated)}

\NormalTok{...}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercise-factor-out-the-package-wide-constant-relative_error}{%
\subsection{\texorpdfstring{Exercise: Factor out the package-wide constant \texttt{RELATIVE\_ERROR}}{Exercise: Factor out the package-wide constant RELATIVE\_ERROR}}\label{exercise-factor-out-the-package-wide-constant-relative_error}}

It's perfectly valid for an \texttt{\_\_init\_\_.py} file to be empty,
but it is also a very useful place to put definitions that are
common to the entire package.

Here, both \texttt{generate} and \texttt{check} use the same default \texttt{RELATIVE\_ERROR};
let's move that into \texttt{\_\_init\_\_.py}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add the line \texttt{RELATIVE\_ERROR\ =\ 0.05} to \texttt{\_\_init\_\_.py};
\item
  Now we can remove the line definining \texttt{RELATIVE\_ERROR} from \texttt{check.py}
  and \texttt{generate.py}, but;
\item
  We still need to import that definition somehow, or else those functions
  won't know what a \texttt{RELATIVE\_ERROR} is; add the line \texttt{from\ .\ import\ RELATIVE\_ERROR}
  to each so that it is visible. When we don't specify a file, it imports the
  definition from \texttt{\_\_init\_\_.py} (just as \texttt{import\ zipfpy} will read the \texttt{\_\_init\_\_.py})
\item
  Run the test suite with \texttt{pytest}
\end{enumerate}

We haven't shortened the code any here---we've replaced a \texttt{RELATIVE\_ERROR\ =\ 0.05}
with an \texttt{from\ .\ import\ RELATIVE\_ERROR}---but now the constant is defined in only
one location so we don't have to worry about editing it in one location and that
value being out of sync in the other.

\hypertarget{exercise-importing-from-within-init.py}{%
\subsection{\texorpdfstring{Exercise: Importing from within \textbf{init}.py}{Exercise: Importing from within init.py}}\label{exercise-importing-from-within-init.py}}

Since any code that does an \texttt{import\ zipf} will read \texttt{\_\_init\_\_.py}, it
can be convenient to put import statments in \texttt{\_\_init\_\_.py} itself, to
ensure that some key definitions are readily available when the package
is imported.

We'll try that here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add the lines \texttt{from\ .generate\ import\ *} and \texttt{from\ .check\ import\ *} to
  \texttt{zipfpy/\_\_init\_\_.py}.
\item
  In the test suite, you can now just call \texttt{import\ zipfpy} and refer to,
  \emph{e.g.}, \texttt{zipfpy.make\_zipf(...)} instead of \texttt{zipfpy.generate\_make\_zipf(...)}.
  Make those changes
\item
  Run the test suite with \texttt{pytest}
\end{enumerate}

Note that it doesn't make sense to import absolutely everything like we
have here into the \texttt{zipfpy} namespace---it kind of defeats the purpose
of having split up the files---but for a modest number of key definitions
this can be very convenient for the package users.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Undo the changes you made in 1. and 2., and make sure the test suite
  still runs.
\end{enumerate}

\hypertarget{rse-package-py-install}{%
\section{What does it mean to install a Python package?}\label{rse-package-py-install}}

By the end of this chapter, you'll have made and published a Python
package that anyone with an internet connection can install. You've
certainly installed Python packages before, such as with \texttt{pip\ install};
the \texttt{pip} tool is the most common way to install Python packages.
The command pip install package
checks to see if the package is already installed (or needs to be upgraded);
if so,
it downloads the package from \href{https://pypi.org/}{PyPI} (the Python Package Index),
unpacks it,
and installs it.

But what does it mean to install a Python package?

You've seen that when you have a Python module or package in the current
directory named, say, \texttt{zipfpy}, Python will successfully find that module
and directory when it runs \texttt{import\ zipfpy}. But clearly having all of the
Python standard library, all installed packages, and our own code in the
same directory gets unwieldly quickly - especially if we want to run our
routines from a directory full of data!

Just as the \texttt{PATH} environment in the shell contains a list of directories that
the shell searches for programs it can execute, in Python a variable \texttt{sys.path}
(in the system-standard \texttt{sys} package) contains a list of the directories to
search. Create a Python file \texttt{showpath.py} containing the following, and
then run it with \texttt{python\ showpath.py}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}

\ControlFlowTok{for}\NormalTok{ directory }\KeywordTok{in}\NormalTok{ sys.path:}
    \BuiltInTok{print}\NormalTok{(directory)}
\end{Highlighting}
\end{Shaded}

If you are using the Anaconda distribution of Python and you run this script
from your desktop, you will see a list that looks like this:

\begin{verbatim}
/Users/pterry/Desktop
/Users/pterry/anaconda3/lib/python36.zip
/Users/pterry/anaconda3/lib/python3.6
/Users/pterry/anaconda3/lib/python3.6/lib-dynload
/Users/pterry/anaconda3/lib/python3.6/site-packages
/Users/pterry/anaconda3/lib/python3.6/site-packages/aeosa
\end{verbatim}

the first will be your current directory - a Python environment
always looks there first - and the rest are system paths for
the Python installation you are using.

To install a package, then means to copy the contents of the
package in some form into one of the directories that Python
will search from.

\hypertarget{exercise-find-a-package-in-pip-list}{%
\subsection{Exercise: Find a package in pip list}\label{exercise-find-a-package-in-pip-list}}

The \texttt{pip\ list} command will generate a list of Python packages
installed using pip in the current environment. Here we'll find
a package that we've recently installed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run \texttt{pip\ list}, and from the list pick something that you've
  installed recently; if you can't think of anything, choose \texttt{pytest}.
\item
  Run the \texttt{showpath.py} script to show the list of paths the package could be in.
\item
  Search the paths in 2. for files corresponding to the package you choose in 1.;
  \texttt{site-packages} is normally a good place to start looking.
\end{enumerate}

\hypertarget{rse-package-py-virtualenv}{%
\section{How can I have only the packages my projects need?}\label{rse-package-py-virtualenv}}

It can be convenient to have several different Python environments that
you can switch between. One reason is that you're probably working on
multiple projects at any given time, each with different requirements
for Python packages, and you want to keep them straight.

Another reason concerns us a bit more immediately. We want to make sure
that other people can successfully install and use our package. That
means:

\begin{itemize}
\tightlist
\item
  We want to be able to easily test install and uninstall our package, without affecting
  the entire Python environment; and
\item
  We want to avoid having to answer problems people have with your package with
  something more helpful than ``I don't know, it works for me'', by making sure we
  can install and run our package in a completely empty environment, so we can tell
  that we're not accidentlly relying on some other package being installed.
\end{itemize}

A very handy answer to both of those needs is a \href{glossary.html\#virtual-environment}{virtual environment}.
A virtual environment is a layer on top of an existing Python installation.
Whenever Python needs to find a library,
This gives us a place to install packages that only some projects need,
or that are still under development,
without affecting the main installation (Figure~\ref{fig:rse-package-py-virtualenv}.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-package-py-virtualenv}Virtual Environments}
\end{figure}

We can create and manage virtual environments using a tool called \texttt{virtualenv}.
To install it,
run \texttt{pip\ install\ virtualenv}.
Once we have done that,
we can create a new virtual environment called \texttt{test} by running:

\begin{verbatim}
$ virtualenv test
\end{verbatim}

\begin{verbatim}
Using base prefix '/Users/pterry/anaconda3'
New python executable in /Users/pterry/test/bin/python
Installing setuptools, pip, wheel...
done.
\end{verbatim}

\texttt{virtualenv} creates a new directory called \texttt{test},
which contains subdirectories called \texttt{bin}, \texttt{lib}, and so on---everything
needed for a minimal Python installation.
Crucially,
\texttt{test/bin/python} checks for packages in \texttt{test/lib} \emph{before} checking the system-wide install.

We can switch to the \texttt{test} environment by running:

\begin{verbatim}
$ source test/bin/activate
\end{verbatim}

\texttt{source} is a Unix shell command meaning ``run all the commands from a file in this currently-active shell''.
We use it because typing \texttt{test/bin/activate} on its own would run those commands in a sub-shell,
which would have no effect on the shell we're in.
Once we have done this,
we're running the Python interpreter in \texttt{test/bin}:

\begin{verbatim}
$ which python
\end{verbatim}

\begin{verbatim}
/Users/pterry/test/bin/python
\end{verbatim}

We can now install packages to our heart's delight.
Everything we install will go under \texttt{test},
and won't affect the underlying Python installation.
When we're done,
we can switch back to the default environment with \texttt{deactivate}.
(We don't need to \texttt{source} this.)

Many developers create a directory called \texttt{\textasciitilde{}/envs}
(i.e., a directory called \texttt{envs} directly below their home directory)
to store their virtual environments:

\begin{verbatim}
$ cd ~
$ mkdir envs
$ which python
\end{verbatim}

\begin{verbatim}
/Users/pterry/anaconda3/bin/python
\end{verbatim}

\begin{verbatim}
$ virtualenv envs/test
\end{verbatim}

\begin{verbatim}
Using base prefix '/Users/pterry/anaconda3'
New python executable in /Users/pterry/envs/test/bin/python
Installing setuptools, pip, wheel...done.
\end{verbatim}

\begin{verbatim}
$ which python
\end{verbatim}

\begin{verbatim}
/Users/pterry/anaconda3/bin/python
\end{verbatim}

\begin{verbatim}
$ source envs/test/bin/activate
\end{verbatim}

\begin{verbatim}
(test)
\end{verbatim}

\begin{verbatim}
$ which python
\end{verbatim}

\begin{verbatim}
/Users/pterry/envs/test/bin/python
(test)
\end{verbatim}

\begin{verbatim}
$ deactivate
\end{verbatim}

\begin{verbatim}
$ which python
\end{verbatim}

\begin{verbatim}
/Users/pterry/anaconda3/bin/python
\end{verbatim}

Notice how every command now displays \texttt{(test)} when that virtual environment is active.
Between Git branches and virtual environments,
it can be very easy to lose track of what exactly you're working on and with.
Having prompts like this can make it a little less confusing;
using virtual environment names that match the names of your projects
(and branches, if you're testing different environments on different branches)
quickly becomes essential.

\hypertarget{exercises-examining-the-virtual-environment}{%
\subsection{Exercises: Examining the virtual environment}\label{exercises-examining-the-virtual-environment}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If you haven't already, create a \texttt{test} virtual environment and activate it. Your prompt should now start with \texttt{(test)}.
\item
  Run \texttt{pip\ list}. What packages are installed?
\item
  Run the \texttt{showpath.py} script from the earlier section, and note that the paths are different from your
  base environment where you looked for packages. They probably look like \texttt{/Users/pterry/envs/test/lib/python-3.7/site-packages}.
\item
  Pick a package you know you have installed in your base environment---pick \texttt{pytest} if you can't think of one---and
  try to import it in a Python interpreter. Can it be found?
\item
  Deactive the environment and try again.
\end{enumerate}

\hypertarget{exercises-installing-a-package-into-the-virtual-environment}{%
\subsection{Exercises: Installing a package into the virtual environment}\label{exercises-installing-a-package-into-the-virtual-environment}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Re-activate the \texttt{test} virtual environment from above, and if necessary,
  re-run the \texttt{showpath.py} script.
\item
  Let's install a simple package, \texttt{click}; it's a commonly used package for
  creating simple command-line tools, but for our purposes it's just a small
  self-contained package. Run \texttt{pip\ install\ click}.
\item
  Search in the directories output from \texttt{showpath.py} for a package named \texttt{click};
  it will be a directory containing an \texttt{\_\_init\_\_.py}.
\item
  Start a Python interpreter, and run \texttt{import\ click}; it is installed in the environment.
\item
  From that same interpretter, run \texttt{print(click.path)}; that is the path to the click package.
  It should report the same directory that you found it in.
\item
  Exit the Python interpretter
\end{enumerate}

Now let's just clarify that the environment is different than the directory you
installed the virtual environment contents in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start another terminal, and go to the directory containing the \texttt{test} virtual environment contents.
\item
  Run the \texttt{showpath.py} script - does it show the directories in the base environment or the virtual environment?
\item
  Start a Python interpretter, and try \texttt{import\ click}.
\item
  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    If you didn't have \texttt{click} installed in the base environment, this will fail.
  \item
    If you did have \texttt{click} installed in the base environment, this will be successful, but the click path will be different: \texttt{print(click.\_\_path\_\_)} to verify.
  \end{enumerate}
\item
  Exit the Python interpreter, and deactivate the \texttt{test} environment in the other terminal.
\end{enumerate}

\hypertarget{rse-package-py-package}{%
\section{How can I create an installable Python package?}\label{rse-package-py-package}}

People can always get your package by cloning your repository and copying files from that
(assuming your repository is accessible,
which is should be for published research),
but it's much friendlier to create something they can install
(and it will be easier for us when it comes to running test suites
and distributing, too!)

For historical reasons,
Python has several ways to build an installable package.
We will show how to use \href{https://setuptools.readthedocs.io/}{setuptools},
which is the lowest common denominator;
\href{https://conda.io/}{conda} is a modern does-everything solution,
but the setuptools approach we describe below will
allow everyone, regardless of what python distribution
they use, to easily use your package.

To use \texttt{setuptools},
we must create a file called \texttt{setup.py} in the directory \emph{above} the root directory of the package:

\begin{verbatim}
+- setup.py
+- test_zipfpy.py
+- zipfpy
    +- __init__.py
    +- check.py
    +- generate.py
\end{verbatim}

The file \texttt{setup.py} must have exactly that name,
and must contain these lines:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ setuptools }\ImportTok{import}\NormalTok{ setup}

\NormalTok{setup(}
\NormalTok{    name}\OperatorTok{=}\StringTok{'zipfpy'}\NormalTok{,}
\NormalTok{    version}\OperatorTok{=}\StringTok{'0.1'}\NormalTok{,}
\NormalTok{    author}\OperatorTok{=}\StringTok{'Greg Wilson'}\NormalTok{,}
\NormalTok{    packages}\OperatorTok{=}\NormalTok{[}\StringTok{'zipfpy'}\NormalTok{]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is enough to be able to install the package from the
local copy source code using standard tools!

Let's create a virtual environment to test the installation:

\begin{verbatim}
$ virtualenv ~/envs/test_zipfpy
...
$ source ~/envs/test_zipfpy/bin/activate
(test_zipfpy) $ pip install .
Processing /Users/pterry/zipfpy/
Building wheels for collected packages: zipfpy
  Building wheel for zipfpy (setup.py) ... done
  Stored in directory: /private/var/folders/44/[etc]
Successfully built zipfpy
Installing collected packages: zipfpy
Successfully installed zipfpy-0.1
\end{verbatim}

Success! If you take a look in \texttt{test\_zipfpy/lib/python3.7/site-packages/}, you will
now see the package with all the other site packages.

We can generate Zipf-distributed data to our hearts content:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>>>} \ImportTok{import}\NormalTok{ zipfpy}
\OperatorTok{>>>} \ImportTok{import}\NormalTok{ zipfpy.generate}
\OperatorTok{>>>}\NormalTok{ zipf_out }\OperatorTok{=}\NormalTok{ zipfpy.generate.make_zipf(}\DecValTok{5}\NormalTok{)}
\OperatorTok{>>>} \BuiltInTok{print}\NormalTok{(zipf_out)}
\NormalTok{[}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.3333333333333333}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

But if we continue and try to check the output, we'll find a problem:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>>>} \ImportTok{import}\NormalTok{ zipfpy.check}
\NormalTok{Traceback (most recent call last):}
\NormalTok{  File }\StringTok{"<stdin>"}\NormalTok{, line }\DecValTok{1}\NormalTok{, }\KeywordTok{in} \OperatorTok{<}\NormalTok{module}\OperatorTok{>}
\NormalTok{  File }\StringTok{"/Users/pterry/zipfpy/zipfpy/check.py"}\NormalTok{, line }\DecValTok{2}\NormalTok{, }\KeywordTok{in} \OperatorTok{<}\NormalTok{module}\OperatorTok{>}
    \ImportTok{from}\NormalTok{ pytest }\ImportTok{import}\NormalTok{ approx}
\NormalTok{ModuleNotFoundError: No module named }\StringTok{'pytest'}
\end{Highlighting}
\end{Shaded}

Ah ha - we're missing a dependency. We use \texttt{pytest} for approximate
value comparison. How best should we document this requirement?

Since a project may depend on many packages,
developers frequently put a list of those dependencies in a file called \texttt{requirements.txt}.
\texttt{pip\ install\ -r\ requirements.txt} will then install the dependencies listed in that file.
(The file can be called anything,
but everyone uses \texttt{requirements.txt},
so you should too.)
This file can just list package names,
or it can specify exact versions, minimum versions, etc.:

\begin{verbatim}
request
scipy==1.1.0
tdda>=1.0
\end{verbatim}

If you want to create a file like this,
\texttt{pip\ freeze} will print the exact versions of all installed packages.
\texttt{pip\ freeze} provides the same output as \texttt{pip\ list}, but in less human-reader-friendly
way and a more easily parsed by computer program format.

So let's install the dependency, ensure that it works, and then create the
\texttt{requirements.txt} file:

\begin{verbatim}
(test_zipfpy) $ pip uninstall zipfpy
(test_zipfpy) $ pip install pytest
Collecting pytest
...[stuff omitted]...
Installing collected packages: py, zipp, importlib-metadata, pluggy, wcwidth, pyparsing, six, packaging, attrs, more-itertools, atomicwrites, pytest
Successfully installed atomicwrites-1.3.0 attrs-19.1.0 importlib-metadata-0.18 more-itertools-7.0.0 packaging-19.0 pluggy-0.12.0 py-1.8.0 pyparsing-2.4.0 pytest-4.6.3 six-1.12.0 wcwidth-0.1.7 zipp-0.5.1
(test_zipfpy) $ pip freeze > requirements.txt
(test_zipfpy) $ cat requirements.txt
atomicwrites==1.3.0
attrs==19.1.0
importlib-metadata==0.18
more-itertools==7.0.0
packaging==19.0
pluggy==0.12.0
py==1.8.0
pyparsing==2.4.0
pytest==4.6.3
six==1.12.0
wcwidth==0.1.7
zipp==0.5.1
\end{verbatim}

(Note that the version numbers, current at the time of writing, will change).

Now we should be able to import \texttt{zipfpy.check}:

\begin{verbatim}
(test_zipfpy) $ pip install .
...
(test_zipfpy) $ python
Python 3.7.2 (default, Mar  9 2019, 23:21:06)
[Clang 10.0.0 (clang-1000.11.45.5)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import zipfpy.check
>>>
\end{verbatim}

Great! Let's make sure it works from scratch. Let's deactivate and
completely delete the previous environment and try again:

\begin{verbatim}
(test_zipfpy) $ deactivate
$ rm -r test_zipfpy
$ virtualenv test_zipfpy_2
$ source test_zipfpy_2/bin/activate
(test_zipfpy_2) $ pip install -r requirements.txt
Installing collected packages: atomicwrites, attrs, zipp, importlib-metadata, more-itertools, pyparsing, six, packaging, pluggy, py, wcwidth, pytest
Successfully installed atomicwrites-1.3.0 attrs-19.1.0 importlib-metadata-0.18 more-itertools-7.0.0 packaging-19.0 pluggy-0.12.0 py-1.8.0 pyparsing-2.4.0 pytest-4.6.3 six-1.12.0 wcwidth-0.1.7 zipp-0.5.1
(test_zipfpy_2) $ pip install .
Processing ...
Building wheels for collected packages: zipfpy
  Building wheel for zipfpy (setup.py) ... done
  Stored in directory: /private/var/folders/44/k2tfp8d12h7ggxc0xz8phfmr0000gn/T/pip-ephem-wheel-cache-ab2atma5/wheels/4b/65/6c/557b5122e27983111d3410a06ef63b7b0f3b848df218bb28ed
Successfully built zipfpy
Installing collected packages: zipfpy
Successfully installed zipfpy-0.1
(test_zipfpy_2) $ python -m pytest
=============================================================== test session starts ================================================================
platform darwin -- Python 3.7.2, pytest-4.6.3, py-1.8.0, pluggy-0.12.0
rootdir: /Users/pterry/Desktop/zipfpy
collected 3 items

test_zipf.py ...                                                                                                                             [100%]

============================================================= 3 passed in 0.02 seconds =============================================================
(test_zipfpy_2)
\end{verbatim}

Success!

\hypertarget{rse-package-py-distrib-scripts}{%
\section{How can I distribute command-line scripts in my Python package?}\label{rse-package-py-distrib-scripts}}

Our package now works and can be installed in Python environments.

One downside of our packaging is that we've now buried the useful command-line
script that is now currently sits in \texttt{zipfpy/check.py}; we used to be able to
run that script as \texttt{python\ zipf.py\ 100\ 50\ 33} to test if a set of counts approximately
matched a zipf distribution:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\NormalTok{...}

\NormalTok{USAGE }\OperatorTok{=} \StringTok{'''zipf num [num...]: are the given values Zipfy?'''}

\NormalTok{...}
\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(sys.argv) }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(USAGE)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        values }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(a) }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ sys.argv[}\DecValTok{1}\NormalTok{:]]}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ is_zipf(values)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{\}}\StringTok{: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(result, values))}
\NormalTok{    sys.exit(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{setuptools} package allows us to install not only the Python
package itself, but also associated scripts that it places with
other executable files. So let's split that command line utility
out into its own tool, \texttt{bin/check\_zipf.py}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env python}
\ImportTok{import}\NormalTok{ sys}
\ImportTok{import}\NormalTok{ zipfpy.check}

\NormalTok{USAGE }\OperatorTok{=} \StringTok{'''zipf num [num...]: are the given values Zipfy?'''}

\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{'__main__'}\NormalTok{:}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(sys.argv) }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(USAGE)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        values }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(a) }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ sys.argv[}\DecValTok{1}\NormalTok{:]]}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ zipfpy.check.is_zipf(values)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{\}}\StringTok{: }\SpecialCharTok{\{\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(result, values))}
\NormalTok{    sys.exit(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That means that the \texttt{zipfpy/check.py} file now contains only
the routines \texttt{is\_zipf()} and \texttt{is\_unsorted\_zipf()}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pytest }\ImportTok{import}\NormalTok{ approx}
\ImportTok{from}\NormalTok{ .generate }\ImportTok{import}\NormalTok{ make_zipf}
\ImportTok{from}\NormalTok{ . }\ImportTok{import}\NormalTok{ RELATIVE_ERROR}


\KeywordTok{def}\NormalTok{ is_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(hist) }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Cannot test Zipfiness without data'}
\NormalTok{    scaled }\OperatorTok{=}\NormalTok{ [h}\OperatorTok{/}\NormalTok{hist[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ h }\KeywordTok{in}\NormalTok{ hist]}
\NormalTok{    perfect }\OperatorTok{=}\NormalTok{ make_zipf(}\BuiltInTok{len}\NormalTok{(hist))}
    \ControlFlowTok{return}\NormalTok{ scaled }\OperatorTok{==}\NormalTok{ approx(perfect, rel}\OperatorTok{=}\NormalTok{rel)}


\KeywordTok{def}\NormalTok{ is_unsorted_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
\NormalTok{    sortedhist }\OperatorTok{=} \BuiltInTok{sorted}\NormalTok{(hist)}
    \ControlFlowTok{return}\NormalTok{ is_zipf(sortedhist)}
\end{Highlighting}
\end{Shaded}

Now we make sure that setuptools knows about this script and
will install it:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ setuptools }\ImportTok{import}\NormalTok{ setup}

\NormalTok{setup(}
\NormalTok{    name}\OperatorTok{=}\StringTok{'zipfpy'}\NormalTok{,}
\NormalTok{    version}\OperatorTok{=}\StringTok{'0.1'}\NormalTok{,}
\NormalTok{    author}\OperatorTok{=}\StringTok{'Greg Wilson'}\NormalTok{,}
\NormalTok{    packages}\OperatorTok{=}\NormalTok{[}\StringTok{'zipfpy'}\NormalTok{],}
\NormalTok{    scripts}\OperatorTok{=}\NormalTok{[}\StringTok{'bin/check_zipf.py'}\NormalTok{]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we we'll install the updated package in a virtual environment:

\begin{verbatim}
coredump-2:08 pterry$ virtualenv test_script
..
Installing setuptools, pip, wheel...
done.
$ source test_script/bin/activate
(test_script) $ pip install -r requirements.txt
...
Installing collected packages: atomicwrites, attrs, zipp, importlib-metadata, more-itertools, six, pyparsing, packaging, pluggy, py, wcwidth, pytest
pip Successfully installed atomicwrites-1.3.0 attrs-19.1.0 importlib-metadata-0.18 more-itertools-7.0.0 packaging-19.0 pluggy-0.12.0 py-1.8.0 pyparsing-2.4.0 pytest-4.6.3 six-1.12.0 wcwidth-0.1.7 zipp-0.5.1
ins
(test_script) $ pip install .
...
Building wheels for collected packages: zipfpy
  Building wheel for zipfpy (setup.py) ... done
  Stored in directory: /private/var/folders/44/k2tfp8d12h7ggxc0xz8phfmr0000gn/T/pip-ephem-wheel-cache-0z_4yd1b/wheels/1c/84/f1/3dedca5ae7f979ead1ca046473b4a1f13552e873ccf1c1d958
Successfully built zipfpy
Installing collected packages: zipfpy
Successfully installed zipfpy-0.1

(test_script) $ check_zipf.py 100 50 33 25
True: [100, 50, 33, 25]
\end{verbatim}

\hypertarget{rse-package-py-document}{%
\section{How should I document my package for others?}\label{rse-package-py-document}}

As we get our package ready for sharing with others, we should
make sure there's enough documentation written for our intended
initial users to successfully install the software and begin to
use key functionality.

Ideally we've been maintaining documentation throughout the development
process. But the documentation we keep through that process is often
intended for a different audience and purpose---that is, it's meant for
us while developing the code, and so includes things that are good and
useful like code comments reminding us why steps are taken, but may
not include overviews of the package and how (and why!) to use it, which
we largely take for granted.

We should certainly have a README for the package, describing how to
install the package and how to use it in a couple of key use cases,
so let's add that.

In the Python community, much documentation is written in
\href{glossary.html\#restructured-text}{reStructuredText}, which is
a markup format for plain text documents which can be rendered
into documents with quite complex indexes and cross-linking
by many documentation tools; GitHub will recognize reST files
ending in \texttt{.rst} and display them nicely. We'll use that here;
titles are underlined and overlined, section headings are underlined,
and code blocks are set off with two colons (\texttt{::}) and indented.
So let's create a \texttt{README.rst}:

\begin{verbatim}
======
Zipfpy
======

This python package provides routines for generating lists of counts
that follow (exactly or approximately) a Zipf distribution, and for
testing whether a counts distribution does or doesn't follow such
a distribution.

Installation
------------

To install, clone this repository, change into the repository directory
and run the commands::

    pip install -r requirements.txt
    pip install .

Use
---

You can use the package after installation with `import zipfpy`, with functions
in the modules `check` and `generate`.

You can also use the command line tool `check_zipf.py` which tests to see if
a provided list of countsfollows a Zipf distribution::

    $ check_zipf.py 100 50 33 25
    True: [100, 50, 33, 25]

Authors
-------

Terry Pratchett
\end{verbatim}

In addition to the README, we should also make sure that there are
\protect\hyperlink{docs-docstrings}{docstrings} written for the package as a whole
(in the \texttt{\_\_init\_\_.py}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{"""}
\CommentTok{The zipfpy package contains tests of distributions of counts to see if}
\CommentTok{they follow a Zipf distribution (https://en.wikipedia.org/wiki/Zipf%27s_law)}
\CommentTok{using the routines in the check module, and routines for generating}
\CommentTok{lists of counts that follow a Zipf distribution in the generate module.}
\CommentTok{"""}
\NormalTok{RELATIVE_ERROR }\OperatorTok{=} \FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

and for key routines like \texttt{is\_zipf} and \texttt{make\_zipf}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ is_zipf(hist, rel}\OperatorTok{=}\NormalTok{RELATIVE_ERROR):}
    \CommentTok{"""Tests if a histogram of counts follows a Zipf distribution.}

\CommentTok{    Given a list of counts as hist, assumed sorted in decreasing order,}
\CommentTok{    and a relative error tolerance (if not provided, the default value}
\CommentTok{    zipfpy.RELATIVE_ERROR is used), tests to see if the counts follow}
\CommentTok{    a Zipf distribution.}

\CommentTok{    Args:}
\CommentTok{        hist: an list or other iterable containing a list of numeric counts}
\CommentTok{        rel: the relative error tolerance used if provided; if not,}
\CommentTok{             the package default is used.}

\CommentTok{    Returns:}
\CommentTok{        True if the list of counts follows a zipf distribution within}
\CommentTok{        the relative tolerance.  False otherwise.}

\CommentTok{    Raises:}
\CommentTok{        AssertionError: raised if an empty list is passed.}
\CommentTok{    """}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(hist) }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Cannot test Zipfiness without data'}
\NormalTok{    scaled }\OperatorTok{=}\NormalTok{ [h}\OperatorTok{/}\NormalTok{hist[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ h }\KeywordTok{in}\NormalTok{ hist]}
\NormalTok{    perfect }\OperatorTok{=}\NormalTok{ make_zipf(}\BuiltInTok{len}\NormalTok{(hist))}
    \ControlFlowTok{return}\NormalTok{ scaled }\OperatorTok{==}\NormalTok{ approx(perfect, rel}\OperatorTok{=}\NormalTok{rel)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ make_zipf(length):}
    \CommentTok{"""Returns a list of counts that follows a Zipf distribution.}

\CommentTok{    Args:}
\CommentTok{        length: the number of counts to be generated}

\CommentTok{    Returns:}
\CommentTok{        A list of the provided length of floating point numbers corresponding}
\CommentTok{        exactly the zipf distribution.  For example, for length=5:}

\CommentTok{        [1.0, 0.5, 0.3333333333333333, 0.25, 0.2]}

\CommentTok{    Raises:}
\CommentTok{        AssertionError: raised if a zero or negative length is provided}
\CommentTok{    """}
    \ControlFlowTok{assert}\NormalTok{ length }\OperatorTok{>} \DecValTok{0}\NormalTok{, }\StringTok{'Zipf distribution must have at least one element'}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\NormalTok{ i) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(length)]}
    \ControlFlowTok{return}\NormalTok{ result}
\end{Highlighting}
\end{Shaded}

Not only is the documentation then useful for those reading the
code, but it helpfully shows up when a user types, for instance,
\texttt{help(zipfpy)}.

\hypertarget{rse-package-py-sphinx}{%
\section{How do I generate web pages of documentation for my package?}\label{rse-package-py-sphinx}}

When you've viewed documentation for large Python packages, they
have very likely been prepared using \href{http://www.sphinx-doc.org/en/master/}{Sphinx},
a package for writing software documentation and generating web packages
for that documentation that is widely used in the Python community.
When combined with \href{https://readthedocs.org}{Read The Docs}, a free service for
hosting that online documentation that includes tools to automatically update
documentation when your package and its documentation changes, and that understands
documents written to support Sphinx, it is an extremely useful way to
build both documentation and an online home for your package. Both tools
recognize reStructuredText files.

We'll start by putting together a minimal set of documentation that includes
your \texttt{README.rst} and the docstrings for your package. Sphinx makes it
relatively straightforwrd to get started, using the \texttt{quickstart} tool. We'll
create doucmentation in a \texttt{docs/} directory at the top of our repository:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{mkdir}\NormalTok{ docs}
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ docs}
\NormalTok{$ }\ExtensionTok{sphinx-quickstart}
\end{Highlighting}
\end{Shaded}

We'll be asked to specify the project's name, our (the author's) name, and
a release. Otherwise we can mostly use the defaults, but we'll want
to use specfiy that we do want autodoc to automatically insert our docstrings:

\begin{verbatim}
...
The project name will occur in several places in the built documentation.
> Project name: zipfpy
> Author name(s): Terry Pratchett
> Project release []: 1.0
...
> autodoc: automatically insert docstrings from modules (y/n) [n]: y
...
\end{verbatim}

Once that is complete, there will be a file called \texttt{conf.py} in the docs
directory that configures Sphinx. We'll make one change to that, so that
autodoc can find our module (and its docstrings).

First, we'll set the ``Path setup'' section, which starts very near the head
of the file:

\begin{verbatim}
# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
\end{verbatim}

Here we those extensions are in another directory, so we'll uncomment those
lines below and add another pointing up one level from \texttt{docs/} so that we have:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ sys}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, os.path.abspath(}\StringTok{'./'}\NormalTok{))}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, os.path.abspath(}\StringTok{'../'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

With that done, we can generate run a sphinx autodoc script which will read
the docstrings from our package and include them into .rst files in the
\texttt{docs/source} directory:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{sphinx-apidoc}\NormalTok{ -o source/ ../zipfpy}
\end{Highlighting}
\end{Shaded}

Now we're ready. In the \texttt{/docs} directory there's a Makefile,
generated by \texttt{sphinx-quickstart};
if we run \texttt{make\ html}
and in a web browser open the file \texttt{docs/\_build/index.html} we'll have a very minimal
documentation in a familiar looking form; if you look under \texttt{modules} you will
see the documentation for the individual modules. But that first page is
still a little sparse.

If we add the line \texttt{..\ include::\ ../README.rst} to the \texttt{docs/index.rst} file
at the beginning, so that page starts with our README, and then include a direct
link to the zipfpy package documentation:

\begin{verbatim}
Welcome to Zipfpy's documentation!
==================================

.. include:: ../README.rst

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   zipfpy

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
\end{verbatim}

and rerun \texttt{make\ html}, we now get an updated set of web pages which
re-uses your README to be the introduction to the documentation.

Note that we needed additional packages to build the documentation
here that is not reflected in our \texttt{requirements.txt}; the \texttt{Sphinx}
package and its dependencies.

They aren't needed for running, developing, or even testing
the package, but it is needed for building the documentation.
To note this requirement, but without requiring everyone installing
the package to have \texttt{Sphinx}
installed, let's create a \texttt{requirements\_docs.txt} file that contains
only those things (where versions come from \texttt{pip\ freeze}):

\begin{verbatim}
Sphinx>=1.7.4
\end{verbatim}

Then anyone wanting to build the documentation (including you, on another computer)
need only run \texttt{pip\ install\ -r\ requirement\_docs.txt}

This breakdown of requirements by use case is often seen with packages
that have a \texttt{requirements\_dev.txt} for additional packaged not needed
for users of the package, but for its development and testing. For this
package, we could usefully pull out \texttt{pytest} from our \texttt{requirements.txt}
into a \texttt{requirements\_dev.txt}

\hypertarget{rse-package-py-rtd}{%
\section{How do I put the documentation for my package online?}\label{rse-package-py-rtd}}

There are a number of ways of hosting the web pages that document
your project. A very common option, particularly for Python
projects, is \href{https://readthedocs.org}{Read The Docs}, a community
supported site which hosts software documentation free of charge.

Read The Docs integrates nicely with GitHub, which will allow
you to have the documentation re-built automatically upon update
of your repository. Thus we recommend
\href{https://readthedocs.org/accounts/signup/}{registering for Read The Docs} with your GitHub
account.

Once you have registered for Read The Docs, confirmed your email, and signed in,
you will be able to add new projects by \href{https://readthedocs.org/dashboard/import/?}{importing a project}
from your github repository; Read The Docs will then build the documentation
(\emph{e.g.}, run make) and host the resulting files. For this to work, all of
the source files (in our case, \texttt{docs/source/modules.rst}, \texttt{docs/source/zipfpy.rst},
\texttt{docs/Makefile}, \texttt{docs.conf.py}) need to be checked into your repository.

Then you should be able to build; after a few minutes, your documents should
be visible! If you named your project, \emph{e.g.}, \texttt{pterry\_zipfpy}, your
documents will be available at
\href{https://pterry_zipfpy.readthedocs.io/en/latest/}{\texttt{https://pterry\_zipfpy.readthedocs.io/en/latest/}}.

As with \href{integrate.html\#integrate-basic}{continuous integration with Travis-CI},
GitHub works with ReadTheDocs so that if you have set up your ReadTheDocs project
with your GitHub account, changes to the documentation will automatically result
in the online documentation being rebuilt.

Also as with continuous integration testing, there are badges for your README linking
both to the documentation and reporting on the status of the documentation build;
you would be able to add that badge and link by adding the following line to your
\texttt{README.rst}:

\begin{verbatim}
.. image:: https://pterry_zipfpy.readthedocs.io/en/latest/?badge=latest
    :target: https://readthedocs.org/projects/pterry_zipfpy/badge/?version=latest
    :alt: Documentation Status
\end{verbatim}

Note the syntax for adding an image to a reST file; \texttt{..\ image::} followed by the link
to the image, and additional, optional fields on indented lines, such as \texttt{:target:} for
a link to follow when the image is clicked, and \texttt{:alt:} for alternate text for the image.

\hypertarget{rse-package-py-distribute}{%
\section{How can I distribute software packages that I have created?}\label{rse-package-py-distribute}}

With our package working, testable, installable, and documented,
we should distribute it so anyone can just \texttt{pip\ install\ pterry\_zipfpy}
and start running!

\texttt{pip} installs packages either from a source distribution (an \texttt{sdist} in
Python packaging jargon) or a binary distribution called a wheel (\texttt{bdist\_wheel}).
We can use setuptools to generate both of these:

\begin{verbatim}
$ python setup.py sdist bdist_wheels
\end{verbatim}

\begin{verbatim}
running sdist
running egg_info
creating zipfpy.egg-info
writing zipfpy.egg-info/PKG-INFO
writing dependency_links to zipfpy.egg-info/dependency_links.txt
...
warning: check: missing required meta-data: url
warning: check: missing meta-data: if 'author' supplied, 'author_email' must be supplied too

creating zipfpy-0.1
creating zipfpy-0.1/bin
creating zipfpy-0.1/zipfpy
...
creating dist
Creating tar archive
removing 'zipfpy-0.1' (and everything under it)

running bdist_wheel
running build
running build_py
creating build
creating build/lib
creating build/lib/zipfpy
...
adding 'zipfpy/__init__.py'
adding 'zipfpy/check.py'
adding 'zipfpy/generate.py'
adding 'zipfpy-0.1.data/scripts/check_zipf.py'
adding 'zipfpy-0.1.dist-info/top_level.txt'
adding 'zipfpy-0.1.dist-info/WHEEL'
adding 'zipfpy-0.1.dist-info/METADATA'
adding 'zipfpy-0.1.dist-info/RECORD'
removing build/bdist.macosx-10.7-x86_64/wheel
\end{verbatim}

We will look at how to clean up the warnings about \texttt{url}, and \texttt{author\_email} in the exercises, as
well as considering other package metadata we should add for our package.

\texttt{python\ setup.py\ sdist} creates a compressed file \texttt{dist/zipf-0.1.tar.gz} that contains the following:

\begin{verbatim}
$ tar ztvf dist/zipf-0.1.tar.gz
\end{verbatim}

\begin{verbatim}
drwxr-xr-x  0 pterry staff       0 Jul 23 16:30 zipfpy-0.1/
-rw-r--r--  0 pterry staff     182 Jul 23 16:30 zipfpy-0.1/PKG-INFO
drwxr-xr-x  0 pterry staff       0 Jul 23 16:30 zipfpy-0.1/bin/
-rwxr-xr-x  0 pterry staff     467 Jul 23 16:24 zipfpy-0.1/bin/check_zipf.py
-rw-r--r--  0 pterry staff     792 Jul 23 16:24 zipfpy-0.1/README.md
drwxr-xr-x  0 pterry staff       0 Jul 23 16:30 zipfpy-0.1/zipfpy.egg-info/
-rw-r--r--  0 pterry staff     182 Jul 23 16:30 zipfpy-0.1/zipfpy.egg-info/PKG-INFO
-rw-r--r--  0 pterry staff     210 Jul 23 16:30 zipfpy-0.1/zipfpy.egg-info/SOURCES.txt
-rw-r--r--  0 pterry staff       7 Jul 23 16:30 zipfpy-0.1/zipfpy.egg-info/top_level.txt
-rw-r--r--  0 pterry staff       1 Jul 23 16:30 zipfpy-0.1/zipfpy.egg-info/dependency_links.txt
-rw-r--r--  0 pterry staff     162 Jul 23 16:24 zipfpy-0.1/setup.py
drwxr-xr-x  0 pterry staff       0 Jul 23 16:30 zipfpy-0.1/zipfpy/
-rw-r--r--  0 pterry staff    1978 Jul 23 16:24 zipfpy-0.1/zipfpy/check.py
-rw-r--r--  0 pterry staff    2022 Jul 23 16:24 zipfpy-0.1/zipfpy/generate.py
-rw-r--r--  0 pterry staff     318 Jul 23 16:24 zipfpy-0.1/zipfpy/__init__.py
-rw-r--r--  0 pterry staff      38 Jul 23 16:30 zipfpy-0.1/setup.cfg
\end{verbatim}

The source files \texttt{\_\_init\_\_.py} and \texttt{generate.py} are in there,
along with the odds and ends that \texttt{pip} will need to install this package properly when the time comes.

Also present in \texttt{dist} is a file \texttt{zipfpy-0.1-py3-none-any.whl}, the ``wheel'' file;
running \texttt{file\ dist/zipfpy-0.1-py3-none-any.whl} will show it to be a zip file. We
can list its contents and see the same source files and some metadata:

\begin{verbatim}
$ unzip -l dist/zipfpy-0.1-py3-none-any.whl
\end{verbatim}

\begin{verbatim}
Archive:  dist/zipfpy-0.1-py3-none-any.whl
  Length      Date    Time    Name
---------  ---------- -----   ----
      318  07-23-2019 20:24   zipfpy/__init__.py
     1978  07-23-2019 20:24   zipfpy/check.py
     2022  07-23-2019 20:24   zipfpy/generate.py
      454  07-23-2019 20:30   zipfpy-0.1.data/scripts/check_zipf.py
        7  07-23-2019 20:30   zipfpy-0.1.dist-info/top_level.txt
       92  07-23-2019 20:30   zipfpy-0.1.dist-info/WHEEL
      172  07-23-2019 20:30   zipfpy-0.1.dist-info/METADATA
      606  07-23-2019 20:30   zipfpy-0.1.dist-info/RECORD
---------                     -------
     5649                     8 files
\end{verbatim}

These distribution files can now be distributed via \texttt{pypi}, the standard Python
repository for Python packages.

There is a \texttt{testpypi} repository that lets us test putting packages
on pypi without having them appear on the standard repository; let's
start with that. Go to \href{https://test.pypi.org}{\texttt{https://test.pypi.org}},
create an account, and confirm your email.

The preferred tool for uploading packages to the PyPI family of repositories
is called \href{https://twine.readthedocs.io/en/latest/}{\texttt{twine}}; we will
install that package:

\begin{verbatim}
$ pip install twine
\end{verbatim}

and now we can upload our distributions:

\begin{verbatim}
$ twine upload --repository-url https://test.pypi.org/legacy/ dist/*
Enter your username: pterry
Enter your passowrd: *********
Uploading zipfpy-0.1-py3-none-any.whl
...
Uploading zipfpy-0.1.tar.gz
...
\end{verbatim}

The \texttt{-\/-repository-url} allows us to specify the test PyPI repository, and we
have now uploaded both types of distribution, allowing users with different
environments to use the wheel distribution if their systems allow it, or
the source distribution otherwise.

We can now test this with a virtual environment and installing with pip:

\begin{verbatim}
$ virtualenv ~/envs/testpypi
Using base prefix '/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7'
New python executable in /Users/pterry/envs/testpypi/bin/python3.7
Also creating executable in /Users/pterry/envs/testpypi/bin/python
sourInstalling setuptools, pip, wheel...
$ source ~/envs/testpypi/bin/activate
(testpypi) $ pip install --index-url https://test.pypi.org/simple/ zipfpy
Looking in indexes: https://test.pypi.org/simple/
Collecting zipfpy
  Using cached https://test-files.pythonhosted.org/packages/f8/ac/d152eed95bff7567954becef22b8fe0f2aae15008fbeb346c69f75b9d2a0/zipfpy-0.1-py3-none-any.whl
Installing collected packages: zipfpy
Successfully installed zipfpy-0.1
(testpypi) $
\end{verbatim}

Success! Once we are happy with how our package appears in testpypi
(including its \href{https://test.pypi.org/project/zipfpy/}{project page}),
we can go through the same process to put it on the main pypi repository.
But first we have a few improvements to make to our setup.py so that
our package webpage is useful and to make it easier for potential users
to find, understand, and install our package.

\hypertarget{exercise-clean-up-warning-messages}{%
\subsection{Exercise: Clean up warning messages}\label{exercise-clean-up-warning-messages}}

\begin{itemize}
\tightlist
\item
  FIXME: clean up warning messages from \texttt{python\ setup.py\ sdist}
\end{itemize}

\hypertarget{exercise-license-longinfo-and-classification-metadata}{%
\subsection{Exercise License, longinfo, and classification metadata}\label{exercise-license-longinfo-and-classification-metadata}}

\begin{itemize}
\tightlist
\item
  FIXME: add additional metadata to setup.py
\end{itemize}

\hypertarget{exercise-requirements-in-setup.py}{%
\subsection{Exercise: Requirements in setup.py}\label{exercise-requirements-in-setup.py}}

\begin{itemize}
\tightlist
\item
  FIXME: add requirements metadata to setup.py
\end{itemize}

\hypertarget{rse-package-py-announce}{%
\section{Announcing Work}\label{rse-package-py-announce}}

FIXME: \url{https://medium.com/indeed-engineering/marketing-for-data-science-a-7-step-go-to-market-plan-for-your-next-data-product-60c034c34d55}

\hypertarget{rse-package-py-summary}{%
\section{Summary}\label{rse-package-py-summary}}

\begin{figure}
\centering
\includegraphics{figures/rse-package-py/concept.pdf}
\caption{\label{fig:p-package-concept}Python Packaging Concept Map}
\end{figure}

\hypertarget{rse-package-py-keypoints}{%
\section{Key Points}\label{rse-package-py-keypoints}}

\begin{itemize}
\tightlist
\item
  A module is simply a file containing Python code; it is executed on \texttt{import}.
\item
  A package named \texttt{mypackage} is a directory named \texttt{mypackage} containing a module with a special name \texttt{\_\_init\_\_.py},
  which may be empty.
\item
  Other modules within the directory are visible after the import.
\item
  A package can contain subpackages.
\item
  Use \texttt{virtualenv} to create a separate virtual environment for each project.
\item
  Use \texttt{pip} to create a distributable package containing your project's software, documentation, and data.
\item
  The default respository for Python packages is \href{https://pypi.org/}{PyPI}
\item
  You can test distributing your package to Pypi using \href{https://test.pypi.org}{TestPyPI}, and when you're ready, publish it to \url{pypi}
\end{itemize}

\hypertarget{rse-correct}{%
\chapter{Correctness}\label{rse-correct}}

We all hope the software we write does what we wrote it to do.
But how can we be sure?
The short is answer is that we can't,
but that as in science,
we can test our expectations against reality to help us decide if we are sure enough.
This chapter explores ways of doing that in both R and Python.

\hypertarget{rse-correct-manual}{%
\section{How can I test functions manually?}\label{rse-correct-manual}}

Suppose we have written a function to get the sign of a number.
In R,
this might be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numSign <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \ControlFlowTok{if}\NormalTok{ (x }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
    \DecValTok{1}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \DecValTok{-1}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

while in Python,
it might be:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ numSign(x):}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{>} \DecValTok{0}\NormalTok{:}
\NormalTok{        out }\OperatorTok{=} \DecValTok{1}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        out }\OperatorTok{=} \DecValTok{-1}
    \ControlFlowTok{return}\NormalTok{ out}
\end{Highlighting}
\end{Shaded}

A simple way to test the function interactively in either language
is to check that it produces the correct value for a few representative cases:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{numSign}\NormalTok{(}\DecValTok{2}\NormalTok{) }\OperatorTok{==}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{numSign}\NormalTok{(}\DecValTok{0}\NormalTok{) }\OperatorTok{==}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{numSign}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{) }\OperatorTok{==}\StringTok{ }\DecValTok{-1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

These tests show that it handles (some) positive and negatives value correctly,
but gives the wrong value for zero
(whose sign should also be 0).

Manual testing is a good place to start,
but when we are creating or modifying a larger function,
we want to be able to re-run our tests with a single command.
That phrase ``with a single command'' suggests that we should write a second function
whose job is to re-run tests for the first.
In R,
this might be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_numSign <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{numSign}\NormalTok{(}\FloatTok{0.1}\NormalTok{) }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{)}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{numSign}\NormalTok{(}\DecValTok{0}\NormalTok{) }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{numSign}\NormalTok{(}\OperatorTok{-}\OtherTok{Inf}\NormalTok{) }\OperatorTok{==}\StringTok{ }\DecValTok{-1}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

As its name suggests,
\texttt{stopifnot} stops the program if its argument is not true.
A Python equivalent could use \texttt{assert} to achieve the same effect:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\KeywordTok{def}\NormalTok{ test_numSign():}
    \ControlFlowTok{assert}\NormalTok{ numSign(}\FloatTok{0.1}\NormalTok{) }\OperatorTok{==} \DecValTok{1}
    \ControlFlowTok{assert}\NormalTok{ numSign(}\DecValTok{0}\NormalTok{) }\OperatorTok{==} \DecValTok{0}
    \ControlFlowTok{assert}\NormalTok{ numSign(math.inf) }\OperatorTok{==} \DecValTok{-1}
\end{Highlighting}
\end{Shaded}

\hypertarget{rse-correct-features}{%
\section{What features do we want a testing framework to have?}\label{rse-correct-features}}

The individual tests in \texttt{test\_numSign} are called \href{glossary.html\#unit-test}{unit tests}
because they test the smallest units of our code.
Writing functions so that we can easily re-run unit tests is a step in the right direction,
but our current approach has two flaws.
First,
if a test fails,
none of the tests that come after it run.
In this case,
for example,
we don't know if \texttt{numSign} handles \texttt{-Inf} correctly or not
because testing stops when \texttt{numSign(0)} gives the wrong answer.
We would have more information if all of the tests ran every time.

The second flaw in \texttt{test\_numSign} is that it doesn't tell us anything if our tests pass.
A one-line summary saying ``3/3 tests passed'' would confirm that the tests actually ran,
while something like ``2/3 tests passed''
followed by a list of the tests that \emph{didn't} so we know where to start investigating
would be even better.

\begin{quote}
\textbf{Too Much Information}

We \emph{don't} want our testing framework to list all of the tests that pass
because if there is too much output,
we will soon start ignoring all of it.
A testing framework should only demand that we pay attention to details
when we actually need to.
\end{quote}

One other requirement for a good set of tests is \href{glossary.html\#test-isolation}{isolation}.
Suppose that we have two functions \texttt{first} and \texttt{second} and we test them in this way:

\begin{verbatim}
first_result = first()
check(first_result)
second_result = second(first_result)
check(second_result)
\end{verbatim}

If \texttt{first} produces a wrong answer,
we won't know if \texttt{second} is working
because we won't know what its actual input is.
Using the output of one test as the input of another
increases the risk of \href{glossary.html\#false-positive}{false positives} (tests passing when they should fail)
and \href{glossary.html\#false-negative}{false negatives} (tests failing when they should pass),
which distract or mislead us.

Putting these requirements together,
a single unit test has:

\begin{itemize}
\tightlist
\item
  a \href{glossary.html\#fixture}{fixture},
  which is the thing being tested (e.g., the number 0 or a list of images);
\item
  an \href{glossary.html\#actual-result}{actual result},
  which is what the code produces when given that fixture; and
\item
  an \href{glossary.html\#expected-result}{expected result}
  that the actual result is compared to.
\end{itemize}

Each test can have one of three results:

\begin{itemize}
\tightlist
\item
  \href{glossary.html\#test-success}{success}: the code passed the test.
\item
  \href{glossary.html\#test-failure}{failure}: the code didn't pass the test.
\item
  \href{glossary.html\#test-error}{error}: something went wrong with the test itself,
  so we don't know anything for certain about the code being tested.
\end{itemize}

A \href{glossary.html\#test-framework}{test framework} (also called a \href{glossary.html\#test-runner}{test runner}) should:

\begin{itemize}
\tightlist
\item
  find and run tests;
\item
  summarize results;
\item
  pinpoint the locations of failures so that users know where to start debugging;
\item
  encourage people to isolate tests; and
\item
  make it easy to write and update tests (because otherwise people won't do it).
\end{itemize}

The next two sections will introduce the most popular frameworks for R and Python.
We will then explore general issues
such as how we can tell which parts of our code have and haven't been tested
and what kinds of tests we should write.

\hypertarget{rse-correct-create-python}{%
\section{How do I create and run unit tests in Python?}\label{rse-correct-create-python}}

\href{http://pytest.org/}{\texttt{pytest}} is the most widely used test framework for Python.
Tests obey three rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All tests are put in files whose names begin with \texttt{test\_}.
\item
  Each test is a function whose name also begins with \texttt{test\_}.
\item
  These functions use \texttt{assert} to check results.
\end{enumerate}

The \texttt{pytest} library comes with a command-line tool that is also called \texttt{pytest}.
When we run it with no options:

\begin{verbatim}
$ pytest
\end{verbatim}

it searches for all files named \texttt{test\_*.py} in the current directory and its subdirectories.
It then imports these files,
runs the tests they contain,
and summarizes their results.
If we only want to run the tests in a particular file,
we can use the command \texttt{pytest\ path/to/test\_file.py}.

We can put our tests wherever we want,
but most projects put them in a subdirectory of the project's root directory called \texttt{tests}.
If the project uses a \href{glossary.html\#build-tool}{build tool} like Make (Chapter~\ref{rse-automate}),
it is common to include a target called \texttt{test} so that \texttt{make\ test} will re-run all of the tests.

So what do tests look like?
To see,
create a file called \texttt{test\_add.py} and insert the following code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ add(x, y):}
\NormalTok{    val }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
    \ControlFlowTok{return}\NormalTok{ val}

\KeywordTok{def}\NormalTok{ test_add_two_small_integers_2():}
    \ControlFlowTok{assert}\NormalTok{ add_two(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{) }\OperatorTok{==} \DecValTok{3}

\KeywordTok{def}\NormalTok{ test_add_integer_and_float():}
\NormalTok{    left }\OperatorTok{=} \DecValTok{1}
\NormalTok{    right }\OperatorTok{=} \FloatTok{2.0}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ add_two(left, right)}
    \ControlFlowTok{assert}\NormalTok{ result }\OperatorTok{==} \DecValTok{3}
    \ControlFlowTok{assert} \BuiltInTok{type}\NormalTok{(result) }\OperatorTok{==} \BuiltInTok{int}
\end{Highlighting}
\end{Shaded}

The function being tested will \emph{not} normally be in
the file of tests---test files will normally import those functions instead---but
we have put \texttt{add} here for convenience.
We don't write comments to explain what each test is doing;
we give each a long name instead,
because that's what \texttt{pytest} will print if and when the test fails.
Finally,
we have shown two styles of testing here:
one in which the fixture is just the values passed into the function
and the result is checked immediately,
and one in which the fixture is constructed step-by-step (the variables \texttt{left} and \texttt{right})
and the function's result is captured in another variable
so that several checks can be done.

To run these tests, we use:

\begin{verbatim}
$ pytest test_add.py
\end{verbatim}

The output is:

\begin{verbatim}
============================= test session starts ==============================
platform darwin -- Python 3.7.3, pytest-5.0.1, py-1.8.0, pluggy-0.12.0
rootdir: /Users/merely/tests
plugins: openfiles-0.3.2, arraydiff-0.3, doctestplus-0.3.0, remotedata-0.3.1
collected 2 items

test_add.py FF                                                          [100%]

=================================== FAILURES ===================================
________________________ test_add_two_small_integers_2 _________________________

    def test_add_two_small_integers_2():
>       assert add_two(1, 2) == 3
E       NameError: name 'add_two' is not defined

test_add.py:6: NameError
__________________________ test_add_integer_and_float __________________________

    def test_add_integer_and_float():
        left = 1
        right = 2.0
>       result = add_two(left, right)
E       NameError: name 'add_two' is not defined

test_add.py:11: NameError
=========================== 2 failed in 0.10 seconds ===========================
\end{verbatim}

The two \texttt{F}'s in the top line tell us there are two failures.
The \texttt{E} in the report for each test tells us why that test failed,
and the listing immediately above this line tells us why:
our function is called \texttt{add},
but we are using \texttt{add\_two} in our tests.
Let's rename the function and re-run our tests:

\begin{verbatim}
============================= test session starts ==============================
platform darwin -- Python 3.7.3, pytest-5.0.1, py-1.8.0, pluggy-0.12.0
rootdir: /Users/merely/tests
plugins: openfiles-0.3.2, arraydiff-0.3, doctestplus-0.3.0, remotedata-0.3.1
collected 2 items

test_add.py .F                                                           [100%]

=================================== FAILURES ===================================
__________________________ test_add_integer_and_float __________________________

    def test_add_integer_and_float():
        left = 1
        right = 2.0
        result = add_two(left, right)
        assert result == 3
>       assert type(result) == int
E       AssertionError: assert <class 'float'> == int
E        +  where <class 'float'> = type(3.0)

test_add.py:13: AssertionError
====================== 1 failed, 1 passed in 0.05 seconds ======================
\end{verbatim}

This time the first line of output tells us that one test passed (the \texttt{.})
and another failed (the \texttt{F}).
\texttt{pytest} then shows where the problem occurred in the failing test:
we claimed on line 13 that the result's type would be \texttt{int}
but it is actually \texttt{float}.

\hypertarget{rse-correct-create-r}{%
\section{How do I create and run unit tests in R?}\label{rse-correct-create-r}}

The easiest way to start testing in R is via the \texttt{usethis} package.
Let's create a new project in RStudio caller \texttt{creater} of type ``package''
(both because \texttt{usethis} is designed to work with packages,
and because our R projects should always be packages).
We can then type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_testthat}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Setting active project to '/Users/merely/creater'
 Adding 'testthat' to Suggests field in DESCRIPTION
 Creating 'tests/testthat/'
 Writing 'tests/testthat.R'
 Call `use_test()` to initialize a basic test file and open it for editing.
\end{verbatim}

The call creates a new directory \texttt{tests/testthat/},
adds \texttt{testthat} to \texttt{Suggests} in the \texttt{DESCRIPTION} file,
and creates a file called \texttt{tests/testthat.R} that tells R how to run the tests.
This may seem like a lot,
but it is all needed to create an R package (Chapter~\ref{rse-package-r}),
so we might as well do things that way from the start.

The file \texttt{tests/testthat.R} contains:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(testthat)}
\KeywordTok{library}\NormalTok{(creater)}

\KeywordTok{test_check}\NormalTok{(}\StringTok{"creater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

I.e.,
it loads the \texttt{testthat} library and our own package,
then calls \texttt{test\_check} to run tests.
If we try to run our tests using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{test}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

we get the message:

\begin{verbatim}
No tests: no files in /Users/merely/creater/tests/testthat match '^test.*\.[rR]$'
\end{verbatim}

This is fair,
since we haven't yet created any tests.
To do that,
we run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{use_test}\NormalTok{(}\StringTok{"mean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Increasing 'testthat' version to '>= 2.1.0' in DESCRIPTION
 Writing 'tests/testthat/test-mean.R'
 Modify 'tests/testthat/test-mean.R'
\end{verbatim}

As the output suggests,
this command creates a file called \texttt{tests/testthat/test-mean.R}.
We could create this file ourselves if we want;
either way,
its name should reflect what we are using it to test,
such as \texttt{test-read-data.R} or \texttt{test-annual-analysis.R}.
Note that we do \emph{not} type the prefix \texttt{test-} in the call to \texttt{usethis::use\_test};
\texttt{usethis} will insert that automatically.

The newly-created file contains a single test case:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_that}\NormalTok{(}\StringTok{"multiplication works"}\NormalTok{, \{}
  \KeywordTok{expect_equal}\NormalTok{(}\DecValTok{2} \OperatorTok{*}\StringTok{ }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We can run this with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{test}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading creater
Testing creater
 |  OK F W S | Context
 |   1       | mean

 Results 
OK:       1
Failed:   0
Warnings: 0
Skipped:  0
\end{verbatim}

or use the RStudio keyboard shortcut Ctrl+Shift+T (on Windows)
or Cmd+Shift+T (on Mac),
which calls the same function.
Either way,
R searches below the \texttt{tests/testthat} directory to find files whose names start with \texttt{test},
loads them,
and runs the tests that it finds.
The output tells us that one test passed,
none failed,
there were no warnings,
and no tests were skipped.

Let's re-open \texttt{tests/testthat/test-mean.R} and replace the test that R wrote for us
with this code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{context}\NormalTok{(}\StringTok{"Computing the mean"}\NormalTok{)}

\KeywordTok{test_that}\NormalTok{(}\StringTok{"the mean of an odd-length vector is correct"}\NormalTok{, \{}
    \KeywordTok{expect_identical}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{), }\FloatTok{2.0}\NormalTok{)}
\NormalTok{\})}

\KeywordTok{test_that}\NormalTok{(}\StringTok{"the mean of an even-length vector is correct"}\NormalTok{, \{}
    \KeywordTok{expect_identical}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{), }\FloatTok{2.0}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading creater
Testing creater
 |  OK F W S | Context
 |   1 1     | Computing the mean

test-mean.R:8: failure: the mean of an even-length vector is correct
mean(1:4) not identical to 2.
1/1 mismatches
[1] 2.5 - 2 == 0.5


 Results 
OK:       1
Failed:   1
Warnings: 0
Skipped:  0
\end{verbatim}

The little table at the top of the output shows that one test passed and one failed
in the context titled \texttt{"Computing\ the\ mean"}.
The lines below tell us what failed and why,
and the final section of the report summarizes the results.
It is redundant in this case,
since both of our tests are in the same context,
but is useful if we have several files full of tests.

\hypertarget{rse-correct-fixtures}{%
\section{How should I set up fixtures my unit tests?}\label{rse-correct-fixtures}}

Test files all have a similar structure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the software to be tested
  (since it normally won't be in the same file as the tests).
\item
  Write one short function to test each feature of the software.
\end{enumerate}

Each test function also has a stereotypical structure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create the fixture.
\item
  Run the test.
\item
  Check the result.
\end{enumerate}

Creating a fixture often requires many lines of code,
so we often write helper functions to do this.
For example,
if we are testing functions that work on multiple dataframes in R,
we might write a function like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{create_three_frames <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{    ones <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
        \OperatorTok{~}\NormalTok{odd, }\OperatorTok{~}\NormalTok{even,}
           \DecValTok{1}\NormalTok{,     }\DecValTok{2}\NormalTok{,}
           \DecValTok{3}\NormalTok{,     }\DecValTok{4}\NormalTok{)}
\NormalTok{    tens <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
        \OperatorTok{~}\NormalTok{odd, }\OperatorTok{~}\NormalTok{even,}
          \DecValTok{10}\NormalTok{,    }\DecValTok{20}\NormalTok{,}
          \DecValTok{30}\NormalTok{,    }\DecValTok{40}\NormalTok{)}
\NormalTok{    hundreds <-}\StringTok{ }\KeywordTok{tribble}\NormalTok{(}
        \OperatorTok{~}\NormalTok{odd, }\OperatorTok{~}\NormalTok{even,}
         \DecValTok{100}\NormalTok{,   }\DecValTok{200}\NormalTok{,}
         \DecValTok{300}\NormalTok{,   }\DecValTok{400}\NormalTok{)}
    \KeywordTok{list}\NormalTok{(}\DataTypeTok{ones=}\NormalTok{ones, }\DataTypeTok{tens=}\NormalTok{tens, }\DataTypeTok{hundreds=}\NormalTok{hundreds)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We could then use the fixture that function produces in many tests.
Here,
for example,
we are using it to test a function called \texttt{columnize}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_that}\NormalTok{(}\StringTok{"columnize combines columns by name with addition"}\NormalTok{, \{}
\NormalTok{    fixture <-}\StringTok{ }\KeywordTok{create_three_frames}\NormalTok{()}
\NormalTok{    result <-}\StringTok{ }\KeywordTok{columnize}\NormalTok{(fixture, }\StringTok{'+'}\NormalTok{, odd)}
    \KeywordTok{expect_equal}\NormalTok{(result}\OperatorTok{$}\NormalTok{odd, }\KeywordTok{c}\NormalTok{(}\DecValTok{111}\NormalTok{, }\DecValTok{333}\NormalTok{))}
\NormalTok{\})}

\KeywordTok{test_that}\NormalTok{(}\StringTok{"columnize combines columns by name with multiplication"}\NormalTok{, \{}
\NormalTok{    fixture <-}\StringTok{ }\KeywordTok{create_three_frames}\NormalTok{()}
\NormalTok{    result <-}\StringTok{ }\KeywordTok{columnize}\NormalTok{(fixture, }\StringTok{'*'}\NormalTok{, odd)}
    \KeywordTok{expect_equal}\NormalTok{(result}\OperatorTok{$}\NormalTok{odd, }\KeywordTok{c}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{9000}\NormalTok{))}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Using helper functions to create fixtures like this
reduces the \href{glossary.html\#cognitive-load}{cognitive load} on whoever has to maintain the tests,
since they only have to form a \href{glossary.html\#mental-model}{mental model} of a few cases
rather than a new one for each test.
Note that we do \emph{not} create the fixture once as a global variable
and then re-use it in many tests.
Tests should be isolated,
and having them share data increases the chances that
one of our tests will modify the data in ways that interfere with other tests.

\hypertarget{rse-correct-check}{%
\section{How should I check the results of tests?}\label{rse-correct-check}}

Once we have a result we need to check it.
The simplest way to do this in Python is the \texttt{assert} statement,
which \href{glossary.html\#raise-exception}{raises} an \href{glossary.html\#exception}{exception} if a condition is not true.
This normally causes Python to halt the program,
but \texttt{pytest} catches these exceptions,
adds them to its summary report,
and then runs the next test function.
As a result,
all of the test functions are run,
but each function only runs as far as the first failed check (Figure~\ref{fig:rse-correct-failed-test}).

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-correct-failed-test}Test Failure}
\end{figure}

This is another reason to recycle fixtures and keep test functions short:
in the example below,
we can't be sure that the second test is run in \texttt{combined\_tests}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ combined_tests():}
\NormalTok{    fixture }\OperatorTok{=}\NormalTok{ make_fixture()}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ function(fixture)}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(result) }\OperatorTok{>} \DecValTok{2}
    \ControlFlowTok{assert}\NormalTok{ result[}\DecValTok{0}\NormalTok{] }\OperatorTok{!=} \DecValTok{0}  \CommentTok{# might not run}
\end{Highlighting}
\end{Shaded}

but we can be sure that both checks are run in \texttt{split\_test\_len} and \texttt{split\_test\_zero}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ split_test_len():}
\NormalTok{    fixture }\OperatorTok{=}\NormalTok{ make_fixture()}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ function(fixture)}
    \ControlFlowTok{assert} \BuiltInTok{len}\NormalTok{(result) }\OperatorTok{>} \DecValTok{2}

\KeywordTok{def}\NormalTok{ split_test_zero():}
\NormalTok{    fixture }\OperatorTok{=}\NormalTok{ make_fixture()}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ function(fixture)}
    \ControlFlowTok{assert}\NormalTok{ result[}\DecValTok{0}\NormalTok{] }\OperatorTok{!=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\textbf{Testing in the Real World}

In practice,
most programmers would write \texttt{combined\_tests} in this case
rather than doubling the amount of reading they have to do
in order to understand these tests.
The purpose of testing is always to draw attention to things that need it;
if the second check in \texttt{combined\_tests} doesn't run because the first one failed
then we will have less information than we could,
but we will still know the most important thing:
our code is broken.
\end{quote}

In contrast to Python,
R's \texttt{testthat} library provides multiple checking functions for different purposes.
\texttt{expect\_identical()} compares that actual and expected results are identical down to the last bit.
\texttt{expect\_equal()} checks that its arguments are equal within a narrow tolerance;
as we discuss in Section~\ref{rse-correct-float},
this is more useful for most data science problems.
Others include \texttt{expect\_length},
which checks that a vector has the right length,
and \texttt{expect\_match},
which checks that a string matches a \href{glossary.html\#regular-expression}{regular expression}.
We can always replace these with \texttt{expect\_true},
i.e.,
use:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{expect_true}\NormalTok{(}\KeywordTok{length}\NormalTok{(result }\OperatorTok{==}\StringTok{ }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

instead of:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{expect_length}\NormalTok{(result, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

but the latter makes our intention clearer.

\hypertarget{rse-correct-failure}{%
\section{Why and how should I test that software fails correctly?}\label{rse-correct-failure}}

Evil clowns are every programmer's worst nightmare.
Their second worst is a \href{glossary.html\#silent-failure}{silent failure}:
something that goes wrong but doesn't crash,
print a warning,
or otherwise signal that human attention is required.
Testing that our code fails when it should
is therefore just as important as testing that it runs correctly.

\begin{quote}
\textbf{We Have the Data}

The need to test error handling is not just folklore:
in their study of failures in data-intensive applications,
Yuan et al. (\protect\hyperlink{ref-Yuan2014}{2014}) found that,
``the majority of catastrophic failures could easily have been prevented
by performing simple testing on error handling code---the last line of defense---even without
an understanding of the software design.''
\end{quote}

In Python,
we can test for an error using a \texttt{with} statement
and the \texttt{pytest.raises} function.
For example,
suppose a function called \texttt{count\_words} is supposed to raise a \texttt{ValueError} exception when given an empty string.
We can test this behavior as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pytest}

\KeywordTok{def}\NormalTok{ test_text_must_not_be_empty():}
    \ControlFlowTok{with}\NormalTok{ pytest.raises(}\PreprocessorTok{ValueError}\NormalTok{):}
\NormalTok{        count_words(}\StringTok{''}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In R,
\texttt{testthat} provides specific functions to test for errors or warnings
called \texttt{expect\_error()} and \texttt{expect\_warning()} respectively:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_that}\NormalTok{(}\StringTok{"text must not be empty"}\NormalTok{, \{}
    \KeywordTok{expect_error}\NormalTok{(}\KeywordTok{count_words}\NormalTok{(}\StringTok{""}\NormalTok{))}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{rse-correct-coverage}{%
\section{How can I tell whether my code has been tested or not?}\label{rse-correct-coverage}}

As we write more code or make changes to code we already have,
we may lose track of which parts we have tested and which we haven't.
Even short programs,
loops or conditionals may make it difficult for us to figure out
what is actually being executed.
For example,
take a few moments to read this function in Python:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ first(left, right):}
    \ControlFlowTok{if}\NormalTok{ left }\OperatorTok{<}\NormalTok{ right:}
\NormalTok{        left, right }\OperatorTok{=}\NormalTok{ right, left}
    \ControlFlowTok{while}\NormalTok{ left }\OperatorTok{>}\NormalTok{ right:}
\NormalTok{        value }\OperatorTok{=}\NormalTok{ second(left, right)}
\NormalTok{        left, right }\OperatorTok{=}\NormalTok{ right, }\BuiltInTok{int}\NormalTok{(right}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ value}

\KeywordTok{def}\NormalTok{ second(check, balance):}
    \ControlFlowTok{if}\NormalTok{ check }\OperatorTok{>} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ balance}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \DecValTok{0}

\BuiltInTok{print}\NormalTok{(first(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

or its equivalent in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(left, right) \{}
  \ControlFlowTok{if}\NormalTok{ (left }\OperatorTok{<}\StringTok{ }\NormalTok{right) \{}
\NormalTok{    tmp <-}\StringTok{ }\NormalTok{left}
\NormalTok{    left <-}\StringTok{ }\NormalTok{right}
\NormalTok{    right <-}\StringTok{ }\NormalTok{tmp}
\NormalTok{  \}}

  \ControlFlowTok{while}\NormalTok{ (left }\OperatorTok{>}\StringTok{ }\NormalTok{right) \{}
\NormalTok{    value <-}\StringTok{ }\KeywordTok{second}\NormalTok{(left, right)}
\NormalTok{    left <-}\StringTok{ }\NormalTok{right}
\NormalTok{    right <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(right }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\NormalTok{  \}}

\NormalTok{  value}
\NormalTok{\}}

\NormalTok{second <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(check, balance) \{}
    \ControlFlowTok{if}\NormalTok{ (check }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
\NormalTok{        balance}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \DecValTok{0}
\NormalTok{    \}}
\NormalTok{\}}

\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\KeywordTok{first}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The only way to know which parts of these functions are and aren't executed
is to trace their execution a line at a time.
The good news is that we don't have to do this ourselves:
a \href{glossary.html\#code-coverage}{code coverage} tool will do this for us.
In essence,
a code coverage tool creates a vector of Boolean flags with one entry for each line of code,
then modifies the code so that the appropriate flag is set to \texttt{true} whenever that line is executed
(Figure~\ref{fig:rse-correct-how-coverage-works}).
When the program finishes,
the tool can show us which lines were touched
and report summary statistics such as the number or percentage of lines that weren't used.

\begin{figure}
\centering
\includegraphics{figures/FIXME.png}
\caption{\label{fig:rse-correct-how-coverage-works}How Coverage Works}
\end{figure}

Most Python programmers use the \texttt{coverage} module as a coverage checking tool.
When we install it we get a command-line utility that is also called \texttt{coverage}.
If the example program above is in a file called \texttt{demo.py},
we can check coverage by running:

\begin{verbatim}
$ coverage run demo_coverage.py
\end{verbatim}

This command doesn't display anything of its own;
instead,
it puts coverage data in a file called \texttt{.coverage} (with a leading \texttt{.}) in the current directory.
To display that data,
we run:

\begin{verbatim}
$ coverage report
\end{verbatim}

\begin{verbatim}
Name      Stmts   Miss  Cover
-----------------------------
demo.py      12      1    92%
\end{verbatim}

To get a fuller report,
we run \texttt{coverage\ html}
and then open \texttt{htmlcov/index.html}.
Clicking on the name of our file produces the colorized line-by-line display shown in Figure~\ref{fig:python-coverage}.

\begin{figure}
\centering
\includegraphics{figures/rse-correct/python-coverage.png}
\caption{\label{fig:python-coverage}Python Coverage}
\end{figure}

R's \texttt{covr} package has several functions for checking coverage.
For example,
we can test the coverage of the tests for a package by running:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covr}\OperatorTok{::}\KeywordTok{package_coverage}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This function outputs a list of all R files in the package
with their corresponding percentage coverage,
which is a good way to figure out what needs to be done next.

\hypertarget{rse-correct-enough}{%
\section{How much testing is enough?}\label{rse-correct-enough}}

If we are writing software for a safety-critical application such as a medical device,
we should aim for 100\% code coverage,
i.e.,
every single line in the application should be tested.
In fact,
we should probably go further and aim for 100\% \href{glossary.html\#path-coverage}{path coverage}
and ensure that every possible path through the code has been checked.

But most of us don't write software that people's lives depend on,
so requiring 100\% code coverage is like asking for ten decimal places of accuracy
when checking the voltage of a household electrical outlet.
We always need to balance the effort required to create tests
against the likelihood that those tests will uncover useful information.

It's important to understand that no amount of testing can prove a piece of software is completely correct.
A function with only two numeric arguments has 2128 possible inputs.
Even if we could write the tests,
how could we be sure we were checking the result of each one correctly?

And testing data analysis pipelines is often harder than testing mainstream software applications,
since data analysts often don't know what the right answer is.
(If we did,
we would have submitted our report and moved on to the next problem already.)
The key distinction is the difference between \href{glossary.html\#validation}{validation},
which asks whether the specification is correct,
while \href{glossary.html\#verification}{verification},
which asks whether we have met that specification.
The difference between them is the difference between building the right thing and building something right,
and the former question is often hard for data scientists to answer.

Luckily,
we can group the test cases for most functions into classes.
For example,
it might be possible to test a function that takes numbers as inputs
as well as we need to
using only a few cases:
zero, a positive number, a negative one, and infinity.
If we want to go further,
we could check that it fails the right way when given a string or a list.
Similarly,
when testing a function that summarizes a table full of data,
we should probably check that it handles tables with:

\begin{itemize}
\tightlist
\item
  no rows
\item
  only one row
\item
  many identical rows
\item
  rows having keys that are supposed to be unique, but aren't
\item
  rows that contain nothing but missing values
\end{itemize}

Some projects develop \href{glossary.html\#checklist}{checklists} to remind programmers what they ought to test.
These checklists can be a bit daunting for newcomers,
but they are a great way to pass on hard-earned experience.

\hypertarget{rse-correct-tdd}{%
\section{When should I write my tests?}\label{rse-correct-tdd}}

Many programmers are passionate advocates of a practice called
\href{glossary.html\#tdd}{test-driven development} (TDD).
Rather than writing code and then writing tests,
they write the tests first and then write just enough code to make those tests pass.
Once the code is working,
they clean it up (Chapter~\ref{refactor}) and then move on to the next task.

TDD's advocates claim that working this way leads to better code in less time because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Writing tests clarifies what the code is actually supposed to do.
\item
  It eliminates \href{glossary.html\#confirmation-bias}{confirmation bias}.
  If someone has just written a function,
  they are predisposed to want it to be right,
  so they will bias their tests towards proving that it is correct
  instead of trying to uncover errors.
\item
  Writing tests first ensures that they actually get written.
\end{enumerate}

These arguments are plausible.
However,
studies such as Fucci et al. (\protect\hyperlink{ref-Fucc2016}{2016}) don't support them:
in practice,
writing tests first or last doesn't appear to affect productivity.
What \emph{does} have an impact is working in small, interleaved increments,
i.e.,
writing just a few lines of code and testing it before moving on
rather than writing several pages of code and then spending hours on testing.

So how do most data scientists figure out if their software is doing the right thing?
The answer is spot checks:
each time they produce an intermediate or final result,
they scan a table, create a chart, or inspect some summary statistics
to see if everything looks OK.
Their heuristics are usually easy to state,
like ``there shouldn't be NAs at this point'' or ``the age range should be reasonable'',
but applying those heuristics to a particular analysis always depends on
their evolving insight into the data in question.

By analogy with test-driven development,
we could call this process \href{glossary.html\#cdd}{checking-driven development} (CDD).
Each time we add a step to our pipeline and look at its output,
we can also add a check of some kind to the pipeline to ensure that
what we are checking for remains true as the pipeline evolves or is run on other data.
This helps reusability---it's amazing how often a one-off analysis
winds up being used many times---but the real goal is comprehensibility.
If someone can get our code and data,
run the former on the latter,
and get the same result that we did,
then our computation is reproducible,
but that doesn't mean they can understand it.
Comments help
(either in the code or as blocks of prose in a \href{glossary.html\#computational-notebook}{computational notebook}),
but they won't check that assumptions and invariants hold.
And unlike comments,
runnable assertions can't fall out of step with what the code is actually doing\ldots{}

We also need to distinguish between testing during development
and testing in production.
During development,
our main concern is whether our answers are (close enough to) what we expect.
We do this by analyzing small datasets
and convincing ourselves that we're getting the right answer in some ad hoc way.

In production,
on the other hand,
our goal is to detect cases where behavior deviates significantly from what we previously decided what right.
We want this to be automated
so that our pipeline will ring an alarm bell to tell us something is wrong
even if we're busy working on something else.
This can happen because real data will never have exactly the same characteristics as the data we used during development.
We also need these checks because the pipeline's environment can change:
for example,
someone could upgrade a library that one of our libraries depends on,
which could lead to us getting slightly different answers than we expected.

\hypertarget{rse-correct-float}{%
\section{Why should I be cautious when using floating-point numbers?}\label{rse-correct-float}}

No matter what the cause,
we need to understand exactly what we mean by ``tolerance'' when talking about testing.
The explanation that follows is simplified to keep it manageable;
To learn more,
please take half an hour to read Goldberg (\protect\hyperlink{ref-Gold1991}{1991}).

Finding a good representation for floating point numbers is hard:
we cannot represent an infinite number of real values with a finite set of bit patterns,
and unlike integers,
no matter what values we \emph{do} represent,
there will be an infinite number of values between each of them that we can't.
These days,
floating point numbers are usually represented using \href{glossary.html\#sign}{sign},
\href{glossary.html\#magnitude}{magnitude} (or \href{glossary.html\#mantissa}{mantissa}),
and an \href{glossary.html\#exponent}{exponent}.
In a 32-bit number,
the IEEE 754 standard calls for 1 bit of sign,
23 bits for the mantissa,
and 8 bits for the exponent.
To illustrate the problems with floating point,
we will use a much simpler 5-bit representation
with 3 bits for the magnitude and 2 for the exponent.
We won't worry about fractions or negative numbers,
since our simple representation will show off the main problems.

The table below shows the possible values we can represent with 5 bits.
(Real floating point representations shift values left and right to avoid the redundancy shown here,
but again,
that doesn't affect our main point.)
Using subscripts to show the bases of numbers,
11022112 in binary
is 623 in decimal,
or 48.

Exponent

Mantissa

00

01

10

11

000

0

0

0

0

001

1

2

4

8

010

2

4

8

16

011

3

6

12

24

100

4

8

16

32

101

5

10

20

40

110

6

12

24

48

111

7

14

28

56

\begin{figure}
\centering
\includegraphics{figures/rse-correct/number-spacing.png}
\caption{\label{fig:verify-spacing}Number Spacing}
\end{figure}

Figure~\ref{fig:verify-spacing} is a clearer view of some of the values our scheme can represent.
It shows that a lot of values are missing from this diagram:
for example,
it includes 8 and 10 but not 9.
This is exactly the same problem as writing out  in decimal:
we have to round that to 0.3333 or 0.3334.

Since this scheme has no representation for 9,
8+1 must be stored as either 8 or 10.
But if 8+1 is 8, what is 8+1+1?
If we add from the left, 8+1 is 8, plus another 1 is 8 again.
If we add from the right, 1+1 is 2, and 2+8 is 10,
so changing the order of operations can make the difference between right and wrong.
In this case,
sorting the values and adding from smallest to largest
gives us the best chance of getting the most accurate answer.
In other situations,
like inverting a matrix,
the rules are more complicated.

To make this more concrete,
consider the short Python program below.
It loops over the integers from 1 to 9
and puts the numbers 0.9, 0.09, 0.009, and so on in \texttt{vals}.
The sums should be 0.9, 0.99, 0.999, and so on, but are they?
To find out,
we can calculate the same values by subtracting .1 from 1,
then subtracting .01 from 1, and so on.
This should create exactly the same sequence of numbers, but it doesn't.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vals }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{):}
\NormalTok{    number }\OperatorTok{=} \FloatTok{9.0} \OperatorTok{*} \FloatTok{10.0} \OperatorTok{**} \OperatorTok{-}\NormalTok{i}
\NormalTok{    vals.append(number)}
\NormalTok{    total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(vals)}
\NormalTok{    expected }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{-}\NormalTok{ (}\FloatTok{10.0} \OperatorTok{**} \OperatorTok{-}\NormalTok{i)}
\NormalTok{    diff }\OperatorTok{=}\NormalTok{ total }\OperatorTok{-}\NormalTok{ expected}
    \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{:2d\}}\StringTok{ }\SpecialCharTok{\{:22.21f\}}\StringTok{ }\SpecialCharTok{\{:22.21f\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(i, total, total}\OperatorTok{-}\NormalTok{expected))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  1 0.900000000000000022204 0.000000000000000000000
##  2 0.989999999999999991118 0.000000000000000000000
##  3 0.998999999999999999112 0.000000000000000000000
##  4 0.999900000000000011013 0.000000000000000000000
##  5 0.999990000000000045510 0.000000000000000000000
##  6 0.999999000000000082267 0.000000000000000111022
##  7 0.999999900000000052636 0.000000000000000000000
##  8 0.999999990000000060775 0.000000000000000111022
##  9 0.999999999000000028282 0.000000000000000000000
\end{verbatim}

As the output shows,
the very first value contributing to our sum is already slightly off:
even with 23 bits for a mantissa,
we cannot exactly represent 0.9 in base 2.
Doubling the size of the mantissa would reduce the error,
but we can't ever eliminate it.

The good news is that 910-1 and 1-0.1 are exactly the same:
the values might not be correct,
but at least they are consistent.
However,
some later values differ,
and sometimes accumulated error makes the result \emph{more} accurate.

\emph{This has nothing to do with randomness.}
The process is completely deterministic,
so the same calculation will produce exactly the same results no matter how many times it is run.
(If we see someone run the same code on the same data with the same parameters many times and average the results,
we should ask if they know what they're doing.)
In practice,
just as electrical engineers trust oscilloscope makers,
data scientists should trust the authors of core libraries to do the best they can.

\hypertarget{rse-correct-numeric}{%
\section{How should I write tests that involved floating-point values?}\label{rse-correct-numeric}}

The absolute spacing between the values we can represent in Figure~\ref{fig:verify-spacing} is uneven.
However,
the relative spacing between each set of values stays the same:
the first group is separated by 1,
then the separation becomes 2,
then 4,
and then 8.
This happens because we're multiplying the same fixed set of mantissas by ever-larger exponents,
and it leads to some useful definitions.
The \href{glossary.html\#absolute-error}{absolute error} in an approximation is the absolute value of
the difference between the approximation and the actual value.
The \href{glossary.html\#relative-error}{relative error} is the ratio of the absolute error to the value we're approximating.
For example,
it we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than in the second.

Relative error is almost always more important than absolute error when we are testing software
because it makes little sense to say that we're off by a hundredth
when the value in question is a billionth.
\href{glossary.html\#accuracy}{Accuracy} is how close our answer is to right,
and \href{glossary.html\#precision}{precision} is how close repeated measurements are to each other.
We can be precise without being accurate (systematic bias),
or accurate without being precise (near the right answer but without many significant digits).

Accuracy is usually more important than precision for human decision making:
a relative error of 10-2 (two decimal places) is good enough for most data science
because the decision a human being would make won't change if the number changes by 1\%.

We now come to the crux of this lesson:
if the function we are testing uses floating point numbers,
what should we compare its result to?
If we compared the sum of the first few numbers in \texttt{vals} to the ``right'' answer,
the answer could be \texttt{False} even though we're doing nothing wrong.
If we compared it to a previously calculated result that we had stored somehow,
the match would be exact.

No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in context.
So what can you do to test your programs?
If you are comparing to a saved result,
and the result was saved at full precision,
you could use exact equality,
because there is no reason for the new number to differ.
However,
any change to your code,
however small,
could trigger a report of a difference.
Experience shows that these spurious warnings quickly lead developers to stop paying attention to their tests.

A much better approach is to write a test that checks whether numbers are the same within some \href{glossary.html\#tolerance}{tolerance},
which is best expressed as a relative error.
R's \texttt{expect\_equal} does this automatically.
In Python,
we can use \texttt{pytest.approx},
which works on lists, sets, arrays, and other collections,
and can be given either relative or absolute error bounds.
To show how it works,
here's an example with an unrealistically tight absolute bound:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pytest }\ImportTok{import}\NormalTok{ approx}

\ControlFlowTok{for}\NormalTok{ bound }\KeywordTok{in}\NormalTok{ (}\FloatTok{1e-15}\NormalTok{, }\FloatTok{1e-16}\NormalTok{):}
\NormalTok{    vals }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{):}
\NormalTok{        number }\OperatorTok{=} \FloatTok{9.0} \OperatorTok{*} \FloatTok{10.0} \OperatorTok{**} \OperatorTok{-}\NormalTok{i}
\NormalTok{        vals.append(number)}
\NormalTok{        total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(vals)}
\NormalTok{        expected }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{-}\NormalTok{ (}\FloatTok{10.0} \OperatorTok{**} \OperatorTok{-}\NormalTok{i)}
        \ControlFlowTok{if}\NormalTok{ total }\OperatorTok{!=}\NormalTok{ approx(expected, }\BuiltInTok{abs}\OperatorTok{=}\NormalTok{bound):}
            \BuiltInTok{print}\NormalTok{(}\StringTok{'}\SpecialCharTok{\{:22.21f\}}\StringTok{ }\SpecialCharTok{\{:2d\}}\StringTok{ }\SpecialCharTok{\{:22.21f\}}\StringTok{ }\SpecialCharTok{\{:22.21f\}}\StringTok{'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(bound, i, total, expected))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
9.999999999999999790978e-17  6 0.999999000000000082267 0.999998999999999971244
9.999999999999999790978e-17  8 0.999999990000000060775 0.999999989999999949752
\end{verbatim}

This tells us that two tests pass with an absolute error of 10-15
but fail when the bound is 10-16,
both of which are unreasonably tight.
Again,
it helps to think of physical experiments:
an absolute error of 10-15= data{[}`word\_count.py', `num\_distinct'){]}
```

Tests like these are simple to write
and catch a surprising number of errors,
particularly when the person using the pipeline isn't its original author.

\begin{quote}
\textbf{Validating Pipelines}

There are several libraries for validating data pipelines in R,
including \href{https://cran.r-project.org/web/packages/assertr/index.html}{assertr},
\href{https://cran.r-project.org/web/packages/checkr/index.html}{checkr},
and \href{https://cran.r-project.org/web/packages/validate/index.html}{validate}.
\end{quote}

\hypertarget{rse-correct-infer}{%
\section{How can I infer and check properties of my data?}\label{rse-correct-infer}}

Writing tests for the properties of data can be tedious,
but some of the work can be automated.
In particular,
the \href{http://www.tdda.info/}{TDDA library} for Python can infer test rules from data,
such as ``\texttt{age} should be less than 100'',
``\texttt{Date} should be sorted in ascending order'',
or ``\texttt{StartDate} should be less than or equal to \texttt{EndDate}''.
The library comes with a command-line tool called \texttt{tdda},
so that the command:

\begin{verbatim}
$ tdda discover training-data.csv properties.tdda
\end{verbatim}

infers rules from data,
while the command:

\begin{verbatim}
$ tdda verify actual-data.csv properties.tdda
\end{verbatim}

verifies data against those rules.
The inferred rules are stored as \href{glossary.html\#json}{JSON},
which is (sort of) readable with a bit of practice.
Reading the generated rules is a good way to get to know your data,
and modifying values
(e.g., changing the maximum allowed value for \texttt{Grade} from the observed 94.5 to the actual 100.0)
is an easy way to make constraints explicit.
For example,
if we run:

\begin{verbatim}
$ tdda discover elements92.csv elements.tdda
\end{verbatim}

(which contains information about the first 92 elements)
the output includes:

\begin{Shaded}
\begin{Highlighting}[]
\ErrorTok{"fields":} \FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \FunctionTok{\{}
        \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"string"}\FunctionTok{,}
        \DataTypeTok{"min_length"}\FunctionTok{:} \DecValTok{3}\FunctionTok{,}
        \DataTypeTok{"max_length"}\FunctionTok{:} \DecValTok{12}\FunctionTok{,}
        \DataTypeTok{"max_nulls"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
        \DataTypeTok{"no_duplicates"}\FunctionTok{:} \KeywordTok{true}
    \FunctionTok{\},}
    \DataTypeTok{"Symbol"}\FunctionTok{:} \FunctionTok{\{}
        \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"string"}\FunctionTok{,}
        \DataTypeTok{"min_length"}\FunctionTok{:} \DecValTok{1}\FunctionTok{,}
        \DataTypeTok{"max_length"}\FunctionTok{:} \DecValTok{2}\FunctionTok{,}
        \DataTypeTok{"max_nulls"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
        \DataTypeTok{"no_duplicates"}\FunctionTok{:} \KeywordTok{true}
    \FunctionTok{\},}
    \DataTypeTok{"ChemicalSeries"}\FunctionTok{:} \FunctionTok{\{}
        \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"string"}\FunctionTok{,}
        \DataTypeTok{"min_length"}\FunctionTok{:} \DecValTok{7}\FunctionTok{,}
        \DataTypeTok{"max_length"}\FunctionTok{:} \DecValTok{20}\FunctionTok{,}
        \DataTypeTok{"max_nulls"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
        \DataTypeTok{"allowed_values"}\FunctionTok{:} \OtherTok{[}
            \StringTok{"Actinoid"}\OtherTok{,}
            \StringTok{"Alkali metal"}\OtherTok{,}
            \StringTok{"Alkaline earth metal"}\OtherTok{,}
            \StringTok{"Halogen"}\OtherTok{,}
            \StringTok{"Lanthanoid"}\OtherTok{,}
            \StringTok{"Metalloid"}\OtherTok{,}
            \StringTok{"Noble gas"}\OtherTok{,}
            \StringTok{"Nonmetal"}\OtherTok{,}
            \StringTok{"Poor metal"}\OtherTok{,}
            \StringTok{"Transition metal"}
        \OtherTok{]}
    \FunctionTok{\},}
    \DataTypeTok{"AtomicWeight"}\FunctionTok{:} \FunctionTok{\{}
        \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"real"}\FunctionTok{,}
        \DataTypeTok{"min"}\FunctionTok{:} \FloatTok{1.007947}\FunctionTok{,}
        \DataTypeTok{"max"}\FunctionTok{:} \FloatTok{238.028913}\FunctionTok{,}
        \DataTypeTok{"sign"}\FunctionTok{:} \StringTok{"positive"}\FunctionTok{,}
        \DataTypeTok{"max_nulls"}\FunctionTok{:} \DecValTok{0}
    \FunctionTok{\},}
    \ErrorTok{...more...}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

We can apply these inferred rules to all elements up to number 118
using the \texttt{-7} option to get pure ASCII output
and the \texttt{-f} option to show only fields with failures:

\begin{verbatim}
$ tdda verify -f elements118.csv elements.tdda
\end{verbatim}

\begin{verbatim}
FIELDS:

Z: 1 failure  5 passes  type   min   max   sign   max_nulls   no_duplicates 

Name: 1 failure  4 passes  type   min_length   max_length   max_nulls   no_duplicates 

Symbol: 1 failure  4 passes  type   min_length   max_length   max_nulls   no_duplicates 

AtomicWeight: 2 failures  3 passes  type   min   max   sign   max_nulls 

...other reports...

SUMMARY:

Constraints passing: 57
Constraints failing: 15
\end{verbatim}

Another way to use TDDA is to generate constraints for two datasets and then look at differences
in order to see how similar the datasets are to each other.
This is especially useful if the constraint file is put under version control.

\hypertarget{rse-correct-summary}{%
\section{Summary}\label{rse-correct-summary}}

\begin{figure}
\centering
\includegraphics{figures/rse-correct/concept.pdf}
\caption{\label{fig:rse-correct-concept}Correctness Concept Map}
\end{figure}

\hypertarget{rse-correct-exercises}{%
\section{Exercises}\label{rse-correct-exercises}}

\hypertarget{rse-correct-ex-numsign}{%
\subsection{\texorpdfstring{Fixing \texttt{numSign}}{Fixing numSign}}\label{rse-correct-ex-numsign}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fix the implementation of \texttt{numSign} in either R or Python so that all tests will succeed.
\item
  Re-run the tests to confirm that the function is working.
\item
  Extend the function so that it does something sensible when passed a string or an empty list.
  Write functions to check your changes.
\end{enumerate}

\hypertarget{rse-correct-ex-simple-calc}{%
\subsection{Checking simple calculations}\label{rse-correct-ex-simple-calc}}

Create a new test by running \texttt{usethis::use\_test("simple-arithmetic")}.
Copy the code below into the newly created test file and fill in the blanks
so that it runs correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{context}\NormalTok{(}\StringTok{"Check simple calculations"}\NormalTok{)}

\KeywordTok{test_that}\NormalTok{(}\StringTok{"plus and minus give correct outputs"}\NormalTok{, \{}
  \KeywordTok{expect_identical}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
  \KeywordTok{___}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\DecValTok{2}\NormalTok{, }\DecValTok{-1}\NormalTok{)}
  \KeywordTok{expect_identical}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\OtherTok{NA}\NormalTok{, __)}
  \KeywordTok{expect_identical}\NormalTok{(___ }\OperatorTok{-}\StringTok{ }\NormalTok{___, }\OtherTok{NA}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{rse-correct-ex-purpose}{%
\subsection{Explain the purpose of tests}\label{rse-correct-ex-purpose}}

Based on the contents of the tests below,
replace the blanks with appropriate \texttt{context()} and \texttt{test\_that()} messages.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{context}\NormalTok{(}\StringTok{"___"}\NormalTok{)}

\KeywordTok{test_that}\NormalTok{(}\StringTok{"___"}\NormalTok{, \{}
  \KeywordTok{expect_identical}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
  \KeywordTok{expect_identical}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\OtherTok{NA}\NormalTok{)}
\NormalTok{\})}

\KeywordTok{test_that}\NormalTok{(}\StringTok{"___"}\NormalTok{, \{}
  \KeywordTok{expect_identical}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\FloatTok{1.5}\NormalTok{)}
  \KeywordTok{expect_identical}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\OtherTok{NA}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{rse-correct-ex-count-words}{%
\subsection{\texorpdfstring{Testing \texttt{count\_words}}{Testing count\_words}}\label{rse-correct-ex-count-words}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a new test file.
\item
  Write unit tests for \texttt{count\_words} to check the following:

  \begin{itemize}
  \tightlist
  \item
    Returns zero when given an empty string.
  \item
    Returns \texttt{NA} when given \texttt{NA}.
  \item
    Returns 2 when given two words separated by a space.
  \item
    Returns 2 when given two words separated by a dash \texttt{-}.
  \end{itemize}
\end{enumerate}

\hypertarget{rse-correct-ex-check-errors}{%
\subsection{Checking for errors}\label{rse-correct-ex-check-errors}}

Add new unit tests for the word-counting function
to check that it raises errors when it should.

\hypertarget{rse-correct-keypoints}{%
\section{Key Points}\label{rse-correct-keypoints}}

\begin{itemize}
\tightlist
\item
  Testing can only ever show that software has flaws, not that it is correct.
\item
  The real purpose is to convince people (including yourself) that software is correct enough
  and to make tolerances on `enough' explicit.
\item
  A testing suite finds and runs tests written in a prescribed fashion and reports their results.
\item
  A unit test can pass (work as expected),
  fail (meaning the software under test is flawed),
  or produce an error (meaning the test itself is flawed).
\item
  Every unit test should be independent of each other so results are comprehensible, contained, and reliable.
\item
  Tests should be built to test that the software fails when and as it is supposed.
\item
  Tests can usefully exercise particular pieces of functionality (unit tests)
  or larger pieces of the code base demonstrating interactions between units (integration tests)
\item
  Tests should verify software (make sure it does what it's supposed to)
  and also validate it (make sure it produces the right answers).
\item
  Data science is normally concerned with approximate correctness of solutions.
\item
  Stochastic statistical variation and deterministic floating point approximations both make answers approximately correct.
\item
  Check that parametric or non-parametric statistics of data
  do not differ from saved values by more than a specified tolerance.
\item
  Infer constraints on data and then check that subsequent data sets obey these constraints.
\item
  Test the data structures used in plotting rather than the plots themselves.
\end{itemize}

\hypertarget{rse-publish}{%
\chapter{Publishing}\label{rse-publish}}

This lesson looks at what should be included in and/or alongside reports and how best to do that.
We use the generic term ``report'' to include research papers,
summaries for clients,
and everything else that is shorter than a book and is going to be read by someone else.

Our motivation is summed up in this quotation:

\begin{quote}
An article about computational science in a scientific publication is \emph{not} the scholarship itself,
it is merely \emph{advertising} of the scholarship.
The actual scholarship is the complete software development environment
and the complete set of instructions which generated the figures.

--- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in Buckheit and Donoho (\protect\hyperlink{ref-Buck1995}{1995})
\end{quote}

As the quote suggests,
modern publishing involves much more than simply producing a report.
It involves providing readers with the data underpinning the report,
as well as any code written in analysing the data.

While the definition of data (and its associated metadata)
is relatively easy to understand,
code can come in many different forms.
Here we distinguish between ``analysis scripts'' written
solely for the purpose of the report (e.g.~to produce a figure)
and ``analysis software'' that is formally packaged and released for use by a wider audience.
Of course, in reality the analysis scripts/software written during the process of preparing
a report can often lie on a continuum between those two definitions.

While some reports, datasets, software packages and/or analysis scripts
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
That means publishing it under an open license (Section~\ref{rse-teams-software-license})
so that people who aren't in academia can find and access it.

\hypertarget{rse-publish-identifiers}{%
\section{Identification}\label{rse-publish-identifiers}}

Before publishing anything,
we need to understand the systems used to identify works
and their authors.

A \href{glossary.html\#doi}{Digital Object Identifier} (DOI)
is a unique identifier for a particular version of a particular digital artifact
such as a report, a dataset, or a piece of software.
DOIs are written as \texttt{doi:prefix/suffix},
but you will often also see them represented as URLs like \texttt{http://dx.doi.org/prefix/suffix}.
In order to be allowed to issue a DOI,
online platforms (e.g.~academic journals, data archives)
must guarantee a certain level of security, longevity and access.

An \href{https://orcid.org/}{ORCID} is an Open Researcher and Contributor ID.
You can get an ORCID for free,
and you should include it in publications
because people's names and affiliations change over time.

\hypertarget{rse-publish-document}{%
\section{Publishing a report}\label{rse-publish-document}}

The best option for publishing a report (with a platform that issues a DOI)
depends on the context.
For academic, peer-reviewed research papers,
numerous open access journals have popped up in recent years.
Many formerly closed-access journals also now offer an open access option
(for an additional fee).
Another option is to publish with an online pre-print server
(e.g. \href{https://www.biorxiv.org/}{bioRxiv}, \href{https://arxiv.org/}{arXiv}).
A preprint is a version of an academic research paper that precedes formal peer review
and publication in a peer-reviewed journal.
The preprint may be available, often as a non-typeset version available free,
before and/or after a paper is published in a journal.

Online writing platforms such as \href{https://authorea.com/}{Authorea} are also an option,
which allow a report to be openly viewable on the web throughout the entire writing process.
At the end of the process Authorea can issue its own DOI for the report,
or the text can be exported in the format required for submission to an academic journal.
Finally, online platforms such as \href{https://figshare.com}{Figshare} and
\href{https://zenodo.org/}{Zenodo}
are a place where any research outputs (reports, datasets, code, supplementary figures)
can be published with a DOI.
It is common for people to upload reports to these platforms so that they can be
easily accessed by others.

\hypertarget{rse-publish-data}{%
\section{Publishing data}\label{rse-publish-data}}

The first step in publishing the data associated with a report
is to determine what (if anything) needs to be published.

If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party (e.g.~open government data),
then it's likely that no data publishing is required.
The report simply needs to document where to access the data
and what version was analyzed,
along with any scripts and software used to download and process the data.
In other words, it's not necessary to re-publish a duplicate of the original dataset
if it's already accessible elsewhere.

Strictly speaking, it's not necessary to publish any data files
produced during the analysis of a publicly available dataset either,
since readers have access to the original data and the scripts/software used to process it.
Having said that, it can be advantageous to publish processed data that is difficult to reproduce
(e.g.~it might require access to a supercomputing facility to run the code)
and/or represents a derived quantity with high re-use potential.
For instance, it would be worthwhile to publish
an estimate of the global average surface temperature derived from
a large database of weather observations,
because a simple metric of global warming could be useful in many subsequent studies.

If a report involves the generation of a new dataset
(e.g.~observations collected during a field experiment),
then clearly that dataset needs to be published.
This section describes how to go about doing that.

\hypertarget{what-is-the-most-useful-way-to-share-my-data}{%
\subsection{What is the most useful way to share my data?}\label{what-is-the-most-useful-way-to-share-my-data}}

Making data useful to other people (including your future self)
is one of the best investments you can make.
The simple version of how to do this is:

\begin{itemize}
\tightlist
\item
  Always use \href{glossary.html\#tidy-data}{tidy data}.
\item
  Include keywords describing the data in the project's \texttt{README.md}
  so that they appear on its home page and can easily be found by search engines.
\item
  Give every dataset and every report a unique identifier (Section~\ref{rse-publish-identifiers}).
\item
  Put data in open repositories.
\item
  Use well-known formats like CSV and HDF5.
\item
  Include an explicit license in every project and every dataset.
\item
  Include units and other metadata.
\end{itemize}

The last point is often the hardest for people to implement,
since many researchers have never seen a well-documented dataset.
We draw inspiration from the data catalog included in \href{https://github.com/the-pudding/data/blob/master/pockets/README.md}{the repository}
for the article ``\href{https://pudding.cool/2018/08/pockets/}{Women's Pockets Are Inferior}''
and include a file \texttt{./data/README.md} in every project
that looks like this:

\begin{verbatim}
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   `Infant_HIV_Testing_2017.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2009-2017
    -   `infant_hiv.csv`
        -   What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_infant_hiv()` to tidy this data.
-   Maternal health indicators disaggregated by age
    -   `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/maternal-health-data/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2000-2014
    -   `at_health_facilities.csv`
        -   What is this?: percentage of births at health facilities by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `c_sections.csv`
        -   What is this?: percentage of Caesarean sections by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `skilled_attendant_at_birth.csv`
        -   What is this?: percentage of births with skilled attendant present by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_maternal_health_adolescents()` to tidy this data.
\end{verbatim}

The catalog above doesn't include column headers or units because the data isn't tidy.
It \emph{does} include the names of the functions used to reformat that data,
and \texttt{./results/README.md} then includes the information that users will want.
One section of that file is shown below:

\begin{verbatim}
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   infant_hiv.csv
      -   What is this?: tidied version of CSV export from spreadsheet.
      -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
      -   Last Modified: September 2018
      -   Contact: Greg Wilson <greg.wilson@rstudio.com>
      -   Spatial Applicability: global
      -   Temporal Applicability: 2009-2017
      -   Generated By: scripts/tidy-24.R

| Header   | Datatype | NA    | Description                                 |
|----------|----------|-------|---------------------------------------------|
| country  | char     | false | ISO3 country code of country reporting data |
| year     | integer  | false | year CE for which data reported             |
| estimate | double   | true  | estimated percentage of measurement         |
| hi       | double   | true  | high end of range                           |
| lo       | double   | true  | low end of range                            |
\end{verbatim}

Note that this catalog includes both units and whether or not a field can be NA.
Note also that calling a field ``NA'' is asking for trouble\ldots{}

\hypertarget{what-standards-of-data-sharing-should-i-aspire-to}{%
\subsection{What standards of data sharing should I aspire to?}\label{what-standards-of-data-sharing-should-i-aspire-to}}

The \href{https://www.go-fair.org/fair-principles/}{FAIR Principles} describe what research data should look like.
They are \href{https://www.natureindex.com/news-blog/what-scientists-need-to-know-about-fair-data}{still aspirational}
for most researchers,
but they tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

\hypertarget{data-should-be-findable.}{%
\subsubsection{\texorpdfstring{Data should be \emph{findable}.}{Data should be findable.}}\label{data-should-be-findable.}}

The first step in using or re-using data is to find it.
You can tell you've done this if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Meta)data is assigned a globally unique and persistent identifier (Section~\ref{rse-publish-identifiers}).
\item
  Data is described with rich metadata (like the catalog shown above).
\item
  Metadata clearly and explicitly includes the identifier of the data it describes.
\item
  (Meta)data is registered or indexed in a searchable resource,
  such as the data sharing platforms described in Section~\ref{rse-publish-data}.
\end{enumerate}

\hypertarget{data-should-be-accessible.}{%
\subsubsection{\texorpdfstring{Data should be \emph{accessible}.}{Data should be accessible.}}\label{data-should-be-accessible.}}

You can't use data if you don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
You can tell you've done this if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Meta)data is retrievable by its identifier using a standard communications protocol like HTTP.
\item
  Metadata is accessible even when the data is no longer available.
\end{enumerate}

\hypertarget{data-should-be-interoperable.}{%
\subsubsection{\texorpdfstring{Data should be \emph{interoperable}.}{Data should be interoperable.}}\label{data-should-be-interoperable.}}

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
You can tell you've done this if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation
\item
  (Meta)data uses vocabularies that follow FAIR principles
\item
  (Meta)data includes qualified references to other (meta)data
\end{enumerate}

\hypertarget{data-should-be-reusable.}{%
\subsubsection{\texorpdfstring{Data should be \emph{reusable}.}{Data should be reusable.}}\label{data-should-be-reusable.}}

This is the ultimate purpose of the FAIR Principles and much other work.
You can tell you've done this if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Meta(data) is described with accurate and relevant attributes.
\item
  (Meta)data is released with a clear and accessible data usage license.
\item
  (Meta)data has detailed \href{glossary.html\#provenance}{provenance}.
\item
  (Meta)data meets domain-relevant community standards.
\end{enumerate}

\hypertarget{how-and-where-do-i-publish-the-data}{%
\subsection{How and where do I publish the data?}\label{how-and-where-do-i-publish-the-data}}

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter~\ref{rse-project}.
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as \href{glossary.html\#data-package}{data packages},
and they are often accompanied by small scripts to clean up and query the data.
Be sure to give the dataset an identifier as discussed in Section~\ref{rse-publish-identifiers}.

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms like the \href{https://osf.io/}{Open Science Framework},
\href{https://datadryad.org/}{Dryad},
and \href{https://figshare.com/}{Figshare}.
Each of these will give the datasets identifiers;
those identifiers should be included in reports
along with scripts to download the data.
Big datasets (i.e., anything more than 5 GB)
may not be yours in the first place,
and probably need the attention of a professional archivist.
Any processed or intermediate data that takes a long time to regenerate
should probably be published as well using these same sizing rules;
all of this data should be given identifiers,
and those identifiers should be included in reports.

\begin{quote}
\textbf{Data journals}

While archiving data at a site like Dryad or Figshare (following the FAIR Principles)
is usually the end of the data publishing process,
there is the option of publishing a journal paper to describe the dataset in detail.
Some research disciplines have journals devoted
to describing particular types of data
(e.g. \href{https://rmets.onlinelibrary.wiley.com/journal/20496060}{Geoscience Data Journal})
and there are also generic data journals
(e.g. \href{https://www.nature.com/sdata/}{Scientific Data}).
\end{quote}

\hypertarget{rse-publish-software}{%
\section{Publishing analysis software}\label{rse-publish-software}}

In the preceding chapters we have learned how to document and package software
so that it can be installed and used by others.
The final step in this process is publication.

It is common practice to have the code associated with a software package
openly available on a hosting service such as GitHub (or GitLab or Bitbucket).
These hosting services are not only a convenient place for people to ask questions
and make contributions/improvements to the software,
they also have built-in functionality for managing the
release of new versions of the software.
One limitation of these sites, however,
is that they don't guarantee persistent long term storage
(e.g.~if you changed the name of your GitHub repository any URLs for
the existing repository would be broken).
Acknowledging this limitation,
GitHub provides \href{https://guides.github.com/activities/citable-code/}{Zenodo integration}
for creating a DOI with each new software release.

\begin{quote}
\textbf{Software journals}

While creating a DOI using a site like Zenodo
is often the end of the software publishing process,
there is the option of publishing
a journal paper to describe the software in detail.
Some research disciplines have journals devoted
to describing particular types of software
(e.g. \href{https://www.geoscientific-model-development.net/}{Geoscientific Model Development}),
and there are also a number of generic software journals such as the
\href{https://openresearchsoftware.metajnl.com/}{Journal of Open Research Software} and
\href{https://joss.theoj.org/}{Journal of Open Source Software}.
\end{quote}

\hypertarget{rse-publish-scripts}{%
\section{Publishing analysis scripts}\label{rse-publish-scripts}}

The final component that needs to be published is the analysis scripts.
Unlike analysis software that has been packaged and released for use by a wider audience,
analysis scripts are simply written to create the figures and tables presented in a given report.
In fact, these scripts would typically make use of analysis software written
by the wider data science community (e.g.~matplotlib, ggplot)
as well discipline-specific packages written by colleagues or co-authors
(e.g.~AstroPy).

Given that analysis scripts will typically leverage
a wide variety of existing software packages,
there's actually three separate items that need to be published:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A detailed description of the analysis software used
\item
  A copy of any analysis scripts written by the authors to produce the key results
  presented in the report
\item
  A description of the data processing steps taken in producing each key result
  (i.e.~a step-by-step account of how the software and scripts were actually implemented)
\end{enumerate}

Earlier we saw that there are well-developed and widely adopted
guidelines for data publishing (e.g.~the FAIR Principles).
The same is not true for analysis scripts.
Librarians, publishers, and regulatory bodies are still trying to determine
the best way for code to be documented and archived.
For the moment,
the best advice we can give for those three key items is discussed below.
That advice ranges from the bare minimum that needs to be done
through to current gold standard practice.

\hypertarget{software-description}{%
\subsection{Software description}\label{software-description}}

In order to document the software packages that were used,
the bare minimum requirement is to list the name and version number
of each software package
that played a critical role in producing the analysis presented in your report.

As Section~\ref{rse-package-py-package} described,
you can get these automatically by running:

\begin{verbatim}
$ pip freeze > requirements.txt
\end{verbatim}

For everything else,
you should write a script or create a rule in your project's Makefile (Chapter~\ref{rse-automate}),
since the commands used to get version numbers will vary from tool to tool:

\begin{verbatim}
## versions : dump versions of software.
versions :
        @echo '# Python packages'
        @pip freeze
        @echo '# dezply'
        @dezply --version
        @echo '# parajune'
        @parajune --status | head 1
\end{verbatim}

While such a list means your software environment is now technically reproducible,
you've left it up to the reader to figure out how to get all those software packages
and libraries installed and playing together nicely.
In some cases this is fine
(e.g.~it might be easy enough for a reader to install the handful of R packages you used),
but in other cases you might want to save the reader (and your future self)
the pain of software installation by making use of a tool
that can automatically install a specified software environment.
The most prominent such tool in data science at the moment is \href{https://docs.conda.io}{conda}.
A conda environment can be exported,

\begin{verbatim}
$ conda env export -n myenv -f myenv.yml
\end{verbatim}

and made available so that readers can use it to install the same environment on their
own computer:

\begin{verbatim}
$ conda env create -f myenv.yml
\end{verbatim}

\begin{quote}
\textbf{Conda environments}

The Python for Atmosphere and Ocean Scientists lesson materials
maintained by Data Carpentry have a section devoted to
\href{https://carpentrieslab.github.io/python-aos-lesson/01-conda/index.html}{Software Installation using Conda}
\end{quote}

Beyond conda there are more complex tools like \href{https://en.wikipedia.org/wiki/Docker_(software)}{Docker} and Nix,
which can literally install your entire environment (down to the precise operating system)
on a different computer.
There's lots of debate
about the potential and suitability of these tools as a solution to reproducible research,
but it's fair to say that their complexity puts them out of reach
for many researchers.

\hypertarget{analysis-scripts}{%
\subsection{Analysis scripts}\label{analysis-scripts}}

The next item you'll need to publish is a copy of the scripts
written to execute those software packages.
Depending on the size or complexity of the scripts you have written,
and whether you re-use them in multiple projects,
you may publish script by script
or create a zip file or tar file that includes everything.
For example,
the Makefile fragment below creates \texttt{\textasciitilde{}/archive/meow-2019-02-21.tgz}:

\begin{verbatim}
ARCHIVE=${HOME}/archive
PROJECT=meow
TODAY=$(shell date "+%Y-%m-%d")
SCRIPTS=./Makefile ./bin/*.py ./bin/*.sh

## archive : create an archive of all the scripts used in this run
archive :
        @mkdir -p ${ARCHIVE}
        @tar zcf ${ARCHIVE}/${PROJECT}-${TODAY}.tgz
\end{verbatim}

\hypertarget{data-processing-steps}{%
\subsection{Data processing steps}\label{data-processing-steps}}

A software description and analysis scripts on their own are not much use to a reader;
they also need to know how those scripts was actually executed.
This means including the configuration files (Chapter~\ref{configure}),
and/or command-line parameters used to generate each key result.

The way in which this information is collected and archived
depends on how your workflow is constructed.
If all of a program's parameters are in a configuration file (Chapter~\ref{configure}),
then that file can be archived.
Alternatively, you might need to have your program print out its configuration parameters
and then use \texttt{grep} or a script to extract information from the logfile (Chapter~\ref{logging}).

If your workflow involves executing a series of command line programs,
then you can keep a log/record of the command line entries
required to produce a given result.
For example, the \href{https://cmdline-provenance.readthedocs.io/en/latest/}{cmdline-provenance}
package generates such records,
including keeping track of the corresponding version control revision number,
so you know exactly which version of your command line program was executed.

As before, while these bare minimum log files ensure that your workflow is reproducible,
they may not be particularly comprehensible.
Manually recreating workflows from them might be a tedious and time consuming process,
even for just moderately complex analyses.
To make things a little easier for the reader (and your future self),
it's a good idea to include a README file in your code library
explaining the sequence of commands required to produce common/key results.
You might also provide a Makefile that automatically builds and executes common workflows.
Beyond that the options get more complex,
with workflow management packages like \href{https://www.vistrails.org/index.php/Main_Page}{VisTrails}
providing a graphical interface
that allows users to drag and drop the various components of their workflow.

\hypertarget{where-to-publish-all-this-stuff}{%
\subsection{Where to publish all this stuff?}\label{where-to-publish-all-this-stuff}}

Following the steps above, you'll be left with a text file
(or perhaps an environment file exported from a conda)
describing your software environment,
a copy of your code library and various log files, and README files and/or Makefiles
that describe your data processing steps.
Sites like Figshare and Zenodo are the ideal place to publish these items,
as they have been specifically setup for archiving the ``long tail'' of reports
(e.g.~supplementary figures, tables, code and data).

\hypertarget{rse-publish-summary}{%
\section{Summary}\label{rse-publish-summary}}

FIXME: create concept map for publishing

\hypertarget{rse-publish-exercises}{%
\section{Exercises}\label{rse-publish-exercises}}

\hypertarget{orcid}{%
\subsection{ORCID}\label{orcid}}

If you don't already have an \href{https://orcid.org/}{ORCID},
go to the website and register now.

If you do have an ORCID,
login at the website and make sure that your details
and publication record are up-to-date.

\hypertarget{a-fair-test}{%
\subsection{A FAIR test}\label{a-fair-test}}

An \href{https://www.ands-nectar-rds.org.au/fair-tool}{online questionnaire}
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.

Take the questionnaire for a dataset you have published
or that you use often.

\hypertarget{publishing-your-code}{%
\subsection{Publishing your code}\label{publishing-your-code}}

Think about a project that you're currently working on.
How would you go about publishing the code associated with that project?
(i.e.~the software description, analysis scripts and data processing steps)

\hypertarget{rse-publish-keypoints}{%
\section{Key Points}\label{rse-publish-keypoints}}

\begin{itemize}
\tightlist
\item
  Include small datasets in repositories; store large ones on data sharing sites, and include metadata in the repository to locate them.
\item
  An ORCID is a unique personal identifier that you can use to identify your work.
\item
  A DOI is a unique identifier for a particular dataset, report, or software release.
\item
  Data should be findable, accessible, interoperable, and reusable (FAIR).
\item
  Use Zenodo to obtain DOIs.
\item
  Publish your software as you would a paper.
\end{itemize}

\hypertarget{rse-finale}{%
\chapter{Finale}\label{rse-finale}}

FIXME: how do we end?

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{license}{%
\chapter{License}\label{license}}

\emph{This is a human-readable summary of (and not a substitute for) the license.
Please see \url{https://creativecommons.org/licenses/by/4.0/legalcode} for the full legal text.}

This work is licensed under the Creative Commons Attribution 4.0
International license (CC-BY-4.0).

\textbf{You are free to:}

\begin{itemize}
\item
  \textbf{Share}---copy and redistribute the material in any medium or
  format
\item
  \textbf{Remix}---remix, transform, and build upon the material for any
  purpose, even commercially.
\end{itemize}

The licensor cannot revoke these freedoms as long as you follow the
license terms.

\textbf{Under the following terms:}

\begin{itemize}
\item
  \textbf{Attribution}---You must give appropriate credit, provide a link
  to the license, and indicate if changes were made. You may do so in
  any reasonable manner, but not in any way that suggests the licensor
  endorses you or your use.
\item
  \textbf{No additional restrictions}---You may not apply legal terms or
  technological measures that legally restrict others from doing
  anything the license permits.
\end{itemize}

\textbf{Notices:}

You do not have to comply with the license for elements of the
material in the public domain or where your use is permitted by an
applicable exception or limitation.

No warranties are given. The license may not give you all of the
permissions necessary for your intended use. For example, other rights
such as publicity, privacy, or moral rights may limit how you use the
material.

\hypertarget{conduct}{%
\chapter{Code of Conduct}\label{conduct}}

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of
experience, education, socio-economic status, nationality, personal appearance,
race, religion, or sexual identity and orientation.

\hypertarget{conduct-standards}{%
\section{Our Standards}\label{conduct-standards}}

Examples of behavior that contributes to creating a positive environment
include:

\begin{itemize}
\tightlist
\item
  using welcoming and inclusive language,
\item
  being respectful of differing viewpoints and experiences,
\item
  gracefully accepting constructive criticism,
\item
  focusing on what is best for the community, and
\item
  showing empathy towards other community members.
\end{itemize}

Examples of unacceptable behavior by participants include:

\begin{itemize}
\tightlist
\item
  the use of sexualized language or imagery and unwelcome sexual
  attention or advances,
\item
  trolling, insulting/derogatory comments, and personal or political
  attacks,
\item
  public or private harassment,
\item
  publishing others' private information, such as a physical or
  electronic address, without explicit permission, and
\item
  other conduct which could reasonably be considered inappropriate in
  a professional setting
\end{itemize}

\hypertarget{conduct-responsibilities}{%
\section{Our Responsibilities}\label{conduct-responsibilities}}

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, or to ban temporarily or permanently any
contributor for other behaviors that they deem inappropriate, threatening,
offensive, or harmful.

\hypertarget{conduct-scope}{%
\section{Scope}\label{conduct-scope}}

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

\hypertarget{conduct-enforcement}{%
\section{Enforcement}\label{conduct-enforcement}}

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by \href{mailto:gvwilson@third-bit.com}{emailing the project team}. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an
incident. Further details of specific enforcement policies may be posted
separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

\hypertarget{conduct-attribution}{%
\section{Attribution}\label{conduct-attribution}}

This Code of Conduct is adapted from the
\href{https://www.contributor-covenant.org}{Contributor Covenant} version 1.4.

\hypertarget{contributing}{%
\chapter{Contributing}\label{contributing}}

Contributions of all kinds are welcome.
By offering a contribution, you agree to abide by our \href{CONDUCT.md}{Code of Conduct}
and that your work may be made available under the terms of \href{LICENSE.md}{our license}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To report a bug or request a new feature,
  please check the \href{https://github.com/tidyblocks/tidyblocks/issues}{list of open issues}
  to see if it's already there,
  and if not,
  file as complete a description as you can.
\item
  If you have made a fix or improvement,
  please create a \href{https://github.com/tidyblocks/tidyblocks/pulls}{pull request}.
  We will review these as quickly as we can (typically within 2-3 days).
  If you are tackling an issue that has already been opened,
  please name your branch \texttt{number-some-description}
  (e.g., \texttt{20-highlighting-active-block})
  and put \texttt{Closes\ \#N} (e.g., \texttt{Closes\ \#20})
  on a line by itself at the end of the PR's long description.
\end{enumerate}

\hypertarget{contributing-style}{%
\section{Style Guide}\label{contributing-style}}

We follow the \href{https://style.tidyverse.org/}{tidyverse style guide} for R and
\href{https://www.python.org/dev/peps/pep-0008/}{PEP 8} for Python as closely as
possible but specify some conventions further. We go against the style guides
only when it is considered that it will improve clarity.

Specific conventions include:

\begin{itemize}
\item
  \texttt{variable\_name} (snake\_case)
\item
  \texttt{function\_name} and \texttt{method\_name} (snake\_case)

  \begin{itemize}
  \tightlist
  \item
    Please do \emph{not} include empty parentheses to indicate a function,
    as this makes it hard to distinguish a function name from a call with no arguments.
  \end{itemize}
\item
  \texttt{folder-name/} (hyphens instead of underscores, trailing slash for clarity)
\item
  \texttt{file-name} (hyphens instead of underscores)
\item
  \texttt{\textquotesingle{}string\textquotesingle{}} and \texttt{"string"}

  \begin{itemize}
  \tightlist
  \item
    We will settle on single vs.~double quotes before we publish :-)
  \end{itemize}
\item
  Method chaining in pandas:

\begin{verbatim}
(dataframe
    .method()
    .method(short_arg)
    .method(
        long_arg1,
        long_arg2))
\end{verbatim}
\end{itemize}

For markdown, we use ATX-headers (\texttt{\#} prefix) rather than Setext headers (\texttt{=/-}
underlines), links with \texttt{{[}linkname{]}{[}tag{]}} rather than \texttt{{[}linkname{]}(url)}, and
fenced code blocks rather than indented blocks.

There are more details for what we recommend for learners in
\url{rse-style.Rmd}. Discuss further in \href{https://github.com/merely-useful/merely-useful.github.io/issues/116}{issue \#116}.

Please note that we use Simplified English rather than Traditional English,
i.e., American rather than British spelling and grammar.

\hypertarget{setting-up-1}{%
\section{Setting Up}\label{setting-up-1}}

This book is written in \href{https://bookdown.org/}{Bookdown}.
If you want to preview builds on your own computer, please:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Follow the instructions for installing Bookdown.
\item
  Run \texttt{make\ everything} to recompile everything.

  \begin{itemize}
  \tightlist
  \item
    Run \texttt{make} on its own to see a list of targets for rebuilding specific volumes as HTML or PDF.
  \end{itemize}
\end{enumerate}

Please note that Bookdown works best with \href{https://yihui.name/tinytex/}{TinyTeX}.
After installing it, you can run \texttt{make\ tex-packages} to install all the packages this book depends on.
You do \emph{not} need to do this if you are only building and previewing the HTML versions of the books.

\hypertarget{glossary}{%
\chapter{Glossary}\label{glossary}}

\begin{description}
\tightlist
\item[\textbf{Abandonware}]
FIXME
\item[\textbf{Absolute error}]
FIXME
\item[\textbf{Absolute path}]
FIXME
\item[\textbf{Accuracy}]
FIXME
\end{description}

\textbf{Action} (in Make):
FIXME

\begin{description}
\tightlist
\item[\textbf{Active listening}]
FIXME
\end{description}

\textbf{Actual output} (of a test):
FIXME

\begin{description}
\tightlist
\item[\textbf{Actual result}]
FIXME
\item[\textbf{Aggregate}]
FIXME
\item[\textbf{Agile development}]
FIXME
\item[\textbf{Ally}]
FIXME
\item[\textbf{Analysis and estimation}]
FIXME
\end{description}

\textbf{Annotated tag} (in version control):
FIXME

\begin{description}
\tightlist
\item[\textbf{Append mode}]
FIXME
\end{description}

\textbf{Application Programming Interface} (API):
FIXME

\begin{description}
\tightlist
\item[\textbf{Assertion}]
FIXME
\item[\textbf{Authentic task}]
A task which contains important elements of things that learners would do in real (non-classroom situations).
To be authentic,
a task should require learners to construct their own answers rather than choose between provided answers,
and to work with the same tools and data they would use in real life.
\item[\textbf{Auto-completion}]
FIXME
\item[\textbf{Automatic variable}]
FIXME
\end{description}

\textbf{Automatic variable} (in Make):
FIXME

\begin{description}
\tightlist
\item[\textbf{Backlog}]
FIXME
\item[\textbf{Bash}]
FIXME
\item[\textbf{Beeswarm plot}]
FIXME
\item[\textbf{Binary code}]
FIXME
\item[\textbf{Bit rot}]
FIXME
\item[\textbf{Boilerplate}]
FIXME
\item[\textbf{Branch}]
FIXME
\item[\textbf{Branch-per-feature workflow}]
FIXME \url{https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow}
\item[\textbf{Breakpoint}]
FIXME
\item[\textbf{Buffer}]
FIXME
\item[\textbf{Bug report}]
FIXME
\item[\textbf{Bug tracker}]
FIXME
\item[\textbf{Build tool}]
FIXME
\item[\textbf{Build tool}]
FIXME \url{https://en.wikipedia.org/wiki/List_of_build_automation_software}
\item[\textbf{Byte code}]
FIXME
\item[\textbf{Call stack}]
FIXME
\item[\textbf{Camel case}]
FIXME
\end{description}

\textbf{Catch} (an exception):
FIXME

\textbf{Checking-driven development} (CDD):
FIXME

\begin{description}
\tightlist
\item[\textbf{Checklist}]
FIXME
\item[\textbf{Code browser}]
FIXME
\item[\textbf{Code coverage}]
FIXME
\item[\textbf{Cognitive load}]
FIXME
\end{description}

\textbf{Comma-separated values} (CSV):
FIXME

\begin{description}
\tightlist
\item[\textbf{Command argument}]
FIXME
\item[\textbf{Command flag}]
FIXME
\item[\textbf{Command history}]
FIXME
\item[\textbf{Command option}]
FIXME
\item[\textbf{Command shell}]
FIXME
\item[\textbf{Command switch}]
FIXME
\end{description}

\textbf{Command-line interface} (CLI):
FIXME

\begin{description}
\tightlist
\item[\textbf{Comment}]
FIXME
\item[\textbf{Commit hash}]
FIXME
\item[\textbf{Commit message}]
FIXME
\item[\textbf{Commit}]
FIXME
\item[\textbf{Commons}]
FIXME
\item[\textbf{Competent practitioner}]
Someone who can do normal tasks with normal effort under normal circumstances.
See also \href{glossary.html\#novice}{novice} and \href{glossary.html\#novice}{expert}.
\item[\textbf{Compiled language}]
FIXME
\item[\textbf{Compiler}]
FIXME
\item[\textbf{Computational competence}]
FIXME
\item[\textbf{Computational notebook}]
FIXME
\item[\textbf{Computational stylometry}]
FIXME
\item[\textbf{Computational thinking}]
FIXME
\item[\textbf{Conditional expression}]
FIXME
\item[\textbf{Configuration object}]
FIXME
\item[\textbf{Confirmation bias}]
FIXME
\item[\textbf{Context manager}]
FIXME
\item[\textbf{Continuation prompt}]
FIXME
\item[\textbf{Continuous integration}]
FIXME
\item[\textbf{Corpus}]
FIXME
\item[\textbf{Coverage}]
FIXME
\end{description}

\textbf{Creative Commons - Attribution License} (CC-BY):
FIXME

\begin{description}
\tightlist
\item[\textbf{Curb cuts}]
\url{https://medium.com/@mosaicofminds/the-curb-cut-effect-how-making-public-spaces-accessible-to-people-with-disabilities-helps-everyone-d69f24c58785}
\item[\textbf{Current working directory}]
FIXME
\item[\textbf{DRY (Don't Repeat Yourself)}]
The general principle when programming that it's typically better to define something (a function, a constant\ldots{}) once
and refer to it consistently as a ``single source of truth'' throughout a piece of software
than to define copies in multiple places,
if only because then you only have to make any changes in one place.
This is useful and helpful principle but should not be thought of as an involate rule.
\item[\textbf{Data engineering}]
FIXME
\item[\textbf{Data package}]
FIXME
\item[\textbf{Declarative programming}]
FIXME
\item[\textbf{Default target}]
FIXME
\end{description}

\textbf{Default target} (in Make):
FIXME

\begin{description}
\tightlist
\item[\textbf{Delimiter}]
FIXME
\item[\textbf{Dependency graph}]
FIXME
\item[\textbf{Design pattern}]
FIXME
\item[\textbf{Design patterns}]
FIXME
\item[\textbf{Destructuring}]
FIXME
\item[\textbf{Dictionary}]
FIXME
\end{description}

\textbf{Digital Object Identifier} (DOI):
FIXME

\begin{description}
\tightlist
\item[\textbf{Directory Tree}]
If the nesting relationships between directories in a filesystem are drawn as arrows from the containing directory to the nested ones,
a tree structure develops.
\item[\textbf{Directory}]
A folder in a filesystem.
\item[\textbf{Disk}]
Disk refers to disk storage, a physical component of a computer that stores information on a disk.
The most common kind of disk storage is a hard disk drive,
which is a storage drive with a non-removable disk.
\item[\textbf{Docstring}]
FIXME
\item[\textbf{Documentation generator}]
FIXME
\item[\textbf{Downvote}]
FIXME
\item[\textbf{Embedded documentation}]
FIXME
\item[\textbf{Eniac}]
FIXME
\item[\textbf{Environment}]
FIXME
\end{description}

\textbf{Error} (result from a unit test):
FIXME

\begin{description}
\tightlist
\item[\textbf{Escape sequence}]
FIXME
\item[\textbf{Exception}]
FIXME
\item[\textbf{Exit status}]
FIXME
\end{description}

\textbf{Expected output} (of a test):
FIXME

\begin{description}
\tightlist
\item[\textbf{Expected result}]
FIXME
\item[\textbf{Expert}]
Someone who can diagnose and handle unusual situations,
knows when the usual rules do not apply,
and tends to recognize solutions rather than reasoning to them.
See also \href{glossary.html\#competent-practitioner}{competent practitioner} and \href{glossary.html\#novice}{novice}.
\item[\textbf{Exploratory programming}]
FIXME
\item[\textbf{Exponent}]
FIXME
\item[\textbf{External error}]
FIXME
\end{description}

\textbf{Failure} (result from a unit test):
FIXME

\begin{description}
\tightlist
\item[\textbf{False beginner}]
Someone who has studied a language before but is learning it again.
False beginners start at the same point as true beginners
(i.e., a pre-test will show the same proficiency)
but can move much more quickly.
\item[\textbf{False negative}]
FIXME
\item[\textbf{False positive}]
FIXME
\item[\textbf{Feature boxing}]
FIXME
\item[\textbf{Feature creep}]
FIXME
\item[\textbf{Feature request}]
FIXME
\item[\textbf{Filename extension}]
FIXME
\item[\textbf{Filename stem}]
FIXME
\item[\textbf{Filesystem}]
Controls how files are stored and retrieved on disk by an operating system.
Also used to refer to the disk that is used to store the files or the type of the filesystem.
\item[\textbf{Filter}]
FIXME
\item[\textbf{Fixture}]
FIXME
\item[\textbf{Flag variable}]
FIXME
\item[\textbf{Flag}]
FIXME
\item[\textbf{Folder}]
FIXME
\item[\textbf{Forge}]
FIXME
\item[\textbf{Fork}]
FIXME
\item[\textbf{Format string}]
FIXME
\end{description}

\textbf{Frequently Asked Questions} (FAQ):
FIXME

\textbf{Full identifier} (in Git):
FIXME

\begin{description}
\tightlist
\item[\textbf{Fully-qualified name}]
FIXME
\item[\textbf{Function attribute}]
FIXME
\end{description}

\textbf{Function} (in Make):
FIXME

\textbf{GNU Public License} (GPL):
FIXME

\begin{description}
\tightlist
\item[\textbf{Git branch}]
FIXME
\item[\textbf{Git clone}]
FIXME
\item[\textbf{Git conflict}]
FIXME
\item[\textbf{Git fork}]
FIXME
\item[\textbf{Git merge}]
FIXME
\item[\textbf{Git pull}]
FIXME
\item[\textbf{Git push}]
FIXME
\item[\textbf{Git stage}]
FIXME
\item[\textbf{Git}]
FIXME
\item[\textbf{GitHub Pages}]
FIXME
\item[\textbf{Globbing}]
FIXME
\end{description}

\textbf{Graphical user interface} (GUI):
FIXME

\begin{description}
\tightlist
\item[\textbf{HTTP status code}]
FIXME
\item[\textbf{Hitchhiker}]
FIXME
\item[\textbf{Home directory}]
FIXME
\item[\textbf{Hot spot}]
FIXME
\item[\textbf{ISO date format}]
FIXME
\item[\textbf{Impostor syndrome}]
FIXME
\item[\textbf{In-place operator}]
FIXME
\item[\textbf{Index}]
FIXME
\item[\textbf{Install}]
FIXME
\end{description}

\textbf{Integrated Development Environment} (IDE):
FIXME

\begin{description}
\tightlist
\item[\textbf{Internal error}]
FIXME
\item[\textbf{Interpeter}]
FIXME
\item[\textbf{Interpreted language}]
FIXME
\item[\textbf{Interruption bingo}]
FIXME
\item[\textbf{Issue tracking system}]
FIXME
\item[\textbf{Issue}]
FIXME
\end{description}

\textbf{Iteration} (in software development):
FIXME

\begin{description}
\tightlist
\item[\textbf{JSON}]
FIXME
\end{description}

\textbf{Jenny} (a repository):
FIXME

\textbf{Join} (of database tables):
FIXME

\begin{description}
\tightlist
\item[\textbf{Kebab case}]
FIXME
\end{description}

\textbf{Label} (in issue tracker):
FIXME

\begin{description}
\tightlist
\item[\textbf{Learned helplessness}]
FIXME
\item[\textbf{Library}]
FIXME
\item[\textbf{Linter}]
FIXME
\item[\textbf{List comprehension}]
FIXME
\item[\textbf{Log file}]
FIXME
\item[\textbf{Logging framework}]
FIXME
\item[\textbf{Loop body}]
FIXME
\end{description}

\textbf{Loop} (in Unix):
FIXME

\begin{description}
\tightlist
\item[\textbf{MIT License}]
FIXME
\item[\textbf{Macro}]
FIXME
\item[\textbf{Magic number}]
FIXME
\item[\textbf{Magnitude}]
FIXME
\item[\textbf{Makefile}]
FIXME
\item[\textbf{Mantissa}]
FIXME
\item[\textbf{Martha's Rules}]
FIXME
\item[\textbf{Memory}]
A physical device on your computer that temporarily stores information for immediate use.
\item[\textbf{Mental model}]
A simplified representation of the key elements and relationships of some problem domain
that is good enough to support problem solving.
\item[\textbf{Method}]
A function that is specific to an object type,
based on qualities of that type,
e.g.~a string method like \texttt{upper()} which turns characters in a string to uppercase.
\item[\textbf{Namespace}]
A way of organizing names of related objects, functions, or variables
to avoid confusion with (for instance) common names that might well occur in multiple packages.
\item[\textbf{Nano}]
FIXME
\item[\textbf{Ngo}]
FIXME
\end{description}

\textbf{Not Invented Here} (NIH):
FIXME

\begin{description}
\tightlist
\item[\textbf{Novice}]
Someone who has not yet built a usable mental model of a domain.
See also \href{glossary.html\#competent-practitioner}{competent practitioner} and \href{glossary.html\#expert}{expert}.
\item[\textbf{ORCID}]
FIXME
\item[\textbf{Object}]
An object is a programming language's way of descrbing and storing values,
usually labeled with a variable name.
\item[\textbf{Object-oriented programming}]
FIXME
\item[\textbf{Open license}]
FIXME
\item[\textbf{Open science}]
FIXME
\item[\textbf{Operating system}]
FIXME
\item[\textbf{Operational test}]
FIXME
\item[\textbf{Oppression}]
FIXME
\item[\textbf{Orthogonality}]
FIXME
\item[\textbf{Overlay configuration}]
FIXME
\item[\textbf{Overloading}]
FIXME
\item[\textbf{Package}]
FIXME
\item[\textbf{Pager}]
FIXME
\item[\textbf{Pair programming}]
FIXME
\item[\textbf{Parent directory}]
FIXME
\item[\textbf{Parking lot}]
FIXME
\item[\textbf{Path coverage}]
FIXME
\item[\textbf{Path}]
FIXME
\item[\textbf{Pattern rule}]
FIXME
\item[\textbf{Pattern rule}]
FIXME
\item[\textbf{Peer action}]
FIXME
\item[\textbf{Phony target}]
FIXME
\item[\textbf{Phony target}]
FIXME
\end{description}

\textbf{Pipe} (in Unix):
FIXME

\begin{description}
\tightlist
\item[\textbf{Post-mortem}]
FIXME
\item[\textbf{Pothole case}]
FIXME
\item[\textbf{Precision}]
FIXME
\end{description}

\textbf{Prerequisite} (in Make):
FIXME

\begin{description}
\tightlist
\item[\textbf{Privilege}]
FIXME
\item[\textbf{Procedural programming}]
FIXME
\item[\textbf{Process}]
FIXME
\item[\textbf{Product manager}]
FIXME
\item[\textbf{Project manager}]
FIXME
\item[\textbf{Prompt}]
FIXME
\item[\textbf{Provenance}]
FIXME
\end{description}

\textbf{Pseudorandom number generator} (PRNG):
FIXME

\textbf{Public domain license} (CC-0):
FIXME

\begin{description}
\tightlist
\item[\textbf{Pull request}]
FIXME
\item[\textbf{Python}]
FIXME
\item[\textbf{Raise}]
FIXME
\end{description}

\textbf{Raise} (exception):
FIXME

\begin{description}
\tightlist
\item[\textbf{Raster image}]
FIXME
\item[\textbf{Rebase}]
FIXME
\item[\textbf{Recursion}]
FIXME
\item[\textbf{Redirection}]
FIXME
\item[\textbf{Refactor}]
FIXME
\item[\textbf{Refactoring}]
FIXME
\item[\textbf{Regular expression}]
FIXME
\item[\textbf{Relative error}]
FIXME
\item[\textbf{Relative import}]
In Python,
the importing of a module relative to the current path
and thus likely from within the current package (e.g., \texttt{from\ .\ import\ generate})
rather than an import from a globally-defined package (e.g., \texttt{from\ zipfpy\ import\ generate}).
\item[\textbf{Relative path}]
FIXME
\item[\textbf{Remote login}]
FIXME
\item[\textbf{Remote repository}]
FIXME
\item[\textbf{Repl}]
FIXME
\item[\textbf{Repository}]
FIXME
\end{description}

\textbf{Representation State Transfer} (REST):
FIXME

\textbf{Reproducible example} (reprex):
FIXME

\begin{description}
\tightlist
\item[\textbf{Reproducible research}]
FIXME
\end{description}

\textbf{Research software engineer} (RSE):
FIXME

\begin{description}
\tightlist
\item[\textbf{Restructured Text (reST)}]
A plain text markup language used by much Python documentation and documentation tooling.
\item[\textbf{Revision}]
FIXME
\item[\textbf{Root directory}]
FIXME
\item[\textbf{Rotating file}]
FIXME
\end{description}

\textbf{Rule} (in Make):
FIXME

\begin{description}
\tightlist
\item[\textbf{SSH key}]
FIXME
\item[\textbf{SSH protocol}]
FIXME
\end{description}

\textbf{Scalable Vector Graphics} (SVG):
FIXME

\begin{description}
\tightlist
\item[\textbf{Script}]
FIXME
\end{description}

\textbf{Seed} (for pseudorandom number generator):
FIXME

\begin{description}
\tightlist
\item[\textbf{Semantic versioning}]
FIXME \url{https://semver.org/}
\item[\textbf{Sense vote}]
FIXME
\end{description}

\textbf{Set and override} (pattern):
FIXME

\begin{description}
\tightlist
\item[\textbf{Shebang}]
FIXME
\item[\textbf{Shell script}]
FIXME
\item[\textbf{Short circuit test}]
FIXME
\end{description}

\textbf{Short identifier} (in Git):
FIXME

\begin{description}
\tightlist
\item[\textbf{Side effects}]
FIXME
\item[\textbf{Sign}]
FIXME
\item[\textbf{Silent error}]
FIXME
\item[\textbf{Silent failure}]
FIXME
\item[\textbf{Situational action}]
FIXME
\item[\textbf{Snake case}]
FIXME
\item[\textbf{Software development process}]
FIXME
\item[\textbf{Source code}]
FIXME
\item[\textbf{Stand-up meeting}]
FIXME
\item[\textbf{Standard error}]
FIXME
\item[\textbf{Standard error}]
FIXME
\item[\textbf{Standard input}]
FIXME
\item[\textbf{Standard input}]
FIXME
\item[\textbf{Standard output}]
FIXME
\item[\textbf{Standard output}]
FIXME
\item[\textbf{Streaming data}]
FIXME
\item[\textbf{Sturdy development}]
FIXME
\item[\textbf{Subcommand}]
FIXME
\item[\textbf{Subdirectory}]
FIXME
\item[\textbf{Subsampling}]
FIXME
\end{description}

\textbf{Success} (result from a unit test):
FIXME

\begin{description}
\tightlist
\item[\textbf{Sustainability}]
FIXME
\item[\textbf{Sustainable software}]
FIXME
\item[\textbf{Symbolic debugger}]
FIXME
\item[\textbf{Syntax highlighting}]
FIXME
\item[\textbf{Synthetic data}]
FIXME
\item[\textbf{Tab completion}]
FIXME
\end{description}

\textbf{Tag} (in version control):
FIXME

\begin{description}
\tightlist
\item[\textbf{Tag}]
FIXME
\item[\textbf{Target}]
FIXME
\end{description}

\textbf{Target} (in Make):
FIXME

\textbf{Target} (of oppression):
FIXME

\begin{description}
\tightlist
\item[\textbf{Technical debt}]
FIXME
\item[\textbf{Test coverage}]
FIXME
\item[\textbf{Test framework}]
FIXME
\item[\textbf{Test isolation}]
FIXME
\item[\textbf{Test runner}]
FIXME
\item[\textbf{Test-driven development}]
FIXME
\item[\textbf{Three stickies}]
FIXME
\item[\textbf{Ticket}]
FIXME
\item[\textbf{Ticketing system}]
FIXME
\item[\textbf{Tidy data}]
As defined in Wickham (\protect\hyperlink{ref-Wick2014}{2014}), tabular data is tidy if (1) each variable is in one column,
(2) each different observation of that variable is in a different row,
(3) there is one table for each kind of variable, and
(4) if there are multiple tables, each includes a key so that related data can be linked.
\item[\textbf{Time boxing}]
FIXME
\end{description}

\textbf{Timestamp} (on a file):
FIXME

\begin{description}
\tightlist
\item[\textbf{Tldr}]
FIXME
\item[\textbf{Tolerance}]
FIXME
\item[\textbf{Transitive dependency}]
FIXME
\item[\textbf{Triage}]
FIXME
\item[\textbf{Tuning}]
FIXME
\item[\textbf{Tuple}]
FIXME
\item[\textbf{Typesetting language}]
FIXME
\item[\textbf{Unit test}]
FIXME
\item[\textbf{Unix shell}]
FIXME
\item[\textbf{Update operator}]
See \href{glossary.html\#in-place-operator}{in-place operator}.
\item[\textbf{Upvote}]
FIXME
\item[\textbf{Validation}]
FIXME
\end{description}

\textbf{Variable} (in Python):
A symbolic name that reserves memory to store a value.

\begin{description}
\tightlist
\item[\textbf{Variable}]
FIXME
\end{description}

\textbf{Variable} (in Make):
FIXME

\begin{description}
\tightlist
\item[\textbf{Vector image}]
FIXME
\item[\textbf{Verification}]
FIXME
\item[\textbf{Violin plot}]
FIXME
\item[\textbf{Virtual environment}]
In Python, the \texttt{virtualenv} package allows you to create virtual, disposable, Python software environments
containing only the packages and versions of packages you want to use for a particular project or task,
and to install new packages into the environment
without affecting other virtual environments or the system-wide default environment.
\end{description}

\textbf{What You See Is What You Get} (WYSIWYG):
FIXME

\begin{description}
\tightlist
\item[\textbf{Wildcard}]
FIXME
\item[\textbf{Working directory}]
FIXME
\item[\textbf{Working memory}]
FIXME
\item[\textbf{Wrap code}]
FIXME
\item[\textbf{Wrapper}]
FIXME
\item[\textbf{YAML}]
FIXME
\end{description}

\hypertarget{rse-objectives}{%
\chapter{Learning Objectives}\label{rse-objectives}}

This appendix lays out the learning objectives for each set of lessons,
and is intended to help instructors who want to use this curriculum.

\hypertarget{the-basics-of-the-unix-shell}{%
\section{The Basics of the Unix Shell}\label{the-basics-of-the-unix-shell}}

\begin{itemize}
\tightlist
\item
  Explain how the shell relates to the keyboard, the screen, the operating system, and users' programs.
\item
  Explain when and why command-line interfaces should be used instead of graphical interfaces.
\item
  Explain the steps in the shell's read-run-print cycle.
\item
  Identify the actual command, options, and filenames in a command-line call.
\item
  Demonstrate the use of tab completion and explain its advantages.
\item
  Explain the similarities and differences between a file and a directory.
\item
  Translate an absolute path into a relative path and vice versa.
\item
  Construct absolute and relative paths that identify specific files and directories.
\item
  Create a directory hierarchy that matches a given diagram.
\item
  Create files in that hierarchy using an editor or by copying and renaming existing files.
\item
  Delete, copy and move specified files and/or directories.
\item
  Redirect a command's output to a file.
\item
  Process a file instead of keyboard input using redirection.
\item
  Construct command pipelines with two or more stages.
\item
  Explain what usually happens if a program or pipeline isn't given any input to process.
\item
  Explain Unix's `small pieces, loosely joined' philosophy.
\item
  Write a loop that applies one or more commands separately to each file in a set of files.
\item
  Trace the values taken on by a loop variable during execution of the loop.
\item
  Explain the difference between a variable's name and its value.
\item
  Explain why spaces and some punctuation characters shouldn't be used in file names.
\item
  Demonstrate how to see what commands have recently been executed.
\item
  Re-run recently executed commands without retyping them.
\end{itemize}

\hypertarget{going-further-with-the-unix-shell}{%
\section{Going Further with the Unix Shell}\label{going-further-with-the-unix-shell}}

\begin{itemize}
\tightlist
\item
  Write a shell script that runs a command or series of commands for a fixed set of files.
\item
  Run a shell script from the command line.
\item
  Write a shell script that operates on a set of files defined by the user on the command line.
\item
  Create pipelines that include shell scripts you, and others, have written.
\item
  Use \texttt{grep} to select lines from text files that match simple patterns.
\item
  Use \texttt{find} to find files whose names match simple patterns.
\item
  Use the output of one command as the command-line argument(s) to another command.
\item
  Explain what is meant by `text' and `binary' files, and why many common tools don't handle the latter well.
\end{itemize}

\hypertarget{git-at-the-command-line}{%
\section{Git at the Command Line}\label{git-at-the-command-line}}

\begin{itemize}
\tightlist
\item
  Explain why someone would use Git at the command line.
\item
  Configure \texttt{git} the first time it is used on a computer.
\item
  Understand the meaning of the \texttt{-\/-global} configuration flag.
\item
  Create a local Git repository at the command line.
\item
  Go through the modify-add-commit cycle for one or more files.
\item
  Explain what the HEAD of a repository is and how to use it.
\item
  Identify and use Git commit numbers.
\item
  Compare various versions of tracked files.
\item
  Restore old versions of files.
\end{itemize}

\hypertarget{advanced-git}{%
\section{Advanced Git}\label{advanced-git}}

\begin{itemize}
\tightlist
\item
  Explain why branches can be useful.
\item
  Merge branches back into the master branch.
\item
  Define the terms fork, clone, origin, remote.
\item
  Understand how to make a pull request and explain what they are useful for.
\end{itemize}

\hypertarget{code-style-review-and-refactoring}{%
\section{Code Style, Review, and Refactoring}\label{code-style-review-and-refactoring}}

\begin{itemize}
\tightlist
\item
  Explain why consistent formatting of code is important.
\item
  Describe standard Python formatting rules and identify cases where code does or doesn't conform to them.
\item
  Write functions whose parameters have default values.
\item
  Explain which parameters should have default values and how to select good ones.
\item
  Write functions that can handle variable numbers of arguments.
\item
  Explain what problems can most easily be solved by creating functions with variable numbers of arguments.
\end{itemize}

\hypertarget{automating-analyses}{%
\section{Automating Analyses}\label{automating-analyses}}

\begin{itemize}
\tightlist
\item
  Explain what a build tool is and how build tools aid reproducible research.
\item
  Describe and identify the three parts of a Make rule.
\item
  Write a Makefile that re-runs a multi-stage data analysis.
\item
  Explain and trace how Make chooses an order in which to execute rules.
\item
  Explain what phony targets are and define a phony target.
\item
  Explain what automatic variables are and correctly identify three commonly-used automatic variables.
\item
  Rewrite Make rules to use automatic variables.
\item
  Explain why and how to write a pattern rule in a Makefile.
\item
  Rewrite Make rules to use patterns.
\item
  Define variables in a Makefile explicitly and by using functions.
\item
  Make a self-documenting Makefile.
\end{itemize}

\hypertarget{working-in-teams}{%
\section{Working in Teams}\label{working-in-teams}}

\begin{itemize}
\tightlist
\item
  Explain what an issue tracking tool does and what it should be used for.
\item
  Explain how to use labels on issues to manage work.
\item
  Describe the information a well-written issue should contain.
\item
  Explain the purpose of a Code of Conduct and the essential features an effective one must have.
\item
  Explain why adding licensing information to a repository is important.
\item
  Explain differences in licensing and social expectations.
\item
  Choose an appropriate license.
\item
  Explain where and how to communicate licensing.
\item
  Explain steps a project lead can take to be a good ally.
\end{itemize}

\hypertarget{project-structure}{%
\section{Project Structure}\label{project-structure}}

\begin{itemize}
\tightlist
\item
  Describe and justify Noble's Rules for organizing projects.
\item
  Explain the purpose of README, LICENSE, CONDUCT, and CITATION files.
\end{itemize}

\hypertarget{continuous-integration}{%
\section{Continuous Integration}\label{continuous-integration}}

\begin{itemize}
\tightlist
\item
  Explain what continuous integration is, how it works, and why it's useful.
\item
  Configure continuous integration for small R and Python software projects.
\item
  Examine and explain output of continuous integration builds.
\item
  Add additional settings to the continuous integration configuration.
\item
  Introduce other uses of continuous integration.
\end{itemize}

\hypertarget{r-packaging}{%
\section{R Packaging}\label{r-packaging}}

\begin{itemize}
\tightlist
\item
  Create and test a citable, shareable R package.
\end{itemize}

\hypertarget{python-packaging}{%
\section{Python Packaging}\label{python-packaging}}

\begin{itemize}
\tightlist
\item
  Create and use virtual environments to manage library versions without conflict.
\item
  Create and test a citable, shareable, documented Pip package.
\item
  Distribute a Pip package with \href{https://pypi.org/}{Pypi}
\end{itemize}

\hypertarget{correctness}{%
\section{Correctness}\label{correctness}}

\begin{itemize}
\tightlist
\item
  Explain the reasons for testing software.
\item
  Write and run unit tests using \texttt{pytest} for Python and \texttt{testthat} for R.
\end{itemize}

\hypertarget{publishing}{%
\section{Publishing}\label{publishing}}

\begin{itemize}
\tightlist
\item
  Explain what to include in publications.
\item
  Explain what DOIs and ORCIDs are.
\item
  Get an ORCID.
\item
  Describe the FAIR Principles and determine whether a dataset conforms to them.\\
\item
  Explain where to publish large, medium, and small datasets.
\item
  Explain where to publish software
\item
  Explain what's involved in publishing analysis code
\item
  Obtain DOIs for datasets, reports, and software packages.
\end{itemize}

\hypertarget{rse-keypoints}{%
\chapter{Key Points}\label{rse-keypoints}}

\hypertarget{the-basics-of-the-unix-shell-1}{%
\section{The Basics of the Unix Shell}\label{the-basics-of-the-unix-shell-1}}

\begin{itemize}
\tightlist
\item
  A shell is a program whose primary purpose is to read commands and run other programs.
\item
  The shell's main advantages are its high action-to-keystroke ratio,
  its support for automating repetitive tasks,
  and its capacity to access networked machines.
\item
  The shell's main disadvantages are its primarily textual nature and how cryptic its commands and operation can be.
\item
  The file system is responsible for managing information on the disk.
\item
  Information is stored in files, which are stored in directories (folders).
\item
  Directories can also store other directories, which forms a directory tree.
\item
  \texttt{cd\ path} changes the current working directory.
\item
  \texttt{ls\ path} prints a listing of a specific file or directory; \texttt{ls} on its own lists the current working directory.
\item
  \texttt{pwd} prints the user's current working directory.
\item
  \texttt{/} on its own is the root directory of the whole file system.
\item
  A relative path specifies a location starting from the current location.
\item
  An absolute path specifies a location from the root of the file system.
\item
  Directory names in a path are separated with \texttt{/} on Unix, but \texttt{\textbackslash{}\textbackslash{}} on Windows.
\item
  \texttt{..} means `the directory above the current one'; \texttt{.} on its own means `the current directory'.
\item
  \texttt{cp\ old\ new} copies a file.
\item
  \texttt{mkdir\ path} creates a new directory.
\item
  \texttt{mv\ old\ new} moves (renames) a file or directory.
\item
  \texttt{rm\ path} removes (deletes) a file.
\item
  \texttt{*} matches zero or more characters in a filename, so \texttt{*.txt} matches all files ending in \texttt{.txt}.
\item
  \texttt{?} matches any single character in a filename, so \texttt{?.txt} matches \texttt{a.txt} but not \texttt{any.txt}.
\item
  Use of the Control key may be described in many ways, including \texttt{Ctrl-X}, \texttt{Control-X}, and \texttt{\^{}X}.
\item
  The shell does not have a trash bin: once something is deleted, it's really gone.
\item
  Most files' names are \texttt{something.extension}.
  The extension isn't required and doesn't guarantee anything,
  but is normally used to indicate the type of data in the file.
\item
  Depending on the type of work you do, you may need a more powerful text editor than Nano.
\item
  \texttt{cat} displays the contents of its inputs.
\item
  \texttt{head} displays the first 10 lines of its input.
\item
  \texttt{tail} displays the last 10 lines of its input.
\item
  \texttt{sort} sorts its inputs.
\item
  \texttt{wc} counts lines, words, and characters in its inputs.
\item
  \texttt{command\ \textgreater{}\ file} redirects a command's output to a file (overwriting any existing content).
\item
  \texttt{command\ \textgreater{}\textgreater{}\ file} appends a command's output to a file.
\item
  \texttt{\textless{}} operator redirects input to a command
\item
  \texttt{first\ \textbar{}\ second} is a pipeline: the output of the first command is used as the input to the second.
\item
  The best way to use the shell is to use pipes to combine simple single-purpose programs (filters).
\item
  A \texttt{for} loop repeats commands once for every thing in a list.
\item
  Every \texttt{for} loop needs a variable to refer to the thing it is currently operating on.
\item
  Use \texttt{\$name} to expand a variable (i.e., get its value). \texttt{\$\{name\}} can also be used.
\item
  Do not use spaces, quotes, or wildcard characters such as '*`or'?' in filenames, as it complicates variable expansion.
\item
  Give files consistent names that are easy to match with wildcard patterns to make it easy to select them for looping.
\item
  Use the up-arrow key to scroll up through previous commands to edit and repeat them.
\item
  Use \texttt{Ctrl-R} to search through the previously entered commands.
\item
  Use \texttt{history} to display recent commands, and \texttt{!number} to repeat a command by number.
\end{itemize}

\hypertarget{going-further-with-the-unix-shell-1}{%
\section{Going Further with the Unix Shell}\label{going-further-with-the-unix-shell-1}}

\begin{itemize}
\tightlist
\item
  Save commands in files (usually called shell scripts) for re-use.
\item
  \texttt{bash\ filename} runs the commands saved in a file.
\item
  \texttt{\$@} refers to all of a shell script's command-line arguments.
\item
  \texttt{\$1}, \texttt{\$2}, etc., refer to the first command-line argument, the second command-line argument, etc.
\item
  Place variables in quotes if the values might have spaces in them.
\item
  Letting users decide what files to process is more flexible and more consistent with built-in Unix commands.
\item
  \texttt{find} finds files with specific properties that match patterns.
\item
  \texttt{grep} selects lines in files that match patterns.
\item
  \texttt{-\/-help} is an option supported by many bash commands, and programs that can be run from within Bash, to display more information on how to use these commands or programs.
\item
  \texttt{man\ command} displays the manual page for a given command.
\item
  \texttt{\$(command)} inserts a command's output in place.
\end{itemize}

\hypertarget{git-at-the-command-line-1}{%
\section{Git at the Command Line}\label{git-at-the-command-line-1}}

\begin{itemize}
\tightlist
\item
  Use git config with the \texttt{-\/-global} option to configure a user name, email address, and other preferences once per machine.
\item
  \texttt{git\ init} initializes a repository.
\item
  Git stores all of its repository data in the \texttt{.git} directory.
\item
  \texttt{git\ status} shows the status of a repository.
\item
  \texttt{git\ add} puts files in the staging area.
\item
  \texttt{git\ commit} saves the staged content as a new commit in the local repository.
\item
  The \texttt{.gitignore} file tells Git what files to ignore.
\item
  \texttt{git\ push} copies changes from a local repository to a remote repository.
\item
  \texttt{git\ pull} copies changes from a remote repository to a local repository.
\item
  \texttt{git\ diff} displays differences between commits.
\item
  \texttt{git\ checkout} recovers old versions of files.
\end{itemize}

\hypertarget{advanced-git-1}{%
\section{Advanced Git}\label{advanced-git-1}}

\begin{itemize}
\tightlist
\item
  \texttt{git\ branch} creates a new branch where new features can be developed while leaving the master branch untouched.
\item
  \texttt{git\ clone} copies a remote repository to create a local repository with a remote called origin automatically set up.
\item
  Pull requests suggest changes to repos where you don't have write privileges.
\item
  Reorganizing code in consistent ways makes errors less likely.
\item
  Replace a value with a name to make code more readable and to forestall typing errors.
\item
  Replace a repeated test with a flag to ensure consistency.
\item
  Turn small pieces of large functions into functions in their own right, even if they are only used once.
\item
  Combine functions if they are always used together on the same inputs.
\item
  Use lookup tables to make decision rules easier to follow.
\item
  Use comprehensions instead of loops.
\end{itemize}

\hypertarget{code-style-review-and-refactoring-1}{%
\section{Code Style, Review, and Refactoring}\label{code-style-review-and-refactoring-1}}

\begin{itemize}
\tightlist
\item
  The brain thinks every difference is significant, so removing unnecessary differences in formatting reduces cognitive load.
\item
  Python software should always conform to the formatting the rules in PEP 8.
\item
  Use \texttt{name=value} to define a default value for a function parameter.
\item
  Use \texttt{*args} to define a catch-all parameter for functions taking a variable number of unnamed arguments.
\item
  Use \texttt{**kwargs} to define a catch-all parameter for functions taking a variable number of named arguments.
\item
  Use destructuring to unpack data structures as needed.
\end{itemize}

\hypertarget{automating-analyses-1}{%
\section{Automating Analyses}\label{automating-analyses-1}}

\begin{itemize}
\tightlist
\item
  A build tool re-runs commands so all files and their dependencies are up-to-date with each other.
\item
  Make is a widely-used build tool that uses files' timestamps to find out-of-date prerequisites.
\item
  A Make rule has targets, prerequisites, and actions.
\item
  A target can correspond to a file or be a phony target (used simply to trigger actions).
\item
  When a target is out of date with respect to its prerequisites, Make executes the actions associated with its rule.
\item
  Make executes as many rules as it needs to when updating files, but always respect prerequisite order.
\item
  Make defines the automatic variables \texttt{\$@} (target), \texttt{\$\^{}} (all prerequisites), and \texttt{\$\textless{}} (first prerequisite).
\item
  Pattern rules can use \texttt{\%} as a placeholder for parts of filenames.
\item
  Makefiles can define variables using \texttt{NAME=value}.
\item
  Makefiles can also use functions such as \texttt{\$(wildcard\ ...)} and \texttt{\$(patsubst\ ...)}.
\item
  Specially-formatted comments can be used to make Makefiles self-documenting.
\end{itemize}

\hypertarget{working-in-teams-1}{%
\section{Working in Teams}\label{working-in-teams-1}}

\begin{itemize}
\tightlist
\item
  Create issues for bugs, enhancement requests, and discussions.
\item
  Add people to issues to show who is responsible for working on what.
\item
  Add labels to issues to identify their purpose.
\item
  Use rules for issue state transitions to define a workflow for a project.
\item
  Create an explicit Code of Conduct for your project modelled on the Contributor Covenant.
\item
  Be clear about how to report violations of the Code of Conduct and who will handle such reports.
\item
  People who are not lawyers should not try to write licenses.
\item
  Every project should include an explicit license to make clear who can do what with the material.
\item
  People who incorporate GPL'd software into their own software must make their software also open under the GPL license; most other open licenses do not require this.
\item
  The Creative Commons family of licenses allow people to mix and match requirements and restrictions on attribution, creation of derivative works, further sharing, and commercialization.
\item
  Be proactive about welcoming and nurturing community members.
\end{itemize}

\hypertarget{project-structure-1}{%
\section{Project Structure}\label{project-structure-1}}

\begin{itemize}
\tightlist
\item
  Put source code for compilation in \texttt{./src/}.
\item
  Put runnable code in \texttt{./bin/}.
\item
  Put raw data in \texttt{./data/}.
\item
  Put results in \texttt{./results/}.
\item
  Put documentation and manuscripts in \texttt{./doc/}.
\item
  Use file and directory names that are easy to match and include dates for the level under \texttt{./data/} and \texttt{./results/}.
\item
  Create README, LICENSE, CONDUCT, and CITATION files in the root directory of the project.
\end{itemize}

\hypertarget{continuous-integration-1}{%
\section{Continuous Integration}\label{continuous-integration-1}}

\begin{itemize}
\tightlist
\item
  Continuous integration re-builds and/or re-tests software every time something changes.
\item
  Use continuous integration to check changes before they are inspected.
\item
  Check style as well as correctness.
\end{itemize}

\hypertarget{r-packaging-1}{%
\section{R Packaging}\label{r-packaging-1}}

\begin{itemize}
\tightlist
\item
  Packages allow software to be shared in manageable ways.
\item
  R packages can be shared through CRAN or GitHub, or managed locally during development.
\item
  Packages can contain code and data.
\item
  A package must contain \texttt{DESCRIPTION} and \texttt{NAMESPACE} files.
\item
  Use \texttt{.Rbuildignore} to control what is and isn't included in a package.
\item
  Include a README, a license, and a citation file in every package.
\item
  Use \texttt{usethis} and \texttt{devtools} to manage package development.
\item
  Put documentation in the \texttt{man} directory and tests in the \texttt{tests} directory.
\item
  Use roxygen2 or Markdown to document the contents of a package.
\end{itemize}

\hypertarget{python-packaging-1}{%
\section{Python Packaging}\label{python-packaging-1}}

\begin{itemize}
\tightlist
\item
  A module is simply a file containing Python code; it is executed on \texttt{import}.
\item
  A package named \texttt{mypackage} is a directory named \texttt{mypackage} containing a module with a special name \texttt{\_\_init\_\_.py},
  which may be empty.
\item
  Other modules within the directory are visible after the import.
\item
  A package can contain subpackages.
\item
  Use \texttt{virtualenv} to create a separate virtual environment for each project.
\item
  Use \texttt{pip} to create a distributable package containing your project's software, documentation, and data.
\item
  The default respository for Python packages is \href{https://pypi.org/}{PyPI}
\item
  You can test distributing your package to Pypi using \href{https://test.pypi.org}{TestPyPI}, and when you're ready, publish it to \url{pypi}
\end{itemize}

\hypertarget{correctness-1}{%
\section{Correctness}\label{correctness-1}}

\begin{itemize}
\tightlist
\item
  Testing can only ever show that software has flaws, not that it is correct.
\item
  The real purpose is to convince people (including yourself) that software is correct enough
  and to make tolerances on `enough' explicit.
\item
  A testing suite finds and runs tests written in a prescribed fashion and reports their results.
\item
  A unit test can pass (work as expected),
  fail (meaning the software under test is flawed),
  or produce an error (meaning the test itself is flawed).
\item
  Every unit test should be independent of each other so results are comprehensible, contained, and reliable.
\item
  Tests should be built to test that the software fails when and as it is supposed.
\item
  Tests can usefully exercise particular pieces of functionality (unit tests)
  or larger pieces of the code base demonstrating interactions between units (integration tests)
\item
  Tests should verify software (make sure it does what it's supposed to)
  and also validate it (make sure it produces the right answers).
\item
  Data science is normally concerned with approximate correctness of solutions.
\item
  Stochastic statistical variation and deterministic floating point approximations both make answers approximately correct.
\item
  Check that parametric or non-parametric statistics of data
  do not differ from saved values by more than a specified tolerance.
\item
  Infer constraints on data and then check that subsequent data sets obey these constraints.
\item
  Test the data structures used in plotting rather than the plots themselves.
\end{itemize}

\hypertarget{publishing-1}{%
\section{Publishing}\label{publishing-1}}

\begin{itemize}
\tightlist
\item
  Include small datasets in repositories; store large ones on data sharing sites, and include metadata in the repository to locate them.
\item
  An ORCID is a unique personal identifier that you can use to identify your work.
\item
  A DOI is a unique identifier for a particular dataset, report, or software release.
\item
  Data should be findable, accessible, interoperable, and reusable (FAIR).
\item
  Use Zenodo to obtain DOIs.
\item
  Publish your software as you would a paper.
\end{itemize}

\hypertarget{rse-solutions}{%
\chapter{Solutions}\label{rse-solutions}}

\hypertarget{chapterrefrse-bash-basics}{%
\section{Chapter~\ref{rse-bash-basics}}\label{chapterrefrse-bash-basics}}

\hypertarget{exerciserefrse-bash-basics-ex-more-ls}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-more-ls}}\label{exerciserefrse-bash-basics-ex-more-ls}}

The \texttt{-l} option makes \texttt{ls} use a \textbf{l}ong listing format, showing not only
the file/directory names but also additional information such as the file size
and the time of its last modification. If you use both the \texttt{-h} option and the \texttt{-l} option,
this makes the file size ``\textbf{h}uman readable'', i.e.~displaying something like \texttt{5.3K}
instead of \texttt{5369}.

\hypertarget{exerciserefrse-bash-basics-ex-ls-rt}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-ls-rt}}\label{exerciserefrse-bash-basics-ex-ls-rt}}

The files/directories in each directory are sorted by time of last change.

\hypertarget{exerciserefrse-bash-basics-ex-paths}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-paths}}\label{exerciserefrse-bash-basics-ex-paths}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No: \texttt{.} stands for the current directory.
\item
  No: \texttt{/} stands for the root directory.
\item
  No: Amanda's home directory is \texttt{/Users/amanda}.
\item
  No: this goes up two levels, i.e.~ends in \texttt{/Users}.
\item
  Yes: \texttt{\textasciitilde{}} stands for the user's home directory, in this case \texttt{/Users/amanda}.
\item
  No: this would navigate into a directory \texttt{home} in the current directory if it exists.
\item
  Yes: unnecessarily complicated, but correct.
\item
  Yes: shortcut to go back to the user's home directory.
\item
  Yes: goes up one level.
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-resolve-rel-path}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-resolve-rel-path}}\label{exerciserefrse-bash-basics-ex-resolve-rel-path}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No: there \emph{is} a directory \texttt{backup} in \texttt{/Users}.
\item
  No: this is the content of \texttt{Users/thing/backup},
  but with \texttt{..} we asked for one level further up.
\item
  No: see previous explanation.
\item
  Yes: \texttt{../backup/} refers to \texttt{/Users/backup/}.
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-reading-ls}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-reading-ls}}\label{exerciserefrse-bash-basics-ex-reading-ls}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No: \texttt{pwd} is not the name of a directory.
\item
  Yes: \texttt{ls} without directory argument lists files and directories
  in the current directory.
\item
  Yes: uses the absolute path explicitly.
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-touch}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-touch}}\label{exerciserefrse-bash-basics-ex-touch}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The \texttt{touch} command generates a new file called \texttt{my\_file.txt} in
  your current directory. You
  can observe this newly generated file by typing \texttt{ls} at the
  command line prompt. \texttt{my\_file.txt} can also be viewed in your
  GUI file explorer.
\item
  When you inspect the file with \texttt{ls\ -l}, note that the size of
  \texttt{my\_file.txt} is 0 bytes. In other words, it contains no data.
  If you open \texttt{my\_file.txt} using your text editor it is blank.
\item
  Some programs do not generate output files themselves, but
  instead require that empty files have already been generated.
  When the program is run, it searches for an existing file to
  populate with its output. The touch command allows you to
  efficiently generate a blank text file to be used by such
  programs.
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-move-dot}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-move-dot}}\label{exerciserefrse-bash-basics-ex-move-dot}}

\begin{verbatim}
$ mv ../analyzed/sucrose.dat ../analyzed/maltose.dat .
\end{verbatim}

Recall that \texttt{..} refers to the parent directory (i.e.~one above the current directory)
and that \texttt{.} refers to the current directory.

\hypertarget{exerciserefrse-bash-basics-ex-renaming-files}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-renaming-files}}\label{exerciserefrse-bash-basics-ex-renaming-files}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No.~While this would create a file with the correct name,
  the incorrectly named file still exists in the directory and would need to be deleted.
\item
  Yes, this would work to rename the file.
\item
  No, the period(.) indicates where to move the file, but does not provide a new file name;
  identical file names cannot be created.
\item
  No, the period(.) indicates where to copy the file, but does not provide a new file name;
  identical file names cannot be created.
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-last-ls}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-last-ls}}\label{exerciserefrse-bash-basics-ex-last-ls}}

We start in the \texttt{/Users/jamie/data} directory, and create a new folder called \texttt{recombine}.
The second line moves (\texttt{mv}) the file \texttt{proteins.dat} to the new folder (\texttt{recombine}).
The third line makes a copy of the file we just moved. The tricky part here is where the file was
copied to. Recall that \texttt{..} means ``go up a level'', so the copied file is now in \texttt{/Users/jamie}.
Notice that \texttt{..} is interpreted with respect to the current working
directory, \textbf{not} with respect to the location of the file being copied.
So, the only thing that will show using ls (in \texttt{/Users/jamie/data}) is the recombine folder.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No, see explanation above. \texttt{proteins-saved.dat} is located at \texttt{/Users/jamie}
\item
  Yes
\item
  No, see explanation above. \texttt{proteins.dat} is located at \texttt{/Users/jamie/data/recombine}
\item
  No, see explanation above. \texttt{proteins-saved.dat} is located at \texttt{/Users/jamie}
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-safe-rm}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-safe-rm}}\label{exerciserefrse-bash-basics-ex-safe-rm}}

\begin{verbatim}
$ rm: remove regular file 'thesis_backup/quotations.txt'? y
\end{verbatim}

The \texttt{-i} option will prompt before (every) removal
(use Y to confirm deletion or N to keep the file).
The Unix shell doesn't have a trash bin, so all the files removed will disappear forever.
By using the \texttt{-i} option, we have the chance to check that we are deleting
only the files that we want to remove.

\hypertarget{exerciserefrse-bash-basics-ex-copy-multi}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-copy-multi}}\label{exerciserefrse-bash-basics-ex-copy-multi}}

If given more than one file name followed by a directory name (i.e.~the destination directory must
be the last argument), \texttt{cp} copies the files to the named directory.

If given three file names, \texttt{cp} throws an error such as the one below, because it is expecting a directory
name as the last argument.

\begin{verbatim}
cp: target morse.txt is not a directory
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-ls-match}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-ls-match}}\label{exerciserefrse-bash-basics-ex-ls-match}}

The solution is \texttt{3.}

\texttt{1.} shows all files whose names contain zero or more characters (\texttt{*}) followed by the letter \texttt{t},
then zero or more characters (\texttt{*}) followed by \texttt{ane.pdb}.
This gives \texttt{ethane.pdb\ \ methane.pdb\ \ octane.pdb\ \ pentane.pdb}.

\texttt{2.} shows all files whose names start with zero or more characters (\texttt{*}) followed by the letter \texttt{t},
then a single character (\texttt{?}), then \texttt{ne.} followed by zero or more characters (\texttt{*}).
This will give us \texttt{octane.pdb} and \texttt{pentane.pdb} but doesn't match anything which ends in \texttt{thane.pdb}.

\texttt{3.} fixes the problems of option 2 by matching two characters (\texttt{??}) between \texttt{t} and \texttt{ne}.
This is the solution.

\texttt{4.} only shows files starting with \texttt{ethane.}.

\hypertarget{exerciserefrse-bash-basics-ex-more-wildcards}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-more-wildcards}}\label{exerciserefrse-bash-basics-ex-more-wildcards}}

\begin{verbatim}
$ cp *calibration.txt backup/calibration
$ cp 2015-11-* send_to_bob/all_november_files/
$ cp *-23-dataset* send_to_bob/all_datasets_created_on_a_23rd/
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-organizing}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-organizing}}\label{exerciserefrse-bash-basics-ex-organizing}}

\begin{verbatim}
mv *.dat analyzed
\end{verbatim}

Jamie needs to move her files \texttt{fructose.dat} and \texttt{sucrose.dat} to the \texttt{analyzed} directory.
The shell will expand *.dat to match all .dat files in the current directory.
The \texttt{mv} command then moves the list of .dat files to the ``analyzed'' directory.

\hypertarget{exerciserefrse-bash-basics-ex-reproduce-structure}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-reproduce-structure}}\label{exerciserefrse-bash-basics-ex-reproduce-structure}}

The first two sets of commands achieve this objective.
The first set uses relative paths to create the top level directory before
the subdirectories.

The third set of commands will give an error because \texttt{mkdir} won't create a subdirectory
of a non-existant directory: the intermediate level folders must be created first.

The final set of commands generates the `raw' and `processed' directories at the same level
as the `data' directory.

\hypertarget{exerciserefrse-bash-basics-ex-sort-n}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-sort-n}}\label{exerciserefrse-bash-basics-ex-sort-n}}

The \texttt{-n} option specifies a numerical rather than an alphanumerical sort.

\hypertarget{exerciserefrse-bash-basics-ex-redirect-append}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-redirect-append}}\label{exerciserefrse-bash-basics-ex-redirect-append}}

In the first example with \texttt{\textgreater{}}, the string ``hello'' is written to \texttt{testfile01.txt},
but the file gets overwritten each time we run the command.

We see from the second example that the \texttt{\textgreater{}\textgreater{}} operator also writes ``hello'' to a file
(in this case\texttt{testfile02.txt}),
but appends the string to the file if it already exists (i.e.~when we run it for the second time).

\hypertarget{exerciserefrse-bash-basics-ex-append-data}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-append-data}}\label{exerciserefrse-bash-basics-ex-append-data}}

Option 3 is correct.
For option 1 to be correct we would only run the \texttt{head} command.
For option 2 to be correct we would only run the \texttt{tail} command.
For option 4 to be correct we would have to pipe the output of \texttt{head} into \texttt{tail\ -n\ 2}
by doing \texttt{head\ -n\ 3\ animals.txt\ \textbar{}\ tail\ -n\ 2\ \textgreater{}\ animals-subset.txt}

\hypertarget{exerciserefrse-bash-basics-ex-piping}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-piping}}\label{exerciserefrse-bash-basics-ex-piping}}

Option 4 is the solution.
The pipe character \texttt{\textbar{}} is used to feed the standard output from one process to
the standard input of another.
\texttt{\textgreater{}} is used to redirect standard output to a file.
Try it in the \texttt{data-shell/molecules} directory!

\hypertarget{exerciserefrse-bash-basics-ex-uniq-adjacent}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-uniq-adjacent}}\label{exerciserefrse-bash-basics-ex-uniq-adjacent}}

\begin{verbatim}
$ sort salmon.txt | uniq
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-reading-pipes}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-reading-pipes}}\label{exerciserefrse-bash-basics-ex-reading-pipes}}

The \texttt{head} command extracts the first 5 lines from \texttt{animals.txt}.
Then, the last 3 lines are extracted from the previous 5 by using the \texttt{tail} command.
With the \texttt{sort\ -r} command those 3 lines are sorted in reverse order and finally,
the output is redirected to a file \texttt{final.txt}.
The content of this file can be checked by executing \texttt{cat\ final.txt}.
The file should contain the following lines:

\begin{verbatim}
2012-11-06,rabbit
2012-11-06,deer
2012-11-05,raccoon
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-pipe-construction}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-pipe-construction}}\label{exerciserefrse-bash-basics-ex-pipe-construction}}

\begin{verbatim}
$ cut -d , -f 2 animals.txt | sort | uniq
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-which-pipe}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-which-pipe}}\label{exerciserefrse-bash-basics-ex-which-pipe}}

Option 4. is the correct answer.
If you have difficulty understanding why, try running the commands, or sub-sections of
the pipelines (make sure you are in the \texttt{data-shell/data} directory).

\hypertarget{exerciserefrse-bash-basics-ex-wildcard-expressions}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-wildcard-expressions}}\label{exerciserefrse-bash-basics-ex-wildcard-expressions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A solution using two wildcard expressions:
  \texttt{shell\ \ \$\ ls\ *A.txt\ \ \$\ ls\ *B.txt}
\item
  The output from the new commands is separated because there are two commands.
\item
  When there are no files ending in \texttt{A.txt}, or there are no files ending in
  \texttt{B.txt}.
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-remove-unneeded}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-remove-unneeded}}\label{exerciserefrse-bash-basics-ex-remove-unneeded}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  This would remove \texttt{.txt} files with one-character names
\item
  This is correct answer
\item
  The shell would expand \texttt{*} to match everything in the current directory,
  so the command would try to remove all matched files and an additional
  file called \texttt{.txt}
\item
  The shell would expand \texttt{*.*} to match all files with any extension,
  so this command would delete all files
\end{enumerate}

\hypertarget{exerciserefrse-bash-basics-ex-loop-dry-run}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-loop-dry-run}}\label{exerciserefrse-bash-basics-ex-loop-dry-run}}

The second version is the one we want to run.
This prints to screen everything enclosed in the quote marks, expanding the
loop variable name because we have prefixed it with a dollar sign.

The first version redirects the output from the command \texttt{echo\ analyze\ \$file} to
a file, \texttt{analyzed-\$file}. A series of files is generated: \texttt{analyzed-cubane.pdb},
\texttt{analyzed-ethane.pdb} etc.

Try both versions for yourself to see the output! Be sure to open the
\texttt{analyzed-*.pdb} files to view their contents.

\hypertarget{exerciserefrse-bash-basics-ex-nested-loops}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-nested-loops}}\label{exerciserefrse-bash-basics-ex-nested-loops}}

We have a nested loop, i.e.~contained within another loop, so for each species
in the outer loop, the inner loop (the nested loop) iterates over the list of
temperatures, and creates a new directory for each combination.

Try running the code for yourself to see which directories are created!

\hypertarget{exerciserefrse-bash-basics-ex-loop-variables}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-loop-variables}}\label{exerciserefrse-bash-basics-ex-loop-variables}}

The first code block gives the same output on each iteration through
the loop.
Bash expands the wildcard \texttt{*.pdb} within the loop body (as well as
before the loop starts) to match all files ending in \texttt{.pdb}
and then lists them using \texttt{ls}.
The expanded loop would look like this:

\begin{verbatim}
$ for datafile in cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> do
>   ls cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> done
\end{verbatim}

\begin{verbatim}
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
\end{verbatim}

The second code block lists a different file on each loop iteration.
The value of the \texttt{datafile} variable is evaluated using \texttt{\$datafile},
and then listed using \texttt{ls}.

\begin{verbatim}
cubane.pdb
ethane.pdb
methane.pdb
octane.pdb
pentane.pdb
propane.pdb
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-limiting-file-sets}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-limiting-file-sets}}\label{exerciserefrse-bash-basics-ex-limiting-file-sets}}

\textbf{Part 1}

4 is the correct answer. \texttt{*} matches zero or more characters, so any file name starting with
the letter c, followed by zero or more other characters will be matched.

\textbf{Part 2}

4 is the correct answer. \texttt{*} matches zero or more characters, so a file name with zero or more
characters before a letter c and zero or more characters after the letter c will be matched.

\hypertarget{exerciserefrse-bash-basics-ex-loop-save}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-loop-save}}\label{exerciserefrse-bash-basics-ex-loop-save}}

\textbf{Part 1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The text from each file in turn gets written to the \texttt{alkanes.pdb} file.
  However, the file gets overwritten on each loop interation, so the final content of \texttt{alkanes.pdb}
  is the text from the \texttt{propane.pdb} file.
\end{enumerate}

\textbf{Part 2}

3 is the correct answer. \texttt{\textgreater{}\textgreater{}} appends to a file, rather than overwriting it with the redirected
output from a command.
Given the output from the \texttt{cat} command has been redirected, nothing is printed to the screen.

\hypertarget{exerciserefrse-bash-basics-ex-list-unique}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-list-unique}}\label{exerciserefrse-bash-basics-ex-list-unique}}

\begin{verbatim}
# Script to find unique species in csv files where species is the second data field
# This script accepts any number of file names as command line arguments

# Loop over all files
for file in $@
do
    echo "Unique species in $file:"
    # Extract species names
    cut -d , -f 2 $file | sort | uniq
done
\end{verbatim}

\hypertarget{exerciserefrse-bash-basics-ex-history-order}{%
\subsection{Exercise~\ref{rse-bash-basics-ex-history-order}}\label{exerciserefrse-bash-basics-ex-history-order}}

If a command causes something to crash or hang, it might be useful
to know what that command was, in order to investigate the problem.
Were the command only be recorded after running it, we would not
have a record of the last command run in the event of a crash.

\hypertarget{chapterrefrse-bash-advanced}{%
\section{Chapter~\ref{rse-bash-advanced}}\label{chapterrefrse-bash-advanced}}

\hypertarget{exerciserefrse-bash-advanced-ex-script-variables}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-script-variables}}\label{exerciserefrse-bash-advanced-ex-script-variables}}

The correct answer is 2.

The special variables \$1, \$2 and \$3 represent the command line arguments given to the
script, such that the commands run are:

\begin{verbatim}
$ head -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb
$ tail -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb
\end{verbatim}

The shell does not expand \texttt{\textquotesingle{}*.pdb\textquotesingle{}} because it is enclosed by quote marks.
As such, the first argument to the script is \texttt{\textquotesingle{}*.pdb\textquotesingle{}} which gets expanded within the
script by \texttt{head} and \texttt{tail}.

\hypertarget{exerciserefrse-bash-advanced-ex-longest-with-extension}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-longest-with-extension}}\label{exerciserefrse-bash-advanced-ex-longest-with-extension}}

\begin{verbatim}
# Shell script which takes two arguments:
#    1. a directory name
#    2. a file extension
# and prints the name of the file in that directory
# with the most lines which matches the file extension.

wc -l $1/*.$2 | sort -n | tail -n 2 | head -n 1
\end{verbatim}

\hypertarget{exerciserefrse-bash-advanced-ex-reading-scripts}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-reading-scripts}}\label{exerciserefrse-bash-advanced-ex-reading-scripts}}

In each case, the shell expands the wildcard in \texttt{*.pdb} before passing the resulting
list of file names as arguments to the script.

Script 1 would print out a list of all files containing a dot in their name.
The arguments passed to the script are not actually used anywhere in the script.

Script 2 would print the contents of the first 3 files with a \texttt{.pdb} file extension.
\texttt{\$1}, \texttt{\$2}, and \texttt{\$3} refer to the first, second, and third argument respectively.

Script 3 would print all the arguments to the script (i.e.~all the \texttt{.pdb} files),
followed by \texttt{.pdb}.
\texttt{\$@} refers to \emph{all} the arguments given to a shell script.

\begin{verbatim}
cubane.pdb ethane.pdb methane.pdb octane.pdb pentane.pdb propane.pdb.pdb
\end{verbatim}

\hypertarget{exerciserefrse-bash-advanced-ex-using-grep}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-using-grep}}\label{exerciserefrse-bash-advanced-ex-using-grep}}

The correct answer is 3, because the \texttt{-w} option looks only for whole-word matches.
The other options will also match ``of'' when part of another word.

\hypertarget{exerciserefrse-bash-advanced-ex-tracking-species}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-tracking-species}}\label{exerciserefrse-bash-advanced-ex-tracking-species}}

\begin{verbatim}
grep -w $1 -r $2 | cut -d : -f 2 | cut -d , -f 1,3  > $1.txt
\end{verbatim}

You would call the script above like this:

\begin{verbatim}
$ bash count-species.sh bear .
\end{verbatim}

\hypertarget{exerciserefrse-bash-advanced-ex-little-women}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-little-women}}\label{exerciserefrse-bash-advanced-ex-little-women}}

\begin{verbatim}
for sis in Jo Meg Beth Amy
do
    echo $sis:
>   grep -ow $sis LittleWomen.txt | wc -l
done
\end{verbatim}

Alternative, slightly inferior solution:

\begin{verbatim}
for sis in Jo Meg Beth Amy
do
    echo $sis:
>   grep -ocw $sis LittleWomen.txt
done
\end{verbatim}

This solution is inferior because \texttt{grep\ -c} only reports the number of lines matched.
The total number of matches reported by this method will be lower if there is more
than one match per line.

\hypertarget{exerciserefrse-bash-advanced-ex-match-subtract}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-match-subtract}}\label{exerciserefrse-bash-advanced-ex-match-subtract}}

The correct answer is 1. Putting the match expression in quotes prevents the shell
expanding it, so it gets passed to the \texttt{find} command.

Option 2 is incorrect because the shell expands \texttt{*s.txt} instead of passing the wildcard
expression to \texttt{find}.

Option 3 is incorrect because it searches the contents of the files for lines which
do not match ``temp'', rather than searching the file names.

\hypertarget{exerciserefrse-bash-advanced-ex-reading-find}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-reading-find}}\label{exerciserefrse-bash-advanced-ex-reading-find}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find all files with a \texttt{.dat} extension recursively from the current directory
\item
  Count the number of lines each of these files contains
\item
  Sort the output from step 2. numerically
\end{enumerate}

\hypertarget{exerciserefrse-bash-advanced-ex-find-properties}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-find-properties}}\label{exerciserefrse-bash-advanced-ex-find-properties}}

Assuming that Nelle's home is our working directory we type:

\begin{verbatim}
$ find ./ -type f -mtime -1 -user ahmed
\end{verbatim}

\hypertarget{exerciserefrse-bash-advanced-ex-combining-options}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-combining-options}}\label{exerciserefrse-bash-advanced-ex-combining-options}}

FIXME: write solution for combining options exercise

\hypertarget{exerciserefrse-bash-advanced-ex-other-wildcards}{%
\subsection{Exercise~\ref{rse-bash-advanced-ex-other-wildcards}}\label{exerciserefrse-bash-advanced-ex-other-wildcards}}

FIXME: write solution for wildcards exercise

\hypertarget{chapterrefrse-automate}{%
\section{Chapter~\ref{rse-automate}}\label{chapterrefrse-automate}}

\hypertarget{exerciserefrse-automate-ex-create-summary-results}{%
\subsection*{Exercise~\ref{rse-automate-ex-create-summary-results}}\label{exerciserefrse-automate-ex-create-summary-results}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-automate-ex-create-summary-results}}

\begin{itemize}
\tightlist
\item
  Add a rule to Makefile to create a summary CSV file from all of the book CSV files.
\item
  Be careful about writing the prerequisites so that it doesn't depend on itself.
\end{itemize}

\hypertarget{exerciserefrse-automate-ex-plot-top-n}{%
\subsection*{Exercise~\ref{rse-automate-ex-plot-top-n}}\label{exerciserefrse-automate-ex-plot-top-n}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-automate-ex-plot-top-n}}

\begin{itemize}
\tightlist
\item
  Make it depend on the summary.
\end{itemize}

\hypertarget{exerciserefrse-automate-ex-mkdir}{%
\subsection*{Exercise~\ref{rse-automate-ex-mkdir}}\label{exerciserefrse-automate-ex-mkdir}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-automate-ex-mkdir}}

\begin{itemize}
\tightlist
\item
  Why is \texttt{mkdir\ -p} useful?
\end{itemize}

\hypertarget{exerciserefrse-automate-ex-report-change}{%
\subsection*{Exercise~\ref{rse-automate-ex-report-change}}\label{exerciserefrse-automate-ex-report-change}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-automate-ex-report-change}}

\begin{itemize}
\tightlist
\item
  Write a rule to report which result files would actually change.
\item
  Hint: use \texttt{diff}.
\end{itemize}

\hypertarget{exerciserefrse-automate-ex-readable-help}{%
\subsection*{Exercise~\ref{rse-automate-ex-readable-help}}\label{exerciserefrse-automate-ex-readable-help}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-automate-ex-readable-help}}

\begin{itemize}
\tightlist
\item
  Modify the command in the \texttt{help} action to remove the leading `\#\#' markers from the output.
\end{itemize}

\hypertarget{exerciserefrse-automate-ex-wildcard-perils}{%
\subsection*{Exercise~\ref{rse-automate-ex-wildcard-perils}}\label{exerciserefrse-automate-ex-wildcard-perils}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-automate-ex-wildcard-perils}}

What is wrong with writing the rule for \texttt{results/collated.csv} like this:

\begin{verbatim}
results/collated.csv : results/*.csv
    $(RUN_COLLATE) $^ > $@
\end{verbatim}

Hint: the fact that the result no longer depends on the program used to create it isn't the only problem.

\hypertarget{chapterrefrse-git-cmdline}{%
\section{Chapter~\ref{rse-git-cmdline}}\label{chapterrefrse-git-cmdline}}

\hypertarget{exerciserefrse-git-cmdline-ex-places}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-places}}\label{exerciserefrse-git-cmdline-ex-places}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-places}}

Frances does not need to make the \texttt{univac} subdirectory a Git repository
because the \texttt{eniac} repository will track everything inside it regardless of how deeply nested.

Frances \emph{shouldn't} run \texttt{git\ init} in \texttt{univac} because nested Git repositories can interfere with each other.
If someone commits something in the inner repository,
Git will not know whether to record the changes in that repository,
the outer one,
or both.

\hypertarget{exerciserefrse-git-cmdline-ex-commit}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-commit}}\label{exerciserefrse-git-cmdline-ex-commit}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-commit}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Would only create a commit if files have already been staged.
\item
  Would try to create a new repository.
\item
  Is correct: first add the file to the staging area, then commit.
\item
  Would try to commit a file ``my recent changes'' with the message myfile.txt.
\end{enumerate}

\hypertarget{exerciserefrse-git-cmdline-ex-multiple}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-multiple}}\label{exerciserefrse-git-cmdline-ex-multiple}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-multiple}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change \texttt{names.txt} and \texttt{old-computers.txt} using an editor like Nano.
\item
  Add both files to the staging area with \texttt{git\ add\ *.txt}.
\item
  Check that both files are there with \texttt{git\ status}.
\item
  Commit both files at once with \texttt{git\ commit}.
\end{enumerate}

\hypertarget{exerciserefrse-git-cmdline-ex-bio}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-bio}}\label{exerciserefrse-git-cmdline-ex-bio}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-bio}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go into your home directory with \texttt{cd\ \textasciitilde{}}.
\item
  Create a new folder called \texttt{bio} with \texttt{mkdir\ bio}.
\item
  Go into it with \texttt{cd\ bio}.
\item
  Turn it into a repository with \texttt{git\ init}.
\item
  Create your biography using Nano or another text editor.
\item
  Add it and commit it in a single step with \texttt{git\ commit\ -a\ -m\ "Some\ message"}.
\item
  Modify the file.
\item
  Use \texttt{git\ diff} to see the differences.
\end{enumerate}

\hypertarget{exerciserefrse-git-cmdline-ex-ignore-nested}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-ignore-nested}}\label{exerciserefrse-git-cmdline-ex-ignore-nested}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-ignore-nested}}

To ignore only the contents of \texttt{results/plots},
add this line to \texttt{.gitignore}:

\begin{verbatim}
results/plots/
\end{verbatim}

\hypertarget{exerciserefrse-git-cmdline-ex-include}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-include}}\label{exerciserefrse-git-cmdline-ex-include}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-include}}

Add the following two lines to \texttt{.gitignore}:

\begin{verbatim}
*.dat           # ignore all data files
!final.dat      # except final.data
\end{verbatim}

The exclamation point \texttt{!} includes a previously-excluded entry.

Note also that if we have previously committed \texttt{.dat} files in this repository
they will not be ignored once these rules are added to \texttt{.gitignore}.
Only future \texttt{.dat} files will be ignored.

\hypertarget{exerciserefrse-git-cmdline-ex-github-interface}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-github-interface}}\label{exerciserefrse-git-cmdline-ex-github-interface}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-github-interface}}

The left button (with the picture of a clipboard)
copies the full identifier of the commit to the clipboard.
In the shell,
\texttt{git\ log} shows the full commit identifier for each commit.

The middle button shows all of the changes that were made in that particular commit;
green shaded lines indicate additions and red lines indicate removals.
We can show the same thing in the shell using \texttt{git\ diff}
or git diff from..to
(where from and to are commit identifiers).

The right button lets us view all of the files in the repository at the time of that commit.
To do this in the shell,
we would need to check out the repository as it was at that commit
using git checkout id.
If we do this,
we need to remember to put the repository back to the right state afterward.

\hypertarget{exerciserefrse-git-cmdline-ex-timestamp}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-timestamp}}\label{exerciserefrse-git-cmdline-ex-timestamp}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-timestamp}}

GitHub displays timestamps in a human-readable relative format
(i.e. ``22 hours ago'' or ``three weeks ago'').
However, if we hover over the timestamp
we can see the exact time at which the last change to the file occurred.

\hypertarget{exerciserefrse-git-cmdline-ex-push-commit}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-push-commit}}\label{exerciserefrse-git-cmdline-ex-push-commit}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-push-commit}}

Committing updates our local repository.
Pushing sends any commits we have made locally
that aren't yet in the remote repository
to the remote repository.

\hypertarget{exerciserefrse-git-cmdline-ex-boilerplate}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-boilerplate}}\label{exerciserefrse-git-cmdline-ex-boilerplate}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-boilerplate}}

When GitHub creates a \texttt{README.md} file while setting up a new repository,
it actually creates the repository and then commits the \texttt{README.md} file.
When we try to pull from the remote repository to our local repository,
Git detects that their histories do not share a common origin and refuses to merge them.

\begin{verbatim}
$ git pull origin master
\end{verbatim}

\begin{verbatim}
warning: no common commits
remote: Enumerating objects: 3, done.
remote: Counting objects: 100% (3/3), done.
remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (3/3), done.
From https://github.com/frances/eniac
 * branch            master     -> FETCH_HEAD
 * [new branch]      master     -> origin/master
fatal: refusing to merge unrelated histories
\end{verbatim}

We can force git to merge the two repositories with the option \texttt{-\/-allow-unrelated-histories}.
Please check the contents of the local and remote repositories carefully before doing this.

\hypertarget{exerciserefrse-git-cmdline-ex-recover}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-recover}}\label{exerciserefrse-git-cmdline-ex-recover}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-recover}}

The answer is (5)-Both 2 and 4.

The \texttt{checkout} command restores files from the repository,
overwriting the files in our working directory.
Answers 2 and 4 both restore the latest version in the repository of the file \texttt{data\_cruncher.sh}.
Answer 2 uses \texttt{HEAD} to indicate the latest,
while answer 4 uses the unique ID of the last commit,
which is what \texttt{HEAD} means.

Answer 3 gets the version of \texttt{data\_cruncher.sh} from the commit before \texttt{HEAD},
which is not what we want.

Answer 1 can be dangerous:
without a filename,
\texttt{git\ checkout} will restore all files in the current directory (and all directories below it)
to their state at the commit specified.
This command will restore \texttt{data\_cruncher.sh} to the latest commit version,
but will also reset any other files we have changed to that version,
which will erase any unsaved changes you may have made to those files.

\hypertarget{exerciserefrse-git-cmdline-ex-history}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-history}}\label{exerciserefrse-git-cmdline-ex-history}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-history}}

The answer is 2.

The command \texttt{git\ add\ history.txt} adds the current version of \texttt{history.txt} to the staging area.
The changes to the file from the second \texttt{echo} command are only applied to the working copy,
not the version in the staging area.

As a result,
when \texttt{git\ commit\ -m\ "Origins\ of\ ENIAC"} is executed,
the version of \texttt{history.txt} committed to the repository is the one from the staging area
with only one line.

However,
the working copy still has the second line.
(\texttt{git\ status} will show that the file is modified.)
\texttt{git\ checkout\ HEAD\ history.txt} therefore replaces the working copy with
the most recently committed version of \texttt{history.txt},
so \texttt{cat\ history.txt} prints:

\begin{verbatim}
ENIAC was the world's first general-purpose electronic computer.
\end{verbatim}

\hypertarget{exericserefrse-git-cmdline-ex-diff}{%
\subsection*{Exericse~\ref{rse-git-cmdline-ex-diff}}\label{exericserefrse-git-cmdline-ex-diff}}
\addcontentsline{toc}{subsection}{Exericse~\ref{rse-git-cmdline-ex-diff}}

FIXME: solution for exercise on git diff

\hypertarget{exerciserefrse-git-cmdline-ex-unstage}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-unstage}}\label{exerciserefrse-git-cmdline-ex-unstage}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-unstage}}

FIXME: solution for exercise on git unstage

\hypertarget{exerciserefrse-git-cmdline-ex-blame}{%
\subsection*{Exercise~\ref{rse-git-cmdline-ex-blame}}\label{exerciserefrse-git-cmdline-ex-blame}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-cmdline-ex-blame}}

FIXME: solution for exercise on git blame

\hypertarget{chapterrefrse-git-advanced}{%
\section{Chapter~\ref{rse-git-advanced}}\label{chapterrefrse-git-advanced}}

\hypertarget{exerciserefrse-git-advanced-ex-additional-pr}{%
\subsection*{Exercise~\ref{rse-git-advanced-ex-additional-pr}}\label{exerciserefrse-git-advanced-ex-additional-pr}}
\addcontentsline{toc}{subsection}{Exercise~\ref{rse-git-advanced-ex-additional-pr}}

\begin{verbatim}
$ git checkout master
$ git checkout -b addItaly
$ cp united_states.txt italy.txt
$ nano italy.txt #Add the right info into the file
$ git add italy.txt
$ git commit -m "Added file on Italy"
$ git push origin addItaly
\end{verbatim}

\hypertarget{chapterrefrse-project}{%
\section{Chapter~@ref(rse-project\}}\label{chapterrefrse-project}}

\hypertarget{exerciserefrse-project-ex-understand-project}{%
\subsection{Exercise~\ref{rse-project-ex-understand-project}}\label{exerciserefrse-project-ex-understand-project}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Who are the participants of this study?

  \begin{itemize}
  \tightlist
  \item
    51 soliciters were interviwed as the participants.
  \end{itemize}
\item
  What types of data was collected and used for analysis?

  \begin{itemize}
  \tightlist
  \item
    Interview data and a data from a database on court decisions.
  \end{itemize}
\item
  Can you find information on the demographics of the interviewees?

  \begin{itemize}
  \tightlist
  \item
    This information is not available within the documentation.
    Information on their jobs and opinions are there,
    but the participant demographics are only described within the associated article.
    The difficulty is that the article is not linked within the documentation or the metadata.
  \end{itemize}
\item
  This dataset is clearly in support of an article. What information can you find about it, and can you find a link to it?

  \begin{itemize}
  \tightlist
  \item
    We can search the dataset name and authorname trying to find this.
    A search for ``National Science Foundation (1228602)'',
    which is the grant information,
    finds the grant page \url{https://www.nsf.gov/awardsearch/showAward?AWD_ID=1228602}.
    Two articles are linked there,
    but both the DOI links are broken.
    We can search with the citation for each paper to find them.
    The Forced Migration article can be found at \url{https://www.fmreview.org/fragilestates/meili}
    but uses a different subset of interviews and does not mention demographics nor links to the deposited dataset.
    The Boston College Law Review article at \url{https://lawdigitalcommons.bc.edu/cgi/viewcontent.cgi?article=3318\&context=bclr}
    has the same two problems of different data and no dataset citation.
  \end{itemize}
\end{enumerate}

Searching more broadly through Meili's work, we can find this article:

\begin{quote}
Stephen Meili: ``Do Human Rights Treaties Help Asylum-Seekers?: Lessons from the United Kingdom''
(October 1, 2015).
Minnesota Legal Studies Research Paper No.~15-41.
Available at SSRN \url{https://ssrn.com/abstract=2668259} or \url{http://dx.doi.org/10.2139/ssrn.2668259}.
\end{quote}

This does list the dataset as a footnote and reports the 51 interviews with demographic data on reported gender of the interviewees.
This paper lists data collection as 2010-2014,
while the other two say 2010-2013.
We might come to a conclusion that this extra year is where the extra 9 interviews come in,
but that difference is not explained anywhere.

\hypertarget{exerciserefrse-project-ex-permanent-links}{%
\subsection{Exercise~\ref{rse-project-ex-permanent-links}}\label{exerciserefrse-project-ex-permanent-links}}

\url{https://web.archive.org/web/20191105173924/https://ukhomeoffice.github.io/accessibility-posters/posters/accessibility-posters.pdf}

\hypertarget{rse-yaml}{%
\chapter{YAML}\label{rse-yaml}}

\href{https://bookdown.org/yihui/rmarkdown/html-document.html}{YAML} is a way to write nested data structures in plain text
that is often used to specify configuration options for software.
The acronym stands for ``YAML Ain't Markup Language'',
but that's a lie:
YAML doesn't use \texttt{\textless{}tags\textgreater{}} like HTML,
but can still be quite fussy about what is allowed to appear where.

A simple YAML file has one key-value pair on each line
with a colon separating the key from the value:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{project-name:}\AttributeTok{ planet earth}
\FunctionTok{purpose:}\AttributeTok{ science fair}
\FunctionTok{moons:}\AttributeTok{ 1}
\end{Highlighting}
\end{Shaded}

Here,
the keys are \texttt{"project-name"}, \texttt{"purpose"}, and \texttt{"moons"},
and the values are \texttt{"planet\ earth"},
\texttt{"science\ fair"},
and (hopefully) the number 1,
since most YAML implementations try to guess the type of data.

If we want to create a list of values without keys,
we can write it either using square brackets (like a Python array)
or dashed items (like a Markdown list),
so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rotation-time:}\AttributeTok{ }\KeywordTok{[}\StringTok{"1 year"}\KeywordTok{,} \StringTok{"12 months"}\KeywordTok{,} \StringTok{"365.25 days"}\KeywordTok{]}
\end{Highlighting}
\end{Shaded}

and:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rotation-time:}
    \KeywordTok{-}\NormalTok{ 1 year}
    \KeywordTok{-}\NormalTok{ 12 months}
    \KeywordTok{-}\NormalTok{ 365.25 days}
\end{Highlighting}
\end{Shaded}

are equivalent.
(The indentation isn't absolutely required in this case,
but helps make the intenton clear.)
If we want to write entire paragraphs,
we can use a marker to show that a value spans multiple lines:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{feedback:}\AttributeTok{ |}
\NormalTok{    Neat molten core concept.}
\NormalTok{    Too much water.}
\NormalTok{    Could have used more imaginative ending.}
\end{Highlighting}
\end{Shaded}

We can also add comments using \texttt{\#} just as we do in many programming languages.

YAML is easy to understand when used this way,
but it starts to get tricky as soon as sub-lists and sub-keys appear.
For example,
this is part of the YAML configuration file for formatting this book:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bookdown:}\AttributeTok{:gitbook:}
  \FunctionTok{pandoc_args:}\AttributeTok{ }\KeywordTok{[}\StringTok{"--csl"}\KeywordTok{,} \StringTok{"./csl/jcb.csl"}\KeywordTok{]}
  \FunctionTok{highlight:}\AttributeTok{ tango}
  \FunctionTok{config:}
    \FunctionTok{download:}\AttributeTok{ }\KeywordTok{[}\StringTok{"pdf"}\KeywordTok{,} \StringTok{"epub"}\KeywordTok{]}
    \FunctionTok{toc:}
      \FunctionTok{collapse:}\AttributeTok{ section}
      \FunctionTok{before:}\AttributeTok{ |}
\NormalTok{        <li><strong><a href=}\StringTok{"./"}\NormalTok{>Merely Useful</a></strong></li>}
    \FunctionTok{sharing:}\AttributeTok{ no}
\end{Highlighting}
\end{Shaded}

It corresponds to the following Python data structure:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\{}
  \StringTok{'bookdown::gitbook'}\NormalTok{: \{}
    \StringTok{'pandoc_args'}\NormalTok{: [}
      \StringTok{'--csl'}\NormalTok{,}
      \StringTok{'./csl/jcb.csl'}
\NormalTok{    ],}
    \StringTok{'highlight'}\NormalTok{: }\StringTok{'tango'}\NormalTok{,}
    \StringTok{'config'}\NormalTok{: \{}
      \StringTok{'download'}\NormalTok{: [}
        \StringTok{'pdf'}\NormalTok{,}
        \StringTok{'epub'}
\NormalTok{      ],}
      \StringTok{'toc'}\NormalTok{: \{}
        \StringTok{'collapse'}\NormalTok{: }\StringTok{'section'}\NormalTok{,}
        \StringTok{'before'}\NormalTok{: }\StringTok{'<li><strong><a href="./">Merely Useful</a></strong></li>}\CharTok{\textbackslash{}n}\StringTok{'}
\NormalTok{      \}}
      \StringTok{'sharing'}\NormalTok{: }\VariableTok{False}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Auro2019}{}%
Aurora, V., and M. Gardiner. 2019. How to respond to code of conduct reports. Version 1.1. Frame Shift Consulting LLC.\\
A short, practical guide to enforcing a Code of Conduct.

\leavevmode\hypertarget{ref-Auro2018}{}%
Aurora, V. et al. 2018. How to respond to code of conduct reports. Frame Shift Consulting.\\
A practical step-by-step guide to handling code of conduct issues.

\leavevmode\hypertarget{ref-Bacc2013}{}%
Bacchelli, A., and C. Bird. 2013. Expectations, outcomes, and challenges of modern code review. \emph{In} Proc.~International conference on software engineering.\\
FIXME

\leavevmode\hypertarget{ref-Beck2016}{}%
Becker, B.A. et al. 2016. Effective compiler error message enhancement for novice programming students. \emph{Computer Science Education}. 26:148--175. doi:\href{https://doi.org/10.1080/08993408.2016.1225464}{10.1080/08993408.2016.1225464}.\\
Reports that improved error messages helped novices learn faster.

\leavevmode\hypertarget{ref-Beni2017}{}%
Beniamini, G. et al. 2017. Meaningful identifier names: The case of single-letter variables. \emph{In} Proc.~2017 international conference on program comprehension (ICPC'17). Institute of Electrical; Electronics Engineers (IEEE).\\
Reports that use of single-letter variable names doesn't affect ability to modify code, and that some single-letter variable names have implicit types and meanings.

\leavevmode\hypertarget{ref-Boll2014}{}%
Bollier, D. 2014. Think like a commoner: A short introduction to the life of the commons. New Society Publishers.\\
A short introduction to a widely-used model of governance.

\leavevmode\hypertarget{ref-Bran1995}{}%
Brand, S. 1995. How buildings learn: What happens after they're built. Penguin USA.\\
FIXME

\leavevmode\hypertarget{ref-Broo2016}{}%
Brookfield, S.D., and S. Preskill. 2016. The discussion book: 50 great ways to get people talking. Jossey-Bass.\\
Describes fifty different ways to get groups talking productively.

\leavevmode\hypertarget{ref-Brow2018}{}%
Brown, N.C., and G. Wilson. 2018. Ten quick tips for teaching programming. \emph{PLOS Computational Biology}. 14. doi:\href{https://doi.org/10.1371/journal.pcbi.1006023}{10.1371/journal.pcbi.1006023}.

\leavevmode\hypertarget{ref-Buck1995}{}%
Buckheit, J.B., and D.L. Donoho. 1995. WaveLab and reproducible research. \emph{In} Wavelets and statistics. Springer New York. 55--81.\\
An early and influential discussion of reproducible research.

\leavevmode\hypertarget{ref-Carr2014}{}%
Carroll, J. 2014. Creating minimalist instruction. \emph{International Journal of Designs for Learning}. 5. doi:\href{https://doi.org/10.14434/ijdl.v5i2.12887}{10.14434/ijdl.v5i2.12887}.\\
A look back on the author's work on minimalist instruction.

\leavevmode\hypertarget{ref-Cohe2010}{}%
Cohen, J. 2010. Modern code review. \emph{In} Making software. A. Oram and G. Wilson, editors. O'Reilly.\\
FIXME

\leavevmode\hypertarget{ref-Deve2018}{}%
Devenyi, G.A. et al. 2018. Ten simple rules for collaborative lesson development. \emph{PLOS Computational Biology}. 14. doi:\href{https://doi.org/10.1371/journal.pcbi.1005963}{10.1371/journal.pcbi.1005963}.

\leavevmode\hypertarget{ref-Dobz1973}{}%
Dobzhansky, T. 1973. Nothing in biology makes sense except in the light of evolution. \emph{The American Biology Teacher}.\\
A forceful statement of the central unifying theme of modern biology.

\leavevmode\hypertarget{ref-Faga1976}{}%
Fagan, M.E. 1976. Design and code inspections to reduce errors in program development. \emph{IBM Systems Journal}. 15:182--211. doi:\href{https://doi.org/10.1147/sj.153.0182}{10.1147/sj.153.0182}.\\
FIXME

\leavevmode\hypertarget{ref-Faga1986}{}%
Fagan, M.E. 1986. Advances in software inspections. \emph{IEEE Transactions on Software Engineering}. 12:744--751. doi:\href{https://doi.org/10.1109/TSE.1986.6312976}{10.1109/TSE.1986.6312976}.\\
FIXME

\leavevmode\hypertarget{ref-Foge2005}{}%
Fogel, K. 2005. Producing open source software: How to run a successful free software project. O'Reilly Media.\\
The definite guide to managing open source software development projects.

\leavevmode\hypertarget{ref-Free1972}{}%
Freeman, J. 1972. The tyranny of structurelessness. \emph{The Second Wave}. 2.\\
Points out that every organization has a power structure: the only question is whether it's accountable or not.

\leavevmode\hypertarget{ref-Fucc2016}{}%
Fucci, D. et al. 2016. An external replication on the effects of test-driven development using a multi-site blind analysis approach. \emph{In} Proc.~10th ACM/IEEE international symposium on empirical software engineering and measurement (ESEM'16). ACM Press.\\
The latest in a long line to find that test-driven development (TDD) has little or no impact on development time or code quality.

\leavevmode\hypertarget{ref-Gold1991}{}%
Goldberg, D. 1991. What every computer scientist should know about floating-point arithmetic. \emph{ACM Computing Surveys}. 23. doi:\href{https://doi.org/10.1145/103162.103163}{10.1145/103162.103163}.\\
FIXME

\leavevmode\hypertarget{ref-Grue2015}{}%
Gruenert, S., and T. Whitaker. 2015. School culture rewired: How to define, assess, and transform it. ASCD.\\
The source of a much-quoted observation on culture.

\leavevmode\hypertarget{ref-Hadd2010}{}%
Haddock, S., and C. Dunn. 2010. Practical computing for biologists. Sinauer Associates.\\
FIXME

\leavevmode\hypertarget{ref-Lind2008}{}%
Lindberg, V. 2008. Intellectual property and open source: A practical guide to protecting code. O'Reilly Media.\\
A thorough dive into intellectual property issues related to open source software

\leavevmode\hypertarget{ref-Marw2018}{}%
Marwick, B. et al. 2018. Packaging data analytical work reproducibly using r (and friends). \emph{PeerJ Preprints}. 6.\\
Describes a simpler project layout scheme.

\leavevmode\hypertarget{ref-Meil2015}{}%
Meili, S. 2016. Do human rights treaties help asylum-seekers: Findings from the u.k. doi:\href{https://doi.org/10.3886/E17507V2}{10.3886/E17507V2}.

\leavevmode\hypertarget{ref-Mill1956}{}%
Miller, G.A. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. \emph{Psychological Review}. 63:81--97. doi:\href{https://doi.org/10.1037/h0043158}{10.1037/h0043158}.\\
FIXME

\leavevmode\hypertarget{ref-Mina1986}{}%
Minahan, A. 1986. Martha's rules. \emph{Affilia}. 1:53--56. doi:\href{https://doi.org/10.1177/088610998600100206}{10.1177/088610998600100206}.\\
Describes a lightweight set of rules for consensus-based decision making.

\leavevmode\hypertarget{ref-Mori2012}{}%
Morin, A. et al. 2012. A quick guide to software licensing for the scientist-programmer. \emph{PLoS Computational Biology}. 8. doi:\href{https://doi.org/10.1371/journal.pcbi.1002598}{10.1371/journal.pcbi.1002598}.\\
A short introduction to software licensing for non-specialists.

\leavevmode\hypertarget{ref-Nobl2009}{}%
Noble, W.S. 2009. A quick guide to organizing computational biology projects. \emph{PLoS Computational Biology}. 5. doi:\href{https://doi.org/10.1371/journal.pcbi.1000424}{10.1371/journal.pcbi.1000424}.\\
How to organize a small to medium-sized bioinformatics project.

\leavevmode\hypertarget{ref-Petr2014}{}%
Petre, M., and G. Wilson. 2014. Code review for and by scientists. \emph{In} Proc.~Second workshop on sustainable software for science: Practice and experience.\\
FIXME

\leavevmode\hypertarget{ref-Quen2018}{}%
Quenneville, J. 2018. Code review.\\
Guidelines for code review

\leavevmode\hypertarget{ref-Sank2018}{}%
Sankarram, S. 2018. Unlearning toxic behaviors in a code review culture.\\
What \emph{not} to do in code review.

\leavevmode\hypertarget{ref-Scop2015}{}%
Scopatz, A., and K.D. Huff. 2015. Effective computation in physics. O'Reilly Media.\\
A comprehensive introduction to scientific computing in Python

\leavevmode\hypertarget{ref-Sega2005}{}%
Segal, J. 2005. When software engineers met research scientists: A case study. \emph{Empirical Software Engineering}. 10:517--536. doi:\href{https://doi.org/10.1007/s10664-005-3865-y}{10.1007/s10664-005-3865-y}.\\
FIXME

\leavevmode\hypertarget{ref-Shol2019}{}%
Sholler, D. et al. 2019. Ten simple rules for helping newcomers become contributors to open projects. \emph{PLOS Computational Biology}. 15:e1007296. doi:\href{https://doi.org/10.1371/journal.pcbi.1007296}{10.1371/journal.pcbi.1007296}.

\leavevmode\hypertarget{ref-Smit2011}{}%
Smith, P. 2011. Software build systems: Principles and experience. Addison-Wesley Professional.\\
A thorough, readable exploration of how software build systems and tools work.

\leavevmode\hypertarget{ref-Stei2014}{}%
Steinmacher, I. et al. 2014. The hard life of open source software project newcomers. \emph{In} Proc.~7th international workshop on cooperative and human aspects of software engineering (CHASE/14).\\
FIXME

\leavevmode\hypertarget{ref-Tasc2017}{}%
Taschuk, M., and G. Wilson. 2017. Ten simple rules for making research software more robust. \emph{PLoS Computational Biology}. 13. doi:\href{https://doi.org/10.1371/journal.pcbi.1005412}{10.1371/journal.pcbi.1005412}.\\
A short guide to making research software usable by other people.

\leavevmode\hypertarget{ref-Wick2016}{}%
Wickes, E., and A. Stein. 2016. Data documentation material.

\leavevmode\hypertarget{ref-Wick2014}{}%
Wickham, H. 2014. Tidy data. \emph{Journal of Statistical Software}. 59. doi:\href{https://doi.org/10.18637/jss.v059.i10}{10.18637/jss.v059.i10}.\\
The defining paper on tidy data.

\leavevmode\hypertarget{ref-Wils2018}{}%
Wilson, G. 2019a. Teaching tech together. Taylor \& Francis.\\
How to create and deliver lessons that work and build a teaching community around them.

\leavevmode\hypertarget{ref-Wils2019}{}%
Wilson, G. 2019b. Ten quick tips for creating an effective lesson. \emph{PLOS Computational Biology}. 15:e1006915. doi:\href{https://doi.org/10.1371/journal.pcbi.1006915}{10.1371/journal.pcbi.1006915}.

\leavevmode\hypertarget{ref-Wils2014}{}%
Wilson, G. et al. 2014. Best practices for scientific computing. \emph{PLoS Biology}. 12. doi:\href{https://doi.org/10.1371/journal.pbio.1001745}{10.1371/journal.pbio.1001745}.\\
Outlines what a mature research software project should look like.

\leavevmode\hypertarget{ref-Wils2017}{}%
Wilson, G. et al. 2017. Good enough practices in scientific computing. \emph{PLoS Computational Biology}. 13. doi:\href{https://doi.org/10.1371/journal.pcbi.1005510}{10.1371/journal.pcbi.1005510}.\\
Outlines what a ``good enough'' research software project should look like.

\leavevmode\hypertarget{ref-Yuan2014}{}%
Yuan, D. et al. 2014. Simple testing can prevent most critical failures: An analysis of production failures in distributed data-intensive systems. \emph{In} Proc.~11th usenix conference on operating systems design and implementation. USENIX Association. 249--265.\\
Found that the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code.

\end{document}
