# Provenance {#provenance}

We are now at the point where we've developed, automated and tested
a workflow for plotting the word count distribution for our collection of classic novels.
In the normal course of events,
outputs from that workflow (e.g. our figures and \(\alpha\) values)
would ultimately be included in a report.
Here we use the term "report" to include research papers,
summaries for clients,
or anything else that is shorter than a book
and aimed at people other than its creators. 

The first and most important point to make
is that modern publishing involves much more than producing a printable PDF.
It also entails providing the data underpinning the report
as well as the code used to do the analysis:

> An article about computational science in a scientific publication
> is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> --- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

While some reports, datasets, software packages and/or analysis scripts
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
Publishing it under an open license (Section \@ref(teams-license)) is the first step;
the sections below describe some of the other steps we can take
to capture the provenance of our data analysis.

> **Identifying Reports and Authors**
>
> Before publishing anything,
> we need to understand how authors and their works are identified.
> A [Digital Object Identifier][doi] (DOI)
> is a unique identifier for a particular version of a particular digital artifact
> such as a report, a dataset, or a piece of software.
> DOIs are written as `doi:prefix/suffix`,
> but are often also represented as URLs like `http://dx.doi.org/prefix/suffix`.
> In order to be allowed to issue a DOI,
> an academic journal, data archive, or other organiation
> must guarantee a certain level of security, longevity and access.
>
> An [ORCID][orcid] is an Open Researcher and Contributor ID.
> Anyone can get an ORCID for free,
> and should include it in publications
> because people's names and affiliations change over time.

## Data Provenance {#provenance-data}

The first step in documenting the data associated with a report
is to determine what (if anything) needs to be published.
If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party,
the report simply needs to document where to access the data
and what version was analyzed:
it's not necessary to publish a duplicate of the dataset.
This is the case for our Zipf's Law analysis,
since the texts we analyze are available at [Project Gutenberg][project-gutenberg].

It's not strictly necessary to publish intermediate data
produced during the analysis of a publicly available dataset either
(e.g. our CSV files produced by `countwords.py`),
so long as readers have access to the original data and the code/software used to process it.
However,
making intermediate data available can save people time and effort,
particularly if it takes a lot of computing power to reproduce it.
For example,
NASA has published
the [Goddard Institute for Space Studies Surface Temperature Analysis][gistemp],
which estimates the global average surface temperature
based on land and ocean weather observations,
because a simple metric of global warming is expensive to produce
and is useful in many research contexts.

If a report involves a new dataset,
such as observations collected during a field experiment,
then they need to be published following the FAIR Principles.

### The FAIR Principles {#provenance-data-fair}

The [FAIR Principles][go-fair] describe what research data should look like.
They are still aspirational for most researchers @Broc2019,
but tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

#### Data should be *findable*.

The first step in using or re-using data is to find it.
We can tell we've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier
    (i.e. a [DOI][doi]).
2.  Data is described with rich metadata
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in Section \@ref(provenance-data-where).

#### Data should be *accessible*.

People can't use data if they don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
We can tell we've done this if:

1.  (Meta)data is retrievable by its identifier
    using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

#### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
We can tell we've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation.
2.  (Meta)data uses vocabularies that follow FAIR principles.
3.  (Meta)data includes qualified references to other (meta)data.

#### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
We can tell we've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance][provenance].
4.  (Meta)data meets domain-relevant community standards.

### Where to Archive Data {#provenance-data-where}

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter \@ref(project).
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as [data packages][data-package],
and they are often accompanied by small scripts to clean up and query the data.

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms
like the [Open Science Framework][osf], [Dryad][dryad], and [Figshare][figshare],
which will give the dataset a DOI.
Big datasets (i.e., anything more than 5 GB)
may not be ours in the first place,
and probably need the attention of a professional archivist.

> **Data Journals**
>
> While archiving data at a site like Dryad or Figshare (following the FAIR Principles)
> is usually the end of the data publishing process,
> there is the option of publishing a journal paper to describe the dataset in detail.
> Some research disciplines have journals devoted
> to describing particular types of data
> (e.g., the [Geoscience Data Journal][geoscience-data-journal])
> and there are also generic data journals
> (e.g., [Scientific Data][scientific-data]).

## Code Provenance {#code-provenance}

Our Zipf's Law analysis represents a typical data science project
in that we've written some code that leverages other pre-existing software packages
(e.g. matplotlib, numpy) in order to produce key figures and results.
To document this code and software in an open, transparent and reproducible manner,
three separate items need to be archived:

1.  A detailed description of our **software environment**
2.  A copy of any **analysis scripts/notebooks** used to produce the key results
    presented in the report.
3.  A description of the **data processing steps** taken in producing each key result
    (i.e. a step-by-step account of how the software and scripts were actually used).

Unfortunately,
librarians, publishers, and regulatory bodies are still trying to determine
the best way to document and archive material like this,
so there is not yet anything like the FAIR Principles.
The best advice we can give is presented below.

### Software Description

FIXME: This section would make more sense if we were using a conda environment
from the beginning (i.e. in the installation instructions).
We should consider the merits of doing that.

In order to document the software packages that were used in our analysis,
the bare minimum requirement is to archive a list of
the names and version numbers of each software package.
We can get version information for the Python packages we are using by running:

```shell
$ pip freeze
```

```text
FIXME: Show output
```

Other command line tools will often have an option like `--version` or `--status`
to access the version information.

Archiving a list of package names and version numbers would mean that our
software environment is technically reproducible,
but it would be left up to the reader of the report
to figure out how to get all those packages installed and working together.
This might be fine for a small number of packages with very few dependencies,
but in more complex cases we probably want to make life easier for the reader
(and for our future selves looking to re-run the analysis).
One way to make things easier is to export a description of a complete
`conda` environment, which can be saved as YAML using:

```shell
$ conda env export > environment.yml
```

That software environment can then be recreated on another computer
with just one line of code:

```
$ conda env create -f environment.yml
```

More complex tools like [Docker][docker]
can literally install our entire environment
(down to the precise version of the operating system)
on a different computer @Nust2020.
However,
their complexity can be daunting,
and there is a lot of debate
about how well (or whether) they actually make research more reproducible in practice.

### Analysis Scripts

Later in this book we will package and release our Zipf's Law code
so that it can be downloaded and installed by the wider research community
just like any other Python package (Chapter \@ref(packaging)).
Going through the process of formally packaging your code can be a good option
if there's a user community out there interested in using and/or building it,
but often the scripts and notebooks we write
to produce a particular figure or table
will not be of broad interest.
In other words,
these scripts were written to get a specific job done
(e.g. to produce Figure 2b in a report)
and do not have a wider application.
To fully capture the provenance of the results presented in a report,
these analysis scripts and/or notebooks can simply be collected up into a zip folder
for archiving.

```shell
$ zip scripts.zip bin/*.py
```
```text
  adding: bin/collate.py (deflated 52%)
  adding: bin/countwords.py (deflated 54%)
  adding: bin/plotcounts.py (deflated 58%)
  adding: bin/test_zipfs.py (deflated 67%)
  adding: bin/utilities.py (deflated 51%)
```

### Data Processing Steps

Finally,
to make our results truly reproducible,
the reader needs to know exactly how (and in what order) we ran our analysis scripts
in our software environment.
The way in which this information is collected and archived
depends on the characteristics of the workflow.
Our Zipf's Law analysis involved executing a series of command line programs,
and a record of the command line entries we executed
can be generated using our Makefile (Chapter \@ref(automate)).
For instance, the sequence of commands involved in plotting
the collated word count distribution is as follows:

```shell
$ make all -n -B
```
```text
python bin/countwords.py data/dracula.txt > results/dracula.csv
python bin/countwords.py data/time_machine.txt > results/time_machine.csv
python bin/countwords.py data/sense_and_sensibility.txt > results/sense_and_sensibility.csv
python bin/countwords.py data/sherlock_holmes.txt > results/sherlock_holmes.csv
python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv
python bin/countwords.py data/frankenstein.txt > results/frankenstein.csv
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
mkdir -p results
python bin/collate.py results/time_machine.csv results/moby_dick.csv results/jane_eyre.csv results/dracula.csv results/sense_and_sensibility.csv results/sherlock_holmes.csv results/frankenstein.csv > results/collated.csv
python bin/plotcounts.py results/collated.csv --outfile results/plotcounts.png --plotparams plotparams.yml
```
The `-n` option is for a dry run (i.e. the commands are displayed but not executed)
and the `-B` option is to run all steps regardless or whether they need updating or not.

We can archive these commands in a text file:

```shell
$ make all -n -B > commands.txt
```

Some of the parameters for `plotcounts.py` are in the `plotparams.yml` configuration file
(Chapter \@ref(config)),
so that file also needs to be archived.

### Where to Archive Code

Now that we have generated information to document our
software environment (`environment.yml`),
analysis scripts (`scripts.zip`),
and data processing steps (`commands.txt` and `plotparams.yml`),
we need to archive them somewhere
(along with a README file to explain what each file is).
There are a number online platforms dedicated to archiving (with a DOI)
the "long-tail" of your research,
including supplementary figures, data and/or code.
Two of the most popular platforms for archiving code
are [Zenodo][zenodo] and [Figshare][figshare].

### Reproducibility Versus Inspectability

In most cases,
documenting your software environment, analysis scripts and data processing steps
will ensure that your computational analysis is reproducible/repeatable 
at the time your report is published.
But what about five or ten years from now?
As we have discussed,
data analysis workflows usually depend on a hierarchy of packages.
Our Zipf's Law analysis depends on a a collection of Python libraries (numpy, matplotlib, scipy, etc),
which in turn depend on the Python language itself.
Some workflows also depend critically on a particular operating system or firmware.
Over time some of these dependencies will inevitably be updated or no longer supported,
meaning the workflow you have documented will no longer be reproducible.
Fortunately,
most readers are not looking to exactly re-run a decade old analysis.
They really just want to be able to figure out what was run 
and what the important decisions were -- something referred to as inspectability (@Gil2016, @Brown2017). 
While exact repeatability has a short shelf-life,
inspectability is the enduring legacy of a well documented computational analysis.

## Summary {#provenance-summary}

The Internet started a revolution in scientific publishing
that shows no sign of ending.
Where an inter-library loan once took weeks to arrive
and data had to be transcribed from published papers
(if it could be found at all),
we can now download one another's work in minutes:
*if* we can find it and make sense of it.
Organizations like [Our Research][our-research] are building tools to help with both;
by using DOIs and ORCIDs,
publishing on preprint servers,
following the FAIR Principles,
and documenting our workflow,
we help ensure that everyone can pursue their ideas as we did.

## Exercises {#provenance-exercises}

### ORCID {#provenance-ex-get-orcid}

If you don't already have an [ORCID][orcid],
go to the website and register now.
If you do have an ORCID,
log in and make sure that your details and publication record are up-to-date.

### A FAIR test {#provenance-ex-fair-test}

An [online questionnaire][fair-questionnaire]
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.
Take the questionnaire for a dataset you have published or that you use often.

### Publishing your code {#provenance-ex-publish-code}

Think about a project that you're currently working on.
How would you go about publishing the code associated with that project
(i.e., the software description, analysis scripts, and data processing steps)?

## Key Points {#publish-keypoints}

```{r, child="keypoints/shared-rse/provenance.md"}
```
