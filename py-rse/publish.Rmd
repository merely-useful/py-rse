# Publishing {#publish}

```{r publish-setup, include=FALSE}
source(here::here("_common.R"))
```

Throughout this book we've produced interesting analysis
on topics including dog licenses in New York City
and word counts in classic English novels.
This final chapter considers how one might go about
publishing a formal report on the sort of data analysis we've conducted.
We use the generic term "report" to include research papers,
summaries for clients,
and everything else that is shorter than a book
and aimed at people other than its creators or contributors.

The first and most important point to make
is that modern publishing involves much more than simply producing a written report.
It involves providing readers with the data underpinning the report,
as well as any code written in analysing the data:

> An article about computational science in a scientific publication
> is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> — Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

Depending on the analysis performed,
the "complete set of instructions" relating to software and code
may range from scripts or notebooks written solely for the purpose of the report
(e.g. our earlier analysis of dog licenses),
to packages that are developed and released for use by a wider audience
(e.g. our Zipf's Law package).

While some reports, datasets, software packages and/or analysis scripts
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
That means publishing it under an open license (Section \@ref(teams-license))
so that people who aren't in academia can find and access it.


## How are reports and authors identified? {#publish-identifiers}

Before publishing anything,
we need to understand the systems used to identify works
and their authors.

A [Digital Object Identifier][doi] (DOI)
is a unique identifier for a particular version of a particular digital artifact
such as a report, a dataset, or a piece of software.
DOIs are written as `doi:prefix/suffix`,
but are often also represented as URLs like `http://dx.doi.org/prefix/suffix`.
In order to be allowed to issue a DOI (i.e., to become a prefix),
academic journals, data archives, and others
must guarantee a certain level of security, longevity and access.

An [ORCID][orcid] is an Open Researcher and Contributor ID.
You can get an ORCID for free,
and you should include it in publications
because people's names and affiliations change over time.

## Where should I publish my reports? {#publish-where}

The best option for publishing a report depends on the context.
Numerous open access journals support academic, peer-reviewed research papers,
and many formerly closed-access journals also now offer an open access option,
though sometimes for an additional fee.

Another option is to publish with an online pre-print server
such as [arXiv][arxiv] (which started it all) or [bioRxiv][biorxiv].
A preprint is a version of an academic research paper that precedes formal peer review
and publication in a peer-reviewed journal.
The preprint may be available, often as a non-typeset version available free,
before and/or after a paper is published in a journal.

Online writing platforms such as [Authorea][authorea] and [Overleaf][overleaf] are also an option.
These allow a report to be openly viewable on the web throughout the entire writing process.
Once work is complete (or as complete as research writing ever is),
they can issue a DOI or the text can be exported for submission to an academic journal.
For everything other than reports,
online platforms such as [Figshare][figshare] and [Zenodo][zenodo]
can issue DOIs as well.
It is common for people to upload datasets and other items to these platforms
so that they can be easily accessed by others.

## How should I publish data? {#publish-data}

The first step in publishing the data associated with a report
is to determine what (if anything) needs to be published.
If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party
(e.g. open government data like the NYC Dog Licensing Dataset),
then it's likely that no data publishing is required.
The report simply needs to document where to access the data
and what version was analyzed,
along with any scripts and software used to download and process the data.
In other words,
it's not necessary to re-publish a duplicate of the original dataset
if it's already accessible elsewhere.

It's not strictly necessary to publish data files
produced during the analysis of a publicly available dataset either,
since readers have access to the original data and the scripts/software used to process it.
However,
it can be advantageous to publish processed data that is difficult to reproduce
(e.g., might require access to a supercomputing facility to run the code)
and/or represents a derived quantity with high re-use potential.
For example,
NASA have published an estimate of the global average surface temperature derived from
land and ocean weather observations
(see the [Goddard Institute for Space Studies Surface Temperature Analysis][gistemp]),
because a simple metric of global warming is useful in many research contexts.

If a report involves a new dataset,
such as observations collected during a field experiment,
then the simple version of how to publish it is:

-   Always use [tidy data][tidy-data].
-   Include keywords describing the data in the project's `README.md`
    so that they appear on its home page and can easily be found by search engines.
-   Give every dataset and every report a unique identifier (Section \@ref(publish-identifiers)).
-   Put data in open repositories.
-   Use well-known formats like CSV and HDF5.
-   Include an explicit license in every project and every dataset.
-   Include units and other metadata.

The last point is often the hardest for people to implement,
since many researchers have never seen a well-documented dataset.
We draw inspiration from the data catalog included in [the repository][womens-pockets-data] for @Dieh2018
and include a file `./data/README.md` in every project
that looks like this:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   `Infant_HIV_Testing_2017.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2009-2017
    -   `infant_hiv.csv`
        -   What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments,
            others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_infant_hiv()` to tidy this data.
-   Maternal health indicators disaggregated by age
    -   `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/maternal-health-data/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2000-2014
    -   `at_health_facilities.csv`
        -   What is this?: percentage of births at health facilities by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `c_sections.csv`
        -   What is this?: percentage of Caesarean sections by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `skilled_attendant_at_birth.csv`
        -   What is this?: percentage of births with skilled attendant present by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   Notes
        -   Data is not tidy:
            some rows are descriptive comments,
            others are blank separators between sections,
            and column headers are inconsistent.
        -   Use `tidy_maternal_health_adolescents()` to tidy this data.
```

The catalog above doesn't include column headers or units because the data isn't tidy.
It *does* include the names of the functions used to reformat that data,
and `./results/README.md` then includes the information that users will want.
One section of that file is shown below:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   infant_hiv.csv
      -   What is this?: tidied version of CSV export from spreadsheet.
      -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
      -   Last Modified: September 2018
      -   Contact: Greg Wilson <greg.wilson@rstudio.com>
      -   Spatial Applicability: global
      -   Temporal Applicability: 2009-2017
      -   Generated By: scripts/tidy-24.R

| Header   | Datatype | NA    | Description                                 |
|----------|----------|-------|---------------------------------------------|
| country  | char     | false | ISO3 country code of country reporting data |
| year     | integer  | false | year CE for which data reported             |
| estimate | double   | true  | estimated percentage of measurement         |
| hi       | double   | true  | high end of range                           |
| lo       | double   | true  | low end of range                            |
```

Note that this catalog includes both units and whether or not a field can be NA.
Note also that calling a field "NA" is asking for trouble…

## What standards of data sharing should I aspire to? {#publish-fair}

The [FAIR Principles][go-fair] describe what research data should look like.
They are still aspirational for most researchers [@Broc2019],
but they tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

### Data should be *findable*.

The first step in using or re-using data is to find it.
You can tell you've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier (Section \@ref(publish-identifiers)).
2.  Data is described with rich metadata (like the catalog shown above).
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in Section \@ref(publish-data).

### Data should be *accessible*.

You can't use data if you don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
You can tell you've done this if:

1.  (Meta)data is retrievable by its identifier using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
You can tell you've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation
2.  (Meta)data uses vocabularies that follow FAIR principles
3.  (Meta)data includes qualified references to other (meta)data

### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
You can tell you've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance][provenance].
4.  (Meta)data meets domain-relevant community standards.

### How and where do I publish the data?

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter \@ref(project).
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as [data packages][data-package],
and they are often accompanied by small scripts to clean up and query the data.
Be sure to give the dataset an identifier as discussed in Section \@ref(publish-identifiers).

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms like the [Open Science Framework][osf],
[Dryad][dryad],
and [Figshare][figshare].
Each of these will give the datasets identifiers;
those identifiers should be included in reports
along with scripts to download the data.
Big datasets (i.e., anything more than 5 GB)
may not be yours in the first place,
and probably need the attention of a professional archivist.
Any processed or intermediate data that takes a long time to regenerate
should probably be published as well using these same sizing rules;
all of this data should be given identifiers,
and those identifiers should be included in reports.

> **Data Journals**
>
> While archiving data at a site like Dryad or Figshare (following the FAIR Principles)
> is usually the end of the data publishing process,
> there is the option of publishing a journal paper to describe the dataset in detail.
> Some research disciplines have journals devoted
> to describing particular types of data
> (e.g., the [Geoscience Data Journal][geoscience-data-journal])
> and there are also generic data journals
> (e.g., [Scientific Data][scientific-data]).

## How should I publish my software packages? {#publish-software}

In Chapter \@ref(package-py) we documented and packaged
our Zipf's Law software.
The final step in this process is publication.
It is common practice to have the code associated with a software package
openly available on a hosting service
such as [GitHub][github], [GitLab][gitlab], or [Bitbucket][bitbucket].
These hosting services are not only a convenient place for people to ask questions
and make contributions/improvements to the software,
they also have built-in functionality for managing the
release of new versions of the software.
One limitation of these sites, however,
is that they don’t guarantee persistent long term storage
(e.g., if you changed the name of your GitHub repository any URLs for
the existing repository would be broken).
Acknowledging this limitation,
GitHub provides [Zenodo integration][github-zenodo-tutorial]
for creating a DOI with each new software release.

> **Software Journals**
>
> While creating a DOI using a site like Zenodo
> is often the end of the software publishing process,
> there is the option of publishing
> a journal paper to describe the software in detail.
> Some research disciplines have journals devoted
> to describing particular types of software
> (e.g., [Geoscientific Model Development][geoscientific-model-development]),
> and there are also a number of generic software journals such as the
> [Journal of Open Research Software][jors] and
> the [Journal of Open Source Software][theoj].

## How should I publish my scripts? {#publish-scripts}

The final component that needs to be published is the analysis scripts (or notebooks).
Unlike analysis software that has been packaged and released for use by a wider audience,
analysis scripts are simply written to create the figures and tables presented in the report.
These scripts typically make use of packages written by the wider community (such as `matplotlib`)
as well discipline-specific packages written by colleagues or co-authors.

Given that analysis scripts will typically leverage
a wide variety of existing software packages,
three separate items need to be published:

1.  A detailed description of the analysis software used
2.  A copy of any analysis scripts written by the authors to produce the key results
    presented in the report
3.  A description of the data processing steps taken in producing each key result
    (i.e. a step-by-step account of how the software and scripts were actually implemented)

Earlier we saw that there are well-developed and widely adopted
guidelines for data publishing (e.g., the FAIR Principles).
The same is not true for analysis scripts.
Librarians, publishers, and regulatory bodies are still trying to determine
the best way for code to be documented and archived.
For the moment,
the best advice we can give for those three key items is discussed below.
That advice ranges from the bare minimum that needs to be done
through to current gold standard practice.

### Software description

In order to document the software packages that were used,
the bare minimum requirement is to list the name and version number
of each software package
that played a critical role in producing the analysis presented in your report.
As Section \@ref(package-py-package) described,
you can get these automatically by running:

```shell
$ pip freeze > requirements.txt
```

For everything else,
you should write a script or create a rule in your project's Makefile (Chapter \@ref(automate)),
since the commands used to get version numbers will vary from tool to tool:

```make
## versions : dump versions of software.
versions :
        @echo '# Python packages'
        @pip freeze
        @echo '# dezply'
        @dezply --version
        @echo '# parajune'
        @parajune --status | head 1
```

While such a list means your software environment is now technically reproducible,
you’ve left it up to the reader to figure out how to get all those software packages
and libraries installed and playing together nicely.
In some cases it might be easy enough for a reader to install the handful of packages you used,
but in other cases you might want to save the reader (and your future self)
the pain of software installation by making use of a tool
that can automatically install a specified software environment.
One tool that's widely used for Python is [`conda`][conda].
A `conda` environment can be exported,

```shell
$ conda env export > environment.yml
```

and made available so that readers can use it to install the same environment on their
own computer:

```
$ conda env create -f environment.yml
```

Beyond `conda`,
more complex tools like [Docker][docker]
can literally install your entire environment (down to the precise operating system) on a different computer.
There’s lots of debate
about the potential and suitability of these tools as a solution to reproducible research,
but their complexity puts them out of reach for many researchers.

### Analysis scripts

The next item you’ll need to publish is a copy of the scripts
written to execute those software packages.
Depending on the size or complexity of the scripts you have written,
and whether you re-use them in multiple projects,
you may publish script by script
or create a zip file or tar file that includes everything.
For example,
the Makefile fragment below creates `~/archive/meow-2019-02-21.tgz`:

```make
ARCHIVE=${HOME}/archive
PROJECT=meow
TODAY=$(shell date "+%Y-%m-%d")
SCRIPTS=./Makefile ./bin/*.py ./bin/*.sh

## archive : create an archive of all the scripts used in this run
archive :
        @mkdir -p ${ARCHIVE}
        @tar zcf ${ARCHIVE}/${PROJECT}-${TODAY}.tgz
```

### Data processing steps

A software description and analysis scripts on their own are not much use to a reader;
they also need to know how those scripts was actually executed.
The way in which this information is collected and archived
depends on how your workflow is constructed.
If all of a program's parameters are in a configuration file (Chapter \@ref(configuration)),
then that file can be archived.
Alternatively, you might need to have your program print out its configuration parameters
and then use `grep` or a script to extract information from the logfile
(Section \@ref(errors-logging)).
If your workflow involves executing a series of command line programs,
then you can keep a log/record of the command line entries
required to produce a given result.
For example,
the [cmdline-provenance][cmdline-provenance] Python package generates such records,
including keeping track of the corresponding version control revision number,
so you know exactly which version of your command line program was executed.
More prosaically,
running:

```shell
$ history | tail -n 20 > generate-figure-5.txt
```

is better than nothing.

As before,
while these outputs from a configuration file, logfile and/or command line history
ensure that your workflow is reproducible,
they may not be particularly comprehensible.
In other words,
manually recreating workflows from these outputs might be a tedious and time consuming process,
even for just moderately complex analyses.
To make things a little easier for the reader (and your future self),
it’s a good idea to include a README file in your code library
explaining the sequence of commands required to produce common/key results.
You might also provide a Makefile that automatically builds and executes common workflows.

## Summary {#publish-summary}

FIXME: summarize publishing chapter

## Exercises {#publish-exercises}

### ORCID {#publish-ex-get-orcid}

If you don't already have an [ORCID][orcid],
go to the website and register now.
If you do have an ORCID,
log in and make sure that your details and publication record are up-to-date.

### A FAIR test {#publish-ex-fair-test}

An [online questionnaire][fair-questionnaire]
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.
Take the questionnaire for a dataset you have published or that you use often.

### Publishing your code

Think about a project that you're currently working on.
How would you go about publishing the code associated with that project
(i.e., the software description, analysis scripts, and data processing steps)?

## Key Points {#publish-keypoints}

```{r, child="keypoints/shared-rse/publish.md"}
```

```{r, child="./links.md"}
```
