[
["index.html", "Merely Useful Chapter 1 Introduction 1.1 Questions 1.2 Objectives 1.3 Who are these lessons for? 1.4 What does “done” look like? 1.5 Why isn’t all of this normal already? 1.6 What will this book accomplish? 1.7 What will we use as running examples? 1.8 Acknowledgments 1.9 Contributing 1.10 Summary 1.11 Key Points", " Merely Useful A Long List of People 2019-05-01 Chapter 1 Introduction It’s still magic even if you know how it’s done. – Terry Pratchett 1.1 Questions What is the difference between open, reproducible, and sustainable? What is computational competence? What is the scope of this training? What are the prerequisites for this training? Be able to do the steps in Yenni et al. (2019). 1.2 Objectives Explain the difference between open science, reproducible research, sustainability, and competence. Determine readiness for using this material. Explain what ‘done’ looks like for the computational component of a small or medium-sized research project. Determine whether a particular research project meets that standard. As research becomes more computing intensive, researchers need more computing skills so that: other people (including your future self) can re-do your analyses; you and the people using your results can be confident that they’re correct; and re-using your software is easier than rewriting it. Most books and courses about software engineering are aimed at product development, but research has different aims and needs. A research programmer’s goal is to answer a question; she might build software in order to do that, but the software is only a means to an end. But just as some astronomers spend their entire careers designing better telescopes, some researchers choose to spend their time building software that will primarily be used by their colleagues. People who do this may be called research software engineers (RSEs) or data engineers, and the aim of these lessons is to help you get ready for these roles—to go from writing code on your own, for your own use, to working in a small team creating tools to help your entire field advance. One of the many challenges you will face is to find the appropriate mix of tools and methods for each problem you have to solve. If you want to reformat a handful of text files so that your program can read them in, you shouldn’t bother writing a comprehensive test suite or setting up automated builds. On the other hand, if you don’t do this, and that “handful of text files” turns into a pile, and then a mountain, you will quickly reach a point where you wish you had. We hope this training will help you understand what challenges have already been solved and where to find those solutions so that when you need them, you’ll be able to find them. 1.3 Who are these lessons for? Amira completed a Master’s in library science five years ago, and has worked since then for a small NGO. She did some statistics during her degree, and has learned some R and Python by doing data science courses online, but has no formal training in programming. Amira would like to tidy up the scripts, data sets, and reports she has created in order to share them with her colleagues. These lessons will show her how to do this and what “done” looks like. Jun completed an Insight Data Science fellowship last year after doing a PhD in Geology, and now works for a company that does forensic audits. He has used a variety of machine learning and visualization software, and has made a few small contributions to a couple of open source R packages. He would now like to make his own code available to others; this guide will show him how such projects should be organized. Sami learned a fair bit of numerical programming while doing a BSc in applied math, then started working for the university’s supercomputing center. Over the past few years, the kinds of applications they are being asked to support have shifted from fluid dynamics to data analysis. This guide will teach them how to build and run data pipelines so that they can teach those skills to their users. 1.4 What does “done” look like? In order to answer the question posed in this section’s title, we need to distinguish between three key ideas. The first is open science, which aims to make research methods and results available for everyone to read and re-use. The second is reproducible research, which means that anyone with access to the raw materials can easily reproduce the results. Openness and reproducibility are closely related, but are not the same thing: If you share my data and analysis scripts, but haven’t documented the manual steps in the analysis, your work is open but not reproducible. If you completely automate the analysis, but it’s only available to people in you company or lab, it’s reproducible but not open. The third key idea is sustainability. A piece of software is being sustained if people are using it, fixing it, and improving it rather than replacing it. Sustainability isn’t just a property of the software: it also depends on the culture of its actual and potential users. If “share, mend, and extend” is woven into the fabric of their culture, even Fortran-77 can thrive (though of course good tooling and packaging can lower costs and barriers to entry). Conversely, it doesn’t matter whether a library has automated tests and is properly packaged if potential users suffer from Not Invented Here syndrome. More importantly, if the software is being maintained by a couple of post-docs who are being paid a fraction of what they could earn in industry, and who have no realistic hope of promotion because their field looks down on tool building, those people will eventually move on and their software will start to suffer from bit rot. What ties these three ideas together is the notion of computational competence, which is the the programming equivalent of good laboratory skills. Software is just another kind of lab equipment; just as an archaeologist should know how to prepare and catalog an artefact, any researcher writing software should know how to make their work reproducible and share it with the world without staying up until dawn. Why “Computational Competence”? The term computational thinking has been widely used since Wing (2006) introduced it a decade ago. It has also been used in such a wide variety of ways that no one really knows what it means. We therefore prefer to talk about computational competence—about someone’s ability to do computing well. 1.5 Why isn’t all of this normal already? Nobody argues that research should be irreproducible or unsustainable, but “not against it” and actively supporting it are very different things. Academia doesn’t yet know how to reward people for writing useful software, so while you may be thanked, the extra effort you put in may not translate into job security or decent pay. And some people still argue against openness, Being open is a big step toward a (non-academic) career path, which is where approximately 80% of PhDs go, and for those staying in academia, open work is cited more often than closed (FIXME: citation). However, some people still worry that if they make their data and code generally available, someone else will use it and publish a result they have come up with themselves. This is almost unheard of in practice, but that doesn’t stop people using it as a boogeyman. Other people are afraid of looking foolish or incompetent by sharing code that might contain bugs. This isn’t just impostor syndrome: members of marginalized groups are frequently judged more harshly than others (FIXME: CITE). 1.6 What will this book accomplish? The goal of this book is to help you produce more correct results in less time and with less effort: stakeholders will be confident that you did things the right way, which in turn will allow them to be confident in your results, and you and others will be able to re-use your data, software, and reports instead of constantly rewriting them. To achieve this, we will cover: Writing code that is readable, testable, and maintainable Automating analyses with build tools Checking and demonstrating correctness via automated tests Publishing science in the 21st Century Using a branch-per-feature workflow, rebasing, and tags to manage work Organizing the code, data, results, and reports in a small or medium-sized project These lessons can be used for self-study by people who are taking part in something like the Insight Data Science Fellows Program, or as part of a one-semester course for graduate students or senior undergraduates. You will know you’re done when: You are reasonably confident that your results are correct. This is not the same as “absolutely sure”: our goal is to make digital work as trustworthy as lab experiments or careful manual analysis. Your software can be used by other people without heroic effort, I.e., people you have never met can find it and figure out how to install it and use it in less time than it would take them to write something themselves. Small changes and extensions are easy so that your software can grow as your problems and questions evolve. 1.7 What will we use as running examples? In order to make this material as accessible as possible, we will use two text processing problems as running examples. The first is an exploration of Zipf’s Law, which states that frequency of a word is inversely proportional to rank, i.e., that the second most common word in some text occurs half as often as most common, the third most common occurs a third as often, and so on. We will write some simple software to test a corpus of text against this rule. The files we will use are taken from the Project Gutenberg and contain this many words: Book Words anne_of_green_gables.txt 105642 common_sense.txt 24999 count_of_monte_cristo.txt 464226 dracula.txt 164424 emma.txt 160458 ethan_frome.txt 37732 frankenstein.txt 78098 jane_eyre.txt 188455 life_of_frederick_douglass.txt 43789 moby_dick.txt 215830 mysterious_affair_at_styles.txt 59604 pride_and_prejudice.txt 124974 sense_and_sensibility.txt 121590 sherlock_holmes.txt 107533 time_machine.txt 35524 treasure_island.txt 71616 This is how often the most common words appear in this corpus as a whole: Word Count the 97278 and 59385 to 56028 of 55190 I 45680 a 40483 in 30030 was 24512 that 24386 you 22123 it 21420 The frequencies aren’t an exact match—we would expect about 48,600 occurrences of “and”, for example—but there certainly seems to be a decay curve of some kind. We’ll look more closely at this data as we go along. The second project is a simple form of computational stylometry. Different writers have different styles; can a computer detect those differences, and if so, can it determine who the likely author of a text actually was? Computational stylometry has been used to explore which parts of Shakespeare’s plays might have been written by other people, which presidential tweets were composed by other people, and who wrote incriminating emails in several high-profile legal cases. The authors of our books are listed below. Three of them were purportedly written by Jane Austen; we will see if the similarity measures we develop show that. Author Book Jane Austen emma.txt Jane Austen pride_and_prejudice.txt Jane Austen sense_and_sensibility.txt Charlotte Brontë jane_eyre.txt Agatha Christie mysterious_affair_at_styles.txt Frederick Douglass life_of_frederick_douglass.txt Arthur Conan Doyle sherlock_holmes.txt Alexandre Dumas count_of_monte_cristo.txt Herman Melville moby_dick.txt Lucy Maud Montgomery anne_of_green_gables.txt Thomas Paine common_sense.txt Mary Wollstonecraft Shelley frankenstein.txt Robert Louis Stevenson treasure_island.txt Bram Stoker dracula.txt H. G. Wells time_machine.txt Edith Wharton ethan_frome.txt 1.8 Acknowledgments This book owes its existence to the hundreds of researchers we met through Software Carpentry and Data Carpentry. We are also grateful to Insight Data Science for sponsoring the early stages of this work, to everyone who has contributed, and to: Practical Computing for Biologists Haddock and Dunn (2010) “A Quick Guide to Organizing Computational Biology Projects” Noble (2009) “Ten Simple Rules for Making Research Software More Robust” Taschuk and Wilson (2017) “Best Practices for Scientific Computing” Wilson et al. (2014) “Good Enough Practices in Scientific Computing” Wilson et al. (2017) Before any of that, though, four books written by Brian Kernighan and his colleagues forever changed the way people think about programming. The Elements of Programming Style Kernighan and Plauger (1978), The C Programming Language Kernighan and Ritchie (1988), The Unix Programming Environment Kernighan and Pike (1983), and Software Tools in Pascal Kernighan and Plauger (1981) didn’t just show an entire generation how to write software we could be proud of: they also showed us that books about programming could be a pleasure to read. 1.9 Contributing Contributions of all kinds are welcome, from errata and minor improvements to entirely new sections and chapters: please email us or submit an issue or pull request to our GitHub repository. Everyone whose work is incorporated will be acknowledged; Please see the contributors’ guide for details, and please note that all contributors are required to abide by our Code of Conduct. 1.10 Summary For researchers and data scientists who can build and run programs that are three or four pages long, and who want to be more productive and have more confidence in their results, this guide provides a pragmatic, tools-based introduction to program design and maintenance. Unlike books and courses aimed at computer scientists and professional software developers, this guide uses data analysis as a motivating example and assumes that the learner’s ultimate goal is to answer questions rather than ship products. Learners must be comfortable with the basics of the Unix shell, Python or R, and Git. They will need a personal computer with Internet access, the Bash shell, Python 3, and a GitHub account. 1.11 Key Points Research is ‘open’ if everyone can read it. Research is ‘reproducible’ if people who have access can regenerate it. Research is ‘sustainable’ if it was built in reasonable time and without heroic effort. Computational competence is the digital equivalent of knowing how to use lab equipment properly. A project is ‘done’ when stakeholders can be reasonably sure the results are correct and the software can be understood, run, and extended by people other than the original authors without heroic effort. References "],
["shell.html", "Chapter 2 The Unix Shell 2.1 Questions 2.2 Objectives 2.3 Introduction 2.4 Working With Files 2.5 Editing Text Files 2.6 Operating on Text 2.7 Shortcuts 2.8 Working with Multiple Files 2.9 Paths 2.10 Pipes 2.11 Variables 2.12 Writing Shell Scripts 2.13 Repeating Things in the Shell 2.14 Key Points", " Chapter 2 The Unix Shell 2.1 Questions What is the Unix shell? When and why would I use the Unix shell? 2.2 Objectives FIXME 2.3 Introduction Look at TLDR pages (which has a command-line client tldr) for content. What is the difference between an operating system and a shell? Why would I use the shell? 2.4 Working With Files How do we see what’s in a folder? How do we move around in the shell? How do we view the contents of a text file? How do we copy a file? How do we move a file? How do we rename a file? How do we delete a file? 2.5 Editing Text Files What’s the difference between a text file and a binary file? How do we run a text editor? How do we edit text in a text editor? How do we move around in a text editor? How do we find and replace in a text editor? 2.6 Operating on Text How do we find out how long a file is? How do we select the top or bottom of a file? How do we save the output of a command in a file? How do we select columns of text? 2.7 Shortcuts How do we see what I’ve done? How do we use tab completion to fill in filenames? How do we repeat previous commands? 2.8 Working with Multiple Files How can we select many files at once (globbing)? How can we make selections more specific? 2.9 Paths What is an absolute path? What is a relative path? What special paths does the shell understand? 2.10 Pipes How do we combine two or more commands? 2.11 Variables What is a variable? How do we create variables? How do we use variables? What variables does the shell automatically create for us? 2.12 Writing Shell Scripts How do we execute commands saved in a file? How do we pass filenames into a script? 2.13 Repeating Things in the Shell What is a loop? How do we write a loop in a shell script? How do we write a loop interactively? 2.14 Key Points FIXME "],
["automate.html", "Chapter 3 Automating Analyses 3.1 Questions 3.2 Objectives 3.3 Introduction 3.4 How can I update a file when its prerequisites change? 3.5 How can I tell Make where to find rules? 3.6 How can I update multiple files when their prerequisites change? 3.7 How can I get rid of temporary files that I don’t need? 3.8 How can I make a target depend on several prerequisites? 3.9 How can I reduce the amount of typing I have to do? 3.10 How can I make one update depend on another? 3.11 How can I abbreviate my update rules? 3.12 How can I write one general rule to update many files in the same way? 3.13 How can I define sets of files automatically? 3.14 How can I document my workflow? 3.15 Summary 3.16 Exercises 3.17 Key Points", " Chapter 3 Automating Analyses 3.1 Questions How can I make my analyses easier to reproduce? How can I make it easier to repeat analyses when I get new data, or when my data or scripts change? 3.2 Objectives Explain what a build tool is and how build tools aid reproducible research. Describe and identify the three parts of a Make rule. Write a Makefile that re-runs a multi-stage data analysis. Explain and trace how Make chooses an order in which to execute rules. Explain what phony targets are and define a phony target. Explain what automatic variables are and correctly identify three commonly-used automatic variables. Rewrite Make rules to use automatic variables. Explain why and how to write a pattern rule in a Makefile. Rewrite Make rules to use patterns. Define variables in a Makefile explicitly and by using functions. Make a self-documenting Makefile. 3.3 Introduction As Chapter 1 said, Zipf’s Law states that the second most common word in a body of text appears half as often as the most common, the third most common appears a third as often, and so on. The analyses we want to do include: Analyze one input file to see how well it conforms to Zipf’s Law. Analyze multiple input files to how well then conform in aggregate. Plot individual and aggregate word frequency distributions and expected values. The project we have inherited as a starting point contains the following: The books we are analyzing are in data/title.txt. A program called bin/countwords.py can read a text file and produce a CSV file with two columns: the words in the text and the number of times that word occurs. bin/countwords.py can analyze several files at once if we provide many filenames on the command line or concatenate the files and send them to standard input in a pipeline using something like cat data/*.txt | bin/countwords.py. Another program, bin/plotcounts.py, will create a visualization for us that shows word rank on the X axis and word counts on the Y axis. (It doesn’t show the actual words.) A third program, bin/collate.py, takes one or more CSV files as input and merges them by combining the counts for words they have in common. Finally, bin/testfit.py will compare actual distributions against expectations and give a fitting score. It’s easy enough to run these programs by hand if we only want to analyze a handful of files, but doing this becomes tedious and error-prone as the number of files grows. Instead, we can write a shell script or another Python program to do multi-file analyses. Doing this documents the pipeline so that our colleagues (and future selves) can understand it, and enables us to re-do the entire analysis with a single command if we get new data or change our methods, parameters, or libraries. It also prevents us from making lots of little errors: there’s no guarantee we’ll get the script right the first time, but once we’ve fixed it, it will stay fixed. However, re-running the entire analysis every time we get a new file is inefficient: we don’t need to re-count the words in our first thousand books when we add the thousand and first. This isn’t a problem when calculations can be done quickly, but many can’t, and anyway, the point of this chapter is to introduce a tool for handling cases in which we really want to avoid doing unnecessary work. What we want is a way to describe which files depend on which other files and how to generate or update a file when necessary. This is the job of a build tool. As the name suggests, a build tool’s job is to build new files out of old ones. The most widely used build tool, Make, was written in 1976 to recompile programs (which at the time was a slow process). GNU Make is a free, fast, and well-documented version of Make; we will use it throughout this book. Alternatives Many better build tools have been developed since Make—so many, in fact, that none has been able to capture more than a small fraction of potential users. Snakemake has a lot of fans, and a future version of this tutorial might well use it. This introduction based on the Software Carpentry lesson on Make maintained by Gerard Capes and on Jonathan Dursi’s introduction to pattern rules. 3.4 How can I update a file when its prerequisites change? Make is based on three key ideas: The operating system automatically records a timestamp every time a file is changed. By checking this, Make can tell whether files are newer or older than other files. A programmer writes a Makefile to tell Make how files depend on each other. For example, the Makefile could say that results/moby_dick.csv depends on data/moby_dick.txt, or that plots/aggregate.svg depends on all of the CSV files in the results/ directory. The Makefile includes shell commands to create or update files that are out of date. For example, it could include a command to (re-)run bin/countwords.py to create results/moby_dick.csv from data/moby_dick.txt. (Make’s use on shell commands is one reason for its longevity, since it allows programmers to write tools for updating files in whatever language they want.) Let’s start by creating a file called Makefile that contains the following three lines: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv The # character starts a comment, which runs to the end of a line (just as it does in Python or R). results/moby_dick.csv is the target of a rule, i.e., something that may need to be created or updated. Every rule in a Makefile has one or more targets, and must be written flush against the left margin. data/moby_dick.txt is a prerequisite in that rule, i.e., something that the target of the rule depends on. A single colon separates the target from its prerequisites, and a rule can have any number of prerequisites—we’ll see examples soon. The indented line that uses Python to run bin/countwords.py is the rule’s action. It creates or updates the target when it is out of date. A rule can have any number of actions, but they must be indented by a single tab character. Notice that the output of bin/countwords.py is redirected using &gt; to create the output file: we will look at modifying the script in Chapter 5 so that it can take the name of an output file as an argument. Together, the three parts of this rule tell Make when and how to re-create results/moby_dick.csv. To test that it works, run this command in the shell: $ make Make automatically looks for a file called Makefile and checks the rules it contains. In this case, one of three things will happen: Make won’t be able to find the file data/moby_dick.csv, so it will run the script to create it. Make will see that data/moby_dick.txt is newer than results/moby_dick.csv, in which case it will run the script to update the results file. Make will see that results/moby_dick.csv is newer than its prerequisite, so it won’t do anything. In the first two cases, Make will show the command it runs, along with anything the command prints to the screen via standard output or standard error. In this case, there is no screen output, so we only see the command. Indentation Errors If Makefile contains spaces instead of tabs to indent the rule’s action, we will see an error message like this: Makefile:3: *** missing separator. Stop. The requirement to use tabs is a legacy of Make’s origins as a student intern project, and no, I’m not kidding. If we run make again right away it doesn’t re-run our script because we’re in situation #3 from the list above: the target is newer than its prerequisites, so no action is required. We can check this by listing the files with their timestamps, ordered by how recently they have been updated: $ ls -l -t data/moby_dick.txt results/moby_dick.csv -rw-r--r-- 1 gvwilson staff 219107 31 Dec 08:58 results/moby_dick.csv -rw-r--r-- 1 gvwilson staff 1276201 31 Dec 08:58 data/moby_dick.txt When Make sees that a target is newer than its prerequisites it displays a message like this: make: `results/moby_dick.csv&#39; is up to date. To test that Make is actually doing the right thing, we can: Delete results/moby_dick.csv and type make again (situation #1). Run the command touch data/moby_dick.txt to update the timestamp on the source file, then run make (situation #2). 3.5 How can I tell Make where to find rules? We don’t have to call our file of rules Makefile. If we want, we can rename the file single_rule.mk and then run it with make -f single_rule.mk. Most people don’t do this in real projects, but in a lesson like this, which includes many example Makefiles, it comes in handy. Using -f doesn’t change our working directory. If, for example, we are in /home/gvwilson/still-magic and run make -f src/automate/single_rule.mk, our working directory remains /home/gvwilson/still-magic. This means that Make will look for the rule’s prerequisite in /home/gvwilson/still-magic/data/moby_dick.txt, not in /home/gvwilson/still-magic/src/automate/data/moby_dick.txt. FIXME: figure 3.6 How can I update multiple files when their prerequisites change? Our Makefile isn’t particularly exciting so far. Let’s add another rule to the end and save the result as double_rule.mk: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv When we ask Make to run this file: $ make -f double_rule.mk we get this rather disappointing message: make: `results/moby_dick.csv&#39; is up to date. Nothing happens because by default Make only attempts to update the first target it finds in the Makefile, which is called the default target. To update something else, we need to tell Make we want it: $ make -f double_rule.mk results/jane_eyre.csv This time Make runs: python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv 3.7 How can I get rid of temporary files that I don’t need? A phony target in a Makefile is one that doesn’t correspond to any files and doesn’t have any prerequisites. Phony targets are just a way to save useful commands in a Makefile, but saying “just” is a bit misleading: what we’re actually doing is recording all of the steps in our workflow, even if those steps don’t create or update files. For example, let’s add another target to our Makefile to delete all of the files we have generated. By convention this target is called clean, and ours looks like this: # Remove all generated files. clean : rm -f results/*.csv (The -f flag to rm means “force removal”. When we use it, rm won’t complain if the files it’s trying to remove are already gone.) Let’s run Make: $ make -f pipeline.mk clean and then use ls to list the contents of results. Sure enough, it’s empty. Phony targets are useful as a way of documenting actions in a project, but there’s a catch. Use mkdir to create a directory called clean, then run make -f clean.mk clean. Make will print: make: `clean&#39; is up to date. The problem is that Make finds something called clean and assumes that’s what the rule is referring to. Since the rule has no prerequisites, it can’t be out of date, so no actions are executed. There are two ways to solve this problem. The first is to make sure we don’t have phony targets with the same names as files or directories. That works as long as our project is small and we’re paying attention, but as the project grows, or we’re rushing to meet a deadline or have inherited the project from someone else and don’t realize that this might be a problem, it’s bound to fail at exactly the worst time. A much better solution is to tell Make that the target is phony by putting this in the Makefile: .PHONY : clean Most people declare all of their phony targets together near the top of the file, though some put the .PHONY declarations right before the rules they refer to. As with most other rules about programming style (Chapter 11), consistency matters more than exactly what you do. 3.8 How can I make a target depend on several prerequisites? Right now, our Makefile says that each result file depends only on the corresponding data file. That’s not accurate: in reality, each result also depends on the script used to generate it. If we change our script, we ought to regenerate our results and then check to see if they’ve changed. (We can rely on version control to tell us that.) Here’s a modified version of the Makefile in which each result depends on both the data file and the script: .PHONY: clean # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt bin/countwords.py python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt bin/countwords.py python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv # ...clean target as before... We can test this by touching the script and then making one or the other result: $ touch bin/countwords.py $ make -f depend_on_script.mk results/jane_eyre.csv python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv 3.9 How can I reduce the amount of typing I have to do? The name of our script now appears four times in our Makefile, which will make for a lot of typing if and when we decide to move it or rename it. We can fix that by defining a variable at the top of our file to refer to the script, then using that variable in our rules: .PHONY: clean COUNT=bin/countwords.py # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt ${COUNT} python ${COUNT} data/moby_dick.txt &gt; results/moby_dick.csv # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt ${COUNT} python ${COUNT} data/jane_eyre.txt &gt; results/jane_eyre.csv # ...clean target as before... The definition takes the form NAME=value. By convention, variables are written in UPPER CASE so that they’ll stand out from filenames (which are usually in lower case), but that’s not required. What is required is using ${NAME} to refer to the variable: if we write $NAME, Make interprets that as “the variable called N followed by the three literal characters ‘AME’.” If no variable called N exists, $NAME becomes AME, which is almost certainly not useful. Using variables doesn’t just cut down on typing. They also make rules easier to understand, since they signal to readers that several things are always and exactly the same. 3.10 How can I make one update depend on another? We can re-create all the results files with a single command by listing multiple targets when we run Make: $ make results/moby_dick.csv results/jane_eyre.csv However, users have to know what files they might want to create in order to do this, and have to type their names exactly right. A better approach is to create a phony target that depends on all of the output files and make it the first rule in the file so that it is the default. By convention, this target is called all, and while we don’t have to list all our phony targets in alphabetical order, it makes them a lot easier to find: .PHONY: all clean COUNT=bin/countwords.py # Regenerate all results. all : results/moby_dick.csv results/jane_eyre.csv # ...rules for moby_dick, jane_eyre, and clean... If we run Make now, it sees that all is only “up to date” if the two CSV files are up to date, so it looks for a rule for each and runs each of those rules. We can draw the prerequisites defined in the Makefile as a dependency graph, with arrows showing what each target depends on. FIXME: figure Note that the Makefile doesn’t define the order in which results/moby_dick.csv and results/jane_eyre.csv are updated, so Make can rebuild them in whatever order it wants. This is called declarative programming: we declare what outcome we want, and the program figures out how to achieve it. 3.11 How can I abbreviate my update rules? We could add a third book to our Makefile, then a fourth, but any time we find ourselves duplicating code, there’s almost certainly a way to write a general rule. In order to create these, though, we first need to learn about automatic variables. The first step is to use the very cryptic expression $@ in the rule’s action to mean “the target of the rule”. We start with this: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv and turn it into this: # Regenerate results for &quot;Moby Dick&quot; results/moby_dick.csv : data/moby_dick.txt python bin/countwords.py data/moby_dick.txt &gt; $@ $@ is an automatic variable: Make defines its value for us separately in each rule. And yes, $@ is an unfortunate name: something like $TARGET would be easier to understand, but we’re stuck with it. Step 2 is to replace the list of prerequisites in the action with $^, which is another automatic variable meaning “all the prerequisites of the current rule”: # Regenerate results for &quot;Jane Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt python bin/countwords.py $^ &gt; $@ But wait: our results files don’t just have books as dependencies. They also depend on bin/countwords.py. What happens if we include that in the rule while using automatic variables? (We’ll do this for a third book to keep the three rules separate in the example Makefile.) # Regenerate results for &quot;The Time Machine&quot; - WRONG results/time_machine.csv : data/time_machine.txt ${COUNT} python bin/countwords.py $^ &gt; $@ This doesn’t do the right thing because $^ includes all of the prerequisites, so the action tries to process the script as if it were a data file: python bin/countwords.py data/time_machine.txt bin/countwords.py results/time_machine.csv This situation comes up so often that Make helpfully provides another automatic variable $&lt; meaning “the first prerequisite”, which lets us rewrite our rules like this: # Regenerate results for &quot;Janey Eyre&quot; results/jane_eyre.csv : data/jane_eyre.txt ${COUNT} python bin/countwords.py $&lt; &gt; $@ And yes, $&lt; &gt; $@ is hard to read, even with practice, and &lt; $&lt; (reading the first prerequisite from standard input) is even harder. Using an editor that does syntax highlighting helps (Chapter 15), and if you are ever designing software for other people to use, remember this case and don’t do it. 3.12 How can I write one general rule to update many files in the same way? We can now replace all the rules for generating results files with one pattern rule that uses % as a wildcard. Whatever part of a filename % matches in the target, it must also match in the prerequisites, so the single rule: results/%.csv : data/%.txt ${COUNT} python bin/countwords.py $&lt; &gt; $@ will handle Jane Eyre, Moby Dick, and The Time Machine. (Unfortunately, % cannot be used in rules’ actions, which is why $&lt; and $@ are needed.) With this rule in place, our entire Makefile is reduced to: .PHONY: all clean COUNT=bin/countwords.py # Regenerate all results. all : results/moby_dick.csv results/jane_eyre.csv results/time_machine.csv # Regenerate result for any book. results/%.csv : data/%.txt ${COUNT} python ${COUNT} $&lt; &gt; $@ # Remove all generated files. clean : rm -f results/*.csv Let’s delete all of the results files and re-create them all: $ make -f pattern_rule.mk clean rm -f results/*.csv $ make -f pattern_rule.mk all python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv python bin/countwords.py data/time_machine.txt &gt; results/time_machine.csv We can still rebuild individual files: $ touch data/jane_eyre.txt $ make -f pattern_rule.mk results/jane_eyre.csv python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv 3.13 How can I define sets of files automatically? Our “automated” analysis is still not fully automated: If we add another book to raw, we have to remember to add it to the all target in the Makefile as well. Once again, we will fix this in steps. To start, imagine that all the results files already exist, and we just want to update them. We can define a variable called RESULTS to be a list of all the results files using the same notation we’d use in the shell to match all the CSV files in the results/ directory: RESULTS=results/*.csv and then make all depend on that: # Regenerate all results. all : ${RESULTS} This works, but only for re-creating files: if a results file doesn’t already exist when we run Make, its name won’t be included in RESULTS, and Make won’t realize that we want to generate it. What we really want to do is generate the list of results files from the list of books in the data/ directory. We can use a function to do this. The syntax is a little odd, because functions were added to Make long after it was first written, but at least they have readable names. Let’s create a variable DATA that holds the names of all of our data files: DATA = $(wildcard data/*.txt) This calls the function wildcard with the argument data/*.txt. The result is a list of all the text files in the raw directory, just as we’d get with data/*.txt in the shell. (We could use a shell wildcard here as we did when defining RESULTS, but we want to show how functions work.) Did this do the right thing? To check, we can add another phony target to the end of the file called settings that uses the shell command echo to print the name and value of a variable: .PHONY: all clean settings # ...everything else... # Show variables&#39; values. settings : echo COUNT: ${COUNT} echo DATA: ${DATA} Let’s run this: $ make -f function_wildcard.mk settings echo COUNT: bin/countwords.py COUNT: bin/countwords.py echo DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt The output appears twice because Make shows us the command it’s going to run before running it. If we put @ before the command, Make doesn’t show it before running it: settings : @echo COUNT: ${COUNT} @echo DATA: ${DATA} $ make -f function_wildcard.mk settings COUNT: bin/countwords.py DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt We now have the names of our input files, but what we want is the names of the corresponding output files. Make has another function called patsubst (short for “pattern substitution”) that uses the same kind of patterns used in rules to do exactly this: RESULTS=$(patsubst data/%.txt,results/%.csv,${DATA}) $(patsubst ...) calls the pattern substitution function. The first argument is what to look for: in this case, a text file in the raw directory. As in a pattern rule, we use % to match the stem of the file’s name, which is the part we want to keep. The second argument is the replacement we want. Ours uses the stem matched by % to construct the name of a CSV file in the results directory. Finally, the third argument is what we’re doing substitutions in, which is our list of books’ names. Let’s check that this has worked by adding to the settings target settings : @echo COUNT: ${COUNT} @echo DATA: ${DATA} @echo RESULTS: ${RESULTS} $ make -f patsubst.mk settings COUNT: bin/countwords.py DATA: data/common_sense.txt data/jane_eyre.txt data/life_of_frederick_douglass.txt data/moby_dick.txt data/sense_and_sensibility.txt data/time_machine.txt RESULTS: results/common_sense.csv results/jane_eyre.csv results/life_of_frederick_douglass.csv results/moby_dick.csv results/sense_and_sensibility.csv results/time_machine.csv Excellent: DATA has the names of all of the files we want to process and RESULTS automatically has the corresponding names of the files we want to generate. Let’s test it: $ make -f patsubst.mk clean rm -f results/*.csv $ make -f patsubst.mk all python bin/countwords.py data/common_sense.txt &gt; results/common_sense.csv python bin/countwords.py data/jane_eyre.txt &gt; results/jane_eyre.csv python bin/countwords.py data/life_of_frederick_douglass.txt &gt; results/life_of_frederick_douglass.csv python bin/countwords.py data/moby_dick.txt &gt; results/moby_dick.csv python bin/countwords.py data/sense_and_sensibility.txt &gt; results/sense_and_sensibility.csv python bin/countwords.py data/time_machine.txt &gt; results/time_machine.csv Our workflow is now just two steps: add a data file and run Make. As we’ll see in Chapter 13, we can even automate the second half in some cases, but this is still a big improvement over running things manually, particularly as we start to add more steps (such as merging data files and generating plots). 3.14 How can I document my workflow? Every well-behaved program can tell people how to use it. If we run make --help, for example, we get a (long) list of things Make can do for us. But how can we document the workflow that our Makefile now embodies? One common choice is to provide a special target like settings that prints a description of available targets: .PHONY: all clean help settings # ...other definitions... # Show help. help : @echo &quot;all : regenerate all out-of-date results files.&quot; @echo &quot;results/*.csv : regenerate a particular results file.&quot; @echo &quot;clean : remove all generated files.&quot; @echo &quot;settings : show the values of all variables.&quot; @echo &quot;help : show this message.&quot; This is easy to set up and does the job, but once again its redundancy should worry us: the same information appears in both the comments on rules and the help, which means that authors have to remember to update the help when adding or changing rules. A better approach, which we will explore in more depth in Chapter 16, is to have people format some comments in a special way and then extract and display those comments when asked for help. We’ll use ## (a double comment marker) to indicate the lines we want displayed and use grep to extract lines that start with that marker. We will use Make’s MAKEFILE_LIST variable to get the path to the Makefile, since we may be using the -f flag to specify which Makefile we’re using. With all that in place, our finished Makefile is: .PHONY: all clean help settings COUNT=bin/countwords.py DATA=$(wildcard data/*.txt) RESULTS=$(patsubst data/%.txt,results/%.csv,${DATA}) ## all : regenerate all results. all : ${RESULTS} ## results/*.csv : regenerate result for any book. results/%.csv : data/%.txt ${COUNT} python ${COUNT} $&lt; &gt; $@ ## clean : remove all generated files. clean : rm -f results/*.csv ## settings : show variables&#39; values. settings : @echo COUNT: ${COUNT} @echo DATA: ${DATA} @echo RESULTS: ${RESULTS} ## help : show this message. help : @grep &#39;^##&#39; ${MAKEFILE_LIST} Let’s test: $ make -f makefile_grep.mk ## all : regenerate all results. ## results/*.csv : regenerate result for any book. ## clean : remove all generated files. ## settings : show variables&#39; values. ## help : show this message. With a bit more work we could strip off the leading ## markers, but this is a good start. How did you know that? FIXME: keep this personal? I had never used the variable MAKEFILE_LIST before writing this lesson. In fact, until about 15 minutes ago, I didn’t even know it existed: I always had my help target’s action grep for ## in Makefile. Once I realized that wouldn’t work in this example (because I’m writing lots of little Makefiles to demonstrate ideas step by step) I searched online for “how to get name of Makefile in make”. The second hit took me to the GNU Make documentation for other special variables, which told me exactly what I needed. I spend anywhere from a quarter to three quarters of my time searching for things when I program these days; one of the goals of these lessons is to give you an idea of what you ought to be searching for yourself so that you can do this more efficiently. 3.15 Summary Figure 3.1: Automation Concept Map Smith (2011) describes the design and implementation of several build tools in detail. 3.16 Exercises 3.16.1 Create a summary CSV file Add rule to Makefile to create a summary CSV file from all of the book CSV files. Careful how to write the dependencies so that it doesn’t depend on itself. 3.16.2 Generate a plot for the top N words FIXME: make it depend on the summary. 3.17 Key Points A build tool re-runs commands to bring files up to date with respect to the things they depend on. Make is a widely-used build tool that uses files’ timestamps to find out-of-date prerequisites. A Make rule has targets, prerequisites, and actions. A target can correspond to a file or be a phony target (used simply to trigger actions). When a target is out of date with respect to its prerequisites, Make executes the actions associated with its rule. Make executes as many rules as it needs to when updating files, but always respect prerequisite order. Make defines the automatic variables $@ (target), $^ (all prerequisites), and $&lt; (first prerequisite). Pattern rules can use % as a placeholder for parts of filenames. Makefiles can define variables using NAME=value. Makefiles can also use functions such as $(wildcard ...) and $(patsubst ...). Specially-formatted comments can be used to make Makefiles self-documenting. References "],
["syndicate.html", "Chapter 4 Syndicating Data 4.1 Questions 4.2 Objectives 4.3 Introduction 4.4 How can I fetch data data from a website? 4.5 How can I handle tabular data? 4.6 How can I generalize data fetching and handle errors? 4.7 How can I compare different data sets? 4.8 How can I publish data so that others can use it? 4.9 How can I make it easy for other people to find my data? 4.10 Summary 4.11 Exercises 4.12 Key Points", " Chapter 4 Syndicating Data 4.1 Questions \"FIXME 4.2 Objectives \"Write Python programs to download data sets using simple REST APIs. \"Parse CSV data using the csv library. \"Test a program that parses CSV using multiline strings. \"Make a function more robust by explicitly handling errors. \"Write Python programs that share static data sets. 4.3 Introduction A growing number of organizations make data sets available on the web in a style called REST, which stands for REpresentational State Transfer. When REST is used, every data set is identified by a URL and can be accessed through a set of functions called an application programming interface (API). This lesson will look at how to use these interfaces, and how to provide data through them ourselves. 4.4 How can I fetch data data from a website? The World Bank’s Climate Data API provides data generated by 15 global circulation models. According to the API’s home page, the data sets containing yearly averages for various values are identified by URLs of the form: http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/VAR/year/ISO3.EXT where: VAR is either pr (for precipitation) or tas (for “temperature at surface”); ISO3 is the International Standards Organization (ISO) 3-letter code for a country, such as “CAN” for Canada or “BRA” for Brazil; and EXT (short for “extension”) specifies the format we want the data in. There are several choices for format, but the simplest is comma-separated values (CSV), in which each record is a row, and the values in each row are separated by commas. (CSV is frequently used for spreadsheet data.) For example, if we want the average annual temperature in Canada as a CSV file, the URL is: http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv If we paste that URL into a browser, it displays: year,data 1901,-7.67241907119751 1902,-7.862711429595947 1903,-7.910782814025879 ... 2007,-6.819293975830078 2008,-7.2008957862854 2009,-6.997011661529541 Behind the Scenes This particular data set might be stored in a file on the World Bank’s server, or that server might: Receive our URL. Break it into pieces. Extract the three key fields (the variable, the country code, and the desired format). Fetch the desired data from a database. Format the data as CSV. Send that to our browser. As long as the World Bank doesn’t change its URLs, we don’t need to know which method it’s using and it can switch back and forth between them without breaking our programs. If we only wanted to look at data for a couple of countries, we could just download those files one by one. But we want to compare data for many different pairs of countries, so we should write a program. Python has a library called Requests for running HTTP requests. To install it, run: $ pip install requests (Note that pip is a program in its own right, so the command above must be run in the shell, and not from within Python itself.) Once it is installed, we can get the data we want like this: import requests url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv&#39; response = requests.get(url) if response.status_code != 200: print(&#39;Failed to get data:&#39;, response.status_code) else: print(&#39;First 100 characters of data are&#39;) print(response.text[:100]) First 100 characters of data are year,data 1901,-7.67241907119751 1902,-7.862711429595947 1903,-7.910782814025879 1904,-8.15572929382 requests.get actually gets our data. More specifically, it: creates a connection to the climatedataapi.worldbank.org server; sends the URL /climateweb/rest/v1/country/cru/tas/year/CAN.csv to that server; creates an object in memory on our computer to hold the response; assigns a number to that object’s status_code member variable to tell us whether the request succeeded or not; and assigns the data sent back by the web server to the object’s text member variable. (We could just pass this URL as an argument to the requests.get call, but assigning it to a variable makes it easier to find.) The server can return many different status codes; the most common are: Code Name Meaning 200 OK The request has succeeded. 204 No Content The server has completed the request, but doesn’t need to return any data. 400 Bad Request The request is badly formatted. 401 Unauthorized The request requires authentication. 404 Not Found The requested resource could not be found. 408 Timeout The server gave up waiting for the client. 418 I’m a teapot No, really… 500 Internal Server Error An error occurred in the server. 200 (OK) is the one we want; if we get anything else, the response probably doesn’t contain actual data (though it might contain an error message). 4.5 How can I handle tabular data? Our little program gets the data we want, but returns it as one long character string rather than as a list of numbers. There are two ways we could convert the former to the latter: Write a function to split that string on newline characters to create lines, then split the lines on commas and convert the second part of each to a number. Use a Python library to do this for us. Most experienced programmers would say that the second approach is easier, but “easy” is relative: using standard libraries is only easier if we know that those libraries exist and how to use them. Let’s try the first approach. To begin, we create a file called test_01.csv that contains the following three lines: 1901,12.3 1902,45.6 1903,78.9 It’s easy to read this file line by line and (for example) report the length of each line: with open(&#39;test_01.csv&#39;, &#39;r&#39;) as reader: for line in reader: print(len(line)) 10 10 10 We can also split each line on commas to turn each one into a list of string fragments: with open(&#39;test_01.csv&#39;, &#39;r&#39;) as reader: for line in reader: fields = line.split(&#39;,&#39;) print(fields) [&#39;1901&#39;, &#39;12.3\\n&#39;] [&#39;1902&#39;, &#39;45.6\\n&#39;] [&#39;1903&#39;, &#39;78.9\\n&#39;] The dates are correct, but the values all end with \\n. This is an escape sequence that represents the newline character at the end of each line. To get rid of it, we should strip leading and trailing whitespace from each line before splitting it on commas: with open(&#39;test_01.csv&#39;, &#39;r&#39;) as reader: for line in reader: fields = line.strip().split(&#39;,&#39;) print(fields) [&#39;1901&#39;, &#39;12.3&#39;] [&#39;1902&#39;, &#39;45.6&#39;] [&#39;1903&#39;, &#39;78.9&#39;] Now let’s have a look at how we could parse the data using standard Python libraries instead. The library we’ll use is called csv. It doesn’t read data itself: instead, it takes the lines read by something else and turns them into lists of values by splitting on commas. Here’s one way to use it: import csv with open(&#39;test_01.csv&#39;, &#39;r&#39;) as raw: cooked = csv.reader(raw) for record in cooked: print(record) [&#39;1901&#39;, &#39;12.3&#39;] [&#39;1902&#39;, &#39;45.6&#39;] [&#39;1903&#39;, &#39;78.9&#39;] Here, raw reads data in the normal way, while cooked is a wrapper that takes a line of text and turns it into a list of fields. We can equally well give a csv.reader a list of strings rather than a file: import csv with open(&#39;test_01.csv&#39;, &#39;r&#39;) as raw: lines = raw.readlines() cooked = csv.reader(lines) for record in cooked: print(record) [&#39;1901&#39;, &#39;12.3&#39;] [&#39;1902&#39;, &#39;45.6&#39;] [&#39;1903&#39;, &#39;78.9&#39;] Using the csv library doesn’t seem any simpler than just splitting strings, but look at what happens when we have data like this: &quot;Meltzer, Marlyn Wescoff&quot;,1922,2008 &quot;Spence, Frances Bilas&quot;,1922,2012 &quot;Teitelbaum,Ruth Lichterman&quot;,1924,1986 With simple string splitting, our output is: [&#39;&quot;Meltzer&#39;, &#39; Marlyn Wescoff&quot;&#39;, &#39;1922&#39;, &#39;2008&#39;] [&#39;&quot;Spence&#39;, &#39; Frances Bilas&quot;&#39;, &#39;1922&#39;, &#39;2012&#39;] [&#39;&quot;Teitelbaum&#39;, &#39;Ruth Lichterman&quot;&#39;, &#39;1924&#39;, &#39;1986&#39;] The double quotes are still there, and the field containing each person’s name has been split into pieces. If we use the csv library, on the other hand, the output is: [&#39;Meltzer, Marlyn Wescoff&#39;, &#39;1922&#39;, &#39;2008&#39;] [&#39;Spence, Frances Bilas&#39;, &#39;1922&#39;, &#39;2012&#39;] [&#39;Teitelbaum,Ruth Lichterman&#39;, &#39;1924&#39;, &#39;1986&#39;] because the library understands how to handle text fields containing commas (and a lot more). We need to do one more thing before using csv with the climate data. When we use the World Bank’s API to get data for a particular country, it comes back as one long string: year,data 1901,-7.67241907119751 1902,-7.862711429595947 1903,-7.910782814025879 ... We have to break this into lines before giving it to csv.reader, and we can do that by splitting the string on the same \\n escape sequence we encountered a few moments ago. To see how this works, let’s read test_01.csv into memory and split it into pieces: with open(&#39;test_01.csv&#39;, &#39;r&#39;) as reader: data = reader.read() lines = data.split(&#39;\\n&#39;) print(lines) [&#39;1901,12.3&#39;, &#39;1902,45.6&#39;, &#39;1903,78.9&#39;, &#39;&#39;] That’s almost right, but why is there an empty string at the end of the list? The answer is that the last line of the file ends in a newline, so Python does the same thing it does in the example below: fields = &#39;a-b-&#39;.split(&#39;-&#39;) print(fields) [&#39;a&#39;, &#39;b&#39;, &#39;&#39;] The solution once again is to strip leading and trailing whitespace before splitting: with open(&#39;test_01.csv&#39;, &#39;r&#39;) as reader: data = reader.read() lines = data.strip().split(&#39;\\n&#39;) print(lines) [&#39;1901,12.3&#39;, &#39;1902,45.6&#39;, &#39;1903,78.9&#39;] Putting this all together, we can get data for Canada like this: import requests import csv url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv&#39; response = requests.get(url) if response.status_code != 200: print(&#39;Failed to get data:&#39;, response.status_code) else: wrapper = csv.reader(response.text.strip().split(&#39;\\n&#39;)) for record in wrapper: print(record) [&#39;year&#39;, &#39;data&#39;] [&#39;1901&#39;, &#39;-7.67241907119751&#39;] [&#39;1902&#39;, &#39;-7.862711429595947&#39;] [&#39;1903&#39;, &#39;-7.910782814025879&#39;] [&#39;1904&#39;, &#39;-8.155729293823242&#39;] [&#39;1905&#39;, &#39;-7.547311305999756&#39;] ... That looks like progress, so let’s convert the data from strings to the numbers we actually want: import requests import csv url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv&#39; response = requests.get(url) if response.status_code != 200: print(&#39;Failed to get data:&#39;, response.status_code) else: wrapper = csv.reader(response.text.strip().split(&#39;\\n&#39;)) for record in wrapper: year = int(record[0]) value = float(record[1]) print(year, value) Traceback (most recent call last): File &quot;api_with_naive_converting.py&quot;, line 11, in &lt;module&gt; year = int(record[0]) ValueError: invalid literal for int() with base 10: &#39;year&#39; The error occurs because the first line of data is: year,data When we try to convert the string 'year' to an integer, Python quite rightly complains. The fix is straightforward: we just need to ignore lines that start with the word year: import requests import csv url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv&#39; response = requests.get(url) if response.status_code != 200: print(&#39;Failed to get data:&#39;, response.status_code) else: wrapper = csv.reader(response.text.strip().split(&#39;\\n&#39;)) results = [] for record in wrapper: if record[0] != &#39;year&#39;: year = int(record[0]) value = float(record[1]) print(year, value) 1901 -7.67241907119751 1902 -7.862711429595947 1903 -7.910782814025879 1904 -8.155729293823242 1905 -7.547311305999756 ... 4.6 How can I generalize data fetching and handle errors? Now that we know how to get the data for Canada, let’s create a function that will do the same thing for an arbitrary country. The steps are simple: copy the code we’ve written into a function that takes a 3-letter country code as a parameter, insert that country code into the URL at the appropriate place, and return the result as a list instead of printing it. The resulting function looks like: def annual_mean_temp(country): &#39;&#39;&#39;Get the annual mean temperature for a country given its 3-letter ISO code (such as &quot;CAN&quot;).&#39;&#39;&#39; url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/&#39; + country + &#39;.csv&#39; response = requests.get(url) if response.status_code != 200: print(&#39;Failed to get data:&#39;, response.status_code) else: wrapper = csv.reader(response.text.strip().split(&#39;\\n&#39;)) results = [] for record in wrapper: if record[0] != &#39;year&#39;: year = int(record[0]) value = float(record[1]) results.append([year, value]) return results This works: canada = annual_mean_temp(&#39;CAN&#39;) print(&#39;first three entries for Canada:&#39;, canada[:3]) first three entries for Canada: [[1901, -7.67241907119751], [1902, -7.862711429595947], [1903, -7.910782814025879]] However, there’s a problem. Look what happens when we pass in an invalid country identifier: latveria = annual_mean_temp(&#39;LTV&#39;) print(&#39;first three entries for Latveria:&#39;, latveria[:3]) first three entries for Latveria: [] Latveria doesn’t exist, so why is our function returning an empty list rather than printing an error message? The non-appearance of an error message must mean that the response code was 200; if it was anything else, we would have gone into the if branch, printed a message, and returned None (which is what functions do when they’re not told to return anything specific). So if the response code was 200 and there was no data, that would explain what we’re seeing. Let’s check: def annual_mean_temp(country): &#39;&#39;&#39;Get the annual mean temperature for a country given its 3-letter ISO code (such as &quot;CAN&quot;).&#39;&#39;&#39; url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/&#39; + country + &#39;.csv&#39; print(&#39;url used is&#39;, url) response = requests.get(url) print(&#39;response code:&#39;, response.status_code) print(&#39;length of data:&#39;, len(response.text)) if response.status_code != 200: print(&#39;Failed to get data:&#39;, response.status_code) else: wrapper = csv.reader(response.text.strip().split(&#39;\\n&#39;)) results = [] for record in wrapper: if record[0] != &#39;year&#39;: year = int(record[0]) value = float(record[1]) results.append([year, value]) return results latveria = annual_mean_temp(&#39;LTV&#39;) print(&#39;number of records for Latveria:&#39;, len(latveria)) url used is http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/LTV.csv response code: 200 length of data: 0 number of records for Latveria: 0 In other words, the World Bank is always saying, “I was able to answer your query,” even when it actually can’t. After a bit more experimenting, we discover that the site always returns a 200 status code. The only way to tell if there’s real data or not is to check if response.text is empty. Here’s the updated function: def annual_mean_temp(country): &#39;&#39;&#39; Get the annual mean temperature for a country given its 3-letter ISO code (such as &quot;CAN&quot;). Returns an empty list if the country code is invalid. &#39;&#39;&#39; url = &#39;http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/&#39; + country + &#39;.csv&#39; response = requests.get(url) results = [] if len(response.text) &gt; 0: wrapper = csv.reader(response.text.strip().split(&#39;\\n&#39;)) for record in wrapper: if record[0] != &#39;year&#39;: year = int(record[0]) value = float(record[1]) results.append([year, value]) return results and here’s a quick check: print(&#39;number of records for Canada:&#39;, len(annual_mean_temp(&#39;CAN&#39;))) print(&#39;number of records for Latveria:&#39;, len(annual_mean_temp(&#39;LTV&#39;))) number of records for Canada: 109 number of records for Latveria: 0 4.7 How can I compare different data sets? Now that we can get surface temperatures for different countries, we can write a function to compare those values. Here’s our first attempt: def diff_records(left, right): &#39;&#39;&#39;Given lists of [year, value] pairs, return list of [year, difference] pairs.&#39;&#39;&#39; num_years = len(left) results = [] for i in range(num_years): left_year, left_value = left[i] right_year, right_value = right[i] difference = left_value - right_value results.append([left_year, difference]) return results Here, we’re using the number of entries in left (which we find with len(left)) to control our loop. Inside the loop we unpack the left and right years and values from the list entries, then append a pair containing a year and a difference to results, which we return at the end. To see if this function works, we can run a couple of tests on made-up data: print(&#39;one record:&#39;, diff_records([[1900, 1.0]], [[1900, 2.0]])) print(&#39;two records:&#39;, diff_records([[1900, 1.0], [1901, 10.0]], [[1900, 2.0], [1901, 20.0]])) one record: [[1900, -1.0]] two records: [[1900, -1.0], [1901, -10.0]] That looks pretty good, but what about these cases? print(&#39;mis-matched years:&#39;, diff_records([[1900, 1.0]], [[1999, 2.0]])) print(&#39;left is shorter&#39;, diff_records([[1900, 1.0]], [[1900, 10.0], [1901, 20.0]])) print(&#39;right is shorter&#39;, diff_records([[1900, 1.0], [1901, 2.0]], [[1900, 10.0]])) IndexError: list index out of rangemis-matched years: [[1900, -1.0]] left is shorter [[1900, -9.0]] right is shorter The first test gives us an answer even though the years didn’t match: we get a result, but it’s meaningless. The second case gives us a partial result, again without telling us there’s a problem, while the third crashes because we’re using left to determine the number of records, but right doesn’t have that many. The first two problems are actually worse than the third because they are silent failures: the function does the wrong thing, but doesn’t indicate that in any way. Let’s fix that: def diff_records(left, right): &#39;&#39;&#39; Given lists of [year, value] pairs, return list of [year, difference] pairs. Fails if the inputs are not for exactly corresponding years. &#39;&#39;&#39; assert len(left) == len(right), \\ &#39;Inputs have different lengths.&#39; num_years = len(left) results = [] for i in range(num_years): left_year, left_value = left[i] right_year, right_value = right[i] assert left_year == right_year, \\ &#39;Record {0} is for different years: {1} vs {2}&#39;.format(i, left_year, right_year) difference = left_value - right_value results.append([left_year, difference]) return results Do our “good” tests pass? print(&#39;one record:&#39;, diff_records([[1900, 1.0]], [[1900, 2.0]])) print(&#39;two records:&#39;, diff_records([[1900, 1.0], [1901, 10.0]], [[1900, 2.0], [1901, 20.0]])) one record: [[1900, -1.0]] two records: [[1900, -1.0], [1901, -10.0]] What about our the three tests that we now expect to fail? print(&#39;mis-matched years:&#39;, diff_records([[1900, 1.0]], [[1999, 2.0]])) AssertionError: Record 0 is for different years: 1900 vs 1999mis-matched years: print(&#39;left is shorter&#39;, diff_records([[1900, 1.0]], [[1900, 10.0], [1901, 20.0]])) AssertionError: Inputs have different lengths. left is shorter print(&#39;right is shorter&#39;, diff_records([[1900, 1.0], [1901, 2.0]], [[1900, 10.0]])) AssertionError: Inputs have different lengths. right is shorter Excellent: the assertions we’ve added will now alert us if we try to work with badly-formatted or inconsistent data. 4.8 How can I publish data so that others can use it? We now have functions to download temperature data for different countries and find annual differences. The next step is to share our findings with the world by publishing the data sets we generate. To do this, we have to answer three questions: How are we going to store the data? How are people going to download it? How are people going to find it? The first question is the easiest to answer: diff_records returns a list of (year, difference) pairs that we can write out as a CSV file: import csv def save_records(filename, records): &#39;&#39;&#39;Save a list of [year, temp] pairs as CSV.&#39;&#39;&#39; with open(filename, &#39;w&#39;) as raw: writer = csv.writer(raw) writer.writerows(records) We use the csv library to write data for the same reason we use it to read: it correctly handles special cases (such as text containing commas). Let’s test it: save_records(&#39;temp.csv&#39;, [[1, 2], [3, 4]]) If we then look in the file temp.csv, we find: 1,2 3,4 as desired. Now, where should this file go? The answer is clearly “a server”, since data on our laptop is only accessible when we’re online (and probably not even then, since most people don’t run a web server on their laptop). But where on the server, and what should we call it? The answer to those questions depends on how the server is set up. On many multi-user Linux machines, users can create a directory called like public_html under their home directory, and the web server will automatically search in those directories. For example, if Nelle has a file called thesis.pdf in her public_html directory, the web server will find it when it gets the URL http://the.server.name/~nelle/thesis.pdf. (In this case, the tilde ~ in front of Nelle’s name tells the web server to look in Nelle’s public_html directory.) The specifics differ from one machine to the next, but the basic idea stays the same. As for what we should call it, here we return to the key idea in REST: every data set should be identified by a “guessable” URL. In our case we’ll use a name like left_right.csv, where left and right are the three-letter codes of the countries whose temperatures we are differencing. We can then tell people that if they want to compare Australia and Brazil, they should look for http://the.server.name/~nelle/AUS_BRA.csv. (We use upper case to be consistent with the World Bank’s API.) But what’s to prevent someone from creating a badly-named (and therefore unfindable) file? Someone could, for example, call save_records('aus+bra.csv', records). To reduce the odds of this happening, let’s modify save_records to take country identifiers as parameters: import csv def save_records(left, right, records): &#39;&#39;&#39;Save a list of [year, temp] pairs as CSV.&#39;&#39;&#39; filename = left + &#39;_&#39; + right + &#39;.csv&#39; with open(filename, &#39;w&#39;) as raw: writer = csv.writer(raw) writer.writerows(records) We can now call it like this: save_records(&#39;AUS&#39;, &#39;BRA&#39;, [[1, 2], [3, 4]]) and then check that the right output file has been created. We are bound to have the country codes anyway (having used them to look up our data), so this should seem natural to our users. 4.9 How can I make it easy for other people to find my data? It’s not enough to tell people what the rule is for creating filenames, since that doesn’t tell them what data sets we’ve actually generated. The final step in this lesson is therefore to make the data we generate findable by creating an index to tell people what files exist. Here’s the format we will use: 2014-05-26,FRA,TCD,FRA_TCD.csv 2014-05-27,AUS,BRA,AUS_BRA.csv 2014-05-27,AUS,CAN,AUS_CAN.csv 2014-05-28,BRA,CAN,BRA_CAN.csv The columns are the date the data set was generated, the identifiers of the two countries being compared, and the name of the data file. We include the date to make it easy for people to see what’s been updated when, but why do we bother to include the filename? After all, we can re-generate it easily given the two country codes. The answer is that while we know the rule for generating filenames, other people’s programs shouldn’t have to. Here’s a function that updates the index file every time we generate a new data file: import time def update_index(index_filename, left, right): &#39;&#39;&#39;Append a record to the index.&#39;&#39;&#39; # Read existing data. with open(index_filename, &#39;r&#39;) as raw: reader = csv.reader(raw) records = [] for r in reader: records.append(r) # Create new record. timestamp = time.strftime(&#39;%Y-%m-%d&#39;) data_filename = left + &#39;_&#39; + right + &#39;.csv&#39; new_record = (timestamp, left, right, data_filename) # Save. records.append(new_record) with open(index_filename, &#39;w&#39;) as raw: writer = csv.writer(raw) writer.writerows(records) Let’s test it. If our index file contains: 2014-05-26,FRA,TCD,FRA_TCD.csv 2014-05-27,AUS,BRA,AUS_BRA.csv 2014-05-27,AUS,CAN,AUS_CAN.csv 2014-05-28,BRA,CAN,BRA_CAN.csv and we run: update_index(&#39;data/index.csv&#39;, &#39;TCD&#39;, &#39;CAN&#39;) then our index file now contains: 2014-05-26,FRA,TCD,FRA_TCD.csv 2014-05-27,AUS,BRA,AUS_BRA.csv 2014-05-27,AUS,CAN,AUS_CAN.csv 2014-05-28,BRA,CAN,BRA_CAN.csv 2014-05-29,TCD,CAN,TCD_CAN.csv Now that all of this is in place, it’s easy for us—and more importantly, for other people—to do new and exciting things with our data. For example, we can easily write a small program that tells us what data sets include information about a particular country and have been published since we last checked: def what_is_available(index_file, country, after): &#39;&#39;&#39;What data files include a country and have been published since &#39;after&#39;?&#39;&#39;&#39; with open(index_file, &#39;r&#39;) as raw: reader = csv.reader(raw) filenames = [] for record in reader: if (after &lt;= record[0]) and (country in (record[1], record[2])): filenames.append(record[3]) return filenames print(what_is_available(&#39;data/index.csv&#39;, &#39;BRA&#39;, &#39;2014-05-27&#39;)) [&#39;AUS_BRA.csv&#39;, &#39;BRA_CAN.csv&#39;] This may not seem like a breakthrough, but it is actually an example of how the web helps researchers do new kinds of science. With a little bit more work, we could create a file on our machine to record when we last ran what_is_available for each of several different sites that are producing data. Each time we run it, our program would: read our local “what to check” file; ask each data source whether it had any new data for us; download and process that data; and present us with a summary of the results. This is exactly how blogs work. Every blog reader keeps a list of blog URLs that it’s supposed to check. When it is run, it goes to each of those sites and asks them for their index file (which is typically called something like feed.xml). It then checks the articles listed in that index against its local record of what has already been seen, then downloads any articles that are new. By automating this process, blogging tools help us focus attention on things that are actually worth looking at. 4.10 Summary Figure 4.1: Syndication Concept Map 4.11 Exercises 4.11.1 Enumerating Python includes a function called enumerate that’s often used in for loops. Look at its documentation, then rewrite diff_records to use it. 4.11.2 When to complain? We committed the same mistake as the World Bank in our differencing code: if someone gives annual_mean_temp an invalid country identifier, it doesn’t report an error, but instead returns an empty list, so the caller has to somehow know to look for that. Should it use an assertion to fail if it doesn’t get data? Why or why not? 4.11.3 Deciding what to check Should save_records check that every record in its input has exactly two fields? Why or why not? What about country codes: should it contain a list of those that match actual countries and check that left and right are in that list? 4.11.4 Metadata for metadata Should the first line of the index file be a header giving the names of the columns? Why or why not? 4.11.5 To automate or not Should update_index be called inside save_records so that the index is automatically updated every time a new data set is generated? Why or why not? 4.11.6 Removing redundant redundancy update_index and save_records both construct the name of the data file. Refactor them to remove this redundancy. 4.12 Key Points \"FIXME \"Make data sets more useful by providing metadata. \"Write Python programs to download data sets using simple REST APIs. \"Parse CSV data using the csv library. \"Test a program that parses CSV using multiline strings. \"Make a function more robust by explicitly handling errors. \"Write Python programs that share static data sets. "],
["configure.html", "Chapter 5 Configuring Software 5.1 Questions 5.2 Objectives 5.3 Introduction 5.4 How can I handle command-line flags consistently? 5.5 What do I do when I run out of memorable single-letter flags? 5.6 How can I manage configuration files consistently? 5.7 How can I implement overlay configuration? 5.8 How can I find configuration files? 5.9 How can I keep a record of the actual configuration that produced particular results? 5.10 Summary 5.11 Exercises 5.12 Key Points", " Chapter 5 Configuring Software 5.1 Questions How can I make it easy for users to configure software? 5.2 Objectives Describe the four levels of configuration typically used by robust software. Explain what an overlay configuration is. Explain why nested configuration options are usually not a good idea. Add flat overlay configuration to a small application. 5.3 Introduction A program that does exactly the same thing every time we run it isn’t as useful as one that can work on different files or analyze data with different thresholds. Software of all kinds needs to be controlled; some things change more often than others, so we need a simple, uniform way to specify some options and leave others alone. The modern Unix convention is to provide four levels of configuration: A system-wide configuration file for general settings. A user-specific configuration file for personal preferences. A job-specific file with settings for a specific run. Command-line options to change things that commonly change. This is sometimes called overlay configuration because each level overrides the ones above it: the user’s configuration file overrides the system settings, the job configuration overrides the user’s defaults, and the command-line options overrides that. 5.4 How can I handle command-line flags consistently? Modern Python programs use the argparse library for handling command-line arguments, but the older and simpler getopt library will illustrate the core ideas, so we will use it. getopt works by matching a specification of what flags are allowed against a list of actual command-line arguments. In simplest form, the spec is string listing all the single-letter flags, with colons showing the ones that take arguments. For example, the string b:q means “the -b flag takes an argument and the -q flag doesn’t”. The list of actual arguments is almost always sys.argv[1:], i.e., all of the command-line arguments except for the name of the program itself. (The name argv stands for “argument vector”, and is a holdover from the days of C.) Given the spec and the list of actual arguments, getopt returns two lists: the first is the (flag, argument) pairs it matched, and the second is everything else—typically a list of files to be processed. from getopt import getopt args = [&#39;-q&#39;, &#39;-b&#39;, &#39;/tmp/log.txt&#39;, &#39;file1.txt&#39;, &#39;file2.txt&#39;] options, extras = getopt(args, &#39;b:q&#39;) print(&#39;options is&#39;, options) print(&#39;extras is&#39;, extras) options is [(&#39;-q&#39;, &#39;&#39;), (&#39;-b&#39;, &#39;/tmp/log.txt&#39;)] extras is [&#39;file1.txt&#39;, &#39;file2.txt&#39;] Once we have these two lists, we can define default values for our configuration and then loop over the (flag, argument) pairs to override those values: # Defaults. logfile = None quiet = False # Override based on command-line options. for (opt, arg) in options: if opt == &#39;-b&#39;: logfile = arg elif opt == &#39;-q&#39;: quiet = True else: assert False, &#39;unrecognized option {}&#39;.format(opt) print(&#39;Log file is {} and quiet is {}&#39;.format(logfile, quiet)) Log file is /tmp/log.txt and quiet is True This pattern is called set and override, and makes programs easier to understand by putting all of the default settings in one place. Since we will often want to pass those settings into functions that do the actual work, it’s very common to put the entire configuration in a dictionary so that we have a single configuration object to pass around. It’s also common to check that some configuration values aren’t accidentally being set twice. After rearranging our code a little, we get this: import sys from getopt import getopt # Defaults. settings = { &#39;logfile&#39; : None, &#39;quiet&#39; : False } # Override based on command-line options. options, extras = getopt(sys.argv[1:], &#39;b:q&#39;) for (opt, arg) in options: if opt == &#39;-b&#39;: assert settings[&#39;logfile&#39;] is None, &#39;cannot set logfile twice&#39; settings[&#39;logfile&#39;] = arg elif opt == &#39;-q&#39;: settings[&#39;quiet&#39;] = True else: assert False, &#39;unrecognized option {}&#39;.format(opt) # Display. print(&#39;Log file {}&#39;.format(settings[&#39;logfile&#39;])) print(&#39;Quiet {}&#39;.format(settings[&#39;quiet&#39;])) print(&#39;Extras {}&#39;.format(extras)) which we can run like this: $ python getopt_dict.py -q first.txt Log file None Quiet True Extras [&#39;first.txt&#39;] We really shouldn’t use assert to handle errors here; Chapter 6 will explore a better approach. 5.5 What do I do when I run out of memorable single-letter flags? Taschuk’s Third Rule says, “Make common operations easy to control.” Taschuk and Wilson (2017) so that users can control everything from a shell script without having to create temporary configuration files. However, there are only so many single-letter flags available, and using -b to specify the name of a log file is hardly intuitive. To allow for this, getopt handles another kind of flag: a double dash followed by a longer name like --erase-temp-files. These flags can take a single argument like their single-letter siblings. To tell getopt what long names it should recognize, we give the function an extra list and use = as a suffix to indicate if the option takes an argument. from getopt import getopt args = [&#39;-q&#39;, &#39;--logfile&#39;, &#39;/tmp/log.txt&#39;, &#39;--overwrite&#39;, &#39;file1.txt&#39;, &#39;file2.txt&#39;] options, extras = getopt(args, &#39;b:q&#39;, [&#39;logfile=&#39;, &#39;overwrite&#39;]) print(&#39;options is&#39;, options) print(&#39;extras is&#39;, extras) options is [(&#39;-q&#39;, &#39;&#39;), (&#39;--logfile&#39;, &#39;/tmp/log.txt&#39;), (&#39;--overwrite&#39;, &#39;&#39;)] extras is [&#39;file1.txt&#39;, &#39;file2.txt&#39;] It’s common to use single-letter flags for the most frequently changed options and long names for things that are changed less frequently, and to provide long-name aliases for the single-letter flags (e.g., to have --all-files mean the same thing as -a). 5.6 How can I manage configuration files consistently? Controlling programs from the command line is useful, but complex programs can have many different configuration options, and it’s very useful to be able to save settings in a file for later reference (and reproducibility). Enabling a program to read its configuration from a file also allows users to set values once and then not worry about them, which is particularly useful when they’re installing the software on their own computer and want to put temporary files in a different location or change the value of the alpha parameter for fitting curves. Programmers have invented many formats for configuration files, so please do not create your own. One possibility is to write the configuration as a Python data structure and then load it as if it was a library. This is clever, but it’s hard for tools in other languages to process. Programers are also fond of JSON, which is a subset of the syntax that JavaScript uses for data structures, but that involves a lot of curly braces. A third option is the Windows INI format, which is laid out like this: [section_1] key_1=value_1 key_2=value_2 [section_2] key_3=value_3 key_4=value_4 INI files are simple to read and write, but the format is slowly falling out of use. What seems to be replacing it is YAML, which stands for “Yet Another Markup Language”. Since YAML is used in GitHub Pages, and (unlike JSON) allows comments, we’ll explore it in this section. Here’s a sample configuration file: # Example configuration file logfile: &quot;/tmp/log.txt&quot; quiet: false overwrite: false fonts: - Verdana - Serif And here’s a short Python program that reads and prints that configuration: import yaml with open(&#39;config.yml&#39;, &#39;r&#39;) as reader: config = yaml.load(reader) print(config) {&#39;logfile&#39;: &#39;/tmp/log.txt&#39;, &#39;quiet&#39;: False, &#39;overwrite&#39;: False, &#39;fonts&#39;: [&#39;Verdana&#39;, &#39;Serif&#39;]} Simple YAML files are simple to write: Lines starting with # are comments. A line key: value defines a value for the given key. Values can be numbers, true or false, or quoted strings. (Strings don’t actually have to be quoted in every case, but the file is a lot easier to understand if you always use quotes.) A point-form list underneath a key becomes an array of values. When a file like this is read in Python, the result is a dictionary. YAML allows nested keys and lists, but if you need them, you’re probably doing something wrong Xu et al. (2015): most users never use most configuration options and find their presence confusing. 5.7 How can I implement overlay configuration? We said at the start that programs often have system-wide, per-user, and per-job configuration files, with each overriding values from the one(s) before and command-line parameters overriding the rest. We can implement this using dict.update, which updates one dictionary with values from another: def get_full_configuration(filenames, command_line={}): &#39;&#39;&#39; Overlay configuration files and command-line parameters, returning configuration object. &#39;&#39;&#39; result = {} for f in filenames: with open(f, &#39;r&#39;) as reader: config = yaml.load(reader) result.update(config) result.update(command_line) return result This function creates an empty dictionary to hold settings. It then reads each specified configuration file in turn and updates the result dictionary with whatever it found in that file. If a file defines values that were previously defined in an earlier file, the update method call automatically overwrites the older values. We end by overriding what we read from the files with whatever was given on the command line; we’ll have to convert getopt’s output to a dictionary, but that’s straightforward—the only trick is to match the command-line flag (like -q) to the configuration variable name (like quiet): def getopt_to_dict(pairs, names): &#39;&#39;&#39; Convert [(flag, value)...] pairs and {flag: config...} names to dictionary. &#39;&#39;&#39; result = {} for (key, value) in pairs: result[names[key]] = value return result We can test this with these three configuration files (we have lined up corresponding values to make them easier to see): system.yml user.yml job.yml quiet: true quiet: false logfile: “/tmp/log.txt” logfile: “./complaints.txt” using this test program: import sys from getopt import getopt from util import get_full_configuration, getopt_to_dict config_files = [&#39;system.yml&#39;, &#39;user.yml&#39;, &#39;job.yml&#39;] options, extras = getopt(&#39;b:q&#39;, sys.argv[1:]) options = getopt_to_dict(options, {&#39;-b&#39;: &#39;logfile&#39;, &#39;-q&#39;: &#39;quiet&#39;}) config = get_full_configuration(config_files, options) print(config) and this command line: $ python test-config.py -q {&#39;quiet&#39;: False, &#39;logfile&#39;: &#39;./complaints.txt&#39;} 5.8 How can I find configuration files? Our configuration files will usually not all be in the same directory. System-wide settings for an application called app are often stored in /etc/app.yml. Alternatively, some programs will set an environment variable APP to the name of the installation directory, and then read the system configuration file from $APP/app.yml. We can use os.getenv('APP') to get the value of the environment variable APP, then append app.yml and load that. (Older programs often use the name app.rc, where “rc” stands for “resource control”.) Similarly, we can get personal settings from $HOME/.app.yml; the leading ‘.’ hides the configuration file from ls. Finally, per-job settings can come from app.yml in the current directory, where again “app” is replaced with the name of the program. Here’s a utility routine that constructs and checks these filenames: def find_config_files(name): &#39;&#39;&#39; Construct a list of configuration files for the named application. &#39;&#39;&#39; app_yml = name + &#39;.yml&#39; locations = [(name.upper(), app_yml), (&#39;HOME&#39;, &#39;.&#39; + app_yml), (&#39;PWD&#39;, app_yml)] result = [] for (var, filename) in locations: value = os.getenv(var) if value: path = os.path.join(value, filename) if os.path.isfile(path): result.append(path) return result Note that we use os.path.isfile to check that file exists before trying to read it. 5.9 How can I keep a record of the actual configuration that produced particular results? Careful record keeping is essential to reproducible science, and if we are careful, the computer can do the record keeping. We can save the entire (merged) configuration object for a particular run of a program using yaml.dump. If we have written our configuration functions correctly, this will let us re-create configuration on another machine even if it has different default settings. The test is whether our program can load a dumped configuration, then dump it again and get the same result. If we’re going to do this, we should always include a version number as a field in the dumped configuration; our program should also print this out when given a --version flag. We need this because how we interpret options will change over time, and if you don’t know what the version of the program was, we’ll have to guess what options mean. 5.10 Summary Many tool also allow longer command-line options to control everything to support scripting Out of scope of this lesson Every tool acts as if it was the only extra thing your project needed So you wind up with lots of configuration files littering your root directory Tempting to move them all to ./etc/, but tools don’t know to look there Figure 5.1: Configuration Concept Map 5.11 Exercises 5.11.1 Dump and check configuration FIXME: how to handle lack of order in keys? (answer: use deep equality) 5.11.2 Read configuration solely from file FIXME: implement flag to read entire configuration from file and nothing else. 5.11.3 Implement a usage function FIXME: implement usage function to display message and exit instead of using assert. 5.11.4 Read standard input if no files specified FIXME: how to handle reading from stdin/stdout if no files specified. 5.12 Key Points Use short command-line parameters to set commonly-changed options. Allow long command-line parameters to change all other options to facilitate scripting. Use the getopt library to parse command-line flags. Read a system-wide configuration file, then a user configuration file, then a job configuration file. Format configuration files using YAML. Dump all configuration values in the same format used for input on request. Include the software version number in the dumped configuration. References "],
["logging.html", "Chapter 6 Logging 6.1 Questions 6.2 Objectives 6.3 Introduction 6.4 What kinds of errors should I try to handle and what should I do? 6.5 How can I detect errors? 6.6 What exceptions should I raise and catch? 6.7 How can my program report its activity consistently? 6.8 How can I control where log messages go and which log messages appear? 6.9 How can I change the format of my log messages? 6.10 How can I handle multiple reporting sources in one program? 6.11 How should I log my program’s configuration? 6.12 Summary 6.13 Exercises 6.14 Key Points", " Chapter 6 Logging 6.1 Questions How should I keep track of what I actually did? How can I get my software to report errors? 6.2 Objectives Explain the advantages of using a logging library rather than print statements in data science pipelines. Describe the intent of the five standard logging levels. Create and configure a simple logger. Define a custom format for log messages. Define a source name to use in consolidated logs. 6.3 Introduction Programs should report things that go wrong. They should also sometimes report things that go right so that people can monitor their progress and down the sources of errors. Adding print statements for debugging is a common approach, but removing them or commenting them out, only to add them again, is tedious—especially when the software is in production. A better approach is to use a logging framework, such as Python’s logging library. This lets you leave your debugging statements in your code and turn them on or off at will. It also lets you send output to any of several destinations, which is helpful when your data analysis pipeline has several stages and you’re trying to figure out which one contains a bug (or whether the problem lies in their interaction). We will start by looking at what kinds of errors you can handle and how you can detect them, then go on to look at how logging can make your work more consistent. 6.4 What kinds of errors should I try to handle and what should I do? To start, we need to distinguish between internal errors, such as calling a function with None instead of a list. and external errors, such as trying to read a file that doesn’t exist. Internal errors should be forestalled by unit testing, but software always encounters new situations in the real world, and those situations can trigger previously unseen bugs. Every time one crops up, we should fix the underlying problem and then write a test to make sure it doesn’t creep back into our code. External errors, on the other hand, are usually caused by interactions between the program and the outside world: for example, a user may mis-type a filename or the network might be down. Section 7.8 described some ways to write unit tests to check that software is doing the right thing in these situations, but we still need to detect them—we’ll look at how in the next section. When an internal error occurs, the only thing we can do in most cases is report it and halt the program. If a function has been passed None instead of a valid list, for example, the odds are good that one of our data structures is corrupted (or at least in an unexpected state). We can try to guess what the problem is and take corrective action, but experience teaches us that our guess will often be wrong and our attempt to correct the problem might actually make things worse. Some external errors fall into the same category. For example, if our program is trying to fetch data from a web service but is unable to connect to it, the most sensible thing is probably to report the connection failure. If we’re interacting with a user, though, it might make sense for us to prompt them to try again: it’s easy to mis-type a password or select the wrong file for analysis, and users shouldn’t have to re-start the program to correct something like this. The one rule we should always follow is to check for errors as early as possible so that we don’t waste the user’s time. Few things are as frustrating as being told at the end of an hour-long calculation that the program doesn’t have permission to write to an output directory. It’s a little extra work to check things like this up front, but the larger your program or the longer it runs, the more useful those checks will be. 6.5 How can I detect errors? Most modern programming languages use exceptions for error handling. As the name suggests, an exception is a way to represent an exceptional or unusual occurrence that doesn’t fit neatly into the program’s expected operation. The code below shows a very simple example that reports attempts to divide by zero: for denom in [-5, 0, 5]: try: result = 1/denom print(&#39;1/{} == {}&#39;.format(denom, result)) except: print(&#39;{} has no reciprocal&#39;.format(denom)) 1/-5 == -0.2 0 has no reciprocal 1/5 == 0.2 try/except looks like if/else and works in a similar fashion. If nothing unexpected happens inside the try block, the except block isn’t run. If something does happen inside the try, the program jumps immediately to the except. This is why the print statement inside the try doesn’t run when denom is 0: as soon as Python tries to calculate 1/denom, it skips directly to the error message. FIXME: diagram We often want to know what exactly went wrong, so Python and other languages store information about the error in an object (also called an exception). We can catch this object and inspect it as follows: for denom in [-5, 0, 5]: try: result = 1/denom print(&#39;1/{} == {}&#39;.format(denom, result)) except Exception as error: print(&#39;{} has no reciprocal: {}&#39;.format(denom, error)) 1/-5 == -0.2 0 has no reciprocal: division by zero 1/5 == 0.2 We can use any variable name we like instead of error; Python will assign the exception object to that variable so that we can do things with it in the except block. Python also allows us to specify what kind of exception we want to catch. For example, we can write code to handle out-of-range indexing and division by zero separately: numbers = [-5, 0, 5] for i in [0, 1, 2, 3]: try: denom = numbers[i] result = 1/denom print(&#39;1/{} == {}&#39;.format(denom, result)) except IndexError as error: print(&#39;index {} out of range&#39;.format(i)) except ZeroDivisionError as error: print(&#39;{} has no reciprocal: {}&#39;.format(denom, error)) 1/-5 == -0.2 0 has no reciprocal: division by zero 1/5 == 0.2 index 3 out of range Exceptions are organized in a hierarchy: for example, FloatingPointError, OverflowError, and ZeroDivisionError are all special cases of ArithmeticError, so an except that catches the latter will catch all three of the former, but an except that catches an OverflowError won’t catch a ZeroDivisionError. The Python documentation describes all ofthe built-in exception types; in practice, the ones that people handle most often are: ArithmeticError: something has gone wrong in a calculation. IndexError and KeyError: something has gone wrong indexing a list or lookup something up in a dictionary. OSError: covers cases like a file not being found, the program not having permission to read or write it, and so on. So where do exceptions come from? The answer is that programmers can raise them explicitly: for number in [1, 0, -1]: try: if number &lt; 0: raise ValueError(&#39;negative values not supported: {}&#39;.format(number)) print(number) except ValueError as error: print(&#39;exception: {}&#39;.format(error)) 1 0 exception: negative values not supported: -1 We can define our own exception types, and many libraries do, but the built-in types are enough to cover common cases. One final note is that exceptions don’t have to be handled where they are raised—in fact, their greatest strength is that they allow long-range error handling. If an exception occurs inside a function and there is no except for it there, Python looks in whoever called that function. It keeps working its way through the call stack until it finds a matching except; if there isn’t one, it takes care of the exception itself. def sum_reciprocals(values): result = 0 for v in values: result += 1/v return result numbers = [-1, 0, 1] try: one_over = sum_reciprocals(numbers) except ArithmeticError as error: print(&#39;Error trying to sum reciprocals: {}&#39;.format(error)) Error trying to sum reciprocals: division by zero This leasd to the rule “throw low, catch high”: write most of your code without exception handlers, since there’s nothing useful you can do in the middle of a small utility function, but put a few handlers in the main body of your program to catch and report all errors. 6.6 What exceptions should I raise and catch? Broadly speaking there are two approaches to error handling: “look before your leap” and “it’s easier to ask for forgiveness than permission”. When using the first, programmers check if an operation will be allowed before trying it: LOG_DIR = &quot;/tmp/mylogdir&quot; def write_message(filename, message): if os.path.isdir(LOG_DIR): path = os.path.join(LOG_DIR, filename) with open(path, &#39;a&#39;) as writer: writer.write(message) else: print(&#39;No log directory {}&#39;.format(LOG_DIR), file=sys.stderr) with the second, they try the operation and catch the exception: LOG_DIR = &quot;/tmp/mylogdir&quot; def write_message(filename, message): try: path = os.path.join(LOG_DIR, filename) with open(path, &#39;a&#39;) as writer: writer.write(message) except OSError as error: print(&#39;Unable to write log message to {}: {}&#39;.format(path, error)) The first approach doesn’t actually need exception handlers, except it does: programmers can never anticipate all of the things that can go wrong in the real world, so there should always be an except somewhere. Since that’s the case, most programmers prefer the second approach. When you are raising exceptions yourself, please make the information as specific as possible. For example, suppose a program contains this: if (val is not None) and (min_val &lt; val &lt; max_val) and (val % 2 == 0) and (val &gt;= prev_val): # OK branch else: raise ValueError(&#39;Bad value: {}&#39;.format(val)) Knowing what value caused the error is helpful, but the message in the exception doesn’t tell us why it was bad. Was it not between min_val and max_val? Was it less than prev_val? Whoever has to debug the problem will want to know, but breaking the test up into cases makes the code harder to read: if val is None: raise ValueError(&#39;Value {} is None&#39;.format(val)) elif (val &lt;= min_val) or (max_val &lt;= val): raise ValueError(&#39;Value {} is out of range ({}, {})&#39;.format(val, min_val, max_val)) elif val % 2 != 0: raise ValueError(&#39;Value {} is odd&#39;.format(val)) elif val &lt; prev_val: raise ValueError(&#39;Value {} is not less than previous value {}&#39;.format(val, prev_val)) else: # OK branch 6.7 How can my program report its activity consistently? Suppose you wanted to turn print statements in your program on or off depending on how much detail you wanted to see. You might wind up writing code like this: DEBUG_LEVEL = 2 # or some other number def complex_function(): # ...lots of complicated code... if (DEBUG_LEVEL &gt; 0): print(&#39;Everything seems to be going well&#39;) # ...more complicated code... if (DEBUG_LEVEL &gt; 1): print(&#39;Current alpha value {}&#39;.format(alpha)) The value of DEBUG_LEVEL acts as a threshold: any debugging output at a lower level isn’t printed. Logging frameworks combine the if and the print statements, and define standard names for the levels. In order of increasing severity, the levels are: DEBUG: very detailed information used for localizing errors. INFO: confirmation that things are working as expected. WARNING: something unexpected happened, but the program will keep going. ERROR: something has gone badly wrong, but the program hasn’t hurt anything. CRITICAL: potential loss of data, security breach, etc. Each of these has a corresponding function: we can use logging.debug, logging.info, etc. to write messages at these levels. By default, only WARNING and above are displayed; messages appear on standard error so that the flow of data in pipes isn’t affected. The source of the message appears as well: by default, the source is called “root” for reasons that will become clear later. Thus, if we run the small program shown below, only the warning message appears: import logging logging.warning(&#39;This is a warning.&#39;) logging.info(&#39;This is just for information.&#39;) WARNING:root:This is a warning. 6.8 How can I control where log messages go and which log messages appear? We can configure logging to send messages to a file instead of to standard error using logging.basicConfig. This has to be done before we make any logging calls: it’s not retroactive. We can use the same function to set the logging level: everything at or above the specified level is displayed. import logging logging.basicConfig(level=logging.DEBUG, filename=&#39;everything.log&#39;) logging.debug(&#39;This is for debugging.&#39;) logging.info(&#39;This is just for information.&#39;) logging.warning(&#39;This is a warning.&#39;) logging.error(&#39;Something went wrong.&#39;) logging.critical(&#39;Something went seriously wrong.&#39;) DEBUG:root:This is for debugging. INFO:root:This is just for information. WARNING:root:This is a warning. ERROR:root:Something went wrong. CRITICAL:root:Something went seriously wrong. By default, basicConfig re-opens the log file in append mode; we can use filemode='w' to overwrite the existing log data. This is useful during debugging, but we should think twice before doing in production, since the information we throw away always turns out to have been exactly what we needed to find a bug. Many programs allow users to specify logging levels and log file names as command-line parameters. At its simplest, this is a single flag -v or --verbose that changes the logging level from WARNING (the default) to DEBUG (the noisiest level). There may also be a corresponding flag -q or --quiet that changes the level to ERROR, and a flag -l or --logfile that specifies a log file name (with standard error being the default for message output). More sophisticated programs may allow –level name, such as --level INFO. If you don’t mind asking users to type in ALL CAPS, you can have them give a logging level name on the command line and pass directly to logging.basicConfig, since strings can be used as level names: basicConfig(level=&quot;DEBUG&quot;) # the same as basicConfig(level=logging.DEBUG) The flag names -v, --quiet, and so on are just conventions, but they are widely used. tail -f A handy trick during development is to configure logging to send messages to a file, and then open another terminal window and run tail -f filename to display changes to that file as information is appended. Doing this gives you a record of the log output while allowing you to monitor your program’s progress interactively. 6.9 How can I change the format of my log messages? By default, logging produces messages with the name of the level (such as WARNING), the name of the logger (we have only seen root so far) and the message. We can put whatever we want in the message, but logging also allows us to specify a format string using the conventions of an older form of string formatting. The general form is %(NAME)s to insert a named value as a string; some of the names supported are: asctime to get the current time, levelname to get the logging level, and message to get the message. As the example below shows, we can also change the format used for displaying dates. We should always use ISO date format: it is unambiguous, easy for other programs to parse, easy to sort: logging.basicConfig(format=&#39;%(asctime)s,%(levelname)s,&quot;%(message)s&quot;&#39;, datefmt=&#39;%Y-%m-%d:%H:%M:%S&#39;) logging.warning(&#39;This is a warning&#39;) 2019-01-05:06:16:58,WARNING,&quot;This is a warning&quot; Our output looks like readable CSV data: the date, a comma, the level, another comma, and then the log message in quotes. This is deliberate: CSV is the lowest common denominator of data formats, so logging in that format means that we don’t have to write a bunch of regular expressions later to pull records out of our log. Remember if you need to write a parser, you’ve done something wrong (Chapter G). 6.10 How can I handle multiple reporting sources in one program? Suppose we have a data analysis pipeline with two stages: one that gets words from files, and another that counts words. If we want to log the actions of both, how will we tell their log messages apart? We could have each stage write to its own log file. but then it would be hard to figure out which activities in stage 1 corresponded to activities in stage 2. If we have them write messages to the same log file, on the other hand, how will we know which messages came from which stage? The answer is that we can configure logging to write to the same output with different source names. By default, the source name is root (which we have been seeing so far), but we can define our own, and set different levels for different loggers. Unfortunately, changing the name of the logging source is the one thing we can’t easily do with basicConfig. Instead, we have to: Ask logging to create a logger, create a handler to send messages to a file, create a formatter to format those messages, and then stitch them together so that the logger knows about the handler and the handler knows about the formatter. Time to write a function: import logging MESSAGE_FORMAT = &#39;%(asctime)s,%(name)s,%(levelname)s,%(message)s&#39; DATE_FORMAT = &#39;%Y-%m-%dT%H:%M:%S&#39; def create_logger(name, level, filename): # Create logger. logger = logging.getLogger(name) logger.setLevel(level) # Send messages to standard output. handler = logging.FileHandler(filename) # Define format. formatter = logging.Formatter(MESSAGE_FORMAT, DATE_FORMAT) # Stitch everything together. handler.setFormatter(formatter) logger.addHandler(handler) return logger To show how it works, here is a function that pretends to get words from a file, sending the words it finds to standard output and writing log messages at WARNING level to log.csv (so the DEBUG-level message shouldn’t appear): from util import create_logger logger = create_logger(&#39;get_words&#39;, &#39;WARNING&#39;, &#39;log.csv&#39;) for word in [&#39;first&#39;, &#39;second&#39;, &#39;third&#39;]: print(word) message = &#39;getting word &quot;{}&quot;&#39;.format(word) logger.debug(message) logger.warn(message) logger.critical(message) And here’s another function that counts words it reads from standard input, writing log messages to the same file at ERROR level (so the DEBUG and WARNING messages shouldn’t appear): import sys from util import create_logger logger = create_logger(&#39;count_words&#39;, &#39;ERROR&#39;, &#39;log.csv&#39;) count = 0 for line in sys.stdin: count += 1 message = &#39;counter {} for {}&#39;.format(count, line.strip()) logger.debug(message) logger.warn(message) logger.error(message) print(count) When we put these to programs in a pipeline, all we see is the count: $ python get_words.py | python count_words.py 3 but when we look in log.csv, we see the messages from both programs: 2019-01-05T06:43:04,get_words,WARNING,getting word &quot;first&quot; 2019-01-05T06:43:04,get_words,CRITICAL,getting word &quot;first&quot; 2019-01-05T06:43:04,get_words,WARNING,getting word &quot;second&quot; 2019-01-05T06:43:04,get_words,CRITICAL,getting word &quot;second&quot; 2019-01-05T06:43:04,get_words,WARNING,getting word &quot;third&quot; 2019-01-05T06:43:04,get_words,CRITICAL,getting word &quot;third&quot; 2019-01-05T06:43:04,count_words,ERROR,counter 1 for first 2019-01-05T06:43:04,count_words,ERROR,counter 2 for second 2019-01-05T06:43:04,count_words,ERROR,counter 3 for third One of the things this shows, by the way, is that Unix doesn’t necessarily pass text from program to program one line at a time. Instead, it usually buffers input and output. Libraries like logging can send messages to many destinations: rotating files so that the system always has messages from the last few hours but doesn’t fill up the disk, for example, or a centralized logging server of some kind that collates logs from many different systems. You probably don’t need any of these, but the data engineers and system administrators who eventually have to install and maintain your programs will be very grateful that you used logging, because then they can set it up the way they want with very little work. 6.11 How should I log my program’s configuration? One of the most important things a program can record is its configuration—its entire configuration, so that its operation can be reproduced exactly later on (Chapter 5). Unfortunately, the configuration for even a moderately complex program consists of many values, and logging frameworks are designed to log one thing at a time. This leaves us with some hard choices: Write the configuration to a separate file. If the program can read configuration files, then the configuration written by an old run can be read in directly for a new run. However, we now have to look in two places to find out what happened in a particular run. Write the configuration to the log, one value at a time. Some programs write name/value pairs, one per setting, at INFO level. This puts all the information in one place (the log file), but someone now has to write software to get configuration values out of the log so that they can be re-used. Write the configuration to the log as one big string. If the configuration is stored in a configuration object, that object can be converted to a JSON string and written to the log as one looooong entry. This puts all of the information in one place, and it’s relatively easy to recover, but the log entry isn’t particularly readable. Use a structured log instead of a line-oriented log. Most log files are written line-by-line because lines of text are the universal language of Unix pipelines. Some people are now experimenting with logs that use more structured storage formats, and if everything in the log is JSON anyway, writing the configuration as a blob of JSON is straightforward. On balance, we recommend the third option: convert the configuration to a JSON structure and write that as a single log entry. 6.12 Summary FIXME: create concept map for logging 6.13 Exercises 6.13.1 Set the logging level FIXME: use getopt and --level NAME to set logging level. 6.13.2 Display logging configuration FIXME: use logging_tree.printout() to display logging configuration. 6.13.3 Log to standard error as well as a file FIXME: modify create_logger to log to standard error as well as to a file. 6.13.4 Write configuration as JSON FIXME: write configuration as JSON. 6.14 Key Points Use logging instead of print to report program activity. Separate messages into DEBUG, INFO, WARNING, ERROR, and CRITICAL levels. Use logging.basicConfig to define basic logging parameters. Always provide timestamps using YYYY-MM-DDTTHH:MM:SS format. Use standard input and standard output for normal input and output, and send log messages to a file. Use tail -f to monitor log files. "],
["unit.html", "Chapter 7 Unit Testing 7.1 Questions 7.2 Objectives 7.3 Introduction 7.4 What are realistic goals for testing? 7.5 What does a systematic software testing framework look like? 7.6 How can I manage tests systematically? 7.7 How can I tell if my software failed as it was supposed to? 7.8 How can I test software that includes randomness? 7.9 How can I test software that does I/O? 7.10 How can I tell which parts of my software have (not) been tested? 7.11 Summary 7.12 Exercises 7.13 Key Points", " Chapter 7 Unit Testing 7.1 Questions How should I write tests for my software? How can I tell how much testing I’ve actually done? 7.2 Objectives Explain what realistic technical and social goals for software testing are. Explain what a test runner is. Explain what a text fixture is. Write and run unit tests using Python’s pytest test runner. Check test coverage using Python’s coverage module. 7.3 Introduction Term frequency-inverse document frequency (TF-IDF) is a way to determine how relevant a document is in a search. A term’s frequency is how often it occurs in a document d divided by the number of words in that document; inverse document frequency is the ratio of the total number of documents to the number of documents in which the word occurs: \\[ \\tau_w = \\frac{n_w(d) / n_*(d)}{D / D_w} \\] If a word is very common in one document, but absent in others, TF-IDF is high for that (word, document) pair. If a word is common in all documents, or if a word is rare in a document, TF-IDF is lower. This simple measure can therefore be used to rank documents’ relevance with respect to that term. Calculating TF-IDF depends on counting words in a document… while ignoring leading/trailing punctuation… except for words like “Ph.D.” And then there’s the question of hyphenation, which can either signal a multi-part word or a line break, unless of course it indicates that a multi-part happened to land at the end of a line. In short, this relatively simple description can be implemented in many subtly different ways, many of which are wrong. In order to tell which one we’ve actually built and whether we’ve built it correctly, we need to write some tests. This lesson therefore introduces some key ideas and tools. This material is drawn in part from Testing and Continuous Integration with Python. 7.4 What are realistic goals for testing? Any discussion of software and correctness has to start with two ideas. The first is that no amount of testing can ever prove that a piece of code is correct. A function that takes three arguments, each of which can have a hundred different values, theoretically needs a million tests. Even if we were to write them, how would we check that the values we’re testing against are correct? And how would we tell that we had typed in those comparison values correctly, or that we were using the right equality checks in our code, or—the list goes on and on. The second big idea is that the real situation is far less dire. Suppose we want to test a function that returns the sign of a number. If it works for the number 3, it will almost certainly work for 4, and for 5, and so on. In fact, there are only a handful of cases that we actually need to test: Value Expected Reason -Inf -1 Only value of its kind Inf 1 Only value of its kind NaN NaN Only value of its kind -5 -1 Or some other negative number6 0 0 The only value that has a sign of 0 127489 1 Or some other positive number As this table shows, we can divide test inputs into equivalence classes and check one member of each. Yes, there’s a chance that we’ll miss something—that the code we’re testing will behave differently for values we have put in the same class—but this approach reduces the number of tests we have to write and makes it easier for the next person reading our code to understand what it does. 7.5 What does a systematic software testing framework look like? A framework for software testing has to: make it easy for people to write tests (because otherwise they won’t do it); run a set of tests; report which ones have failed; give some idea of where or why they failed (to help debugging); and give some idea of whether any results have changed since the last run. Any single test can have one of three results: success, meaning that the test passed correctly; failure, meaning that the software being tested didn’t do what it was supposed to; or error, meaning that the test itself failed (in which case, we don’t know anything about the software being tested). A unit test is a function that runs some code and produces one of these three results. The input data to the unit test is called the fixture; we tell if the test passed or not by comparing the actual output to the expected output. For example, here’s a very badly written version of numSign and an equally badly written pair of tests for it: numSign &lt;- function(x) { if (x &gt; 0) { 1 } else { -1 } } stopifnot(numSign(1) == 1) stopifnot(numSign(-Inf) == -1) stopifnot(numSign(NULL) == 0) Here, the fixtures are 1, NULL, and -Inf, and the corresponding expected outputs are 1, 0, and -1. These tests are badly written for two reasons: Execution halts at the first failed test, which means we get less information than we could about the state of the system we’re testing. Each test prints its output to the screen: there is no overall summary and no easy way to tell which test produced which result. This isn’t a problem when there are only three tests, but experience shows that if the output isn’t comprehensible at a glance, developers will stop paying attention to it. One other mistake that’s often made in testing is making tests depend on each other. Good tests are independent: they should produce the same results no matter what other tests are run in what order, so that a failure in an early tests doesn’t cause a later test to fail when it should pass or pass when it should fail. This means that every test should start from a freshly-generated fixture rather than using the output of previous tests. 7.6 How can I manage tests systematically? A test framework is a library that provides us with functions that help us write tests, and includes a test runner that will find tests, execute them, and report both individual results that require attention and a summary of overall results. There are many test frameworks for Python. One of the most popular is pytest, which structures tests according to three simple rules: All tests are put in files whose names begin with test_. Each test is a function whose name also begins with test_. Test functions use assert to check that results are as expected. For example, we could test the count_words function by putting the following code in test_count.py: from tf_idf import count_word def test_single_word(): assert count_words(&#39;word&#39;) == {&#39;word&#39; : 1} def test_single_repeated_word(): assert count_words(&#39;word word&#39;) == {&#39;word&#39; : 2} def test_two_words(): assert count_words(&#39;another word&#39;) == {&#39;another&#39; : 1, &#39;word&#39; : 1} def test_trailing_punctuation(): assert count_words(&quot;anothers&#39; word&quot;) == {&#39;anothers&#39; : 1, &#39;word&#39; : 1} The fixture in the last test is the string \"anothers' word\", and the expected result is the dictionary {'anothers' : 1, 'word' : 1}. Note that the assert statement doesn’t include an error message; pytest will include the name of test function in its output if the test fails, and that name should be all the documentation we need. (If the test is so complicated that more is needed, we should probably write a simpler test.) We can run our tests from the shell with a single command: $ pytest As it runs tests, pytest prints . for each one that passes and F for each one that fails. After all of the tests have been run, it prints a summary of the failures: ============================= test session starts ============================== platform darwin -- Python 3.6.5, pytest-3.5.1, py-1.5.3, pluggy-0.6.0 rootdir: /Users/pterry/still-magic/src/unit, inifile: plugins: remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2 collected 4 items test_count.py ...F [100%] =================================== FAILURES =================================== __________________________ test_trailing_punctuation ___________________________ def test_trailing_punctuation(): &gt; assert count_words(&quot;anothers&#39; word&quot;) == {&#39;anothers&#39; : 1, &#39;word&#39; : 1} E assert {&quot;anothers&#39;&quot;: 1, &#39;word&#39;: 1} == {&#39;anothers&#39;: 1, &#39;word&#39;: 1} E Omitting 1 identical items, use -vv to show E Left contains more items: E {&quot;anothers&#39;&quot;: 1} E Right contains more items: E {&#39;anothers&#39;: 1} E Use -v to get the full diff test_count.py:13: AssertionError ====================== 1 failed, 3 passed in 0.05 seconds ====================== pytest searches for all files named test_*.py or *_test.py in the current directory and its sub-directories. We can use command-line options to narrow the search: for example, pytest test_count.py runs only the tests in test_count.py. It automatically records and reports pass/fail/error counts and gives us a nicely-formatted report, but most importantly, it works the same way for everyone, so we can test without having think about how (only about what). That said, fitting tests into this framework sometimes requires a few tricks, which we will explore in the sections that follow. 7.7 How can I tell if my software failed as it was supposed to? Many errors in production systems happen because people don’t test their error handling code. Yuan et al. (2014) found that almost all (92%) of catastrophic system failures were the result of incorrect handling of non-fatal errors explicitly signalled in software, and that in 58% of the catastrophic failures, the underlying faults could easily have been detected through simple testing of error handling code. Our tests should therefore check that the software fails as it’s supposed to and when it’s supposed to; if it doesn’t, we run the risk of a silent error. We can check for exceptions manually using this pattern: # Expect count_words to raise ValueError for empty input. def test_text_not_empty(): try: count_words(&#39;&#39;) assert False, &#39;Should not get this far&#39; except ValueError: pass This code runs the test (i.e., calls count_words) and then fails on purpose if execution reaches the next line. If the right kind of exception is raised, on the other hand, it does nothing. (We’ll take a look in the exercises at how to improve this to catch the case where the wrong kind of exception is raised.) This pattern works, but it violates our first rule of testing: if writing tests is clumsy, developers won’t do it. To make life easier, pytest provides a context manager called pytest.raises to handle tests for exceptions. A context manager creates an object that lives exactly as long as a block of code, and which can do setup and cleanup actions at the start and end of that block. We can use pytest.raises with Python’s with keyword to say that we expect a particular exception to be raised in that code block, and should report an error if one isn’t raised: import pytest def test_text_not_empty(): with pytest.raises(ValueError): count_words(&#39;&#39;) ============================= test session starts ============================== platform darwin -- Python 3.6.5, pytest-3.5.1, py-1.5.3, pluggy-0.6.0 rootdir: /Users/gvwilson/merely-useful/still-magic/src/unit, inifile: plugins: remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2 collected 1 item test_exception.py F [100%] =================================== FAILURES =================================== _____________________________ test_text_not_empty ______________________________ def test_text_not_empty(): with pytest.raises(ValueError): &gt; count_words(&#39;&#39;) E Failed: DID NOT RAISE &lt;class &#39;ValueError&#39;&gt; test_exception.py:6: Failed =========================== 1 failed in 0.04 seconds =========================== The output tells us that count_words doesn’t raise an exception when given an empty string, so we should either decide that the count in this case is zero, or go back and fix our function. 7.8 How can I test software that includes randomness? Data scientists use a lot of random numbers; testing code that relies on them makes use of the fact that they aren’t actually random. A pseudorandom number generator (PRNG) uses a complex algorithm to create a stream of values that have all the properties of a truly random sequence. A key feature of PRNGs is that they can be initialized with a seed, and that given the same seed, the PRNG will always produce the same sequence of values. If you want your work to be reproducible, you should always seed your PRNG, and always record the seed somewhere so that you can re-run exactly the same calculations. Pseudorandom numbers aren’t the only unpredictable things in programs. If you rely on the current date and time, that counts as “randomness” as well. For example, suppose you want to test that a function correctly counts the number of dates between the start of the year and the current date. As time goes by, the correct answer will change, so how can you write a reusable function? The answer is to write a wrapper for the function in question that either calls the actual function or does what you need for testing, and then to use that wrapper, and only that wrapper, everywhere in your program. The example below uses this approach: if the value TESTING_DATE has been set, weeks_since_01 returns the number of weeks from start to that date, and if not, it returns the number of weeks from `start to the current (unpredictable) date: import datetime DAYS_PER_WEEK = 7 TESTING_DATE = None def weeks_since_01(start): current = TESTING_DATE if current is None: current = datetime.date.today() return round((current - start).days / DAYS_PER_WEEK) If this code is in a file called wrappers.py, we would use it like this: print(&#39;current&#39;, wrappers.weeks_since_01(datetime.date(2018, 8, 1))) wrappers.TESTING_DATE = datetime.date(2018, 8, 30) print(&#39;fixed&#39;, wrappers.weeks_since_01(datetime.date(2018, 8, 1))) current 25 fixed 4 A cleaner approach is to make the test control an attribute of the function. (Remember, functions are objects in memory like anything else, so we can attach other data to them.) Using this approach allows us to import the function on its own as a self-contained unit, and avoids making those functions depend on externally-defined (global) variables. The method works because the body of a function isn’t executed as the function is defined, so it’s OK to refer to values that are added afterward. Here’s what defining a wrapper function looks like: def weeks_since_02(start): current = weeks_since_02.testing_date if current is None: current = datetime.date.today() return round((current - start).days / DAYS_PER_WEEK) weeks_since_02.testing_date = None and here’s how we would use it: # demo_test_weeks.py print(&#39;first&#39;, weeks_since_02(datetime.date(2018, 8, 1))) weeks_since_02.testing_date = datetime.date(2018, 8, 30) print(&#39;second&#39;, weeks_since_02(datetime.date(2018, 8, 1))) current 25 fixed 4 7.9 How can I test software that does I/O? A lot of early books on unit testing said that tests shouldn’t rely on external files, both because file I/O was slow and because those files could easily get lost. Neither stricture applies today: file I/O is much faster than it was in the 1990s, and if files of test data are stored in version control, they’re no more likely to be lost than the source code. That said, it’s often easier to read unit tests if the “files” used as fixtures are included right next to the tests, and it’s easier to test output if the “files” that are created only ever live in memory: temporary output files can always be cleaned up after tests complete, but it’s one extra burden on test authors. A good way to avoid all of these problems is to treat strings in memory as if they were files, which is what Python’s StringIO module does. A StringIO object has the same methods as a file object, but reads and writes text in memory instead of bytes on disk: from io import StringIO writer = StringIO() for word in &#39;first second third&#39;.split(): writer.write(&#39;{}\\n&#39;.format(word)) print(writer.getvalue()) first second third StringIO objects can also be initialized with some data that a program can read. (Notice that the lengths reported below include the newline character at the end of each line.) DATA = &#39;&#39;&#39;first second third&#39;&#39;&#39; for line in StringIO(DATA): print(len(line)) 6 7 5 In order to use StringIO in tests, we may need to refactor our code a bit (Chapter 17). It’s common to have a function open a file, read its contents, and return the result like this: def main(infile, outfile): with open(infile, &#39;r&#39;) as reader: with open(outfile, &#39;w&#39;) as writer: # ...read from reader and write to writer... However, this main function is hard to test, since there’s no easy way to substitute a StringIO for the file inside that function. What we can do is reorganize the software so that file opening is done separately from reading and writing. This is good practice anyway for handling stdin and stdout in command-line tools, which don’t need to be opened: def main(infile, outfile): if infile == &#39;-&#39;: reader = stdin else: reader = open(infile, &#39;r&#39;) if outfile == &#39;-&#39;: writer = stdout else: writer = open(outfile, &#39;r&#39;) process(reader, writer) if infile == &#39;-&#39;: reader.close() if outfile == &#39;-&#39;: writer.close() def process(reader, writer): # ...read from reader and write to writer... After moving the reading and writing into process, we can easily pass in a couple of StringIO objects for testing. 7.10 How can I tell which parts of my software have (not) been tested? Take a moment to study the code shown below. Can you tell which lines are and aren’t being executed? def first(left, right): if left &lt; right: left, right = right, left while left &gt; right: value = second(left, right) left, right = right, int(right/2) return value def second(check, balance): if check &gt; 0: return balance else: return 0 def main(): final = first(3, 5) print(3, 5, final) if __name__ == &#39;__main__&#39;: main() The answer is probably “no”, but the second half of the answer should be “that’s what computers are for”. Coverage measures which parts of program are and are not executed. In principle, a coverage tool keep a list of Booleans, one per line, all of which are initialized to False. Each time a line is executed, the coverage tool sets the corresponding flag to True. After the program finishes, the tool reports which lines have and have not been executed, along with summary statistics like the percentage of code executed. It’s easy and wrong to obsess about meeting specific targets for test coverage. However, anything that isn’t tested should be assumed to be wrong, and drops in coverage often indicate new technical debt. Use pip install coverage to install the standard Python coverage tool. Once you have done that, use coverage run filename.py instead of python filename.py to run your program. This creates a file in the current directory called .coverage; once your program completes, you can run coverage report to get a summary of the most recent report: $ coverage run demo_coverage.py Name Stmts Miss Cover -------------------------------------- demo_coverage.py 16 1 94% If you want the details, you can use coverage html to generate an HTML listing: FIXME: include unit/coverage.html A more advanced tool called a profiler can give you even more information; we will discuss profilers briefly in Section 15.6. 7.11 Summary One practice we haven’t described in this section is test-driven development (TDD). Rather than writing code and then writing tests, many developers believe we should write tests first to help us figure out what the code is supposed to do, and then write just enough code to make those tests pass. Once the code works, we should clean it up and commit it, then move on to the next task. TDD’s advocates claim that writing tests first focuses people’s minds on what code is supposed to so that they’re not subject to confirmation bias when viewing test results. They also claim that TDD ensures that code actually is testable, and that tests are actually written. However, the evidence backing these claims is contradictory: empirical studies have not found a strong effect Fucci et al. (2016), and at least one study suggests that it may not be the order of testing and coding that matters, but whether developers work in short, interleaved bursts Fucci et al. (2017). Many productive programmers still believe in TDD, so it’s possible that we are measuring the wrong things. FIXME: create concept map for unit testing 7.12 Exercises 7.12.1 Handle the wrong kind of exception FIXME: modify explicit exception testing code to handle the wrong kind of exception. 7.13 Key Points Testing can only ever show that software has flaws, not that it is correct. Its real purpose is to convince people (including yourself) that software is correct enough, and to make tolerances on ‘enough’ explicit. A test runner finds and runs tests written in a prescribed fashion and reports their results. A unit test can pass (work as expected), fail (meaning the software under test is flawed), or produce an error (meaning the test itself is flawed). A fixture is the data or other input that a test is run on. Every unit test should be independent of every other to keep results comprehensible and reliable. Programmers should check that their software fails when and as it is supposed to in order to avoid silent errors. Write test doubles to replace unpredictable inputs such as random numbers or the current date or time with a predictable value. Use string I/O doubles when testing file input and output. Use a coverage tool to check how well tests have exercised code. References "],
["verify.html", "Chapter 8 Verification 8.1 Questions 8.2 Objectives 8.3 Introduction 8.4 What is the difference between testing in software engineering and in data analysis? 8.5 Why should I be cautious when using floating-point numbers? 8.6 How can I express how close one number is to another? 8.7 How should I write tests that involved floating-point values? 8.8 How can I test plots and other graphical results? 8.9 How can I test the steps in a data analysis pipeline during development? 8.10 How can I check the steps in a data analysis pipeline in production? 8.11 How can I infer and check properties of my data? 8.12 Summary 8.13 Exercises 8.14 Key Points", " Chapter 8 Verification 8.1 Questions How should I test a data analysis pipeline? 8.2 Objectives Explain why floating point results aren’t random but can still be unpredictable. Explain why it is hard to test code that produces plots or other graphical output. Describe and implement heuristics for testing data analysis. Describe the role of inference in data analysis testing and use the tdda library to find and check constraints on tabular data. 8.3 Introduction The previous lesson explained how to test software in general; this one focuses on testing data analysis. 8.4 What is the difference between testing in software engineering and in data analysis? Testing data analysis pipelines is often harder than testing mainstream software applications. The reason is that data analysts often don’t know what the right answer is, which makes it hard to check correctness. The key distinction is the difference between validation and verification. Validation asks, “Is specification correct?” while verification asks, It’s the difference between building the right thing and building something right; the former question is often much harder for data scientists to answer. Instead of unit testing, a better analogy is often physical experiments. When high school students are measuring acceleration due to gravity, they should get a figure close to \\[10 m/sec^2\\]. Undergraduates might get \\[9.8 m/sec^2\\] depending on the equipment used, but if either group gets \\[9.806 m/sec^2\\] with a stopwatch, a marble, and an ramp, they’re either incredibly lucky or cheating. Similarly, when testing data analysis pipelines, we often have to specify tolerances. Does the answer have to be exactly the same as a hand-calculated value or a previously-saved value? If not, how close is good enough? We also need to distinguish between development and production. During development, our main concern is whether our answers are (close enough to) what we expect. We do this by analyzing small datasets and convincing ourselves that we’re getting the right answer in some ad hoc way. In production, on the other hand, our goal is to detect cases where behavior deviates significantly from what we previously decided what right. We want this to be automated so that our pipeline will ring an alarm bell to tell us something is wrong even if we’re busy working on something else. We also have to decide on tolerances once again, since the real data will never have exactly the same characteristics as the data we used during development. We also need these checks because the pipeline’s environment can change: for example, someone could upgrade a library that one of our libraries depends on, which could lead to us getting slightly different answers than we expected. 8.5 Why should I be cautious when using floating-point numbers? Every tutorial on testing numerical software has to include a discussion of the perils of floating point, so we might as well get ours out of the way. The explanation that follows is simplified to keep it manageable; if you want to know more, please take half an hour to read Goldberg (1991). Finding a good representation for floating point numbers is hard: we cannot represent an infinite number of real values with a finite set of bit patterns, and unlike integers, no matter what values we do represent, there will be an infinite number of values between each of them that we can’t. These days, floating point numbers are usually represented using sign, magnitude (or mantissa), and an exponent. In a 32-bit word, the IEEE 754 standard calls for 1 bit of sign, 23 bits for the mantissa, and 8 bits for the exponent. To illustrate the problems with floating point, we will use a much simpler 5-bit representation with 3 bits for the magnitude and 2 for the exponent. We won’t worry about fractions or negative numbers, since our simple representation will show off the main problems. The table below shows the possible values (in decimal) that we can represent with 5 bits. Real floating point representations don’t have all the redundancy that you see in this table, but it illustrates the point. Using subscripts to show the bases of numbers, \\[110_2 \\times 2^{11_2)\\] \\[6 \\times 2^3\\] or 48. Exponent Mantissa 00 01 10 11 000 0 0 0 0 001 1 2 4 8 010 2 4 8 16 011 3 6 12 24 100 4 8 16 32 101 5 10 20 40 110 6 12 24 48 111 7 14 28 56 Figure 8.1 is a clearer view of some of the values our scheme can represent: Figure 8.1: Number Spacing A lot of values are missing from this diagram: for example, it includes 8 and 10 but not 9. This is exactly like the problem writing out 1/3 in decimal: we have to round that to 0.3333 or 0.3334. But if this scheme has no representation for 9, then 8+1 must be stored as either 8 or 10. This raises an interesting question: if 8+1 is 8, what is 8+1+1? If we add from the left, 8+1 is 8, plus another 1 is 8 again. If we add from the right, 1+1 is 2, and 2+8 is 10, so changing the order of operations can make the difference between right and wrong. In this case, if we sort the values and then add from smallest to largest, it gives us the best chance of getting the best answer. In other situations, like inverting a matrix, the rules are more complicated. Just as electrical engineers trust oscilloscope makers, almost all data scientists should trust the authors of core libraries to get this right. To make this more concrete, consider the short Python program below: vals = [] for i in range(1, 10): number = 9.0 * 10.0 ** -i vals.append(number) total = sum(vals) expected = 1.0 - (10.0 ** -i) diff = total - expected print(&#39;{:2d} {:22.21f} {:22.21f}&#39;.format(i, total, total-expected)) This program loop runs over the integers from 1 to 9 inclusive and puts the numbers 0.9, 0.09, 0.009, and so on in vals. The sums should be 0.9, 0.99, 0.999, and so on, but are they? To find out, we can calculate the same values by subtracting .1 from 1, then subtracting .01 from 1, and so on. This should create exactly the same sequence of numbers, but it doesn’t: index total difference 1 0.900000000000000022204 0.000000000000000000000 2 0.989999999999999991118 0.000000000000000000000 3 0.998999999999999999112 0.000000000000000000000 4 0.999900000000000011013 0.000000000000000000000 5 0.999990000000000045510 0.000000000000000000000 6 0.999999000000000082267 0.000000000000000111022 7 0.999999900000000052636 0.000000000000000000000 8 0.999999990000000060775 0.000000000000000111022 9 0.999999999000000028282 0.000000000000000000000 As this table shows, the very first value contributing to our sum is already slightly off. Even with 23 bits for a mantissa, we cannot exactly represent 0.9 in base 2, any more than we can exactly represent 1/3 in base 10. Doubling the size of the mantissa would reduce the error, but we can’t ever eliminate it. The good news is, \\[9 {\\times} 10^{-1}\\] and \\[1 - 0.1\\] are exactly the same: the value might not be precisely right, but at least they are consistent. But some later values differ, and sometimes accumulated error makes the result more accurate. It’s very important to note that this has nothing to do with randomness. The same calculation will produce exactly the same results no matter how many times it is run, because the process is completely deterministic, just hard to predict. If you see someone run the same code on the same data with the same parameters many times and average the results, you should ask if they know what they’re doing. (That said, doing this can be defensible if there is parallelism, which can change evaluation order, or if you’re changing platform, e.g., moving computation to a GPU.) 8.6 How can I express how close one number is to another? The absolute spacing in the diagram above between the values we can represent is uneven. However, the relative spacing between each set of values stays the same: the first group is separated by 1, then the separation becomes 2, then 4, and then 8. This happens because we’re multiplying the same fixed set of mantissas by ever-larger exponents, and it leads to some useful definitions. The absolute error in an approximation is the absolute value of the difference between the approximation and the actual value. The relative error is the ratio of the absolute error to the value we’re approximating. For example, it we are off by 1 in approximating 8+1 and 56+1, we have the same absolute error, but the relative error is larger in the first case than in the second. Relative error is almost always more important than absolute error when we are testing software because it makes little sense to say that we’re off by a hundredth when the value in question is a billionth. 8.7 How should I write tests that involved floating-point values? Accuracy is how close your answer is to right, and precision is how close repeated measurements are to each other. You can be precise without being accurate (systematic bias), or accurate without being precise (near the right answer, but without many significant digits). Accuracy is usually more important than precision for human decision making, and a relative error of \\[10^{-3}\\] (three decimal places) is more than good enough for most data science because the decision a human being would make won’t change if the number changes by 0.1%. We now come to the crux of this lesson: if the function you’re testing uses floating point numbers, what do you compare its result to? If we compared the sum of the first few numbers in vals to what it’s supposed to be, the answer could be False even though we’re doing nothing wrong. If we compared it to a previously calculated result that we had stored somehow, the match would be exact. No one has a good generic answer to this problem because its root cause is that we’re using approximations, and each approximation has to be judged in context. So what can you do to test your programs? If you are comparing to a saved result, and the result was saved at full precision, you could use exact equality, because there is no reason for the new number to differ. However, any change to your code, however small, could trigger a report of a difference. Experience shows that these spurious warnings quickly lead developers to stop paying attention to their tests. A much better approach is to write a test that checks whether numbers are the same within some tolerance, which is best expressed as a relative error. In Python, you can do this with pytest.approx, which works on lists, sets, arrays, and other collections, and can be given either relative or absolute error bounds. To show how it works, here’s an example with an unrealistically tight absolute bound: from pytest import approx for bound in (1e-15, 1e-16): vals = [] for i in range(1, 10): number = 9.0 * 10.0 ** -i vals.append(number) total = sum(vals) expected = 1.0 - (10.0 ** -i) if total != approx(expected, abs=bound): print(&#39;{:22.21f} {:2d} {:22.21f} {:22.21f}&#39;.format(bound, i, total, expected)) 9.999999999999999790978e-17 6 0.999999000000000082267 0.999998999999999971244 9.999999999999999790978e-17 8 0.999999990000000060775 0.999999989999999949752 This tells us that two tests pass with an absolute error of \\[10^{-15}\\] but fail when the bound is \\[10^{-16}\\], both of which are unreasonably tight. (Again, think of physical experiments: an absolute error of \\[10^{-15}\\] is one part in a trillion, which only a handful of high-precision experiments have ever achieved.) 8.8 How can I test plots and other graphical results? Testing visualizations is hard: any change to the dimension of the plot, however small, can change many pixels in a raster image, and cosmetic changes such as moving the legend up a couple of pixels will similarly generate false positives. The simplest solution is therefore not to test the generated image, but to test the data used to produce it. Unless you suspect that the plotting library contains bugs, feeding it the correct data should produce the correct plot. If you do need to test the generated image, the only practical approach is to compare it to a saved image that you have visually verified. pytest-mpl does this by calculating the root mean square (RMS) difference between images, which must be below a threshold for the comparison to pass. It also allows you to turn off comparison of text, because font differences can throw up spurious failures. As with choosing tolerances for floating-point tests, your rule for picking thresholds should be, “If images are close enough that a human being would make the same decision about meaning, the test should pass” FIXME: example Another approach is to save the plot in a vector format like SVG that stores the coordinates of lines and other elements as text in a structure similar to that of HTML. You can then check that the right elements are there with the right properties, although this is less rewarding than you might think: again, small changes to the library or to plotting parameters can make all of the tests fail by moving elements by a pixel or two. Vector-based tests therefore still need to have thresholds on floating-point values. 8.9 How can I test the steps in a data analysis pipeline during development? We can’t tell you how to test your math, since we don’t know what math you’re using, but we can tell you where to get data to test it with. The first method is subsampling: choose random subsets of your data, analyze it, and see how close the output is to what you get with the full dataset. If output doesn’t converge as sample size grows, something is probably unstable—which is not necessarily the same as wrong. Instability is often a problem with the algorithm, rather than with the implementation. If you do this, it’s important that you select a random sample from your data rather than (for example) the first N records or every N’th record. If there is any ordering or grouping in your data, those techniques can produce samples that are biased, which may in turn invalidate some of your tests. FIXME: add an exercise that subsamples the Zipf data. The other approach is to test with synthetic data. With just a few lines of code, you can generate uniform data (i.e., data having the same values for all observations), strongly bimodal data (which is handy for testing clustering algorithms), or just sample a known distribution. If you do this, you should also try giving your pipeline data that doesn’t fit your expected distribution and make sure that something, somewhere, complains. Doing this is the data science equivalent of testing the fire alarm every once in a while. For example, we can write a short program to generate data that conforms to Zips’ Law and use it to test our analysis. Real data will be integers (since words only occur or not), and distributions will be fractional. We will use 5% relative error as our threshold, which we pick by experimentation: 1% excludes a valid correct value. The test function is called is_zipf: from pytest import approx RELATIVE_ERROR = 0.05 def is_zipf(hist): scaled = [h/hist[0] for h in hist] print(&#39;scaled&#39;, scaled) perfect = [1/(1 + i) for i in range(len(hist))] print(&#39;perfect&#39;, perfect) return scaled == approx(perfect, rel=RELATIVE_ERROR) Here are three tests that use this function with names that suggest their purpose: def test_fit_correct(): actual = [round(100 / (1 + i)) for i in range(10)] print(&#39;actual&#39;, actual) assert is_zipf(actual) def test_fit_first_too_small(): actual = [round(100 / (1 + i)) for i in range(10)] actual[0] /= 2 assert not is_zipf(actual) def test_fit_last_too_large(): actual = [round(100 / (1 + i)) for i in range(10)] actual[-1] = actual[1] assert not is_zipf(actual) 8.10 How can I check the steps in a data analysis pipeline in production? An operational test is one that is kept in place during production to tell users if everything is still working as it should. A common pattern for such tests is to have every tool append information to a log (Chapter 6) and then have another tool check that log file after the run is over. Logging and then checking makes it easy to compare values between pipeline stages, and ensures that there’s a record of why a problem was reported. Some common operational tests include: Does this pipeline stage produce the same number of output records as input records? Or fewer if the stage is aggregating? If two or more tables are being joined, is the number of output records equal to the product of the number of input records? Is the standard deviation be smaller than the range of the data? Are there any NaNs or NULLs where there aren’t supposed to be any? To illustrate these ideas, here’s a script that reads a document and prints one line per word: import sys num_lines = num_words = 0 for line in sys.stdin: num_lines += 1 words = [strip_punctuation(w) for w in line.strip().split()] num_words += len(words) for w in words: print(w) with open(&#39;logfile.csv&#39;, &#39;a&#39;) as logger: logger.write(&#39;text_to_words.py,num_lines,{}\\n&#39;.format(num_lines)) logger.write(&#39;text_to_words.py,num_words,{}\\n&#39;.format(num_words)) Here’s a complementary script that counts how often words appear in its input: import sys num_words = 0 count = {} for word in sys.stdin: num_words += 1 count[word] = count.get(word, 0) + 1 for word in count: print(&#39;{} {}&#39;, word, count[word]) with open(&#39;logfile.csv&#39;, &#39;a&#39;) as logger: logger.write(&#39;word_count.py,num_words,{}\\n&#39;.format(num_words)) logger.write(&#39;word_count.py,num_distinct,{}\\n&#39;.format(len(count))) Both of these scripts write records to logfile.csv. When we look at that file after a typical run, we see records like this: text_to_words.py,num_lines,431 text_to_words.py,num_words,2554 word_count.py,num_words,2554 word_count.py,num_distinct,1167 We can then write a small program to check that everything went as planned: # read CSV file into the variable data check(data[&#39;text_to_words.py&#39;][&#39;num_lines&#39;] &lt;= data[&#39;word_count.py&#39;][&#39;num_words&#39;]) check(data[&#39;text_to_words.py&#39;][&#39;num_words&#39;] == data[&#39;word_count.py&#39;][&#39;num_words&#39;]) check(data[&#39;word_count.py&#39;][&#39;num_words&#39;] &gt;= data[&#39;word_count.py&#39;][&#39;num_distinct&#39;]) 8.11 How can I infer and check properties of my data? Writing tests for the properties of data can be tedious, but some of the work can be automated. In particular, the TDDA library can infer test rules from data, such as age &lt;= 100, Date should be sorted ascending, or StartDate &lt;= EndDate. The library comes with a command-line tool called tdda, so that the command: $ tdda discover elements92.csv elements.tdda infers rules from data, while the command: tdda verify elements92.csv elements.tdda verifies data against those rules. The inferred rules are stored as JSON, which is (sort of) readable with a bit of practice. Reading the generated rules is a good way to get to know your data, and modifying values (e.g., changing the maximum allowed value for Grade from the observed 94.5 to the actual 100.0) is an easy way to make constraints explicit: &quot;fields&quot;: { &quot;Name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;min_length&quot;: 3, &quot;max_length&quot;: 12, &quot;max_nulls&quot;: 0, &quot;no_duplicates&quot;: true }, &quot;Symbol&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;min_length&quot;: 1, &quot;max_length&quot;: 2, &quot;max_nulls&quot;: 0, &quot;no_duplicates&quot;: true }, &quot;ChemicalSeries&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;min_length&quot;: 7, &quot;max_length&quot;: 20, &quot;max_nulls&quot;: 0, &quot;allowed_values&quot;: [ &quot;Actinoid&quot;, &quot;Alkali metal&quot;, &quot;Alkaline earth metal&quot;, &quot;Halogen&quot;, &quot;Lanthanoid&quot;, &quot;Metalloid&quot;, &quot;Noble gas&quot;, &quot;Nonmetal&quot;, &quot;Poor metal&quot;, &quot;Transition metal&quot; ] }, &quot;AtomicWeight&quot;: { &quot;type&quot;: &quot;real&quot;, &quot;min&quot;: 1.007947, &quot;max&quot;: 238.028913, &quot;sign&quot;: &quot;positive&quot;, &quot;max_nulls&quot;: 0 }, ... } We can apply these inferred rules to all elements using the -7 flag to get pure ASCII output and the -f flag to show only fields with failures: $ tdda verify -7 -f elements118.csv elements92.tdda FIELDS: Name: 1 failure 4 passes type OK min_length OK max_length X max_nulls OK no_duplicates OK Symbol: 1 failure 4 passes type OK min_length OK max_length X max_nulls OK no_duplicates OK AtomicWeight: 2 failures 3 passes type OK min OK max X sign OK max_nulls X ...others... SUMMARY: Constraints passing: 57 Constraints failing: 15 Another way to use TDDA is to generate constraints for two datasets and then look at differences in order to see how similar the datasets are to each other. This is especially useful if the constraint file is put under version control. 8.12 Summary FIXME: create concept map for verification 8.13 Exercises FIXME: create exercises for verification 8.14 Key Points Programmers should use tolerances when comparing floating-point numbers (not just in tests). Test the data structures used in plotting rather than the plots themselves. Check that parametric or non-parametric statistics of data do not differ from saved values by more than a specified tolerance. Infer constraints on data and then check that subsequent data sets obey these constraints. References "],
["branches.html", "Chapter 9 A Branching Workflow 9.1 Questions 9.2 Objectives 9.3 Introduction 9.4 How can I use branches to manage development of new features? 9.5 How can I switch between branches when work is only partly done? 9.6 How can I keep my project’s history clean when working on many branches? 9.7 How can I make it easy for people to review my work before I merge it? 9.8 What is a feature? 9.9 How can I label specific versions of my work? 9.10 Summary 9.11 Exercises 9.12 Key Points", " Chapter 9 A Branching Workflow 9.1 Questions How can a growing number of people coordinate work on a single project? 9.2 Objectives Explain what rebasing is and use it interactively to collapse a sequence of commits into a single commit. Describe a branch-per-feature workflow and explain why to use it. Describe what a repository tag is and create an annotated tag in a Git repository. 9.3 Introduction A common Git workflow for single-author/single-user projects is: Make some changes, running git add and git commit as you go along. git push to send those changes to a remote repository for backup and sharing. Switch computers (e.g., go from your laptop to the cluster where you’re going to process full-sized data sets). git pull to get your changes from the remote repository, and possibly resolve merge conflicts if you made some changes on the cluster the last time you were there but forgot to push them to the remote repo. Occasionally, you’ll run git checkout -- . or git reset --hard VERSION to discard changes. Less frequently, you’ll use git diff -r abcd1234 path/to/file or git show abcd1234:path/to/file to look at what’s changed in files, or git checkout abcd1234 path/to/file followed by git add and git commit to restore an old version of a file. And every once in a while, you’ll jenny the repository and clone a new copy because everything is messed up. This workflow essentially uses Git to do backups and as a replacement for scp (or ftp, if you’re of an older generation). But sometimes you have to work on several things at once in a single project, or need to set aside current work for a high-priority interrupt. And even if those situations don’t arise, this workflow doesn’t provide guidance for collaborating with others. This lesson introduces a few working practices that will help you do these things. Along the way, it shows you how to use some of Git’s more advanced capabilities. As with everything involving Git, the syntax for commands is more complicated than it needs to be Perez De Rosso and Jackson (2013),Perez De Rosso and Jackson (2016), but if used carefully, they can make you much more productive. 9.4 How can I use branches to manage development of new features? Experienced developers tend to use a branch-per-feature workflow, even when they are working on their own. Whenever they want to create a new feature, they create a new branch from master and do the work there. When it’s done, they merge that branch back into `master. More specifically, a branch-per-feature workflow looks like this: git checkout master to make sure you’re working in the master branch. git pull origin master (or git pull upstream master) to make sure that your starting point is up to date (i.e., that the changes you committed while working on the cluster yesterday are now in your local copy, or that the fix your colleague did last week has been incorporated). git checkout -b name-of-feature to create a new branch. The branch’s name should be as descriptive as the name of a function or of a file containing source code: \"stuff\" or \"fixes\" saves you a couple of seconds of typing when you create the branch, and will cost you much more time than that later on when you’re juggling half a dozen branches and trying to figure out what’s in each. Do the work you want to do. If some other task occurs to you while you’re doing it—for example, if you’re writing a new function and realize that the documentation for some other function should be updated—don’t do that work in this branch just because you happen to be there. Instead, commit your changes, switch back to master, create a new branch for the other work, and carry on. Periodically, you may git push origin name-of-feature to save your work to GitHub. When the work is done, you git pull origin master to get any changes that anyone else has put in the master copy into your branch and merge any conflicts. This is an important step: you want to do the merge and test that everything still works in your branch, not in master. Finally, use git checkout master to switch back to the master branch on your machine and git merge name-of-feature master to merge your changes into master. Test it one more time, and then git push origin master to send your changes to the remote repository for everyone else to admire and use. It takes longer to describe this workflow than to use it. Right now, for example, the repository for this material currently has six active branches: master (which is the published version), explain-branch-per-feature (where this is being written), and four others created from master as reminders to fix a glossary entry, rewrite the description in README.md of how to generate a PDF of these notes, and so on. This approach essentially uses branches as a to-do list, which takes no more time than typing a brief note about the feature into a text file or writing it in a lab notebook, and is much easier to track. Chapter 10 will look at better ways to manage this. 9.5 How can I switch between branches when work is only partly done? So what happens if we are in the middle of making changes on branch X and realize that we need to do some reorganizing that has nothing to do with the feature we’re currently working on? The easy answer is to commit our half-finished work, switch branches, do the other task, and then switch back. However, that workflow leaves a lot of commits in our history corresponding to half-done work, and ideally we want every commit to move the system from one useful state to another. And if we make a dozen commits while working on a feature and then merge that branch into master, the overall history of the project can become very difficult to read. There is a command called git stash that will temporarily save work on a branch. We don’t recommend using it, since it creates yet another place where valuable information might be stored (or forgotten). Instead, commit changes in the branch as you work, then squash those commits into one single comprehensible commit using the command git rebase (discussed in the next section). After squashing, you can merge that single large (meaningful) commit into master. Like many things involving Git, this is easier to understand with a diagram. Suppose we have created a branch to work on a feature: FIXME: figure We make several small changes, then realize we need to fix something else, so we commit those changes: FIXME: figure switch to master, and create a new branch to do the other work. This happens several times, eventually leaving the repository in this state: FIXME: figure If we were to merge now, four commits would wind up in the history of the master branch. Instead, we squash those four commits into one: FIXME: figure and then merge that: FIXME: figure 9.6 How can I keep my project’s history clean when working on many branches? The workflow described above depends on rebasing, which means moving or combining some commits from one branch to another. git rebase is a powerful command: it can replay changes made in one branch on top of changes made to another: FIXME: figure or collapse several consecutive commits into a single commit, as we saw in the previous section. We will only use it for the latter ability. The command we want is git rebase -i BASE, where BASE is the most recent commit before the sequence to be compressed (i.e., the one that everything else will be based on). Many people find this confusing, and frequently ask git rebase to start with the first commit they want changed rather than the last one that they don’t. When we run git rebase -i, it brings up a display of recent commits in the same editor we would use for writing commit messages: FIXME: figure The help text in this display tells us what we can do; in most cases, people choose to pick the first commit and squash the rest. When we save this file, Git immediately launches the editor again to show us the combination of recent changes and messages: FIXME: figure We can now edit this text to create a commit message for the unified commit. After we have done this, git log will only show us this single commit, because it’s the only one left in our history: FIXME: figure git rebase is a complex command, and if we have merged changes from other branches into the branch we’re rebasing, Git can become confused (or rather, it can confuse us). A good rule to follow is, “Don’t rebase branches that are shared with other people.” Instead, do all the work required for a feature, rebase, and then merge in recent changes from master and merge back to master: FIXME: figure What if you have pushed a branch to GitHub (or elsewhere) and then combine the commits to change its history as shown below: FIXME: figure In this case, Git realizes that your merge would lose information and prevents the operation from going through. You can use git push --force to overwrite the remote history, but this is usually a sign that you should have done something differently a while back. Remember, git push --force will also overwrite any work that other people have pushed to the repository, so it’s a good way to end friendships. 9.7 How can I make it easy for people to review my work before I merge it? The biggest benefit of having a second person work on a programming project is not getting twice as much code written, but having a second pair of eyes look at the software. Chapter 11 will discuss how to do code review, but a good first step depends on using a branch-per-feature workflow. In order for someone to review your change, that change and the original have to somehow be put side-by-side. The usual way to do this is to create a pull request, which is a marker saying, “Someone would like to merge branch A into branch X”. The pull request does not contain the changes: instead, it points at two branches, so that if either branch is changed, the differences displayed are always up to date. FIXME: figure Pull requests can be created between two branches in a single repository, or between branches of different repositories. The latter is common in projects with many contributors: each contributor works in a branch in their own fork of the repository, and when they’re done, they create a pull request offering to merge their changes into the master branch of the original repository. In order to update their own repository, they pull changes from the original: FIXME: figure A pull request can store more than the location of the source and destination branch. In particular, it can store comments that people have made about the proposed merge. GitHub and other forges allow users to add comments on the pull request as a whole, or on particular lines, and can mark old comments as out of date if the lines of code the comment is attached to are updated. Pull requests aren’t just for code. If you are using a typesetting language like Markdown or LaTeX to write your papers, you can create pull requests for changes so that your colleagues can comment on them, then revise and push again when you incorporate their suggestions. This workflow has all the benefits of adding comments to Google Docs, but scales to large projects or large numbers of contributors. The downside, of course, is that you have to use a typesetting language rather than a WYSIWYG editor, because the programmers who created Git and other version control systems still don’t support anything that couldn’t be put on a punchcard in 1965. 9.8 What is a feature? But what is a “feature”, exactly? What’s large enough to merit creation of a new branch? These rules make sense for small projects with or without collaborators: Anything cosmetic that is only one or two lines long can be done in master and committed right away. “Cosmetic” means changes to comments or documentation: nothing that affects how code runs—not even a simple variable renaming—is done this way, because experience has taught that things that aren’t supposed to often do. A pure addition that doesn’t change anything else is a feature and goes into a branch. For example, if you run a new analysis and save the results, that should be done on its own branch because it might take several tries to get the analysis to run, and you might interrupt yourself several times to fix things that you’ve discovered aren’t working. Every change to code that someone might want to undo later in one step gets done as a feature. For example, if a parameter is added to a function, every call to the function has to be updated; since neither alteration makes sense without the other, it’s considered a single feature and should be done in one branch. The hardest thing about using a branch-per-feature workflow is actually doing it for small changes. As the first point in the list above suggests, most people are pragmatic about this on small projects; on large ones, where dozens of people might be committing, even the smallest and most innocuous change needs to be in its own branch so that it can be reviewed (which we discuss below). The other thing that’s hard to do with a branch-per-feature workflow is a major code reorganization. If many files are being moved, renamed, and altered in order to restructure the project, merging branches where those changes haven’t been made can be tedious and error-prone. The solution is to not get into this situation: as Chapter 17 says, code should be reorganized in many small steps, not one big one. 9.9 How can I label specific versions of my work? A tag is a permanent label on a particular state of the repository. Tags are theoretically redundant, since the commit hash identifies that state as well, but commit hashes are (deliberately) random and therefore hard to remember or find. Experienced developers Use annotated tags to mark every major event in the project’s history. These tags are called “annotated” because they allow their creator to specify a message, just like a commit. Research projects often use report-date-event for tag names, such as jse-2018-06-23-response or pediatrics-2018-08-15-summary. If you do this, please don’t tempt fate by calling something -final. Most software projects use semantic versioning for software releases, which produces three-part version numbers major.minor.patch: Increment major every time there’s an incompatible externally-visible change. Increment minor when adding functionality without breaking any existing code. Increment patch for bug fixes that don’t add any new features (“now works as previously documented”). Simple projects only tag the master branch because everything that is finished is merged to master. Larger software projects may create a branch for each released version and do minor or patch updates on that branch, but this outside the scope of this lesson. 9.10 Summary FIXME: create concept map for workflow 9.11 Exercises FIXME: exercises for branching 9.12 Key Points Create a new branch for every feature, and only work on that feature in that branch. Always create branches from master, and always merge to master. Use rebasing to combine several related commits into a single commit before merging. References "],
["backlog.html", "Chapter 10 Managing Backlog 10.1 Questions 10.2 Objectives 10.3 Introduction 10.4 How can I manage the work I still have to do? 10.5 How can I write a good bug report? 10.6 How can I use labels to organize work? 10.7 How can I use labels to prioritize work? 10.8 How can I use issues to enforce a workflow for a project? 10.9 Summary 10.10 Exercises 10.11 Key Points", " Chapter 10 Managing Backlog 10.1 Questions How can I tell what needs to be done and who is doing it? 10.2 Objectives Explain what an issue tracking tool does and what it should be used for. Explain how to use labels on issues to manage work. Describe the information a well-written issue should contain. 10.3 Introduction Version control tells us where we’ve been; issues tells us where we’re going. Issue tracking tools are often called ticketing systems or bug trackers because they were created to keep track of work that needs to be done and bugs that needed fixing. However, they can be used to manage any kind of work and are often a convenient way to manage discussions as well. 10.4 How can I manage the work I still have to do? Like other forges, GitHub records a set of issues for each project, and allows members of that project to create new ones, modify existing ones, and search both. Every issue has: A unique ID, such as #123, which is also part of its link. This makes issues easy to find and refer to: in particular, GitHub automatically translates the expression #123 in a commit message into a link to that issue. A one-line title to aid browsing and search. The issue’s current status. In simple systems (like GitHub’s), each issue is either open or closed, and by default, only open issues are displayed. The ID of the issue’s creator. The IDs of people who have commented on it or modified it are also embedded in the issue’s history, which helps when using issues to manage discussions. A full description that may include screenshots, error messages, and just about anything else. Replies, counter-replies, and so on from people with an interest in the issue. Broadly speaking, people create three kinds of issues: Bug reports to describe problems they have encountered. A good bug report is a work of art, so we discuss them in detail below. Feature requests (for software packages) or tasks (for projects). These issues are the other kind of work backlog in a project: not “what needs to be fixed” but “what do we want to do next”. Questions. Many projects encourage people to ask questions on a mailing list or in a chat channel, but answers given there can be hard to find later, which leads to the same questions coming up over and over again. If people can be persuaded to ask questions by filing issues, and to respond to issues of this kind, then the project’s old issues become a customized Stack Overflow for the project. (In practice, most projects find that it’s easier to create a page of links to old questions and answers, or to rely on search to find things, since using issues this way requires someone to put in curation time.) 10.5 How can I write a good bug report? The better the bug report, the faster the response, and the more likely the response will actually address the issue Bettenburg et al. (2008). Make sure the problem actually is a bug. It’s always possible that you have called a function the wrong way or done an analysis using the wrong configuration file; if you take a minute to double-check, you could well fix the problem yourself. Try to come up with a reproducible example, or reprex. A reprex includes only the steps or lines of code needed to make the problem happen; again, you’ll be surprised how often you can solve the problem yourself as you trim down your steps to create one. Write a one-line title for the issue and a longer (but still brief) description that includes relevant details). Attach any screenshots that show the problem or (slimmed-down) input files needed to re-create it. Describe the version of the software you were using, the operating system you were running on, and anything else that might affect behavior. Describe each problem separately so that each one can be tackled on its own. (This parallels the rule about creating a branch in version control for each bug fix or feature discussed in Chapter 9.) Here’s an example of a well-written bug report with all of the fields mentioned above: ID: 1278 Creator: standage Owner: malvika Labels: Bug, Assigned Summary: wordbase.py fails on accented characters Description: 1. Create a text file called &#39;accent.txt&#39; containing the word &quot;Pumpernickel&quot; with an umlaut over the &#39;u&#39;. 2. Run &#39;python wordbase.py --all --message accent.txt&#39; Program should print &quot;Pumpernickel&quot; on a line by itself with the umlaut, but instead it fails with the message: No encoding for [] on line 1 of &#39;accent.txt&#39;. ([] shows where a solid black box appears in the output instead of a printable character.) Versions: - `python wordbase.py --version` reports 0.13.1 - Using on Windows 10. 10.6 How can I use labels to organize work? There is always more work to do than there is time to do it. Issue trackers let project members add labels to issues to manage this. A label is just a word or two; GitHub allows project owners to create any labels they want that users can then add to their issues. A small project should always use these four labels: Bug: something should work but doesn’t. Enhancement: something that someone wants added. Task: something needs to be done, but won’t show up in code (e.g., organizing the next team meeting). Discussion: something the team needs to make a decision about. (All issues can have discussion—this category is for issues that start that way.) It might also use: Question: how is something supposed to work (discussed earlier). Suitable for Newcomer or Beginner-Friendly: to identify an easy starting point for someone who has just joined the project. If you help potential new contributors find places to start, they’re more likely to do so Steinmacher et al. (2014). 10.7 How can I use labels to prioritize work? The labels listed above described what kind of work an issue describes; a separate set of labels can be used to indicate how important that work is. These labels typically have names like “High Priority” or “Low Priority”; their purpose is to help triage, which is the process of deciding what is going to be worked on in what order. In a large project, this is the responsibility of the product manager (Chapter 12); in a small one, it’s common for the project’s lead to decide this, or for project members to use up-votes and down-votes on an issue to indicate how important they think it is. However the decision is made, it’s helpful to use six more labels to record the outcomes: Urgent: this needs to be done now (typically reserved for security fixes). Current: this issue is included in the current round of work. Next: this issue is (probably) going to be included in the next round. Eventually: someone has looked at the issue and believes it needs to be tackled, but there’s no immediate plan to do it. Won’t Fix: someone has decided that the issue isn’t going to be addressed, either because it’s out of scope or because it’s not actually a bug. Once an issue is marked this way, it is usually then closed. Duplicate: this issue is a duplicate of one that’s already in the system. Again, issues marked this way are usually then closed. Larger projects will replaced “Current”, “Next”, and “Eventually” with labels corresponding to upcoming software releases, journal issues, or conferences. Unfortunately, this doesn’t work well on GitHub’s issue tracking system, since there’s no way to “retire” a label without deleting it. After a while, a project that uses labels for specific events can have a lot of old labels… 10.8 How can I use issues to enforce a workflow for a project? If you’re using GitHub, the short answer to this section’s title question is, “You can’t,” but you can create conventions, and more sophisticated issue tracking systems will let you enforce those conventions. The basic idea is to only allow issues to move from some states to others, and to only allow some people to move them. For example, Figure 10.1 shows a workflow for handling bugs in a medium-sized project: Figure 10.1: Issue State Transitions An “Open” issue becomes “Assigned” when someone is made responsible for it. An “Assigned” issue becomes “Active” when that person starts to work on it. If they stop work for any length of time, it becomes “Suspended”. (That way, people who are waiting for it know not to hold their breath.) An “Active” bug can either be “Fixed” or “Cancelled”, where the latter state means that the person working on it has decided it’s not really a bug. Once a bug is “Fixed”, it can either be “Closed” or, if the fix doesn’t work properly, moved back into the “Open” state and go around again. Workflows like this are more complex than small projects need, but as soon as the team is distributed—particularly distributed across timezones—it’s very helpful to be able to find out what’s going on without having to wait for someone to be awake. 10.9 Summary FIXME: create concept map for workflow 10.10 Exercises FIXME: exercises for backlog chapter. 10.11 Key Points Create issues for bugs, enhancement requests, and discussions. Add people to issues to show who is responsible for working on what. Add labels to issues to identify their purpose. Use rules for issue state transitions to define a workflow for a project. References "],
["style.html", "Chapter 11 Code Style and Review 11.1 Questions 11.2 Objectives 11.3 Introduction 11.4 What are the standard rules of good style for Python programs? 11.5 How can I check that code follows style guidelines? 11.6 How should I review someone else’s code: 11.7 How should I collaborate with people when doing code reviews? 11.8 In what order should functions be defined? 11.9 How can I specify default values for functions’ parameters? 11.10 How can I write functions to handle a variable number of arguments? 11.11 How can I pass an unknown number of parameters? 11.12 How can I get values out of structured data more easily? 11.13 Summary 11.14 Exercises 11.15 Key Points", " Chapter 11 Code Style and Review 11.1 Questions How should I name my variables and functions? How should I organize my code so that other people can understand it? How can I make functions more reusable? What should my code do when an error occurs? 11.2 Objectives Explain why consistent formatting of code is important. Describe standard Python formatting rules and identify cases where code does or doesn’t conform to them. Write functions whose parameters have default values. Explain which parameters should have default values and how to select good ones. Write functions that can handle variable numbers of arguments. Explain what problems can most easily be solved by creating functions with variable numbers of arguments. 11.3 Introduction To paraphrase Dobzhansky (1973), nothing in software development makes sense except in light of human psychology. This is particularly true when we look at programming style. Computers don’t need to understand programs in order to execute them, but people do in order to create them, maintain them, and fix them. The more clearly those programs are laid out, the easier it is to find things and make sense of them. This lesson therefore describes some widely-used rules of Python programming style, and some features of the language that you can use to make your programs more flexible and more readable. The biggest benefit of having a second person work on a programming project is therefore not getting twice as much code written, but having code reviewed. Study after study over more than 40 years has shown that code review is the most effective way to find bugs in software Fagan (1976),Fagan (1986),Cohen (2010)Bacchelli and Bird (2013). Despite this, it still isn’t common in research software development, in part because it isn’t part of the culture Segal (2005), but also because code review is mostly useful when the reviewers understand the problem domain well enough to comment on algorithms and design choices rather than indentation and variable naming, and the number of people who can do that for a research project is typically very small—sometimes as small as one Petre and Wilson (2014). This lesson will look at the mechanics of code review and present some short examples of the kinds of things reviewers should look for. 11.4 What are the standard rules of good style for Python programs? The single most important rule of style is to be consistent, both internally and with other programs. Python’s standard style is called PEP-8; the term “PEP” is short for “Python Enhancement Proposal”, and PEP-8 has become the most widely used standard for Python coding. Some of its rules are listed below, along with others borrowed from “Code Smells and Feels”: FIXME: add before-and-after examples for each of these. 11.4.1 Always indent code blocks using 4 spaces, and use spaces instead of tabs. 11.4.2 Do not put spaces inside parentheses I.e., don’t write ( 1+2 ). This applies to function calls as well: do not write max( a, b ). 11.4.3 Always use spaces around comparisons like &gt; and &lt;=. Use your own judgment for spacing around common arithmetic operators like + and /. 11.4.4 Use ALL_CAPS_WITH_UNDERSCORES for constants. 11.4.5 Use lower_case_with_underscores for the names of functions and variables. This naming convention is sometimes called pothole case. You should only use CamelCase for classes, which are outside the scope of this lesson. 11.4.6 Put two blank links between each function definition. This helps them stand out. 11.4.7 Avoid abbreviations in function and variable names. They can be ambiguous, and can be be hard for non-native speakers to understand. This rule doesn’t necessarily you will have to do more typing: a good programming editor will auto-complete names for you. 11.4.8 Use short names for short-lived local variables but longer names for things with wider scope. Loop indices Beniamini et al. (2017) can be i and j (provided the loop is only a few lines long). Anything that is used at a greater distance or whose purpose isn’t immediately clear (such as a function) should have a longer name. 11.4.9 Put everything in a file in order. The order of items in each file should be: The shebang line (because it has to come first to work). The file’s documentation string (Chapter 11). All of the import statements, one per line. Constant definitions. Function definitions. If the file can be run as a program, the if __name__ == '__main__' statement discussed in Section 20.5. 11.4.10 Do not comment and uncomment sections of code to change behavior. If you need to do something in some runs of the program and not do it in others, use an if statement to enable or disable that block of code. It’s much more reliable—you’re far less likely to accidentally comment out one too many lines—and you may find that you want to leave those conditional sections in the finished program for logging purposes (Chapter 6). 11.4.11 Keep functions short. Nothing should be more than a page long or have more than three levels of indentation because of nested loops and conditionals. Anything longer or more deeply nested will be hard for readers to fit into working memory; when you find yourself needing to break these limits, extract a function (Section 17.9). 11.4.12 Handle special cases at the start of the function. This helps readers mentally get them out of the way and focus on the “normal” case. Section 17.8 discusses this in more detail. 11.5 How can I check that code follows style guidelines? Checking that code conforms to guidelines like the ones above can be time consuming, but luckily it doesn’t have to be done by hand. Most languages have tools that will check code style rules and report violations. These are often called linters, after an early tool called [lint][lint] that found lint (or fluff) in C code. Python has a tool that used to be called pep8 and is now called pycodestyle that will do this. For example, this program is supposed to count the number of stop words in a document: stops = [&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;] def count(ln): n = 0 for i in range(len(ln)): line = ln[i] stuff = line.split() for word in stuff: # print(word) j = stops.count(word) if (j &gt; 0) == True: n = n + 1 return n import sys lines = sys.stdin.readlines() # print(&#39;number of lines&#39;, len(lines)) n = count(lines) print(&#39;number&#39;, n) When we run this command: $ pycodestyle count_stops.py it prints this report: src/style/count_stops_before.py:3:1: E302 expected 2 blank lines, found 1 src/style/count_stops_before.py:11:24: E712 comparison to True should be &#39;if cond is True:&#39; or &#39;if cond:&#39; src/style/count_stops_before.py:12:13: E101 indentation contains mixed spaces and tabs src/style/count_stops_before.py:12:13: W191 indentation contains tabs src/style/count_stops_before.py:15:1: E305 expected 2 blank lines after class or function definition, found 1 src/style/count_stops_before.py:15:1: E402 module level import not at top of file Fixing these issues gives us this: import sys stops = [&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;] def count(ln): n = 0 for i in range(len(ln)): line = ln[i] stuff = line.split() for word in stuff: # print(word) j = stops.count(word) if j &gt; 0: n = n + 1 return n lines = sys.stdin.readlines() # print(&#39;number of lines&#39;, len(lines)) n = count(lines) print(&#39;number&#39;, n) This program gets a clean bill of health, so it’s worth looking at in more detail. Here are things that should be changed: The commented-out print statements should either be removed or turned into proper logging statements (Chapter 6). The variables ln, i, and j should be given clearer names. The outer loop in count loops over the indices of the line list rather than over the lines. It should do the latter (which will allow us to get rid of the variable i). There’s no reason to store the result of line.split in a temporary variable: the inner loop of count can use it directly. Rather than counting how often a word occurs in the list of stop words and then adding 1 to n, we can create a set of stop words and use a simple membership test. This will be more readable and more efficient. Since the set of stop words is a global variable, it should be written in upper case. We should use += to increment the counter n. Rather than reading the input into a list of lines and then looping over that, we can give count a stream and have it process the lines one by one. Since we might want to use count in other programs some day, we should put the two lines at the bottom that handle input into a conditional so that they aren’t executed when this script is imported. After making all these changes, our little program looks like this: import sys STOPS = {&#39;a&#39;, &#39;A&#39;, &#39;the&#39;, &#39;The&#39;, &#39;and&#39;} def count(reader): n = 0 for line in reader: for word in line.split(): if word in STOPS: n += 1 return n if __name__ == &#39;__main__&#39;: n = count(sys.stdin) print(&#39;number&#39;, n) 11.6 How should I review someone else’s code: FIXME: how to review on GitHub. 11.7 How should I collaborate with people when doing code reviews? How you review is just as important as what you look for: being dismissive or combative are good ways to ensure that people don’t pay attention to your reviews, or avoid having you review their work. Equally, being defensive when someone offers suggestions politely and sincerely is very human, but can stunt your development as a programmer. Lots of people have written guidelines for doing reviews, which are also useful when reviewing written work Quenneville (2018),Sankarram (2018). A few key points are: Work in small increments. As Cohen (2010) and others have found, code review is most effective when done in short bursts. That means that change requests should also be short: anything that’s more than a couple of screens long should be broken into smaller pieces. Look for algorithmic problems first. Code review isn’t just (or even primarily) about style: its real purpose is to find bugs before they can affect anyone. The first pass over any change hould therefore look for algorithmic problems. Are the calculations right? Are any rare cases going to be missed? Are errors being caught and handled Chapter 6? Use a rubric. Linters are great, but can’t decide when someone should have used a lookup table instead of conditionals. A list of things to check for can make review faster and more comprehensible, especially when you can copy-and-paste or drag-and-drop specific comments onto specific lines (something that GitHub unfortunately doesn’t yet support). Ask for clarification. If you don’t understand something, or don’t understand why the author did it, ask. (And when the author explains it, think about suggesting that the explanation ought to be documented somewhere.) Offer alternatives. Telling authors that something is wrong is helpful; telling them what they might do instead is more so. Don’t be sarcastic or disparaging. “Did you maybe think about testing this garbage?” is a Code of Conduct violation in any well-run project. Don’t present opinions as facts. “Nobody uses X any more” might be true. If it is, the person making the claim ought to be able to point at download statistics or a Google Trends search; if they can’t, they should say, “I don’t think anybody uses X any more” and explain why they think that. Don’t feign surprise or pass judgment. “Gosh, didn’t you know [some obscure fact]?” isn’t helpful; neither is, “Geez, why don’t you [some clever trick] here?” Don’t overwhelm people with details. If someone has used the letter x as a variable name in several places, and they shouldn’t have, comment on the first two or three and simply put a check beside the others—the reader won’t need the comment repeated. Don’t ask people to do extra work. Nobody enjoys fixing bugs and style violations. Asking them to add a few features while they’re at it is rude. Don’t let people break these rules just because they’re frequent contributors or in positions of power. The culture of any organization is shaped by the worst behavior it is willing to tolerate Gruenert and Whitaker (2015). If you let people be rude to one another, that is your culture. Be specific in replies to reviewers. If someone has suggested a better variable name, you can probably simply fix it. If someone has suggested a major overhaul to an algorithm, you should reply to their comment to point at the commit that includes the fix. Thank your reviewers. If someone has taken the time to read your code carefully, thank them for doing it. 11.8 In what order should functions be defined? When encountering code for the first time, most people scan it from top to bottom. If that code is a program or script, rather than a library, its main function should be put first, and should probably be called main. After reading that function, someone should have a good idea of what the program does in what order. Three common patterns that people might match against are: Figure out what the user has asked it to do (Chapter 5). Read all input data. Process it. Write output. or: Figure out what the user has asked for. For each input file: Read. Process. Write file-specific output (if any). Write summary output (if any). or: Figure out what the user has asked for. Repeatedly: Wait for user input. Do what the user has asked. Exit when a “stop” command of some sort is received. Each step in each of the outlines above usually becomes a function. Those functions depend on others, some of which are written to break code into comprehensible chunks and are then called just once, others of which are utilities that may be called many times from many different places. FIXME: figure Different people order these differently; our preferred order is: Put all of the single-use functions in the first half of the file in the order in which they are likely to be called. Put all of the multi-use utility functions in the bottom of the file in alphabetical order. If any of those utility functions are used by other scripts or programs, they should go in a file of their own. In fact, this is a good practice even if they’re only used by one program, since it signals even more clearly which functions are in the “structural” layer and which are in the “utility” layer. 11.9 How can I specify default values for functions’ parameters? Working memory can only hold a few items at once: initial estimates in the 1950s put the number at 7 plus or minus 2 Miller (1956), and more recent estimates put it as low as 4 or 5. If your function requires two dozen parameters, the odds are very good that users will frequently forget them or put them in the wrong order. One solution is to give parameters default values (Chapter 11); another is to bundle them together so that (for example) people pass three point objects instead of nine separate x, y, and z values. A third approach (which can be combined with the preceding two) is to specify default values for some of the parameters. Doing this gives users control over everything while also allowing them to ignore details; it also codifies what you consider “normal” for the function. For example, suppose we are comparing images to see if they are the same or different. We can specify two kinds of tolerance: how large a difference in color value to notice, and how many differences above that threshold to tolerate (as a percentage of the total number of pixels). By default, any color difference is considered significant, and only 1% of pixels are allowed to differ: def image_diff(left, right, per_pixel=0, fraction=0.01): # ...implementation... When this function is called using image_diff(old, new), those default values apply. However, it can also be called like this: image_diff(old, new, per_pixel=2) allows pixels to differ slightly without those differences being significant. image_diff(old, new, fraction=0.05) allows more pixels to differ. image_diff(old, new, per_pixel=1, fraction=0.005) raises the per-pixel threshold but decrease number of allowed differences. Default parameter values make code easier to understand and use, but there is a subtle trap. When Python executes a function definition like this: def collect(new_value, accumulator=set()): accumulator.add(new_value) return accumulator it calls set() to create a new empty set, and then uses that set as the default value for accumulator every time the function is called. It does not call set() anew for each call, so all calls using the default will share the same set: &gt;&gt;&gt; collect(&#39;first&#39;) {&#39;first&#39;} &gt;&gt;&gt; collect(&#39;second&#39;) {&#39;first&#39;, &#39;second&#39;} A common way to avoid this is to pass None to the function to signal that the user didn’t provide a value: def collect(new_value, accumulator=None): if accumulator is None: accumulator = set() accumulator.add(new_value) return accumulator 11.10 How can I write functions to handle a variable number of arguments? We can often make programs simpler by writing functions that take a variable number of arguments, just like print and max. One way to to require user to stuff those arguments into a list, e.g., to write find_limits([a, b, c, d]). However, Python can do this for us. If we declare a single argument whose name starts with a single *, Python will put all “extra” arguments into a tuple and pass that as the argument. By convention, this argument is called args: def find_limits(*args): print(args) find_limits(1, 3, 5, 2, 4) (1, 3, 5, 2, 4) This catch-all parameter can be used with regular parameters, but must come last in the parameter list to avoid ambiguity: def select_outside(low, high, *values): result = [] for v in values: if (v &lt; low) or (v &gt; high): result.add(v) return result print(select_outside(0, 1.0, 0.3, -0.2, -0.5, 0.4, 1.7)) [-0.2, -0.5, 1.7] An equivalent special form exists for named arguments: the catch-all variable is conventionally called kwargs (for “keyword arguments”) and its name is prefixed with ** (i.e., two asterisks instead of one). When this is used, the function is given a dictionary of names and values rather than a list: def set_options(tag, **kwargs): result = &#39;&lt;{}&#39;.format(tag) for key in kwargs: result += &#39; {}=&quot;{}&quot;&#39;.format(key, kwargs[key]) result += &#39;/&gt;&#39; return result print(set_options(&#39;h1&#39;, color=&#39;blue&#39;)) print(set_options(&#39;p&#39;, align=&#39;center&#39;, size=&#39;150%&#39;)) &lt;h1 color=&quot;blue&quot;/&gt; &lt;p align=&quot;center&quot; size=&quot;150%&quot;/&gt; Notice that the names of parameters are not quoted: the call is color='blue' and not 'color'='blue'. 11.11 How can I pass an unknown number of parameters? We can use the inverse of *args and *kwargs to match a list of values to arguments. In this case, we put the single * in front of the list when calling the function rather than in front of the parameter when defining it: def trim_value(data, low, high): print(data, &quot;with&quot;, low, &quot;and&quot;, high) parameters = [&#39;some matrix&#39;, &#39;lower bound&#39;, &#39;upper bound&#39;] trim_value(*parameters) some matrix with lower bound and upper bound 11.12 How can I get values out of structured data more easily? Modern programming languages have lots of other tools to make life more convenient for programmers. One that’s particularly useful is destructuring: [first, [second, third]] = [1, [2, 3]] print(first) print(second) print(third) 1 2 3 As this example shows, if the variables on the left are arranged in the same way as the values on the right, Python will automatically unpack the values and assign them correctly. This is particularly useful when looping over lists of structured values: people = [ [[&#39;Kay&#39;, &#39;McNulty&#39;], &#39;mcnulty@eniac.org&#39;], [[&#39;Betty&#39;, &#39;Jennings&#39;], &#39;jennings@eniac.org&#39;], [[&#39;Marlyn&#39;, &#39;Wescoff&#39;], &#39;mwescoff@eniac.org&#39;] ] for [[first, last], email] in people: print(&#39;{} {} &lt;{}&gt;&#39;.format(first, last, email)) Kay McNulty &lt;mcnulty@eniac.org&gt; Betty Jennings &lt;jennings@eniac.org&gt; Marlyn Wescoff &lt;mwescoff@eniac.org&gt; 11.13 Summary George Orwell laid out six rules for good writing, the last and most important of which is, “Break any of these rules sooner than say anything outright barbarous.” There will always be cases where your code will be easier to understand if you don’t do the things described in this lesson, but there are probably fewer of them than you think. FIXME: create concept map for review. 11.14 Exercises FIXME: create exercises for review. 11.15 Key Points The brain thinks every difference is significant, so removing unnecessary differences in formatting reduces cognitive load. Python software should always conform to the formatting the rules in PEP 8. Use name=value to define a default value for a function parameter. Use *args to define a catch-all parameter for functions taking a variable number of unnamed arguments. Use **kwargs to define a catch-all parameter for functions taking a variable number of named arguments. Use destructuring to unpack data structures as needed. References "],
["process.html", "Chapter 12 Development Process 12.1 Questions 12.2 Objectives 12.3 Introduction 12.4 What is agile development? 12.5 What are the feedback loops in agile development? 12.6 What are the risks associated with agile development? 12.7 What is sturdy development? 12.8 What is the role of a product manager? 12.9 What is the role of a project manager? 12.10 What are the risks associated with sturdy development? 12.11 Why, when, and how should our team start to cut corners? 12.12 Summary 12.13 Key Points", " Chapter 12 Development Process 12.1 Questions \"How can a team develop software systematically? 12.2 Objectives \"Explain what a software development process is. \"Define the key features of an agile development process. \"Describe the key roles in a planning-based software development process. \"Explain the true purpose of a schedule. 12.3 Introduction A software development process is the steps a team goes through to create, deliver, and maintain software. Broadly speaking, software development processes can be divided into three groups: Chaotic: everyone’s doing something, but there’s no overall plan or consistency. An agile process based on lots of short steps with frequent feedback and course correction. A sturdy process that invests a lot of effort in planning out work. (This label is made up: historically, planning-based approaches were described first and didn’t have a particular name, but then agile came along and we needed one.) Most self-taught teams start with a chaotic process. This material is meant to help you move to something better. 12.4 What is agile development? Agile development starts from three related premises: You can’t plan a software project very far in advance because requirements and technology are constantly changing. You shouldn’t try to plan everything in advance because you’re going to learn what’s possible, and your users are going to learn what they actually want, as the work progresses. You can afford not to lock yourself into a long-term plan because software is much more malleable than concrete or steel. This approach came into prominence with the rise of the web in the 1990s: The web made it possible to release software weekly, daily, or even hourly, since updating a server is a lot faster, and a lot less expensive, than shipping CDs to thousands of people. Multi-year development plans don’t make a lot of sense when everything they depend on will be obsolete by the time work starts, much less by the time it finishes. The growth of the web was aided by, and fuelled, the growth of the open source movement. People couldn’t help noticing that most open source projects didn’t have long-range plans, but nevertheless produced high-quality software faster than many closed-source commercial projects. 12.5 What are the feedback loops in agile development? At its core, agile development relies on continuous feedback. Agile methods break development down into short iterations, typically no more than two weeks long, and often as short as a single day. In each iteration, the team decides what to build next, then designs it, builds it, tests it, and delivers it. Feedback loops at several scales help them get this right and do it better next time. The iteration itself is the primary feedback loop. Users often don’t know what they want until they see it, so short cycles are a way to avoid spending too much time building what turns out to be the wrong thing. Short iterations help improve efficiency in two other ways as well. First, most people can keep track of what they’re doing for a few days at a time without elaborate Gantt charts, so short cycles allow them to spend proportionally less time coordinating with one another. Second, finding bugs becomes easier: instead of looking through weeks’ or months’ worth of software to find out where the problem is, developers usually only have to look at what’s been written in the last few days. A typical working day starts with a stand-up meeting where everyone in the team reports what they did since the last meeting, what they’re planning to do next, and what’s blocking them (if anything). (It’s called a “stand-up” meeting because it’s usually held standing up, which encourages people to stay focused.) For example, my report earlier this week was: Yesterday: fixed a bug in citation handling; got the Shiny example to update properly; added slides to the tutorial on loading data in Shiny applications. Today: update the script that checks for unused bibliography entries; add tests for handling missing data files in the Shiny tutorial. Blockers: should we report “no permission” separately from “missing file” in the Shiny tutorial? Stand-up meetings are another agile feedback loop. Each day, the team gets feedback on the progress they’re making, whether they’re still on track to meet the iteration’s goals, whether the technical decisions they’re making are paying off, and so on. The key to making this work is that each task is at most a day long. Anything longer is broken into sub-tasks so that there’s something to report at every meeting. Without this rule, it’s all too easy for someone to say, “Still working on X,” several days in a row, so feedback and the possibility of early course correction are lost. Once the stand-up meeting is over, everyone gets back to work. In many agile teams, this means sitting with a partner and doing pair programming. One person, called the driver, does the typing, while the other person, called the navigator, watches and comments; every hour or so, the pair switch roles. Pair programming is beneficial for several reasons. First, the navigator will often notice mistakes in the driver’s code, or remember design decisions that the driver is too busy typing to recall. This is the tightest of the feedback loops that make agile work, since feedback is nearly continuous. Second, pair programming spreads knowledge around: every piece of code has been seen by at least two people, which reduces the risk of “But I didn’t know” mistakes. It also helps people pick up new skills: if you have just seen someone do something with two clicks, you will probably do it that way when it’s your turn to drive rather than spending two minutes doing it the way you always have. And finally, most people are less likely to check Facebook every five minutes if someone else is working with them… As well as pair programming, most agile teams use two other practices. Test-driven development (TDD), discussed in Chapter 7, is the practice of writing unit tests before writing application code. The second is continuous integration. Every time someone commits code to the version control repository, an automated process checks out a clean copy of the code, builds it, runs all the tests, and posts the results somewhere for the whole team to see. If any of the tests fail, the continuous integration system notifies people by sending out email or texting them. We will explain how to implement continuous integration in Chapter 13. From a developer’s point of view, its key benefit is that it ensures the project is always in a runnable state. This may not seem important if you don’t have to hand it in until next week, but is critical as soon as two or more people are involved in development. It’s very frustrating (and unproductive) for someone to be blocked because someone else has broken something but not realized it. The final key practice in agile development is the post-mortem. At the end of every sprint, the team should get together and ask themselves what went well, what could be improved, and what new things they want to try (if any). 12.6 What are the risks associated with agile development? The biggest risk associated with agile is that people can use the term to excuse a process that’s still actually chaotic. Most people don’t like writing plans before they code or documenting what they’ve done; coincidentally, agile doesn’t require them to do much of either. It’s therefore all too common for developers to say “we’re agile” when what they mean is “we’re not going to bother doing anything we don’t want to”. In reality, agile requires more discipline, not less, just as improvising well requires even more musical talent than playing a score exactly. At the same time, many people don’t like having to make decisions: after two decades of schooling, they want to be told what the assignment is and exactly what they have to do to get an ‘A’, a ‘B’, or whatever grade they’re shooting for. Some become quite defensive when told that figuring out what to do is now part of their job, but that’s as essential to agile development as it is to scientific research. Finally, FIXME: customer buy-in. 12.7 What is sturdy development? As we said in the introduction, the alternative to agile development doesn’t have a widely-accepted name; we call it “sturdy”, since “traditional”, “old-fashioned”, and “process-heavy” all sound pejorative. Whatever it’s called, it’s key feature is planning work in advance. If you’re going to spend three days driving across the country, it makes sense to spend half an hour figuring out a good route. Equally, if you’re going to spend several months building a complex piece of software, and you know what the final result is supposed to look like, it makes sense to spend some time figuring out what you’re going to do and how long it ought to take. In order to explain how to do this systematically, we need to look at two roles in large software projects: the product manager and the project manager. 12.8 What is the role of a product manager? The product manager is the person responsible for the software’s feature list. Typically, while developers are building Version N, she is talking to users in order to find out what should go into Version N+1. She doesn’t ask them what features they want, because if she does, what she’ll get is a mish-mash of buzzwords plucked from the lead paragraphs of popular articles. (Blockchain! Blockchain all the things!) Instead, she asks, “What can’t you do right now that you want to?”, “What do you find irritating in the current product?”, and, “Why are you using other software instead of ours?” She then translates the answers into a list of changes to be considered for Version N+1. The product manager also talks to the software’s developers to find out what they don’t like about the current software and adds their wishes to the pile. Typically, these are things like “reorganize the database interface” and “clean up the build process”. So, it’s Monday morning. Version N shipped last Thursday (because you should never, ever ship on a Friday). the team has had a weekend to catch its collective breath and is ready to start work once again. (If people are so burned out from the previous round of work that they need a whole week to recover, read the discussion of crunch mode in Chapter 23.) At this point, the product manager divides up the list of desired features and assigns a few to each developer. They then have some time—typically a few days to a couple of weeks—to do a little research, write some throwaway prototype code, and most importantly to think. How could this feature be implemented? Is there a quick-and-dirty alternative that would only deliver half of what was asked for but could be done in a tenth of the time? What impact will each alternative have on the installation process? And how will the new feature be tested? This process is called analysis and estimation (A&amp;E). The result is a collection of short proposals, each typically half a page to half a dozen pages long. There’s no set form for these, but they usually include whatever background information a well-informed developer is unlikely to already know, a discussion of the alternatives, lessons learned from any prototyping that was done, and most importantly, an estimate of how much time would be needed to build each alternative. This time includes estimates from QA (for testing), the technical writer (for documenting), the people responsible for the build and creating the installer, and so on. So now it’s Monday morning again. Three weeks have gone by and all the A&amp;E’s are done. When the time estimates are totalled, they come to 700 developer-days. Unfortunately, there are only 240 available: the size of the team is fixed, and the next release has to be available in May. This is normal: there is never enough time to add everything that everyone wants. What the product manager does now is draw a 3x3 grid. The X axis is labeled “effort”, the Y axis, “importance”, and each axis is divided into “low”, “medium”, and “high”. (Finer-grained divisions, such as a 1–10 scale, add no value: nobody has an algorithm for distinguishing priority 6 items from priority 7 items, and anyway, the grid is just to get conversation going.) Each feature’s name now goes in one of the nine grid cells, and the product manager steps out of the way. 12.9 What is the role of a project manager? The project manager’s role is to turn priorities and effort estimates into a schedule and then ensure that the schedule is met. First, the high-effort, low-importance features in the grid are crossed off: they’re simply not worth doing. The project manager may decide to tackle some low-effort, high-value items first in order to ensure that if things go badly wrong, something useful will still have been accomplished Alternatively, she may decide to go after some high-effort items at the start, on the theory that if they aren’t tackled early, they’ll keep getting pushed off. Either way, the items on the diagonal are the ones that have to be argued over. Should the team tackle Feature 14 (high effort, high importance) or Features 18, 19, and 22 (individually lower importance, but the same total effort)? It can take rounds of discussion to sort this out; what’s important is that people don’t tweak effort estimates in order to squeeze things in, because if they do, the people doing the estimating will start padding their numbers in self defense. Since project managers aren’t stupid, they’ll shave the estimates even more, so developers will add even more padding, and pretty soon the whole thing becomes science fiction. 12.10 What are the risks associated with sturdy development? The hardest step in this process for beginners is coming up with time estimates. How can you possibly guess how long it will take to write a database interface if you’ve never used one before? There are two answers: You’re not expected to pull an number out of thin air (at least, not by managers who know what they’re doing). Instead, you should budget enough time to write some throwaway code, or download and try out an open source tool, in order to get a feel for it. You’ve had to learn other new technologies before. A guess based on that experience might be off by a factor of two or three, but it probably won’t be off by a factor of ten. Even if it is, it’s better than no guess at all, provided everyone involved knows that it’s a guess. (You will meet people who will be very critical every time one of your estimates is wrong. In my experience, they are no better at estimating than anyone else.) The more estimating you do, the better you’ll get, and the better you are, the more smoothly your projects will run. If you’re an undergraduate, your project will probably have to fit in one or two terms. You will therefore probably be asked to go around the planning loop once or twice, which in turn determines how much you’ll be expected to deliver in each iteration. This is called time boxing: you specify how long a cycle will last, then see how much work you can fit into that interval. The alternative is feature boxing: decide what you want to do, then build a schedule that gives you enough time to do it. Time boxing generally works better even in projects that don’t have to line up with semester boundaries, since it encourages developers to take smaller steps, and allows them to give customers more frequent demos (which serve as course corrections). 12.11 Why, when, and how should our team start to cut corners? Contrary to popular belief, a schedule’s primary purpose is not to tell you what you’re supposed to be doing on any given day. Instead, it is is to tell you when you should start cutting corners. Suppose, for example, that you have ten weeks in order to accomplish some task. Five weeks after you start, you’ve only done the first third of work. You have several options: Ignore the problem. This is very popular, but doesn’t actually solve the problem. Work longer hours. This is also very popular, but as Chapter 23 explains, it is self-defeating. Enlarge the team. This is a good strategy if you’re peeling carrots, but usually doesn’t work on other kinds of projects because it takes more time to bring someone up to speed than it does to do the work yourself. Move the deadline back. Product groups and research teams often do this (usually in combination with the previous solution), but it usually isn’t an option for course projects. Instructors have to submit marks at the end of the term, and conference deadlines are fixed as well; sometimes, whatever hasn’t been done by the due date might as well not be done at all. Cut corners. The best approach is to update the schedule to reflect the rate at which you’re actually working, then drop features that it tells you can’t be finished in time. Let’s go back to our example. At the start of the project you believed it would take ten weeks. You’re now at week five, but you’ve done only the first four weeks’ worth of work, so your estimates for how long tasks would take were too optimistic by about 25%. You should therefore go back to your schedule and add 1/4 to each task’s estimate. That inevitably means that some of the things you originally planned to do now spill off the end of your ten-week window. That’s OK; it’s a shame you won’t get to them, but at least you can start taking action now rather than trying to recover from a disaster. People will thank you for this. “I’m sorry, we’re not going to have the frobnosticator for May 1” is OK in January, since it gives whoever was counting on the frobnosticator time to make other plans. It is not OK on April 30; neither is saying that it’s “done”, but full of bugs. These calculations are the responsibility of the project manager. Her job is to make sure everyone is doing what they’re supposed to be doing, to shield the team from interruptions (there are always interruptions), and to track the team’s progress. She periodically compares how much has actually been done with how much was supposed to be done and adjusts plans accordingly. Taking scheduling seriously is one of the things that distinguishes good teams from bad ones. It’s unfortunate that most students only get to do it once or twice in their courses, since you only really see the benefits with practice, but even a couple of rounds of practice can make a big difference. 12.12 Summary FIXME: summarize process 12.13 Key Points \"A software development process can be chaotic, agile, or sturdy. \"Agile development depends on feedback loops at several scales. \"Pair programming provides realtime feedback on code as it is written. \"The primary responsibility of a product manager is to translate users’ needs into features and priorities. \"The primary responsibility of a project manager is to create and maintain a schedule. "],
["integrate.html", "Chapter 13 Continuous Integration 13.1 Questions 13.2 Objectives 13.3 Introduction 13.4 How can I run commands automatically every time a repository is updated? 13.5 How can I tell Travis that something went wrong? 13.6 How can I display the status of a repository? 13.7 How can I automatically run tests for my project when things change? 13.8 Summary 13.9 Exercises 13.10 Key Points", " Chapter 13 Continuous Integration 13.1 Questions How can I tell what state my project is actually in? 13.2 Objectives Explain how continuous integration works. Configure continuous integration for a small software project. 13.3 Introduction Continuous integration (CI) is a simple idea: every time someone commits code to a repository, that code should automatically be compiled and checked and the results posted where people can see them. If the build or the tests fail, the person responsible for the problem should be notified immediately so that they can fix problems while the changes are still fresh in their head (and before anyone else has to deal with them). Taking it one step further, all of these automatic checks can be run on every branch so that people only create pull requests for things that are working (Chapter 9). The most widely used continuous integration system is Travis CI. It integrates well with Github, and will run tests on multiple platforms and with multiple versions of tools. Developers still have to build the tests—CI only as good as the tests it runs—but those tests can check the code’s style as well as its correctness (Chapter 11). This lesson will show you how to set up Travis for your project. 13.4 How can I run commands automatically every time a repository is updated? The first step is to tell Travis about your repository. Log in to Travis-CI with your GitHub credentials; on the left, beside “My Repositories”, click “+” to add a repository that you have already created on GitHub. Once you have found it, flick the switch. Next, create a file called .travis.yml in the root directory of the repository. Note the leading . in the name: like many systems, Travis puts its configuration file in the project’s root directory (Chapter 5), but uses a leading dot so that the file won’t clutter the listing from ls. A simple .travis.yml configuration file looks like this: language: &quot;python&quot; python: - &quot;3.6&quot; install: - &quot;pip install -r requirements.txt&quot; script: - &quot;python src/continuous/hello.py&quot; The language key tells Travis what language we’re using. We then use python to specify the version (or versions) of Python to use (we can ask Travis to test our project with several different versions). install tells Travis how to install the software needed for testing; by convention, we put a list of packages in requirements.txt for pip to use (Chapter 20). Finally, script tells Travis how to actually run the tests. We can put almost anything here, provided it does not need to interact with a human being (e.g., provided it doesn’t prompt with yes/no questions during package installation). The third and final step is to create that test script. Ours is called hello.py, and is in the ./src/continuous directory of the project containing this lesson: #!/usr/bin/env python print(&#39;Hello, continuous&#39;) The first line is a shebang that tells the operating system to run our script with Python rather than with the Bash shell. #!/Users/pterry/anaconda3/bin/python would run a particular version of Python, but /usr/bin/env some_program_name finds the program you want, so if Python is installed somewhere else, this still works. Once we commit these changes, Travis is set up. Every time a commit is made to this branch, it will: Create a new Linux image. Install the desired version of Python (or clone an existing image that has it, which is faster). Install the software described by requirements.txt. Run the hello.py script. Report the results at https://travis-ci.org/user/repo. Travis’s summary report tells us what happened (Figure 13.1). The detailed log has lots of information: there are 397 lines hidden under “Build system information” and another 23 under “pip install” heading (Figure 13.2). Figure 13.1: Travis Summary Report Figure 13.2: Travis Log 13.5 How can I tell Travis that something went wrong? The most important thing is the test program’s exit status. An exit status of 0 means “nothing went wrong”, and it is the default if we don’t specify anything else. We can force this exit status by ending our script with sys.exit(0). The operating system interprets any non-zero exit status as a shell error code. sys.exit(1) means “something went wrong”, and you will rarely have to worry about other codes like 127 for “command not found” and 130 for “terminated with Control-C”. We can test this by telling Travis to run a second script: language: &quot;python&quot; python: - &quot;3.6&quot; install: - &quot;pip install -r requirements.txt&quot; script: - &quot;python src/continuous/hello.py&quot; - &quot;python src/continuous/failure.py&quot; and by making that script report failure on purpose: #!/usr/bin/env python import sys print(&#39;And this command fails&#39;) sys.exit(1) When we commit these changes and view the repository’s status page, we are initially told that our build is queued (Figure 13.3). We don’t need to refresh the page: when the build starts, the page updates automatically. When when the build finishes, the summary turns red and the log displays an error message (Figure 13.4). Figure 13.3: Travis Queued Figure 13.4: Travis Failure 13.6 How can I display the status of a repository? Travis’s dashboard is very useful, but we would also like to display the status of the build on GitHub because that’s where most people will look. To do this, we can at the top of the status page and find the build icon (Figure 13.5). Figure 13.5: Travis Build Icon Clicking on it brings up a dialog where we can select the Markdown we need for the master branch. Paste this into the file README.md in the root directory of the master branch of your repository, commit, and push to GitHub. (Note that the ./README.md is the project’s home page on GitHub, not the root of the GitHub Pages site. While we are waiting for Travis to finish building our site, we can take a look at the “Branches” tab of our repository. Clicking on a check mark or an X will bring up details of that build on that branch (Figure 13.6). Sure enough, the page displays a red X once the build on master completes. We can now modify .travis.yml to remove the failing script, commit, wait for an email to arrive to tell us that the build has completed, and then go to the project on GitHub (Figure 13.7). Figure 13.6: Travis Overall Figure 13.7: Travis GitHub Icon 13.7 How can I automatically run tests for my project when things change? FIXME: describe how to run actual tests with Travis-CI - will this require a project or projects in a separate repository? 13.8 Summary FIXME: create concept map for integration. FIXME: forward ref to Section 15.8. 13.9 Exercises FIXME: exercises for integration. 13.10 Key Points Continuous integration rebuilds and/or re-tests software every time something changes. Use continuous integration to check changes before they are inspected. Check style as well as correctness. "],
["remote.html", "Chapter 14 Working Remotely 14.1 Questions 14.2 Objectives 14.3 Introduction 14.4 How are connections to remote computers implemented? 14.5 How can I run commands on a remote computer? 14.6 How can I execute commands on a remote computer without logging in? 14.7 How can I copy files to and from remote computers? 14.8 How can I avoid typing my password over and over again? 14.9 Summary 14.10 Exercises 14.11 Key Points", " Chapter 14 Working Remotely 14.1 Questions How can I work with remote computers? 14.2 Objectives Explain what SSH is and when it should be used. Use ssh to connect to a remote computer. Use scp to copy files to and from remote computers. Generate and install an SSH key pair. 14.3 Introduction Computers keep getting faster, but data keeps getting bigger. While today’s laptops would have been considered supercomputers a decade ago, they cannot store or process all of the information that researchers have available to them. Even if they could, much of that information is too sensitive to be put on anyone’s personal machine. This lesson will therefore introduce you to some tools that can be used to connect to and work on remote computers. 14.4 How are connections to remote computers implemented? When we use the Unix shell on a desktop or laptop computer, the first step is to log in so that the operating system knows who we are and what we’re allowed to do. We do this by typing our username and password; the operating system checks those values against its records, and if they match, runs a shell for us. As we type commands, the characters we’re typing are sent from the keyboard to the shell. The shell displays those characters on the screen to represent what we type, and then, if what we typed was a command, the shell executes it and displays its output (if any). FIXME: diagram What if we want to run some commands on another machine, such as the cluster in the basement? To do this, we have to first log in to that machine. We call this a remote login. In order for us to do this, we have to run a client program that can talk to that server. The client program passes our login credentials to the remote server; if we are allowed to login, that server then runs a shell for us on the remote computer. Once our local client is connected to the remote server, everything we type is passed to the shell running on the remote computer. That remote shell runs those commands on our behalf, just as a local shell would, then sends back output, via the server, to our client, for our computer to display: FIXME: diagram The most important thing about the connection between the local and remote computers isn’t its reliability, but how secure it is. In the early days of the Internet, two programs called rsh and rcp were frequently used to run an interactive shell on a remote computer and copy files back and forth between computers. However, since the messages they sent were not encrypted, anyone who wanted to snoop on the traffic between the two machines could do so. The SSH protocol was invented to solve this problem. It uses several sophisticated (and heavily tested) encryption protocols to keep the traffic between computers confidential. It can be used interactively via two programs called ssh (which stands for “secure shell”) and scp (which stands for “secure copy”). 14.5 How can I run commands on a remote computer? To log in to a remote computer, we use the command ssh username@hostname. If the login is successful, we will be presented with a shell prompt that just happens to be coming from somewhere else. Typing exit or Control-D terminates the remote shell, just as it would terminate a local shell, and returns us to our previous shell. FIXME: diagram In the example below, the local machine’s command prompt is earth$ and the remote computer’s is moon$. # Check current directory on local computer. earth$ pwd /users/pterry # Log in to remote computer `moon.euphoric.edu` as `tp`. earth$ ssh tp@moon.euphoric.edu Password: ******** # Check name of computer on which remote shell is running. moon$ hostname moon # Check current directory on remote computer. moon$ pwd /home/tp # List contents of directory on remote computer. moon$ ls -F bin/ cheese.txt dark_side/ rocks.cfg # End remote shell session. moon$ exit # Confirm that we are back in the starting directory on the local computer. earth$ pwd /users/pterry 14.6 How can I execute commands on a remote computer without logging in? Suppose we want to check whether the file backups/results-2011-11-12.dat exists on a remote computer. Instead of logging in and then typing ls, we can tell ssh to run just that single command: $ ssh pterry@moon.euphoric.edu &quot;ls results*&quot; Password: ******** results-2018-09-18.dat results-2018-10-04.dat results-2018-10-28.dat results-2018-11-11.dat ssh takes the argument after username@computer and passes it to the remote shell; in this case the command contains a space, so we have to enclose it in quotes. The remote shell runs ls results for us and sends the output back to our local shell for display. 14.7 How can I copy files to and from remote computers? To copy a file with cp, we specify the source and destination paths. The secure copying command scp works exactly the same way, except either or both of the paths may contain a hostname. If we leave out the hostname, scp assumes we mean the machine we’re running on. For example, this command copies our latest results to the backup server, printing out the status as it does so: $ scp results.dat pterry@moon.euphoric.edu:backups/results-2018-11-11.dat Password: ******** results.dat 100% 9 1.0 MB/s 00:00 Note the colon :, separating the hostname of the server and the pathname of the file we are copying to. It is this character that informs scp that the source or target of the copy is on the remote machine. Copying a whole directory uses the same syntax as the cp command: we just add the -r option to signal that we want to copy recursively. For example, this command copies all of our results from the backup server to our laptop: $ scp -r pterry@moon.euphoric.edu:backups ./backups Password: ******** results-2018-09-18.dat 100% 7 1.0 MB/s 00:00 results-2018-10-04.dat 100% 9 1.0 MB/s 00:00 results-2018-10-28.dat 100% 8 1.0 MB/s 00:00 results-2018-11-11.dat 100% 9 1.0 MB/s 00:00 ssh can even be used in a pipe: earth$ ssh pterry@moon.euphoric.edu &quot;ls results&quot; | sort -r results-2018-11-11.dat results-2018-10-28.dat results-2018-10-04.dat results-2018-09-18.dat FIXME: diagram 14.8 How can I avoid typing my password over and over again? Typing our password over and over again is annoying, especially if the commands we want to run remotely are in a loop. To avoid doing this, we can create an SSH key to tell the remote machine that it should always trust us. SSH keys come in pairs: a public key that gets shared with services like GitHub, and a private key that is stored only on your computer. If the keys match, you’re granted access. The cryptography behind SSH keys ensures that no one can reverse engineer your private key from the public one. You might already have an SSH key pair on your machine. To check, see if the directory ~/.ssh exists (i.e., if there is a directory called .ssh with a leading . in its name directly below your home directory): $ ls ~/.ssh If a file called id_rsa.pub is there, you already have a key pair and don’t need to create a new one. If you don’t see id_rsa.pub, use the following command to generate a new key pair (making sure to replace your@email.com with your own email address): $ ssh-keygen -t rsa -C &quot;your@email.com&quot; When asked where to save the new key, hit enter to accept the default location. You should see a message like this, with your username instead of pterry: Generating public/private rsa key pair. Enter file in which to save the key (/Users/pterry/.ssh/id_rsa): You will then be asked to provide an optional passphrase. This can be used to make your key even more secure, but if what you want is avoiding type your password every time you can skip it by hitting enter twice. Enter passphrase (empty for no passphrase): Enter same passphrase again: When the key generation is complete you should see the following confirmation: Your identification has been saved in /Users/pterry/.ssh/id_rsa. Your public key has been saved in /Users/pterry/.ssh/id_rsa.pub. The key fingerprint is: 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your@email.com The key&#39;s randomart image is: +--[ RSA 2048]----+ | | | | | . E + | | . o = . | | . S = o | | o.O . o | | o .+ . | | . o+.. | | .+=o | +-----------------+ (The random art image is an alternative way to match keys, which we won’t use.) You now you need to place a copy of your public key on any remote computer you would like to connect to without typing your password. The content of your public key is in ~/.ssh/id_rsa.pub: $ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA879BJGYlPTLIuc9/R5MYiN4yc/YiCLcdBpSdzgK9Dt0Bkfe3rSz5cPm4wmehdE7GkVFXrBJ2YHqPLuM1yx1AUxIebpwlIl9f/aUHOts9eVnVh4NztPy0iSU/Sv0b2ODQQvcy2vYcujlorscl8JjAgfWsO3W4iGEe6QwBpVomcME8IU35v5VbylM9ORQa6wvZMVrPECBvwItTY8cPWH3MGZiK/74eHbSLKA4PY3gM4GHI450Nie16yggEg2aTQfWA1rry9JYWEoHS9pJ1dnLqZU3k/8OWgqJrilwSoC5rGjgp93iu0H8T6+mEHGRQe84Nk1y5lESSWIbn6P636Bl3uQ== your@email.com Copy this to your clipboard, then log in to the remote server with your username and password: $ ssh pterry@moon.euphoric.edu Password: ******** and paste the content of the clip at the end of ~/.ssh/authorized_keys on the remote computer. After you have appended the public key, logout of the remote machine and try login again. If you set up your SSH key correctly you won’t need to type your password. $ ssh pterry@moon.euphoric.edu &quot;ls results&quot; results-2018-09-18.dat results-2018-10-04.dat results-2018-10-28.dat results-2018-11-11.dat There are several other ways to accomplish this task. For example, you can scp the file ~/.ssh/id_rsa.pub from your local computer to the remote computer, then use a text editor to add it to ~/.ssh/authorized_keys on the remote computer. Whatever you do, be careful not to overwrite the content of the remote authorized_keys file. 14.9 Summary FIXME: create concept map for SSH and its kin. 14.10 Exercises FIXME: create exercises for SSH and its kin. 14.11 Key Points ssh is a secure way to log in to a remote computer. scp is a secure way to copy files to and from a remote computer. SSH is a secure alternative to username/password authorization. SSH keys are generated in pairs: the public key can be shared with others, but the private keys stays on your machine only. "],
["tools.html", "Chapter 15 Other Tools 15.1 Questions 15.2 Objectives 15.3 Introduction 15.4 What should I look for in an editor? 15.5 What are symbolic debuggers and how can they help me? 15.6 How can I tell why my program is running slowly? 15.7 How can I get all of these tools in one go? 15.8 How can I run tasks at regular times? 15.9 Summary 15.10 Exercises 15.11 Key Points", " Chapter 15 Other Tools 15.1 Questions What other tools can help me be more productive? 15.2 Objectives List the criteria for choosing a good program editor. Explain what a symbolic debugger is and demonstrate its use. Explain what profiling is and describe two approaches to profiling. Explain what integrated development environments are and what advantages they have over standalone tools. Explain how to execute tasks at specified times. 15.3 Introduction The previous chapters have explored several key tools in depth, but there are many others that will improve your productivity. You shouldn’t try to pick them all up at once, but each will help you do something in less time and with less pain. 15.4 What should I look for in an editor? The next most important tool after a version control system is your editor. There are literally thousands to choose from; if you want a plain text editor, your choices range from the very small (such as Pico, which is included in most Linux installations) to the very large (like Emacs, whose name doesn’t actually stand for “eighty megabytes and constantly swapping” and which isn’t actually a Lisp-based operating system in disguise). There are also WYSIWYG tools like Microsoft Word, LibreOffice, and Google Docs, though these usually can’t be used for programming, since they insert non-ASCII characters and formatting information in files (even files that look unformatted). You probably already have a favorite editor. If you’re like most programmers, you will change jobs, languages, operating systems, and nationality before you’ll switch to another, because it’s taken weeks or months for your hands to master the current one. However, if your editor doesn’t pass the following tests, switching now will save you enough time and grief in the future to make the temporary loss of productivity worthwhile. FIXME: keep this personal? Is it kind to your wrists? If ergonomics was a standard part of the undergraduate curriculum, programming editors would probably be used as examples of bad design. I was an Emacs power user for many years, and have paid a heavy price for it: I can’t type for more than half an hour now before my hands start to hurt from all those control-key combinations and long reaches for the escape key. Is it portable? I’m writing this on Windows; an hour from now I’ll be using my Mac laptop, and on Monday I’ll be back on a Linux machine at work. I don’t know what I’ll be using next year, but what I do know is that I won’t want to have to retrain my hands to use it. You should therefore pick an editor that’s portable across all three major operating systems. Is it syntax-aware? Human beings shouldn’t have to indent code or count parentheses to make sure they have closed all the ones they opened. They equally shouldn’t have to type every character in the name of every function or method they call: their editor should do these things for them. This won’t just save wear and tear on your wrists: letting the machine do it will make the text you create more uniform, and hence more readable (Chapter 11). Is it programmable? Just like spreadsheets, games, and other applications, editors are built out of functions. Good ones let you call those functions yourself so that you can write small programs (usually called macros) to automate common operations. As a programmer, you have the skills to make your tools do more than their creators intended, so you should find tools that allow you to. Does it handle Unicode characters? Most programmers don’t speak English as a first language, and spell their names using symbols that aren’t in the old 7-bit ASCII character set. Programming languages haven’t caught up yet—they still insist on using &lt;=, for example—but at the very least, you have to be able to edit data and documentation that include Greek, Tamil, and the like. (See [Spolsky2003] for a developer-oriented introduction to character sets in general and Unicode in particular.) One thing that isn’t on the list above is syntax highlighting. Most programming editors do it by default, but the most recent empirical studies show that it has little or no benefit Hannebauer, Hesenius, and Gruhn (2018). On the other hand, many programmers claim to find it useful, and there’s no evidence it does any harm, so we have turned it on for the code samples in these lessons. 15.5 What are symbolic debuggers and how can they help me? A symbolic debugger is a program that allows you to control and inspect the execution of other programs. You can step through the target program a line at a time, display the values of variables or expressions, look at the call stack, or (my personal favorite) set breakpoints to say, “Pause the program when it reaches this line”. Depending on the language you’re using, you may have to compile your program with certain options turned on to make it debuggable, but that’s a small price to pay for the hours or days a debugger can save you when you’re trying to track down a problem. Some debuggers, like GDB, are standalone programs, while others are build into integrated development environments (discussed below). Both are better than adding print statements to your program and re-running it because: adding print statements takes longer than clicking on a line and setting a breakpoint; adding print statements distorts the code you’re debugging by moving things around in memory, altering the flow of control, and/or changing the timing of thread execution; and it’s all too easy to make a mistake in the print statement—few things are as frustrating as wasting an afternoon debugging a problem, only to realize that the print statement you copied and pasted isn’t displaying the values you thought it was. Debuggers and logging (Chapter 6) are complementary. A debugger is what you use when you are trying to track down a specific problem; logging is what you put into your code so that if there’s a problem you’ll have the information you need to track it down more quickly. Over the years, I’ve been surprised by how strongly some programmers resist using a debugger. The reason can’t be the five or ten minutes it takes to learn how to use one—that pays for itself almost immediately. The only explanation I’ve been able to come up with is that some people enjoy being inefficient. Typing in print statements and paging through screens of output lets them feel like they’re being productive, when in fact they’re just being busy. If your brain needs a break (which it sometimes will), then take a break: stretch your legs, stare out a window, practice your juggling, or do whatever else you can to take your mind away from your problem for a few minutes. Don’t drag out the process of finding and fixing your bug by using sloppy technique just to let your brain idle for a while. 15.6 How can I tell why my program is running slowly? Section 7.10 described how a coverage tool can keep track of which lines of a program have and have not been executed so that programmers can figure out which bits of code haven’t been tested. A profiler is a more sophisticated tool that records how much time was spent executing each line so that programmers can figure out which parts are slowest. As a very rough guide, 20% of the code accounts for 80% of execution time, so finding and tuning a few hot spots is the fastest way to make programs faster. There are two ways to do profiling. instrumenting profilers insert extra statements into the application to record the clock time, while sampling profilers interrupt execution every few clock ticks and see where the program is. Both approaches distort the program’s behavior slightly, so there is an unavoidable tradeoff between how much information is collected and how accurately that information represents the behavior of the unmodified program. FIXME: figure showing two kinds of profilers Profilers typically present two kinds of information: how much time was spent in a statement, and how much was spent in that statement and any functions it called. They may also present time per statement and total time, so that if a line of code executes quickly, but is called millions of times, it will show up as a hot spot. Once you have found where your program is slow, the next step is to speed it up. That’s beyond the scope of this tutorial, but the “Performance” section of Wickham (2014) or Gorelick and Ozsvald (2014),Lanaro (2017) are good places to start. 15.7 How can I get all of these tools in one go? A build system, a smart editor, and a debugger all talking to one another: that’s a pretty good description of an integrated development environment or IDE. These were invented in the 1970s, but didn’t really catch on until Borland released Turbo Pascal in the 1980s. Along with the tools described above, modern IDEs usually include: a code browser that displays an overview of the packages, classes, methods, and data in your program; an interactive prompt so that you can type in expressions or call functions and see the results without having to start (or restart) your program; a linter that can warn you when your code doesn’t meet naming and indentation conventions (discussed in Chapter 11); some refactoring tools to help you reorganize your code (discussed in Chapter 17); and a test runner to display the results of tests and let you jump directly to ones that have failed (Chapter 7). In short, an IDE is to programming what a well-equipped workbench is to a carpenter. Microsoft’s Visual Studio Code, Apple’s XCode, and IntelliJ IDEA from JetBrains are three popular commercial IDEs; WingIDE is a good place to start for Python programmers, and RStudio is a full-featured IDE for R that can also be used online. There are dozens of others, any of which will make you more productive than their disconnected counterparts. Since most of these store project data (including build instructions) in a proprietary format, your team will do much better if you all adopt the same IDE. This will also let you help one another solve problems and share plugins. (Having to agree on which IDE to use may be another reason why some programmers resist adopting any IDE at all, since they require even more investment to master than editors.) 15.8 How can I run tasks at regular times? One other kind of tool that many programmers rely on is one that runs tasks at pre-defined times. The best known of these is the venerable Unix utility cron, and a related tool is at, which runs a single job at some time in the future. cron’s configuration files are famously cryptic, and it’s more common these days to run jobs when something changes in a version control repository (Chapter 13) than at a specific time, but you may find yourself in need. 15.9 Summary FIXME: summary of tools 15.10 Exercises FIXME: exercises for tools 15.11 Key Points Choose an editor that is kind to your wrists, portable, syntax-aware, programmable, and can handle Unicode characters. A symbolic debugger is a program that allows you to control and inspect the execution of another program. Symbolic debuggers are a much more efficient way to find most bugs than print statements. A profiler records and reports the execution time of the statements in a program. Instrumenting profilers modify the source code to check the clock and record start and end times for statements. Sampling profilers interrupt a program’s execution to build a histogram of execution times. An integrated development environment (IDE) combines editing, execution, profiling, code checking, and many other tools in one. Use cron or at to execute tasks periodically or at some future time respectively. References "],
["docs.html", "Chapter 16 Documentation 16.1 Questions 16.2 Objectives 16.3 Introduction 16.4 How can I write useful error messages? 16.5 What should I document for whom? 16.6 How should I write documentation for code? 16.7 What should I document? 16.8 How can I create a useful FAQ? 16.9 Summary 16.10 Exercises 16.11 Key Points", " Chapter 16 Documentation 16.1 Questions What kind of documentation should I write and where should I put it? 16.2 Objectives Explain what docstrings are and add correctly-formatted docstrings to short Python programs. Extract and format documentation from docstrings using pydoc. Explain why consistent formatting of code is important. 16.3 Introduction An old proverb says, “Trust, but verify.” The equivalent in programming is, “Be clear, but document.” No matter how well software is written, it always embodies decisions that aren’t explicit in the final code or accommodates complications that aren’t going to be obvious to the next reader. Putting it another way, the best function names in the world aren’t going to answer the questions “Why does the software do this?” and “Why doesn’t it do this in a simpler way?” This lesson will explore who we should write documentation for, what we should write for them, and where it should go. 16.4 How can I write useful error messages? The error message shown in Figure 16.1 is not helpful: Figure 16.1: Error Message Neither is this: System.InvalidOperationException: Nullable object must have a value. or this: I tried really hard but was unable to complete your request. You probably need to talk to a human - have you tried calling Dave? Error messages are often the first thing people actually read about a piece of software (or possibly the second if they had to install it themselves), so they should therefore be the most carefully written documentation for that software. A quick web search for “writing good error messages” turns up hundreds of hits, but recommendations are often more like gripes than solid guidelines and are usually not backed up by evidence. What research there is Traver (2010),Becker et al. (2016) gives us the following rules: Do not tell the user what the program did that caused the problem, but what the user did. Putting it another way, the message shouldn’t state the effect of the error, it should state the cause. Be spatially correct, i.e., point at the actual location of the error. Few things are as frustrating as being pointed at line 28 when the problem is really on line 35. Do not provide tips or potential solutions. In most languages it is not possible to determine what the actual error is from the message with 100% certainty. Therefore it is better to give an as-specific-as-possible message on what went wrong without offering guidance on fixing it. Tips and hints could be provided by a different tool, but they should be based on the error message and not part of it. Be as specific as possible without ever being (or seeming) wrong: from a user’s point of view, “file not found” is very different from “don’t have permissions to open file” or “file is empty”. Write for your audience’s level of understanding. For example, error messages should never use programming terms more advanced than those you would use to describe the code the user wrote. Do not blame the user, and do not use words like fatal, illegal, etc. The former can frustrate—in many cases, “user error” actually isn’t—and the latter can make people worry that the program has damaged their data, their computer, or their reputation. Do not try to make the computer sound like a human being. In particular, avoid humor: very few jokes are funny on the dozenth re-telling, and most users are going to see error messages at least that often. Use a consistent vocabulary. This rule can be hard to enforce when error messages are written by several different people, but putting them all in one module makes review easier. That last suggestion deserves a little elaboration. Most people write error messages directly in their code: try: # ...do something complicated... except OSError as e: print(&#39;Unable to find or read file {}&#39;.format(filename)) sys.exit(1) A better approach for large projects is to put all of the error messages in a catalog: ERROR_MESSAGES = { &#39;cannot_read_file&#39; : &#39;Unable to find or read file {}&#39;, &#39;config_corrupted&#39; : &#39;Configuration file {} corrupted&#39;, # ...more error messages... } and then only use messages from that catalog: from error_messages import ERROR_MESSAGES try: # ...do something complicated... except OSError as e: print(ERROR_MESSAGES[&#39;cannot_read_file&#39;].format(filename)) sys.exit(1) Doing this makes it much easier to ensure that messages are consistent. It also makes it much easier to give messages in the user’s preferred language: ERROR_MESSAGES = { &#39;en&#39; : { &#39;cannot_read_file&#39; : &#39;Unable to find or read file {}&#39;, &#39;config_corrupted&#39; : &#39;Configuration file {} corrupted&#39;, # ...more error messages in English... }, &#39;fr&#39; : { &#39;cannot_read_file&#39; : &#39;Impossible d&#39;acceder au fichier {}&#39;, &#39;config_corrupted&#39; : &#39;Fichier de configuration {} corrompu&#39;, # ...more error messages in French... } # ...other languages... } The error report is then looked up as: ERROR_MESSAGES[user_language][&#39;cannot_read_file&#39;] where user_language is a two-letter code for the user’s preferred language. 16.5 What should I document for whom? There are three kinds of people in any domain: novices, competent practitioners, and experts Wilson (2018). A novice doesn’t yet have a mental model of the domain; they don’t know what the key terms are, how they relate, what the causes of their problems are, or how to tell whether a solution to their problem is appropriate or not. Competent practitioners know enough to accomplish routine tasks with routine effort: they may need to check [Stack Overflow(stack-overflow) every few minutes, but they know what to search for and what “done” looks like. Finally, experts have such a deep and broad understanding of the domain that they can solve routine problems at a glance and are able to handle the one-in-a-thousand cases that would baffle the merely competent. Each of these three groups needs a different kind of documentation. A novice needs a tutorial that introduces her to key ideas one by one and shows how they fit together. A competent practitioner needs reference guides, cookbooks, and Q&amp;A sites; these give her solutions close enough to what she needs that she can tweak them the rest of the way. Experts need this material as well—nobody’s memory is perfect—but they may also paradoxically want tutorials. The difference between them and novices is that experts want tutorials on how things work and why they were designed that way. The first thing to decide when writing documentation is therefore to decide which of these needs you are trying to meet. Tutorials like this one should be long-form prose that contain code samples and diagrams. They should use authentic tasks to motivate ideas, i.e., show people things they actually want to do rather than printing the numbers from 1 to 10, and should include regular check-ins so that learners and instructors alike can tell if they’re making progress. If you would like to know more about creating tutorials, please see Wilson (2018). Tutorials help novices build a mental model, but competent practitioners and experts will be frustrated by their slow pace and low information density. They will want single-point solutions to specific problems like how to find cells in a spreadsheet that contain a certain string or how to configure the web server to load an access control module. They can make use of an alphabetical list of the functions in a library, but are much happier if they can search by keyword to find what they need; one of the signs that someone is no longer a novice is that they’re able to compose useful queries and tell if the results are on the right track or not. That observation brings us to the notion of a false beginner, which is someone who appears not to know anything, but who has enough prior experience in other domains to be able to piece things together much more quickly than a genuine novice. Someone who is proficient with MATLAB, for example, will speed through a tutorial on Python’s numerical libraries much more quickly than someone who has never programmed before. In an ideal world, we would satisfy these needs with a chorus of explanations, some long and detailed, others short and to the point. In our world, though, time and resources are limited, so all but the most popular packages must make do with single explanations. The rest of this section will therefore look at how to create reference guides and FAQs. 16.6 How should I write documentation for code? Instead of writing comments to document code, Python encourages us to write docstrings (short for “documentation string”). A docstring is a string placed at the start of a file or function but not assigned to a variable. If it appears at the start of a file, and the file is loaded using import module, the string is made available as module.__doc__ (with two underscores). If it is placed at the start of a function called func, the string becomes func.__doc__. Python’s built-in help function looks up this string, formats it using the rules like Markdown’s, and displays it, so help(module) or help(func) displays something useful. Docstrings are usually written in triple quotes so that they can span multiple lines, and to make them stand out from strings that are being used as data. For example, here’s a file trim.py that has a one-sentence docstring for the module as a whole and a multi-line docstring for the function trim: &#39;&#39;&#39; Tools for trimming values to lie in a specified range. &#39;&#39;&#39; def trim(values, low, high, in_place=False): &#39;&#39;&#39; Ensure that all values in the result list lie in low...high (inclusive). If &#39;in_place&#39; is &#39;True&#39;, modifies the input instead of creating a new list. Args: values: List of values to be trimmed. low: Lower bound on values (inclusive). high: Upper bound on values (inclusive). in_place: If true, modify input list (default False). Returns: List of trimmed values (which may be the input list). Raises: AssertionError: if &#39;low&#39; is greater than &#39;high&#39;. &#39;&#39;&#39; assert low &lt;= high, &#39;Nonsensical trim range {}..{}&#39;.format(low, high) result = values if in_place else values[:] for (i, x) in enumerate(result): result[i] = min(high, max(low, x)) return result If we run the shell command: pydoc bin/trim.py` Python will read the Python file, extract the docstrings, and create a nicely-formatted listing: NAME trim - Tools for trimming values to lie in a specified range. FUNCTIONS trim(values, low, high, in_place=False) Ensure that all values in the result list lie in low...high (inclusive). If &#39;in_place&#39; is &#39;True&#39;, modifies the input instead of creating a new list. Args: values: List of values to be trimmed. low: Lower bound on values (inclusive). high: Upper bound on values (inclusive). in_place: If true, modify input list (default False). Returns: List of trimmed values (which may be the input list). Raises: AssertionError: if &#39;low&#39; is greater than &#39;high&#39;. FILE /Users/pterry/magic/bin/trim.py We get the same output from within the Python interpreter with: import trim help(trim) or we can run pydoc -w bin/trim.py to generate an HTML page (with some rather garish coloring). We can go further and use a more sophisticated (i.e., more powerful but also more complicated) tool called Sphinx. It reads a superset of Markdown called reStructredText and generates. cross-indexed documentation that is more nicely formatted than pydoc’s default output. Sphinx is used by by ReadTheDocs, which extracts and formats documentation from GitHub repositories and other places. One benefit of ReadTheDocs is that it puts documentation in an easily-findable place; the other is that it automatically regenerates that documentation every time there’s a change to the repository (an example of continuous integration). It’s a great service, but it’s out of the scope of this lesson. To learn more about documenting Python code, see this tutorial by James Mertz. 16.7 What should I document? The answer to the question in this section’s title depends on what stage of development you are in. If you are doing exploratory programming, a short docstring to remind yourself of each function’s purpose is good enough. (In fact, it’s probably better than what most people do.) That one- or two-liner should begin with an active verb and describe either how inputs are turned into outputs, or what side effects the function has; as we discuss below, if you need to describe both, you should probably rewrite your function. An active verb is something like “extract”, “normalize”, or “find”. For example, these are all good one-line docstrings: “Create a list of current ages from a list of birth dates.” “Clip signals to lie in [0…1].” “Reduce the red component of each pixel.” You can tell your one-liners are useful if you can read them aloud in the order the functions are called in place of the function’s name and parameters. Once you start writing code for other people—including yourself three months from now—your docstrings should describe: The name and purpose of every public class, function, and constant in your code. The name, purpose, and default value (if any) of every parameter to every function. Any side effects the function has. The type of value returned by every function. What exceptions those functions can raise and when. The word “public” in the first rule is important. You don’t have to write full documentation for helper functions that are only used inside your package and aren’t meant to be called by users, but these should still have at least a comment explaining their purpose. You also don’t have to document unit testing functions: as discussed in Chapter 7, these should have long names that describe what they’re checking so that failure reports are easy to scan. 16.8 How can I create a useful FAQ? An FAQ is a list of frequently-asked questions and corresponding answers. A good FAQ uses the terms and concepts that people bring to the software rather than the vocabulary of its authors; putting it another way, the questions should be things that people might search for online, and the answers should give them enough information to solve their problem. Creating and maintaining a FAQ is a lot of work, and unless the community is large and active, a lot of that effort may turn out to be wasted, because it’s hard for the authors or maintainers of a piece of software to anticipate what newcomers will be mystified by. A better approach is to leverage sites like Stack Overflow, which is where most programmers are going to look for answers anyway: Post every question that someone actually asks you, whether it’s online, by email, or in person. Be sure to include the name of the software package in the question so that it’s findable. Answer the question, making sure to mention which version of the software you’re talking about (so that people can easily spot and discard stale answers in the future). With a bit of work, the Stack Exchange Data Explorer can be used to download questions and answers about your software if you want to put them all in an offline guide. You can also use Stack Printer for this; for example, the URL http://www.stackprinter.com/topvoted?service=stackoverflow&amp;tagged=rstudio will bring up a paged view of top-voted questions about RStudio. Stack Overflow’s guide to asking a good question has been refined over many years, and is a good guide for any project: Write the most specific title you can. “Why does division sometimes give a different result in Python 2.7 and Python 3.5?” is much better than, “Help! Math in Python!!” Give context before giving sample code. A few sentences to explain what you’re trying to do and why will help people determine if their question is a close match to yours or not. Provide a minimal reprex. Chapter 10 explained the value of a reproducible example (reprex), and why reprexes should be as short as possible. Readers will have a much easier time figuring out if this question and its answers are for them if they can see and understand a few lines of code. Tag, tag, tag. Keywords make everything more findable, from scientific papers and left-handed musical instruments to solutions for programming problems. Use “I” and question words (how/what/when/where/why). The section headings in these lessons follow this rule for the same reason that questions in a FAQ should: writing this way forces you to think more clearly about what someone might actually be thinking when they need help. Keep each item short. The “minimal manual” approach to instructional design Carroll (2014) breaks everything down into single-page steps, with half of that page devoted to troubleshooting. This may feel like baby steps to the person doing the writing, but is often as much as a person searching and reading can handle. It also helps writers realize just how much implicit knowledge they are assuming. Allow for a chorus of explanations. As discussed earlier, users are all different from one another, and are therefore best served by a chorus of explanations. Do not be afraid of providing multiple explanations to a single question that suggest different approaches or are written for different prior levels of understanding. 16.9 Summary How can I tell you what I think till I see what I say? – E.M. Forster Writing documentation will sometimes give you ideas about how to improve the code. For example, if you need to document something in the middle of a function, you should probably put it in a separate function. Similarly, if you have to use the word “and” more than once to describe what a function does, your documentation is probably telling you that the function is trying to do too many things. FIXME: create concept map for docs 16.10 Exercises FIXME: create exercises for docs 16.11 Key Points Documentation strings (docstrings) can be placed at the start of a file or at the start of a function. Docstrings can be formatted using a superset of Markdown. Tools like pydoc and Sphinx can extract and format docstrings to create documentation for software. References "],
["refactor.html", "Chapter 17 Refactoring 17.1 Questions 17.2 Objectives 17.3 Introduction 17.4 How can I avoid repeating values in code? 17.5 How can I avoid repeating calculations in code? 17.6 How can I make repeated conditional tests clearer? 17.7 How can I avoid duplicating expressions in assignment statements? 17.8 How can I make special cases easier to spot? 17.9 How can I divide code into more comprehensible chunks? 17.10 When and how should I combine code into a single function? 17.11 How can I replace code with data? 17.12 What can I use instead of explicit loops? 17.13 Summary 17.14 Exercises 17.15 Key Points", " Chapter 17 Refactoring 17.1 Questions How can I improve code that already works? 17.2 Objectives Describe at least four common refactorings and correctly apply each. 17.3 Introduction Refactoring means changing the structure of code without changing what it does, like refactoring an equation to simplify it. It is just as much a part of programming as writing code in the first place: nobody gets things right the first time Brand (1995), and needs or insights can change over time. Most discussions of refactoring focus on object-oriented programming, but many patterns can and should be used to clean up procedural code. This lesson describes and motivates some of the most useful patterns; These rules are examples of design patterns: general solutions to commonly occurring problems in software design. Knowing them and their names will help you create better software, and also make it easier for you to communicate with your peers. 17.4 How can I avoid repeating values in code? Our first and simplest refactoring is called “replace value with name”. It tells us to replace magic numbers with names, i.e., to define constants. This can seem ridiculous in simple cases (why define and use inches_per_foot instead of just writing 12?). However, what may be obvious to you when you’re writing code won’t be obvious to the next person, particularly if they’re working in a different context (most of the world uses the metric system and doesn’t know how many inches are in a foot). It’s also a matter of habit: if you write numbers without explanation in your code for simple cases, you’re more likely to do so for complex cases, and more likely to regret it afterward. Using names instead of raw values also makes it easier to understand code when you read it aloud, which is always a good test of its style. Finally, a single value defined in one place is much easier to change than a bunch of numbers scattered throughout your program. You may not think you will have to change it, but then people want to use your software on Mars and you discover that constants aren’t Mak (2006). FIXME: include refactor/replace_value_with_name.html 17.5 How can I avoid repeating calculations in code? It’s inefficient to calculate the same value over and over again. It also makes code less readable: if a calculation is inside a loop or a function, readers will assume that it might change each time the code is executed. Our second refactoring, “hoist repeated calculation out of loop”, tells us to move the repeated calculation out of the loop or function. Doing this signals that its value is always the same. And by naming that common value, you help readers understand what its purpose is. FIXME: include refactor/hoist_repeated_calculation.html 17.6 How can I make repeated conditional tests clearer? Novice programmers frequently write conditional tests like this: if (a &gt; b) == True: # ...do something... The comparison to True is unnecessary because a &gt; b is a Boolean value that is itself either True or False. Like any other value, Booleans can be assigned to variables, and those variables can then be used directly in tests: was_greater = estimate &gt; 0.0 # ...other code that might change estimate... if was_greater: # ...do something... This refactoring is called “replace repeated test with flag”. When it is used, there is no need to write if was_greater == True: that always produces the same result as if was_greater. Similarly, the equality tests in if was_greater == False is redundant: the expression can simply be written if not was_greater. Creating and using a flag instead of repeating the test is therefore like moving a calculation out of a loop: even if that value is only used once, it makes our intention clearer—these really are the same test. FIXME: include refactor/replace_repeated_test_with_flag.html If it takes many lines of code to process data and create a score, and the test then needs to change from &gt; to &gt;=, we’re more likely to get the refactored version right the first time, since the test only appears in one place and its result is given a name. 17.7 How can I avoid duplicating expressions in assignment statements? An in-place operator, sometimes called an update operator, does a calculation with two values and overwrites one of the values. For example, instead of writing: step = step + 1 we can write: step += 1 In-place operators save us some typing. They also make the intention clearer, and most importantly, they make it harder to get complex assignments wrong. For example: samples[least_factor_index, max(current_offset, offset_limit)] *= scaling_factor is much easier to read than the equivalent expression: samples[least_factor_index, max(current_offset, offset_limit)] = \\ scaling_factor * samples[least_factor_index, max(current_limit, offset_limit)] (The proof of this claim is that you probably didn’t notice on first reading that the long form uses different expressions to index samples on the left and right of the assignment.) The refactoring “use in-place operator” does what its name suggests: converts normal assignments into their briefer equivalents. FIXME: include refactor/use-in-place-operator.html 17.8 How can I make special cases easier to spot? A short circuit test is a quick check to handle a special case, such as checking the length of a list of values and returning math.nan for the average if the list is empty. “Place short circuits early” tells us to put short-circuit tests near the start of functions so that readers can mentally remove special cases from their thinking while reading the code that handles the usual case. FIXME: include refactor/place-short-circuits-early.html A related refactoring pattern is called “default and override”. To use it, find cases where a value is set conditionally; assign the default or most common value unconditionally, and then override it in a special case. The result is fewer lines of code and clearer control flow; however, it does mean executing two assignments instead of one, so it shouldn’t be used if the common case is expensive (e.g., involves a database lookup or a web request). FIXME: include refactor/default-and-override.html In simple cases, people will sometimes put the test and assignment on a single line: scale = 1.0 if configuration[&#39;threshold&#39;] &gt; UPPER_BOUND: scale = 0.8 Some programmers take this even further and use a conditional expression: scale = 0.8 if configuration[&#39;threshold&#39;] &gt; UPPER_BOUND else 1.0 However, this puts the default last instead of first, which is less clear. 17.9 How can I divide code into more comprehensible chunks? Functions were created so that programmers could write common operations and re-use them in order to reduce the amount of code that needed to be compiled. It turns out that moving complex operations into functions also reduces cognitive load: by reducing the number of things that have to be understood simultaneously. FIXME: include refactor/extract-function.html You should always extract functions when code can be used in other contests. Even if it can’t, you should extract functions whenever it makes the function clearer when it is read aloud. Multi-part conditionals, parts of long equations, and the bodies of loops are good candidates for extraction; if you can’t think of a plausible name, or if a lot of data has to be passed into the function after it’s extracted, the code should probably be left where it is. Finally, it’s often helpful to keep using the original variable names as parameter names during refactoring to reduce typing. 17.10 When and how should I combine code into a single function? “Combine functions” is the opposite of “extract function”. If operations are always done together, it can sometimes be be more efficient to do them together, and might be easier to understand. However, combining functions often reduces their reusability and readability; one sign that functions shouldn’t have been combined is how often people use the combination and throw some results away. The fragment below shows how two functions can be combined: FIXME: include refactor/combine-functions.html One thing you may not notice about the combination is that it assumes characters are either vowels or consonants, which means it might work differently than separate calls to the two original functions. Issues like this are why experienced developers write unit tests (Chapter 7) before starting to refactor. 17.11 How can I replace code with data? It is sometimes easier to understand and maintain lookup tables than complicated conditionals, so the “create lookup table” refactoring tells us to turn the latter into the former: FIXME: include refactor/create-lookup-table.html The more distinct cases there are, the greater the advantage lookup tables have over multi-branch conditionals. Those advantages multiply when items can belong to more than one category, in which case the table is often best written as a dictionary with items as keys and sets of categories as values: LETTERS = { &#39;A&#39; : {&#39;vowel&#39;, &#39;upper_case&#39;}, &#39;B&#39; : {&#39;consonant&#39;, &#39;upper_case&#39;}, # ...other upper-case letters... &#39;a&#39; : {&#39;vowel&#39;, &#39;lower_case&#39;}, &#39;b&#39; : {&#39;consonant&#39;, &#39;lower_case&#39;}, # ...other lower-case letters... &#39;+&#39; : {&#39;punctuation&#39;}, &#39;@&#39; : {&#39;punctuation&#39;}, # ...other punctuation... } def count_vowels_and_consonants(text): num_vowels = num_consonants = 0 for char in text: num_vowels += int(&#39;vowel&#39; in LETTERS[char]) num_consonants += int(&#39;consonant&#39; in LETTERS[char]) return num_vowels, num_consonants The expressions used to update num_vowels and num_consonants make use of the fact that in produces either True or False, which the function int converts to either 1 or 0. We will explore ways of making this code more readable in the exercises. 17.12 What can I use instead of explicit loops? When programmers see a pattern in many different contexts, they will often add features to the language to support it. Many language features therefore exist to give programmers something to refactor to. One such feature is called a list comprehensions, which can be used in place of a loop that processes one list to create another. The word “comprehension” is used in the sense of “comprehensive”: every element of the original list is processed, and in simple cases, produces exactly one element in the result. FIXME: include refactor/simple-comprehension.html List comprehensions can be easier to read than loops for simple calculations, but they become more complicated to understand when conditional expressions are included. The if case isn’t too base: FIXME: include refactor/comprehension-if.html but if-else is structured differently in comprehensions than it is in a one-line conditional: FIXME: include refactor/comprehension-else.html Similarly, cross-product loops are straightforward (for some definition of “straightforward”): FIXME: include refactor/cross-product-comprehension.html but the equivalent of nested loops in which the inner loop depends on the value of the outer loop always feels back-to-front: FIXME: include refactor/nested-comprehension.html Python comprehensions work for sets, dictionaries, and anything else that can be iterated over. 17.13 Summary A good test of code quality: each plausible small change to functionality requires one change in one place FIXME: create concept map for refactoring 17.14 Exercises 17.14.1 Make lookup tables easier to read FIXME: make last example of lookup tables easier to read. 17.15 Key Points Reorganizing code in consistent ways makes errors less likely. Replace a value with a name to make code more readable and to forestall typing errors. Replace a repeated test with a flag to ensure consistency. Turn small pieces of large functions into functions in their own right, even if they are only used once. Combine functions if they are always used together on the same inputs. Use lookup tables to make decision rules easier to follow. Use comprehensions instead of loops. References "],
["project.html", "Chapter 18 Project Structure 18.1 Questions 18.2 Objectives 18.3 Introduction 18.4 What are Noble’s Rules? 18.5 How should files and sub-directories be named? 18.6 How should I manage a mix of compiled programs and scripts? 18.7 Should I separate documentation from manuscripts? 18.8 How should I handle data can’t be stored in version control? 18.9 What other files should every project contain? 18.10 What is a project? 18.11 Summary 18.12 Exercises 18.13 Key Points", " Chapter 18 Project Structure 18.1 Questions How should I organize the files and directories in my project? 18.2 Objectives Describe and justify Noble’s Rules for organizing projects. Explain the purpose of README, LICENSE, CONDUCT, and CITATION files. 18.3 Introduction Project organization is like a diet: there is no such thing as “no diet”, just a good one or a bad one. Similarly, there is no such thing as “no project organization”: your project is either organized well or poorly. As with coding style (Chapter 11), small pieces in predictable places with readable names are easier to find and use than large chunks that vary from project to project and have names like stuff. 18.4 What are Noble’s Rules? Noble (2009) described a way to organize small bioinformatics projects that is equally useful for other kinds of research computing. Each project is put in a separate Git repository, and the directories in the root of this repository are organized according to purpose. The original specification included five top-level directories: The ./src/ directory (short for “source”) holds source code for programs written in languages like C or C++ that need to be compiled. Many projects don’t have this directory because all of their code is written in languages that don’t need compilation. Runnable programs go in ./bin/ (an old Unix abbreviation for “binary”, meaning “not text”). This includes the compiled and runnable versions of C and C++ programs, and also shell scripts, Python or R programs, and everything else that can be executed. Raw data goes in in ./data/ and is never modified after being stored. Results are put in ./results/. This includes cleaned-up data, figures, and everything else that can be rebuilt using what’s in ./bin/ and ./data/. If intermediate results can be re-created quickly and easily, they might not be stored in version control, but anything that is included in a manuscript should be here. Finally, documentation and manuscripts go in ./doc/. Figure 18.1: Project Layout Figure 18.1 below shows this layout for a project called g-trans. A few things to notice are: The documentation for the regulate script appears in the root of ./doc/, while the paper for JCMB is stored in a sub-directory, since it contains several files. The ./src/ directory contains a Makefile to re-build the regulate program (Chapter 3). Some projects put the Makefile in the root directory, reasoning that since it affects both ./src/ and ./bin/, it belongs above them both rather than in either one. There are several sub-directories underneath ./data/ and ./results/, which we will discuss in the next section. Each of the sub-directories in ./results/ has its own Makefile, which will re-create the contents of that directory. 18.5 How should files and sub-directories be named? While the directories in the top level of each project are organized by purpose. the directories within ./data/ and ./results/ are organized chronologically so that it’s easy to see when data was gathered and when results were generated. These directories all have names in ISO date format like YYYY-MM-DD to make it easy to sort them chronologically. This naming is particularly helpful when data and results are used in several reports. At all levels, filenames should be easy to match with simple shell wildcards. For example, a project might use speciesorgantreatment.csv as a file-naming convention, giving filenames like human_kidney_cm200.csv. This allows human_*_cm200.csv to match all human organs or *_kidney_*.csv to match all kidney data. It does produce long filenames, but tab completion means you only have to type them once. Long filenames are just as easy to match in programs: Python’s glob and R’s Sys.glob will both take a pattern and return a list of matching filenames. 18.6 How should I manage a mix of compiled programs and scripts? Noble’s Rules puts everything runnable in a single directory called ./bin/. That makes things easy to find, but most software engineers would say that only the source code for programs should be in version control, not the output of the compiler. There are three ways to handle this: Put the compiled program under version control. In theory this makes research more reproducible, since anyone who wants to re-run the analysis can be sure they’re using exactly the same program as the author, but in practice, many “compiled” programs load libraries dynamically, so their results can still be affected by changes to their environment. Putting the compiled programs in version control does make it easier for people to re-run analyses, though, since they don’t need to be able to compile things themselves. Put everything in ./bin/, then edit .gitignore to tell Git to ignore the compiled programs. This keeps everything runnable in one place, but requires a little bit of extra bookkeeping. Put compiled programs in ./bin/ and everything that doesn’t require compilation in ./scripts/. This makes management simpler (just ignore everything in ./bin/), but means there are two places where runnable programs might live. We usually go with the third option, but they’re all good as long as you are consistent between projects and document your rule. 18.7 Should I separate documentation from manuscripts? Noble’s Rules place documentation and manuscripts in ./docs/. As with compiled programs and scripts, some people separate these, so that (for example) ./docs/ has the project’s website and the documentation for its software, while ./reports/ has one sub-directory for each paper, thesis chapter, or other manuscript. As more researchers [work in the open(#g:open-science), and as tools like R Markdown and the Jupyter Notebook blur the distinction between software, documentation, and reports, separating the two makes less sense. 18.8 How should I handle data can’t be stored in version control? Small datasets that don’t contain sensitive information can and should be stored in version control. As a rule of thumb, anything that you would send in an email attachment is probably small enough to be put into Git, while anything that might reveal someone’s identity when combined with other data sets should not be. If data is large or sensitive, there should still be something in ./data/ to show its existence, and that “something” should be easy for programs to read. One option is a CSV file whose columns are: the name of the dataset, its URL or other unique identifier, the date it was last checked, and its size (so that users will have some idea of how much work is involved in processing it). Another option is to have one file per dataset, so that instead of reading human_genome.bam, the program reads human_genome.yml and then uses the url key in that file to find the data it actually wants. If your data is complicated, you may also want to include a README.md file in ./data/ modelled on the one that accompanies The Pudding article “Women’s Pockets are Inferior” (see Chapter 21). 18.9 What other files should every project contain? Most projects’ repositories contain a few files that weren’t mentioned in Noble’s paper, but which have become conventional in open source projects. All of these files may be plain text or Markdown, and may have a .txt or .md suffix (or no suffix at all), but please use the principal names given, in all caps, since a growing number of tools expect them. README: the project’s title and a one-paragraph description of its purpose or content. This file is displayed by GitHub Pages as the project’s home page. LICENSE: the project’s license (discussed in Chapter 19). CONDUCT: its code of conduct (also discussed in Chapter 19). CITATION: how the work should be cited. This file usually contains a plain text citation, and may also include entries formatted for various bibliographic systems like BibTeX. Some projects also have a separate CONTRIBUTORS file listing everyone who has contributed to it, while others include this as a section in CITATION. If setting up the project or contributing to it are complex, there may also be a file called BUILD that explains how to install the required software, the formatting conventions for new data entries, and so on. These instructions can also be included as a section in README. Whichever convention is used, remember that the easier it is for people to get set up and contribute, the more likely they are to do so Steinmacher et al. (2014). 18.10 What is a project? Like features (Chapter 9), what exactly constitutes a “project” requires a bit of judgment, and different people will make different decisions. Some common criteria are one project per publication, one project per deliverable piece of software, or one project per team. The first tends to be too small: a good data set will result in several reports, and the goal of some projects is to produce a steady stream of reports (such as monthly forecasts). The second is a good fit for software engineering projects whose primary aim is to produce tools rather than results, but is inappropriate for most data analysis work. The third tends to be too large: a team of half a dozen people may work on many different things at once, and a repository that holds them all quickly looks like someone’s basement. The best rule of thumb we have found for deciding what is and isn’t a project is to ask what you have meetings about. If the same set of people need to get together on a regular basis to talk about something, that “something” probably deserves its own repository. And if the list of people changes slowly over time but the meetings continue, that’s an even stronger sign. 18.11 Summary FIXME: create concept map for project structure 18.12 Exercises FIXME: exercises for structure chapter. 18.13 Key Points Put source code for compilation in ./src/. Put runnable code in ./bin/. Put raw data in ./data/. Put results in ./results/. Put documentation and manuscripts in ./doc/. Use file and directory names that are easy to match and include dates for the level under ./data/ and ./results/. Create README, LICENSE, CONDUCT, and CITATION files in the root directory of the project. References "],
["inclusive.html", "Chapter 19 Including Everyone 19.1 Questions 19.2 Objectives 19.3 Introduction 19.4 Why does a project need explicit rules? 19.5 How should I license my software? 19.6 How should I license my data and reports? 19.7 Why should I establish a code of conduct for my project? 19.8 Why can I be a good ally for members of marginalized groups? 19.9 How can I be a good ally for members of marginalized groups? 19.10 Summary 19.11 Exercises 19.12 Key Points", " Chapter 19 Including Everyone 19.1 Questions Why should I make my project welcoming for everyone? Why do I need a license for my work? What license should I use for my work? How should I tell people what they can and cannot do with my work? Why should my project have an explicit Code of Conduct? How can I be a good ally? 19.2 Objectives Explain the purpose of a Code of Conduct and the essential features an effective one must have. Explain why adding licensing information to a repository is important. Explain differences in licensing and social expectations. Choose an appropriate license. Explain where and how to communicate licensing. Explain steps a project lead can take to be a good ally. 19.3 Introduction The previous lesson talked about the physical organization of projects. This one talks about the social structure, which is more important to the project’s success. A project can survive badly-organized code; none will survive for long if people are confused, pulling in different directions, or hostile. This lesson therefore talks about what projects can do to make newcomers feel welcome and to make things run smoothly after that. It draws on Fogel (2005), which describes how good open source software projects are run, and on Bollier (2014), which explains what a commons is and when it’s the right model to use. 19.4 Why does a project need explicit rules? Jo Freeman’s influential essay “The Tyranny of Structurelessness” pointed out that every group has a power structure; the only question is whether it is formal and accountable or informal and unaccountable Freeman (1972). Thirty-five years after the free software movement took on its modern, self-aware form, its successes and failures have shown that if a project doesn’t clearly state who has the right to do what, it will wind up being run by whoever argues loudest and longest. 19.5 How should I license my software? It might seem strange to put licensing under discussion of inclusivity, but if the law or a publication agreement prevents people from reading your work or using your software, you’re excluding them (and probably hurting your own career). You may need to do this in order to respect personal or commercial confidentiality, but the first and most important rule of inclusivity is to be open by default. However, that is easier said than done, not least because the law hasn’t kept up with everyday practice. Morin and Sliz (2012) and this blog post are good starting points from a scientist’s point of view, while Lindberg (2008) is a deeper dive for those who want details. In brief, creative works are automatically eligible for intellectual property (and thus copyright) protection. This means that every creative work has some sort of license: the only question is whether authors and users know what it is. Every project should therefore include an explicit license. This license should be chosen early: if you don’t set it up right at the start, then each collaborator will hold copyright on their work and will need to be asked for approval when a license is chosen. By convention, the license is usually put in a file called LICENSE or LICENSE.txt in the project’s root directory. This file should clearly state the license(s) under which the content is being made available; the plural is used because code, data, and text may be covered by different licenses. Don’t write your own license, even if you are a lawyer: legalese is a highly technical language, and words don’t mean what you think they do. To make license selection as easy as possible, GitHub allows you to select one of the most common licenses when creating a repository. The Open Source Initiative maintains a list of licenses, and choosealicense.com will help you find a license that suits your needs. Some of the things you will need to think about are: Do you want to license the code at all? Is the content you are licensing source code? Do you require people distributing derivative works to also distribute their code? Do you want to address patent rights? Is your license compatible with the licenses of the software you depend on? For example, as we will discuss below, you can use MIT-licensed code in a GPL-licensed project but not vice versa. The two most popular licenses for software are the MIT license and the GNU Public License (GPL). The MIT license (and its close sibling the BSD license) say that people can do whatever they want to with the software as long as they cite the original source, and that the authors accept no responsibility if things go wrong. The GPL gives people similar rights, but requires them to share their own work on the same terms: You may copy, distribute and modify the software as long as you track changes/dates in source files. Any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL along with build &amp; install instructions. — tl;dr We recommend the MIT license: it places the fewest restrictions on future action, it can be made stricter later on, and the last thirty years shows that it’s good enough to keep work open. 19.6 How should I license my data and reports? The MIT license and the GPL apply to software. When it comes to data and reports, the most widely used family of licenses are those produced by Creative Commons, which have been written and checked by lawyers and are well understood by the community. The most liberal license is referred to as CC-0, where the “0” stands for “zero restrictions”. CC-0 puts work in the public domain, i.e., allows anyone who wants to use it to do so however they want with no restrictions. This is usually the best choice for data, since it simplifies aggregate analysis. For example, if you choose a license for data that requires people to cite their source, then anyone who uses that data in an analysis must cite you; so must anyone who cites their results, and so on, which quickly becomes unwieldy. The next most common license is the Creative Commons - Attribution license, usually referred to as CC-BY. This allows people to do whatever they want to with the work as long as they cite the original source. This is the best license to use for manuscripts, since you want people to share them widely but also want to get credit for your work. Other Creative Commons licenses incorporate various restrictions on specific use cases: ND (no derivative works) prevents people from creating modified versions of your work. Unfortunately, this also inhibits translation and reformatting. NC (no commercial use) does not mean that people cannot charge money for something that includes your work, though some publishers still try to imply that in order to scare people away from open licensing. Instead, the NC clause means that people cannot charge for something that uses your work without your explicit permission, which you can give under whatever terms you want. Finally, SA (share-alike) requires people to share work that incorporates yours on the same terms that you used. Again, this is fine in principle, but in practice makes aggregation a headache. 19.7 Why should I establish a code of conduct for my project? You don’t expect to have a fire, but every large building or event should have a fire safety plan. Similarly, having a Code of Conduct for your project reduces the uncertainty that participants face about what is acceptable and unacceptable behavior. You might think this is obvious, but long experience shows that articulating it clearly and concisely reduces problems caused by have different expectations, particularly when people from very different cultural backgrounds are trying to collaborate. An explicit Code of Conduct is particularly helpful for newcomers, so having one can help your project grow and encourage people to give you feedback. Having a Code of Conduct is particularly important for people from marginalized or under-represented groups, who have probably experienced harassment or unwelcoming behavior before. By adopting one, you signal that your project is trying to be a better place than YouTube, Twitter, and other online cesspools. Some people may push back claiming that it’s unnecessary, or that it infringes freedom of speech, but in our experience, what they often mean is that thinking about how they might have benefited from past inequity makes them feel uncomfortable, or that they like to argue for the sake of arguing. If having a Code of Conduct leads to them going elsewhere, that will probably make your project run more smoothly. Just as you shouldn’t write your own license for a project, you probably shouldn’t write your own Code of Conduct. We recommend using the Contributor Covenant for development projects and the model code of conduct from the Geek Feminism Wiki for in-person events. Both have been thought through carefully and revised in the light of experience, and both are now used widely enough that many potential participants in your project will not need to have them explained. Rules are meaningless if they aren’t enforced. If you adopt a Code of Conduct, it is therefore important to be clear about how to report issues and who will handle them. Aurora, Gardiner, and Flower Horne (2018) is a short, practical guide to handling incidents; like the Contributor Covenant and the model code of conduct, it’s better to start with something that other people have thought through and refined than to try to create something from scratch. 19.8 Why can I be a good ally for members of marginalized groups? Setting out rules and handling incidents when they arise is what projects can do; if you have power (even or especially the power that comes from being a member of the majority group), what you can do personally is be a good ally for members of marginalized groups. Much of this discussion is drawn from the Frameshift Consulting Ally Skills workshop, which you should attend if you can. First, some definitions. Privilege is an unearned advantage given to some people but not all; oppression is systemic, pervasive inequality that benefits the privileged and harms those without privilege. A straight, white, physically able, economically secure male is less likely to be interrupted when speaking, more likely to be called on in class, and more likely to get a job interview based on an identical CV than someone who is perceived as being outside these categories. The unearned advantage may be small in any individual case, but compound interest quickly amplifies these differences: someone who is called on more often in class is more likely to be remembered by a professor, who in turn is therefore more likely to recommend them to a potential employer, who is more likely to excuse the poor grades on their transcripts, and on and on it goes. People who are privileged are often not aware of it for the same reason that most fish don’t know what water tastes like. A target is someone who suffers from oppression. Targets are often called “members of a marginalized group”, but that phrasing is deliberately passive. Targets don’t choose to be marginalized: those with privilege marginalize them. An ally is a member of a privileged group who is working to understand their own privilege and end oppression. For example, privilege is being able to walk into a store and have the owner assume you’re there to buy things, not to steal them. Oppression is the stories told about (for example) indigenous people being thieves, and the actions people take as a result of them. A target is an indigenous person who wants to buy milk, and an ally is a white person who pays attention to a lesson like this one (raising their own awareness), calls out peers who spread racist stories (a peer action), or asks the shopkeeper whether they should leave too (a situational action). Why should you be an ally? You could do it out of a sense of fairness because you realize that you have benefited from oppression even if you haven’t taken part (or don’t think you have). And you should do it because you can: taking action to value diversity results in worse performance ratings for minority and female leaders, while ethnic majority or male leaders who do this aren’t penalized Hekman et al. (2017). As soon as you acknowledge that (for example) women are called on less often than men, or are less likely to get an interview or a publication given identical work, you have to acknowledge that white and Asian males are more likely to get these benefits than their performance alone deserves. 19.9 How can I be a good ally for members of marginalized groups? So much for the theory: what should you actually do? A few simple rules will go a long way: Be short, simple, and firm. Don’t try to be funny: it almost always backfires, or will later be used against you. Play for the audience: you probably won’t change the mind of the oppressor you’re calling out, but you might change the minds or give heart to people who are observing. Pick your battles. You can’t challenge everyone, every time, without exhausting yourself and deafening your audience. An occasional sharp retort will be much more effective than constant criticism. Don’t shame or insult one group when trying to help another. For example, don’t call someone stupid when what you really mean is that they’re racist or homophobic. Change the terms of the debate. The last rule is best explained by example. Suppose someone says, “Why should we take diversity into account when hiring? Why don’t we just hire the best candidate?” Your response could be, “Because taking diversity into account is hiring the best candidate. If you can run a mile in four minutes and someone else can do it in 4:15 with a ball and chain on their leg, who the better athlete? Who will perform better if the impediment is removed? If you intend to preserve an exclusionary culture in this lab, considering how much someone has achieved despite systemic unfairness might not make sense, but you’re not arguing for that, are you?” And if someone then says, “But it’s not fair to take anything other than technical skill into account when hiring for a technical job,” you can say, “You’re right, which means that what you’re really upset about is the thought that you might be treated the way targets have been treated their whole lives.” Captain Awkward has useful advice, and Charles’ Rules of Argument are very useful online: Don’t go looking for an argument. State your position once, speaking to the audience. Wait for absurd replies. Reply once more to correct any misunderstandings of your original statement. Do not reply again—go do something fun instead. Finally, it’s important to recognize that good principles sometimes conflict. For example, consider this scenario: A manager consistently uses male pronouns to refer to software and people of unknown gender. When you tell them it makes you uncomfortable to treat maleness as the norm, they say that male is the default gender in their first language and you should be more considerate of people from other cultures. On the one hand, you want to respect other people’s cultures; on the other hand, you want to be inclusive of women. In this case, the manager’s discomfort about changing pronouns matters less than the career harm caused by them being exclusionary, but many cases are not this clear cut. Like any written rules, a Code of Conduct requires constant interpretation; like everything else, discussion about specific cases becomes easier with practice. 19.10 Summary FIXME: create concept map for making an inclusive project 19.11 Exercises FIXME: exercises for creating an inclusive project. 19.12 Key Points Create an explicit Code of Conduct for your project modelled on the Contributor Covenant. Be clear about how to report violations of the Code of Conduct and who will handle such reports. People who are not lawyers should not try to write licenses. Every project should include an explicit license to make clear who can do what with the material. People who incorporate GPL’d software into their own software must make their software also open under the GPL license; most other open licenses do not require this. The Creative Commons family of licenses allow people to mix and match requirements and restrictions on attribution, creation of derivative works, further sharing, and commercialization. Be proactive about welcoming and nurturing community members. References "],
["package.html", "Chapter 20 Packaging 20.1 Questions 20.2 Objectives 20.3 Introduction 20.4 How can I turn a set of Python source files into a module? 20.5 How can I control what is executed during import and what isn’t? 20.6 How can I install a Python package? 20.7 How can I create an installable Python package? 20.8 How can I manage the source code of large packages? 20.9 How can I distribute software packages that I have created? 20.10 How can I manage the packages my projects need? 20.11 How can I test package installation? 20.12 Announcing Work 20.13 Summary 20.14 Exercises 20.15 Key Points", " Chapter 20 Packaging 20.1 Questions How can I manage the libraries my project relies on? How can I package up my work for others to use? How should I announce my work? 20.2 Objectives Create and use virtual environments to manage library versions without conflict. Create and test a citable, shareable Pip package. 20.3 Introduction Another response of the wizards, when faced with a new and unique situation, was to look through their libraries to see if it had ever happened before. This was…a good survival trait. It meant that in times of danger you spent the day sitting very quietly in a building with very thick walls. – Terry Pratchett The more software you write, the more you realize that a programming language is a way to build and combine software libraries. Every widely-used language now has an online repository from which people can download and install those libraries. This lesson shows you how to use Python’s tools to create and share libraries of your own. This material is based in part on Python 102 by Ashwin Srinath. 20.4 How can I turn a set of Python source files into a module? Any Python source file can be imported by any other. When a file is imported, the statements in it are executed as it loads. Variables defined in the file are then available as module.name. (This is why Python files should be named using pothole_case instead of kebab-case: an expression like import some-thing isn’t allowed because some-thing isn’t a legal variable name.) As an example, we can put a constant and two functions used in our Zipf’s Law study in a file called zipf.py: from pytest import approx RELATIVE_ERROR = 0.05 def make_zipf(length): assert length &gt; 0, &#39;Zipf distribution must have at least one element&#39; result = [1/(1 + i) for i in range(length)] return result def is_zipf(hist, rel=RELATIVE_ERROR): assert len(hist) &gt; 0, &#39;Cannot test Zipfiness without data&#39; scaled = [h/hist[0] for h in hist] perfect = make_zipf(len(hist)) return scaled == approx(perfect, rel=rel) and then use import zipf, from zipf import is_zipf, and so on: from zipf import make_zipf, is_zipf generated = make_zipf(5) print(&#39;generated distribution: {}&#39;.format(generated)) generated[-1] *= 2 print(&#39;passes test with default tolerance: {}&#39;.format(is_zipf(generated))) print(&#39;passes test with tolerance of 1.0: {}&#39;.format(is_zipf(generated, rel=1.0))) Running this program produces the following output: generated distribution: [1.0, 0.5, 0.3333333333333333, 0.25, 0.2] passes test with default tolerance: False passes test with tolerance of 1.0: True It also creates a sub-directory called __pycache__ that holds the compiled versions of the imported files. The next time Python imports zipf, it checks the timestamp on zipf.py and the timestamp on the corresponding file in __pycache__. If the latter is more recent, Python doesn’t bother to recompile the file: it just loads the bytes in the cached version and uses those. To avoid confusing it, we (almost) always put __pycache__ in .gitignore. 20.5 How can I control what is executed during import and what isn’t? Sometimes it’s handy to be able to import code and also run it as a program. For example, we may have a file full of useful functions for extracting keywords from text that we want to be able to use in other programs, but also want to be able to run keywords somefile.txt to get a listing. To help us do this (and other things we’ll see later), Python automatically creates a variable called __name__ in each module. If the module is the main program, that variable is assigned the string '__main__'. Otherwise, it is assigned the module’s name. Using this leads to modules like this: import sys from pytest import approx USAGE = &#39;&#39;&#39;zipf num [num...]: are the given values Zipfy?&#39;&#39;&#39; RELATIVE_ERROR = 0.05 def make_zipf(length): ...as before... def is_zipf(hist, rel=RELATIVE_ERROR): ...as before... if __name__ == &#39;__main__&#39;: if len(sys.argv) == 1: print(USAGE) else: values = [int(a) for a in sys.argv[1:]] result = is_zipf(values) print(&#39;{}: {}&#39;.format(result, values)) sys.exit(0) Here, the code guarded by if __name__ == '__main__' isn’t executed when the file loaded by something else. We can test this by re-running use.py as before: the usage message doesn’t appear, which means the main block wasn’t executed, which is what we want. 20.6 How can I install a Python package? The most common way to install Python packages is to use a tool called pip. The command pip install package checks to see if the package is already installed (or needs to be upgraded); if so, it downloads the package from PyPI (the Python Package Index), unpacks it, and installs it. Depending on where Python is installed, completing that installation may require administrative privileges; for example, if Python is installed in /usr/bin/python, you may need to run sudo to overwrite previously-installed libraries. This is a bad idea, since system tools may depend on particular versions of those packages, and may break if you overwrite them. Section 20.10 shows how to avoid these problems. Since a project may depend on many packages, developers frequently put a list of those dependencies in a file called requirements.txt. pip install -r requirements.txt will then install the dependencies listed in that file. (The file can be called anything, but everyone uses requirements.txt, so you should too.) This file can just list package names, or it can specify exact versions, minimum versions, etc.: request scipy==1.1.0 tdda&gt;=1.0 If you want to create a file like this, pip freeze will print the exact versions of all installed packages. You usually won’t use this directly in requirements.txt, since your project probably doesn’t depend on all of the listed files, but it’s a good practice to save this in version control when producing reports so that you can reproduce your results later (Chapter 21). 20.7 How can I create an installable Python package? Packages have to come from somewhere, and that “somewhere” is mostly developers like you. Creating a Python package is fairly straightforward, and mostly comes down to having the right directory structure. A package is a directory that contains a file called __init__.py, and may contain other files or sub-directories containing files. __init__.py can contain useful code or be empty, but either way, it has to be there to tell Python that the directory is a package. For our Zipf example, we can reorganize our files as follows: +- use.py +- zipf +- __init__.py zipf/__init__.py contains RELATIVE_ERROR and the functions we’ve seen before. use.py doesn’t change—in particular, it can still use import zipf or from zipf import is_zipf even though there isn’t a file called zipf.py any longer. When we run use.py, it loads zipf/__init__.py when import zipf executes and creates a __pycache__ directory inside zipf. 20.8 How can I manage the source code of large packages? As our package grows, we should split its source code into multiple files. To show how this works, we can put the Zipf generator in zipf/generate.py: def make_zipf(length): assert length &gt; 0, &#39;Zipf distribution must have at least one element&#39; result = [1/(1 + i) for i in range(length)] return result and then import that file in __init__.py. import sys from pytest import approx from . import generate RELATIVE_ERROR = 0.05 def is_zipf(hist, rel=RELATIVE_ERROR): assert len(hist) &gt; 0, &#39;Cannot test Zipfiness without data&#39; scaled = [h/hist[0] for h in hist] perfect = generate.make_zipf(len(hist)) return scaled == approx(perfect, rel=rel) We write that import as from . import generate to make sure that we will get the generate.py file in the same directory as __init__.py (the . means “current directory”, just as it does in the Unix shell). When we arrange our code like this, the code that uses the library must refer to zipf.generate.make_zipf: import zipf generated = zipf.generate.make_zipf(5) print(&#39;generated distribution: {}&#39;.format(generated)) generated[-1] *= 2 print(&#39;passes test with default tolerance: {}&#39;.format(zipf.is_zipf(generated))) print(&#39;passes test with tolerance of 1.0: {}&#39;.format(zipf.is_zipf(generated, rel=1.0))) 20.9 How can I distribute software packages that I have created? People can always get your package by cloning your repository and copying files from that (assuming your repository is accessible, which is should be for published research), but it’s much friendlier to create something they can install. For historical reasons, Python has several ways to do this. We will show how to use setuptools, which is the lowest common denominator; conda is a modern does-everything solution, but has larger startup overhead. To use setuptools, we must create a file called setup.py in the directory above the root directory of the package: +- setup.py +- use.py +- zipf +- __init__.py +- generate.py The file setup.py must have exactly that name, and must contain these lines: from setuptools import setup, find_packages setup( name=&#39;zipf&#39;, version=&#39;0.1&#39;, author=&#39;Greg Wilson&#39;, packages=find_packages() ) The name, version, and author parameters to setup are self-explanatory; you should modify the values assigned to them for your package; the function find_packages returns a list of things worth packaging. Once you have created this file, you can run python setup.py sdist to create your package. The verb sdist stands for “source distribution”, meaning that the source of the Python files is included in the package: $ python setup.py sdist running sdist running egg_info creating zipf.egg-info writing zipf.egg-info/PKG-INFO writing dependency_links to zipf.egg-info/dependency_links.txt writing top-level names to zipf.egg-info/top_level.txt writing manifest file &#39;zipf.egg-info/SOURCES.txt&#39; reading manifest file &#39;zipf.egg-info/SOURCES.txt&#39; writing manifest file &#39;zipf.egg-info/SOURCES.txt&#39; warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md running check warning: check: missing required meta-data: url warning: check: missing meta-data: if &#39;author&#39; supplied, &#39;author_email&#39; must be supplied too creating zipf-0.1 creating zipf-0.1/zipf creating zipf-0.1/zipf.egg-info copying files to zipf-0.1... copying setup.py -&gt; zipf-0.1 copying zipf/__init__.py -&gt; zipf-0.1/zipf copying zipf/generate.py -&gt; zipf-0.1/zipf copying zipf.egg-info/PKG-INFO -&gt; zipf-0.1/zipf.egg-info copying zipf.egg-info/SOURCES.txt -&gt; zipf-0.1/zipf.egg-info copying zipf.egg-info/dependency_links.txt -&gt; zipf-0.1/zipf.egg-info copying zipf.egg-info/top_level.txt -&gt; zipf-0.1/zipf.egg-info Writing zipf-0.1/setup.cfg creating dist Creating tar archive removing &#39;zipf-0.1&#39; (and everything under it) We will look at how to clean up the warnings about README.md, url, and author_email in the exercises. python setup.py sdist creates a compressed file dist/zipf-0.1.tar.gz that contains the following: $ tar ztvf dist/zipf-0.1.tar.gz drwxr-xr-x 0 pterry staff 0 20 Aug 15:36 zipf-0.1/ -rw-r--r-- 0 pterry staff 180 20 Aug 15:36 zipf-0.1/PKG-INFO -rw-r--r-- 0 pterry staff 38 20 Aug 15:36 zipf-0.1/setup.cfg -rw-r--r-- 0 pterry staff 145 20 Aug 13:40 zipf-0.1/setup.py drwxr-xr-x 0 pterry staff 0 20 Aug 15:36 zipf-0.1/zipf/ -rw-r--r-- 0 pterry staff 317 20 Aug 13:34 zipf-0.1/zipf/__init__.py -rw-r--r-- 0 pterry staff 163 20 Aug 13:34 zipf-0.1/zipf/generate.py drwxr-xr-x 0 pterry staff 0 20 Aug 15:36 zipf-0.1/zipf.egg-info/ -rw-r--r-- 0 pterry staff 1 20 Aug 15:36 zipf-0.1/zipf.egg-info/dependency_links.txt -rw-r--r-- 0 pterry staff 180 20 Aug 15:36 zipf-0.1/zipf.egg-info/PKG-INFO -rw-r--r-- 0 pterry staff 154 20 Aug 15:36 zipf-0.1/zipf.egg-info/SOURCES.txt -rw-r--r-- 0 pterry staff 5 20 Aug 15:36 zipf-0.1/zipf.egg-info/top_level.txt The source files __init__.py and generate.py are in there, along with the odds and ends that pip will need to install this package properly when the time comes. 20.10 How can I manage the packages my projects need? We want to test the package we just created, but we don’t want to affect the packages we already have installed, and as noted earlier, we may not have permission to write into the directory that contains system-wide packages. (For example, we may be testing something out on a cluster shared by our whole department.) The solution is to use a virtual environment. These are slowly being superceded by more general solutions like Docker, but they are still the easiest solution for most of us. A virtual environment is a layer on top of an existing Python installation. Whenever Python needs to find a library, it looks in the virtual environment first. If it can satisfy its needs there, it’s done; otherwise, it looks in the underlying environment. This gives us a place to install packages that only some projects need, or that are still under development, without affecting the main installation. FIXME: figure We can create and manage virtual environments using a tool called virtualenv. To install it, run pip install virtualenv. Once we have done that, we can create a new virtual environment called test by running: $ virtualenv test Using base prefix &#39;/Users/pterry/anaconda3&#39; New python executable in /Users/pterry/test/bin/python Installing setuptools, pip, wheel... done. virtualenv creates a new directory called test, which contains sub-directories called bin, lib, and so on—everything needed for a minimal Python installation. Crucially, test/bin/python checks for packages in test/lib before checking the system-wide install. We can switch to the test environment by running: $ source test/bin/activate source is a Unix shell command meaning “run all the commands from a file in this currently-active shell”. We use it because typing test/bin/activate on its own would run those commands in a sub-shell, which would have no effect on the shell we’re in. Once we have done this, we’re running the Python interpreter in test/bin: $ which python /Users/pterry/test/bin/python We can now install packages to our heart’s delight. Everything we install will go under test, and won’t affect the underlying Python installation. When we’re done, we can switch back to the default environment with deactivate. (We don’t need to source this.) Most developers create a directory called ~/envs (i.e., a directory called envs directly below their home directory) to store their virtual environments: $ cd ~ $ mkdir envs $ which python /Users/pterry/anaconda3/bin/python $ virtualenv envs/test Using base prefix &#39;/Users/pterry/anaconda3&#39; New python executable in /Users/pterry/envs/test/bin/python Installing setuptools, pip, wheel...done. $ which python /Users/pterry/anaconda3/bin/python $ source envs/test/bin/activate (test) $ which python /Users/pterry/envs/test/bin/python (test) $ deactivate $ which python /Users/pterry/anaconda3/bin/python Notice how every command now displays (test) when that virtual environment is active. Between Git branches and virtual environments, it can be very easy to lose track of what exactly you’re working on and with. Having prompts like this can make it a little less confusing; using virtual environment names that match the names of your projects (and branches, if you’re testing different environments on different branches) quickly becomes essential. 20.11 How can I test package installation? Now that we have a virtual environment set up, we can test the installation of our Zipf package: $ pip install ./src/package/05/dist/zipf-0.1.tar.gz Processing ./src/package/05/dist/zipf-0.1.tar.gz Building wheels for collected packages: zipf Running setup.py bdist_wheel for zipf ... done Stored in directory: /Users/pterry/Library/Caches/pip/wheels/6b/de/80/d72bb0d6e7c65b6e413f0cf10f04b4bbccb329767853fe1644 Successfully built zipf Installing collected packages: zipf Successfully installed zipf-0.1 (test) $ python &gt;&gt;&gt; import zipf &gt;&gt;&gt; zipf.RELATIVE_ERROR 0.05 $ pip uninstall zipf Uninstalling zipf-0.1: Would remove: /Users/pterry/envs/test/lib/python3.6/site-packages/zipf-0.1.dist-info/* /Users/pterry/envs/test/lib/python3.6/site-packages/zipf/* Proceed (y/n)? y Successfully uninstalled zipf-0.1 (test) Again, one environment per project and one project per environment might use more disk space than is absolutely necessary, but it will still be less than most of your data sets, and will save a lot of debugging. 20.12 Announcing Work FIXME: https://medium.com/indeed-engineering/marketing-for-data-science-a-7-step-go-to-market-plan-for-your-next-data-product-60c034c34d55 20.13 Summary FIXME: create concept map for packages 20.14 Exercises 20.14.1 Clean up warning messages FIXME: clean up warning messages from python setup.py sdist 20.15 Key Points Use virtualenv to create a separate virtual environment for each project. Use pip to create a distributable package containing your project’s software, documentation, and data. "],
["publish.html", "Chapter 21 Publishing 21.1 Questions 21.2 Objectives 21.3 Introduction 21.4 What is the most useful way to share my data? 21.5 What standards of data sharing should I aspire to? 21.6 What data should I publish and how? 21.7 How can I identify my work for citation? 21.8 How can I identify myself in my publications? 21.9 What software should I publish and how? 21.10 Summary 21.11 Exercises 21.12 Key Points", " Chapter 21 Publishing 21.1 Questions How and where should I publish my reports? What should I include in my publications? 21.2 Objectives Explain what to include in publications, and where to publish large, medium, and small datasets. Explain what DOIs and ORCIDs are. Get an ORCID. Describe the FAIR Principles and determine whether a dataset conforms to them. Obtain DOIs for datasets, reports, and software packages. 21.3 Introduction This lesson looks at what should be included in reports and how best to do that. We use the generic term “report” to include research papers, summaries for clients, and everything else that is shorter than a book and is going to be read by someone else. Our motivation is summed up in this quotation: An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures. – Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in Buckheit and Donoho (1995) While some things can’t be published without violating personal or commercial confidentiality, every researcher’s default should be to make their work as widely available as possible. That means publishing it in an open access venue (Chapter 19) so that people who aren’t in academia can find it and read it. Software can be published in JOSS or F1000 Research, and you can use the Unpaywall browser extension to find what other people have been willing to share. 21.4 What is the most useful way to share my data? Making data useful to other people (including your future self) is one of the best investments you can make. The simple version of how to do this is: Always use tidy data. Include keywords describing the data in the project’s README.md so that they appear on its home page and can easily be found by search engines. Give every dataset and every report a unique identifier (Section 21.7). Put data in open repositories (Section 21.6). Use well-known formats like CSV and HDF5. Include an explicit license in every project and every dataset. Don’t just describe how to get the data: include scripts that do it. Include units and other metadata. The last point is often the hardest for people to implement, since many researchers have never seen a well-documented dataset. We draw inspiration from the data catalog included in the repository for the article “Women’s Pockets Are Inferior” and include a file ./data/README.md in every project that looks like this: - Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017 - `Infant_HIV_Testing_2017.xlsx` - What is this?: Excel spreadsheet with summarized data. - Source(s): UNICEF, &lt;https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/&gt; - Last Modified: July 2018 (according to website) - Contact: Greg Wilson &lt;greg.wilson@rstudio.com&gt; - Spatial Applicability: global - Temporal Applicability: 2009-2017 - `infant_hiv.csv` - What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx` - Notes - Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent. - Use `tidy_infant_hiv()` to tidy this data. - Maternal health indicators disaggregated by age - `maternal_health_adolescents_indicators_April-2016_250d599.xlsx` - What is this?: Excel spreadsheet with summarized data. - Source(s): UNICEF, &lt;https://data.unicef.org/resources/dataset/maternal-health-data/&gt; - Last Modified: July 2018 (according to website) - Contact: Greg Wilson &lt;greg.wilson@rstudio.com&gt; - Spatial Applicability: global - Temporal Applicability: 2000-2014 - `at_health_facilities.csv` - What is this?: percentage of births at health facilities by country, year, and mother&#39;s age - Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx` - `c_sections.csv` - What is this?: percentage of Caesarean sections by country, year, and mother&#39;s age - Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx` - `skilled_attendant_at_birth.csv` - What is this?: percentage of births with skilled attendant present by country, year, and mother&#39;s age - Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx` - Notes - Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent. - Use `tidy_maternal_health_adolescents()` to tidy this data. The catalog above doesn’t include column headers or units because the data isn’t tidy. It does include the names of the functions used to reformat that data, and ./results/README.md then includes the information that users will want. One section of that file is shown below: - Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017 - infant_hiv.csv - What is this?: tidied version of CSV export from spreadsheet. - Source(s): UNICEF, &lt;https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/&gt; - Last Modified: September 2018 - Contact: Greg Wilson &lt;greg.wilson@rstudio.com&gt; - Spatial Applicability: global - Temporal Applicability: 2009-2017 - Generated By: scripts/tidy-24.R | Header | Datatype | NA | Description | |----------|----------|-------|---------------------------------------------| | country | char | false | ISO3 country code of country reporting data | | year | integer | false | year CE for which data reported | | estimate | double | true | estimated percentage of measurement | | hi | double | true | high end of range | | lo | double | true | low end of range | Note that this catalog includes both units and whether or not a field can be NA. Note also that calling a field “NA” is asking for trouble… 21.5 What standards of data sharing should I aspire to? The FAIR Principles describe what research data should look like. They are still aspirational for most researchers, but they tell us what to aim for. The most immediately important elements of the FAIR Principles are outlined below. 21.5.1 Data should be findable. The first step in using or re-using data is to find it. You can tell you’ve done this if: (Meta)data is assigned a globally unique and persistent identifier (Section 21.7). Data is described with rich metadata (like the catalog shown above). Metadata clearly and explicitly includes the identifier of the data it describes. (Meta)data is registered or indexed in a searchable resource, such as the data sharing platforms described in Section 21.6. 21.5.2 Data should be accessible. You can’t use data if you don’t have access to it. In practice, this rule means the data should be openly accessible (the preferred solution) or that authenticating in order to view or download it should be free. You can tell you’ve done this if: (Meta)data is retrievable by its identifier using a standard communications protocol like HTTP. Metadata is accessible even when the data is no longer available. 21.5.3 Data should be interoperable. Data usually needs to be integrated with other data, which means that tools need to be able to process it. You can tell you’ve done this if: (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation (Meta)data uses vocabularies that follow FAIR principles (Meta)data includes qualified references to other (meta)data 21.5.4 Data should be reusable. This is the ultimate purpose of the FAIR Principles and much other work. You can tell you’ve done this if: Meta(data) is described with accurate and relevant attributes. (Meta)data is released with a clear and accessible data usage license. (Meta)data has detailed provenance. (Meta)data meets domain-relevant community standards. 21.6 What data should I publish and how? Small datasets (i.e., anything under 500 MB) can be stored in version control using the conventions described in Chapter 18. If the data is being used in several projects, it may make sense to create one repository to hold only the data; the R community refers to these as data packages, and they are often accompanied by small scripts to clean up and query the data. Be sure to give the dataset an identifier as discussed in Section 21.7. For medium-sized datasets (between 500 MB and 5 GB), it’s better to put the data on platforms like the Open Science Framework, Dryad, and Figshare. Each of these will give the datasets identifiers; those identifiers should be included in reports along with scripts to download the data. Big datasets (i.e., anything more than 5 GB) may not be yours in the first place, and probably need the attention of a professional archivist. Any processed or intermediate data that takes a long time to regenerate should probably be published as well using these same sizing rules; all of this data should be given identifiers, and those identifiers should be included in reports. 21.7 How can I identify my work for citation? A Digital Object Identifier (DOI) is a unique identifier for a particular version of a particular digital artifact such as a report, a dataset, or a piece of software. DOIs are written as doi:prefix/suffix, but you will often also see them represented as URLs like http://dx.doi.org/prefix/suffix. We can use Zenodo to get DOIs for free: Log in to Zenodo using your GitHub ID. (The first time you do this, you will have to authorize the application so that Zenodo can read data from your GitHub repositories.) Once you have logged in, pick a repository and click the “On” button. Go back to that GitHub repository and go to the “Releases” tab. Create a new release, give it a version number (discussed in Section 9.9), and then click “Publish release”. Finally, go back to Zenodo and look under the “Upload” tab for a new upload corresponding to this release. Once it appears, fill in the required information and publish it. A Zenodo DOI badge will automatically appear on the GitHub project’s site, and anyone with the DOI will be able to find it. FIXME: screenshots and a diagram showing how this process works 21.8 How can I identify myself in my publications? An ORCID is an Open Researcher and Contributor ID. You can get an ORCID for free, and you should include it in publications because people’s names and affiliations change over time. 21.9 What software should I publish and how? Including a link to a GitHub repository in your publications is a good first step, but it is only the first step. Software changes over time, and your scripts almost certainly depend on specific versions of other packages (Chapter 20). It therefore seems logical to include the exact version numbers of everything used in your analysis in each publication, but that’s impractical. Should you include the version of the compiler used to build the R or Python interpreter you used? What about the operating system? Is a hardware specification needed in case it turns out that the processor you used had a bug? Some people now believe that researchers should put everything in a virtual machine and share that, but a Docker image is like a screenshot: all the bits are there, but it’s a lot of work to get the information out. Librarians, publishers, and regulatory bodies are still trying to find useful answers to these questions. For the moment, the best advice we can give it to publish the version IDs of the standard software used in the analysis. As Section 20.6 described, you can get these automatically by running: $ pip freeze &gt; requirements.txt For everything else, you should write a script or create a rule in your project’s Makefile (Chapter 3), since the commands used to get version numbers will vary from tool to tool: ## versions : dump versions of software. versions : @echo &#39;# Python packages&#39; @pip freeze @echo &#39;# dezply&#39; @dezply --version @echo &#39;# parajune&#39; @parajune --status | head 1 The scripts, notebooks, and/or Makefiles used to produce results tend to evolve at a different rate than data, so they should get separate DOIs. Depending on the size or complexity of the software you have written, and whether you re-use it in multiple projects, you may publish script by script or create a zip file or tar file that includes everything. For example, the Makefile fragment below creates ~/archive/meow-2019-02-21.tgz: ARCHIVE=${HOME}/archive PROJECT=meow TODAY=$(shell date &quot;+%Y-%m-%d&quot;) SCRIPTS=./Makefile ./bin/*.py ./bin/*.sh ## archive : create an archive of all the scripts used in this run archive : @mkdir -p ${ARCHIVE} @tar zcf ${ARCHIVE}/${PROJECT}-${TODAY}.tgz Finally, it’s imperative that you include the configuration files and command-line parameters that you used to generate particular runs. If all of the program’s parameters are in a configuration file (Chapter 5), you can archive that. Otherwise, have your program print out its configuration parameters and use grep or a script to extract from the logfile (Chapter 6). If you can’t do that because you’re using someone else’s software, write a small shell script that logs the configuration and then runs the program. 21.10 Summary FIXME: create concept map for publishing 21.11 Exercises FIXME: create exercises for publishing 21.12 Key Points Include small datasets in repositories; store large ones on data sharing sites, and include metadata in the repository to locate them. An ORCID is a unique personal identifier that you can use to identify your work. A DOI is a unique identifier for a particular dataset, report, or software release. Data should be findable, accessible, interoperable, and reusable (FAIR). Use Zenodo to obtain DOIs. Publish your software as you would a paper. References "],
["teamwork.html", "Chapter 22 Teamwork 22.1 Questions 22.2 Objectives 22.3 Introduction 22.4 How can we run meetings more efficiently? 22.5 What should we do when the meeting is over? 22.6 How can we keep people from talking too much or too little? 22.7 How should we run online meetings? 22.8 What sort of people make teamwork hard? 22.9 How should we handle conflict within the team? 22.10 Summary 22.11 Key Points", " Chapter 22 Teamwork 22.1 Questions FIXME 22.2 Objectives FIXME 22.3 Introduction FIXME: introduction 22.4 How can we run meetings more efficiently? Most people do meetings poorly: they don’t have an agenda going in, they don’t take minutes, they waffle on or wander off into irrelevancies, they repeat what others have said or recite banalities simply so that they’ll have said something, and they hold side conversations (which pretty much guarantees that the meeting will be a waste of time). Knowing how to run a meeting efficiently is a core skill for anyone who wants to get things done. Knowing how to take part in someone else’s meeting is just as important, but gets far less attention: everyone offers leadership training, nobody offers followership training. The rules for running meetings quickly and smoothly are well known but rarely followed: Decide if there actually needs to be a meeting. If the only purpose is to share information, have everyone send a brief email instead. Remember, people can read faster than they can speak: if someone has facts for the rest of the team to absorb, the most polite way to communicate them is to type them in. Write an agenda. If nobody cares enough about the meeting to prepare a point-form list of what’s to be discussed, the meeting itself probably doesn’t need to happen. Include timings in the agenda. Timings help you prevent early items stealing time from later ones. Your first estimates with any new group will be wildly optimistic, so revise them upward for subsequent meetings. However, you shouldn’t plan a second or third meeting just because the first one ran over-time: instead, try to figure out why you’re running over and fix the underlying problem. Prioritize. Tackle issues that will have high impact but take little time first, and things that will take more time but have less impact later. That way, if the first things run over time, the meeting will still have accomplished something. Make one person responsible for keeping things moving. One person should be made chair, and be responsible for keeping items to time, chiding people who are having side conversations or checking email, and asking people who are talking too much to get to the point. The chair should not do all the talking; in fact, whoever is in charge will talk less in a well-run meeting than most other participants. Require politeness. No one gets to be rude, no one gets to ramble, and if someone goes off topic, it’s the chair’s job to say, “Let’s discuss that elsewhere.” No interruptions. Participants should raise a finger, put up a sticky note, or make one of the other gestures people use at high-priced auctions when they want to speak. The chair should keep track of who wants to speak and give them time in turn. No side conversations. This makes meetings more efficient because Nobody can actually pay attention to two things at once. If distractions are tolerated, people will miss things or they’ll have to be repeated. More importantly, not paying attention is insulting—chatting with a friend while someone explains what they did last week is a really effective way to say, “I don’t think you or your work is important.” No technology. Insist that everyone put their phones, tablets, and laptops into politeness mode (i.e., close them). If this is too stressful, let participants hang on to their electronic pacifiers but turn off the network so that they really are using them just to take notes or check the agenda. The one exception is accessibility needs: if someone needs their phone, their laptop, or some other aid in order to take part in the meeting, nobody has a right to tell them “no”. Take minutes. Someone other than the chair should take point-form notes about the most important information that was shared, and about every decision that was made or every task that was assigned to someone. Take notes. While other people are talking, participants should take notes of questions they want to ask or points they want to make. (You’ll be surprised how smart it makes you look when it’s your turn to speak.) End early. If your meeting is scheduled for 10:00-11:00, aim to end at 10:55 to give people time to get where they need to go next. 22.5 What should we do when the meeting is over? As soon as the meeting is over, circulate the minutes (i.e., emailed them to everyone or post them to a wiki): People who weren’t at the meeting can keep track of what’s going on. You and your teammates all have to juggle tasks from other projects or courses, which means that sometimes you won’t be able to make it to meetings. A wiki page, email message, or blog entry is a much more efficient way to catch up than asking a team mate, “Hey, what did I miss?” Everyone can check what was actually said or promised. More than once, one of us has looked over the minutes of a meeting and thought, “Did I say that?” or, “Wait a minute, I didn’t promise to have it ready then!” Accidentally or not, people will often remember things differently; writing it down gives the team a chance to correct mis-recollection, mis-interpretation, or mis-representation, which can save a lot of anguish later on. People can be held accountable at subsequent meetings. There’s no point making lists of questions and action items if you don’t follow up on them later. If you’re using a ticketing system (Chapter 10), create a ticket for each new question or task right after the meeting and update those that are being carried forward. That way, your agenda for the next meeting can start by rattling through the list of open tickets. Run all your meetings like this for a month, with the goal of making each one a minute shorter than the one before, and we promise that you’ll build better software. 22.6 How can we keep people from talking too much or too little? Some people are so used to the sound of their own voice that they will talk half the time no matter how many other people are in the room. One way to combat this is to give everyone three sticky notes at the start of the meeting. Every time they speak, they have to give up one sticky note. When they’re out of stickies, they aren’t allowed to speak until everyone has used at least one, at which point everyone gets all of their sticky notes back. This ensures that nobody talks more than three times as often as the quietest person in the meeting, which completely changes group dynamics: people who have given up trying to be heard because they always get trampled suddenly have space to contribute, and the overly-frequent speakers quickly realize just how unfair they have been. Another useful technique is called interruption bingo. Draw a grid and label the rows and columns with the participants’ names. Each time one person interrupts another, add a tally mark to the appropriate cell; halfway through the meeting, take a moment to look at the results. In most cases, you will see that one or two people are doing all of the interrupting, often without being aware of it. After that, saying, “All right, I’m adding another tally to the bingo card,” is often enough to get them to throttle back. (Note that this technique is for managing interruptions, not speaking time. It may be completely appropriate for people with more knowledge of a subject to speak about it more often in a meeting, but it is never appropriate to repeatedly cut people off.) 22.7 How should we run online meetings? Chelsea Troy’s discussion of why online meetings are often frustrating and unproductive makes an important point: in most online meetings, the first person to speak during a pause gets the floor. As a result, “If you have something you want to say, you have to stop listening to the person currently speaking and instead focus on when they’re gonna pause or finish so you can leap into that nanosecond of silence and be the first to utter something. The format…encourages participants who want to contribute to say more and listen less.” The solution is to run a text chat beside the video conference where people can signal that they want to speak. The chair can then select people from the waiting list. This practice can be reinforced by having everyone mute themselves, and only allowing the moderator to unmute people. 22.8 What sort of people make teamwork hard? FIXME: introduction (without Tolstoy) Anna knows more about every subject than everyone else on the team put together—at least, she thinks she does. No matter what you say, she’ll correct you; no matter what you know, she knows better. Anna is pretty easy to spot: if you keep track of how often people interrupt one another in team meetings, her score is usually higher than everyone else’s put together. Bao is a contrarian: no matter what anyone says, he’ll take the opposite side. This can be healthy in small doses, but if someone plays devil’s advocate at every turn, they’re just impeding progress. Caitlin has so little confidence in her own ability (despite her good grades) that she won’t make any decision, no matter how small, until she has checked with someone else. Everything has to be spelled out in detail for her so that there’s no possibility of her getting anything wrong. Frank believes that knowledge is power. He enjoys knowing things that other people don’t—or to be more accurate, he enjoys it when people know he knows things they don’t. Frank is good at making things work, but when asked how he did it, he’ll grin and say, “Oh, I’m sure you can figure it out.” Gary is a hitchhiker. He has discovered that most people would rather shoulder some extra work than snitch, and he takes advantage of it at every turn. The frustrating thing is that he’s so damn plausible when someone finally does confront him. “There have been mistakes on all sides,” he says, or, “Well, I think you’re nit-picking.” The only way to deal with Kenny is to stand up to him: remember, if he’s not doing his share, he’s the bad guy, not you. Hediyeh is quiet—very quiet. She never speaks up in meetings, even when she knows that what other people are saying is wrong. She might contribute to the mailing list, but she’s very sensitive to criticism, and will always back down rather than defending her point of view. Hediyeh isn’t a troublemaker, but rather a lost opportunity. Melissa would easily have made the varsity procrastination team if she’d bothered to show up to tryouts. She means well, and she really does feel bad about letting people down, but somehow something always comes up, and her tasks are never finished until the last possible moment. Of course, that means that everyone who is depending on her can’t do their work until after the last possible moment… Petra has a favorite phrase: “why don’t we”. Why don’t we write a GUI to help people edit the program’s configuration files? Hey, why don’t we invent our own little language for designing GUIs? Her energy and enthusiasm are hard to argue with, but argue you must. Otherwise, for every step you move forward, the project’s goalposts will recede by two. This is called feature creep, and has ruined many projects that might otherwise have delivered something small but useful. Raj is rude. “That’s stupid,” and a mixed bag of obscenities are his favorite phrases. “It’s just the way I talk,” he says, not knowing or not caring that for a lot of people, that kind of language has often been a prelude to bullying or worse. His only redeeming grace is that he can’t dissemble as well as Gary, so he’s easier to get rid of. Sergei is simply incompetent. He doesn’t understand the problem, he hasn’t bothered to master the tools and libraries he’s supposed to be using, the code he checks in doesn’t run, and his thirty-second bug fixes introduce more problems than they solve. If he means well, try to re-partition the work so that he’ll do less damage. If he doesn’t, he should be treated like any other hitchhiker. 22.9 How should we handle conflict within the team? FIXME: this is not about harassment or abuse—see Chapter 19 for that. You just missed an important deadline, and people are unhappy. The sick feeling in the pit of your stomach has turned to anger: you did your part, but Marta didn’t finish her stuff until the very last minute, which meant that no one else had time to spot the two big mistakes she’d made. As for Chul, well, he didn’t deliver at all—again. If something doesn’t change, this project is going to pull down your performance review so far that you might have to start looking for a new job. Situations like this come up all the time. Broadly speaking, there are four ways you can deal with them: Cross your fingers and hope that things will get better on their own, even though the last three times you hoped they would, they didn’t. Do extra to make up for others’ shortcomings. This sometimes works in the short term, and saves you the mental anguish of confronting others, but the time for that “extra” has to come from somewhere; what usually ends up happening is that other parts of the project, or your personal life, suffer. Lose your temper and start shouting. Unfortunately, people often wind up displacing their anger into other parts of their life: I’ve seen developers yell at waitresses for bringing incorrect change when what they really needed to do was tell their boss, “No, I won’t work through another holiday weekend to make up for your decision to short-staff the project.” Take constructive steps to fix the underlying problem. Most of us find number four hard because we don’t like confrontation. If you manage it properly, though, it is a lot less bruising, which means that you don’t have to be as afraid of initiating it. Also, if people believe that you will actually take steps when they bully, lie, procrastinate, or do a half-assed job, they will usually avoid making it necessary. (A colleague once said that she had never met anyone who wished they had waited longer before putting their foot down.) Here are the steps you should take when you feel that a teammate isn’t pulling their weight: Make sure you’re not guilty of the same sin. You won’t get very far complaining about someone else interrupting in meetings if you do it just as frequently. Check expectations. Are you sure the offender knows what standards they are supposed to be meeting? This is where things like job descriptions or up-front discussion of who’s responsible for what come in handy. Check the situation. Is someone dealing with an ailing parent or immigration woes? Have they been put to work on three other projects that you don’t know about? Document the offense. Write down what the offender has actually done and why it’s not good enough. Doing this will help you clarify matters in your own mind. It’s also absolutely necessary if you have to escalate. Check with other team members. Are you alone in feeling that the offender is letting the team down? If so, you aren’t necessarily wrong, but it’ll be a lot easier to fix things if you have the support of the rest of the team. Finding out who else on the team is unhappy can be the hardest part of the whole process, since you can’t even ask the question without letting on that you’re upset, and word will almost certainly get back to whoever you’re asking about, who might then turn around and accuse you of stirring up trouble. After a couple of unhappy experiences of this kind, I’ve learned that it’s best to raise the issue for the first time in a group of three: you, the person you want to talk to, and a third person who can act as unofficial moderator. Talk with the offender. This should be a team effort: put it on the agenda at a team meeting, present your complaint, and make sure that the offender understands it. In most cases this is enough: human beings are herd animals, and if someone realizes that they’re going to be called on their hitchhiking or bad manners, they will usually change their ways. Escalate as soon as there’s a second offense. Hitchhikers and others who really don’t have good intentions are counting on you giving them one last chance after another until the project is finished and they can go suck the life force out of their next victim. Don’t fall into this trap. If someone stole your laptop, you’d report it right away. If someone steals your time, you’re being pretty generous giving them more than one chance to mend their ways. In the context of a research project, “escalation” means “taking the issue to your supervisor”. (If you’re reluctant to do this because you don’t want to be a snitch, go back and re-read the bit about people stealing your laptop.) Of course, your supervisor has probably had dozens of students complain to her over the years about teammates not doing their share—it isn’t uncommon to have both halves of a pair tell the instructor that they’re doing all the work. (This is yet another reason to use version control: it makes it easy to check who’s actually written what.) In order to get her to take you seriously and help you fix your problem, you should send her an email, signed by several people describing the problem and the steps you have already taken to resolve it. Make sure the offender gets a copy as well, and ask your instructor to arrange a meeting to resolve the issue. This is where documentation is crucial. Hitchhikers are usually very good at appearing reasonable; they’re very likely to nod as you present your case, then say, “Well, yes, but…” and rhyme off a bunch of minor exceptions or cases where others on the team have also fallen short of expectations. If you can’t back up your complaint, your supervisor will likely be left with the impression that the whole team is dysfunctional, and nothing will improve. One technique your supervisor may ask you to use in a meeting like this is active listening. As soon as one person makes a point, the person on the opposite side of the issue explains it back to them, as in, “So what I think Igor is saying is…” This guarantees that the second person has actually paid attention to what the first person said. It can also defuse a lot of tension, since explaining someone’s position back to them forces you to see the world through their eyes, if only for a few moments. 22.10 Summary Brown (2007) has lots of good advice on running meetings. Brookfield and Preskill (2016) is a catalog of ways to get people talking productively. 22.11 Key Points FIXME References "],
["pacing.html", "Chapter 23 Pacing 23.1 Questions 23.2 Objectives 23.3 Introduction 23.4 How much work is overwork and what are its effects? 23.5 How can I manage my time so that I can keep to a productive schedule? 23.6 What happens if I don’t manage my workload sensibly? 23.7 Summary 23.8 Key Points", " Chapter 23 Pacing 23.1 Questions FIXME 23.2 Objectives FIXME 23.3 Introduction FIXME: introduction 23.4 How much work is overwork and what are its effects? FIXME: keep this personal? I used to brag about the hours I was working. Not in so many words, of course—I had some social skills—but I’d show up for class around noon, unshaven and yawning, and casually mention how I’d been up ’til 6:00 a.m. hacking away at a monster bug. Looking back, I can’t remember who I was trying to impress. Instead, what I remember is how much of the code I wrote in those all-nighters I threw away once I’d had some sleep, and how much damage those bleary-eyed stupors did to my mental health (and grades). My mistake was to confuse “working” with “being productive”. You can’t produce software (or anything else) without doing some work, but you can easily do lots of work without producing anything of value. Scientific study of the issue goes back to at least the 1890s; Robinson (2005) is a good short summary, and the most important facts are: Working more than eight hours a day for an extended period of time lowers your total productivity, not just your hourly productivity—i.e., you get less done in total when you’re in crunch mode than when you work regular hours. Working over 21 hours in a stretch increases the odds of you making a catastrophic error just as much as being legally drunk. These facts have been reproduced and verified hundreds of time, and the data behind them is as solid as the data linking smoking to lung cancer. However, while most smokers now acknowledge that their habit is killing them, many people in academia and the software industry still act as if they were somehow immune to the effects of sleep deprivation. To quote Robinson: When Henry Ford famously adopted a 40-hour workweek in 1926, he was bitterly criticized by members of the National Association of Manufacturers. But his experiments, which he’d been conducting for at least 12 years, showed him clearly that cutting the workday from ten hours to eight hours—and the workweek from six days to five days—increased total worker output and reduced production cost… the core of his argument was that reduced shift length meant more output. …many studies, conducted by businesses, universities, industry associations and the military, …support the basic notion that, for most people, eight hours a day, five days per week, is the best sustainable long-term balance point between output and exhaustion. Throughout the 30s, 40s, and 50s, these studies were apparently conducted by the hundreds; and by the 1960s, the benefits of the 40-hour week were accepted almost beyond question in corporate America. In 1962, the Chamber of Commerce even published a pamphlet extolling the productivity gains of reduced hours. But, somehow, Silicon Valley didn’t get the memo… I worked for a data visualization startup in the mid-1990s. Three months before our first release, the head of development “asked” us to start coming in on Saturdays. We were already pulling one late night a week at that point (with no mention of overtime pay—our boss seemed to think that ten dollars’ worth of pizza was adequate compensation for four hours of work), and most of us were also working at least a couple of hours at home in the evenings. It’s hardly surprising that we missed our “can’t miss” deadline by ten weeks, and had to follow up our 1.0 release with a 1.1, and then a 1.2, in order to patch the bugs we’d created. We were all zombies, and zombies can’t code. Those kinds of hours are sadly still normal in many parts of the software industry, and also in university programs. Designing and building software is a creative act that requires a clear head, but many otherwise-intelligent people act as if it was like digging a ditch. The difference is that it’s hard to lose ground when digging (though not impossible). In software, on the other hand, it only takes me a couple of minutes to create a bug that will take hours to track down later. This is summarized in Robinson’s first rule: Productivity varies over the course of the workday, with the greatest productivity occurring in the first four to six hours. After enough hours, productivity approaches zero; eventually it becomes negative. Ah, you say, but that’s long-term total output. What about short bursts now and then, like pulling an all-nighter to meet a deadline? Well, that’s been studied too, and the results aren’t pleasant. Your ability to think drops by 25% for each 24 hours you’re awake. Put it another way, the average person’s IQ is only 75 after one all-nighter, which puts them in the bottom 5% of the population. Do two all nighters in a row, and your effective IQ is 50, the level at which people are usually judged incapable of independent living. The catch in all of this is that people usually don’t notice their abilities declining. Just like drunks who think they’re still able to drive, people who are deprived of sleep don’t realize that they’re not finishing their sentences (or thoughts). They certainly don’t realize that they’re passing parameters into function calls the wrong way around, or that what they’re typing in will all have to be deleted and re-done tomorrow. So think very hard about what’s more important to you: the amount of good work you produce, or how hard your friends think you’re pushing yourself. Then think about which of those other people are actually going to care about, and pace yourself accordingly. 23.5 How can I manage my time so that I can keep to a productive schedule? “But I have so many assignments to do!”, you say. “And they’re all due at once! I have to work extra hours to get them all done!” No. In order to be productive, you have to prioritize and focus. The first is important because people naturally spend hours on things that don’t need to be done, then find themselves with too little time for the things that actually count. It can actually be expressed as an algorithm: Make a list of the things you have to do. I used to use a hardcover lab notebook for this, since I could doodle in it while I was on the subway, but these days it’s just a text file on my laptop. Other people use their phone or send themselves email messages that then go into a folder titled “To Do”. However you do it, the important thing is to write it down. You can only hold half a dozen things in working memory at once Hock (2004),Wilson (2018); if you try to manage a to-do list longer than that in your head, you will forget things. Weed out everything that you don’t need to do right away. Notice that I said “need”, not “want”: if you want to mess around with a new theme for your personal website, that’s fine, but that’s play time, not work time, and we’re talking about getting work done. Prioritize what’s left. Sort the list so that the most important tasks are at the top. I don’t worry about getting the stuff below the first three or four lines into exact order, since I’m going to re-check my list before I get to them anyway. Make sure you have everything you need to do the first task. Update your repository from version control, make sure you have an up-to-date copy of the assignment specification, install whatever libraries and documentation you need, and adjust your chair. Don’t give yourself an excuse to interrupt your own work: the world will provide enough of those. Turn off interruptions. Shut down email and put your phone in “Do Not Disturb” mode. (Most phones will let you disable notifications from all but a handful of people, so you won’t miss a call from daycare while you’re working.) Don’t panic, it’s only for an hour—most people can’t stay focused longer than that, and anyway, you’ll need to stretch your muscles and get rid of that tea you drank. Set an alarm to go off in fifty minutes. Don’t switch tasks in that time unless you absolutely have to. Instead, if you remember an email message that you need to send, or discover a couple of tests that really should be written, add notes to your to-do list. (This is another reason to keep a lab notebook: the few seconds it takes to pick up a pen and jot something down gives your hands a rest from the keyboard.) Take a break every hour. At the end of the fifty minutes, get up and stretch, check mail (but don’t reply to anything that isn’t urgent), go to the washroom, stretch again, and then re-order your to-do list and start the next round. Try to work six sprints each day. Yes, your day will be eight or nine hours long, but there are always interruptions, and you should plan realistically, not optimistically. If you can actually spend 75% of your time on the things you’re supposed to be doing, you’re a lot better at staying focused than most people. Keep track of what you accomplished. It’s easy to feel like you’re not making any progress on a large project. Marking a task as “done” may not feel like much in the moment, but when you look back at the end of the week, or are reporting what you’ve done to your teammates, having those notes will remind you that you’re actually making headway. If any task on your list is more than an hour long, break it down into into smaller pieces and prioritize those separately. Keep in mind that the future is approaching at a fixed rate of one day every 24 hours: if something’s going to take sixty hours to do, you’d better allow at least ten working days for it, which means you’d better tackle the first piece two weeks before the deadline. And since breaking large tasks down into small ones takes time, don’t be embarrassed by having “plan XYZ” as a task in your list. Remember, a week of hard work can sometimes save you an hour of thought (Chapter G). The point of all this organization and preparation is to get yourself into the most productive mental state possible. Psychologists call it flow Csikszentmihalyi (2008); athletes call it “being in the zone”, while musicians talk about losing themselves in what they’re playing. Whatever name you use, you will produce much more per hour in this state than normal. That’s the good news. The bad news is that it takes roughly ten minutes to get back into a state of flow after an interruption, no matter how short the interruption was. This means that if you are interrupted half a dozen times per hour, you are never at your productive peak. It’s very much like processes being paged in and out by an operating system: if it happens too often, the CPU spends all its time moving things around and none doing useful work. Making lists and setting one-hour alarms will probably seem a little earnest at first, but trust me: your friends will stop mocking you once they see that you’re able to finish your work and still have time to play some badminton and catch a movie. They may even start to imitate you. If timeslicing is bad, why do schools require you to do it all the time? Doing one course at a time for three weeks would be more efficient. However, it would be harder on instructors, and some things need time to digest. Similarly, companies may know that meetings distract from work as often as they move it forward (Chapter 22), but some things really do need to be discussed. I find I’m most productive when I do three back-to-back sprints on one project in the morning, then give three other projects an hour each in the afternoon. (I will also often schedule a meeting with a colleague so that we both have some time blocked out in our calendars to focus on what we’re supposed to deliver.) 23.6 What happens if I don’t manage my workload sensibly? We were eating dinner one Friday night when my daughter asked me, “Daddy, why don’t you ever laugh?” Coincidentally (or perhaps not) I had just finished reading Jesse Noller’s post A Lot Happens, in which he said: You can’t be emotionally all in on everything. You can’t make another 24 hours appear to be “present” for everything… I stole time and ran my emotional credit card like it was limitless. I stole time from my family, from work, from everything. In a companion piece written a month later, he showed the price of being “all in” by turning a classic XKCD cartoon (Figure 23.1) into something more personal (Figure 23.2). Figure 23.1: XKCD’s Version Figure 23.2: Jesse Noller’s Version It took a lot of courage to be as honest as Jesse was. What follows is my attempt to live up to his example. Starting when I was 19, I had bouts of depression every 12-18 months for a little over 20 years. Some were triggered by overwork or big life changes; others hit me out of the blue. Whatever the cause (and the cause might just have been neurochemistry—there’s history on both sides of my family), I would feel bleak and exhausted for anything from a couple of weeks to a couple of months. If you’ve had mononucleosis (glandular fever to the British), it feels exactly like that, but without the swollen glands and other physical symptoms—so much so that my first couple of bouts were misdiagnosed as recurrences of mono. I didn’t even try to get help until I was in my mid-30s. At first, it was because I didn’t know I could: two different doctors in two different countries told me that was a virus and all I could do was wait it out. And then, well, people didn’t talk about this stuff back then—not the people I knew—so I didn’t even know where to start. But I eventually met someone who’d been through it, and she pointed me in the right direction, and I finally got some counseling. Things were better for more than ten years, but in April 2015 I had a week of panic attacks and insomnia. I recovered fairly quickly, but had a relapse from late June until early August during which I could barely string three thoughts together. I was OK again until four very bleak days in early November while on holiday with family. I did this to myself. When I changed jobs on short notice in the summer of 2014 I expected things would be difficult, but it was even worse than I’d feared—so much so that I wound up working without pay in November and December. Not long after I found out that a major funder had blacklisted me because I was “difficult”. Things were so bad at one point that I decided I’d better polish my rusty programming skills, just in case I had to get a real job. The last thing I’d worked on was an e-commerce application written in Django, so I started building a web application to keep track of Software Carpentry workshops. All of that might have been manageable, but while it was going on my wife and I decided to sell our house to get away from a toxic neighbor. And since we were doing that, it seemed only sensible to go and spend a year in England while our daughter was still young enough to actually want to hang out with her parents. Meanwhile, angered by Gamergate and a bunch of other things, I approached a bunch of people about organizing a two-day workshop for grassroots get-into-tech groups trying to help people who didn’t fit the Silicon Valley mold. Each of these things was worth doing, but together they were too much. As Jesse said, you can’t be all in on everything. All you can do is run dry, and I did. When we discovered that visa rules for the United Kingdom had changed and we’d only be able to spend six months overseas instead of the year we’d hoped for, I had no reserves left. When we had to find a place in England long-distance, pack up a house, and deal with my father’s cancer and then his death, I had nothing to fall back on. The sensible response would have been to scale back, but I couldn’t. I mean that literally: every time I switched off email for a few hours my panic attacks returned. I was hooked on being plugged in every bit as much as my two-packs-a-day father was hooked on cigarettes, and it was every bit as unhealthy. And of course when email withdrawal made me twitchy I started making more mistakes, which meant more things went wrong, which made me feel even more that I had to get online and sort it all out—all of it, whether it was important or not, and my family paid the price. Programmers still don’t talk about this very often, but it’s more important than programming languages or business plans. No one should be ashamed to say, “That’s worth doing but I’m not going to do it.” And no one should be ashamed to say that they’ve hit their limit, or measure what they’ve done against what they could have done if they were smarter, faster, luckier, and never needed to sleep. My father never managed to quit smoking. I don’t think I’ll ever stop wanting to do everything that needs doing, but for my family’s sake as well as my own I’m going to try to get it under control. And if you’re who I was when I was 25 or 35, please don’t wait until you’re in your forties or fifties to start talking about this kind of thing. Who knows? Maybe once you get it out there, you’ll start laughing once in a while. 23.7 Summary If you want to understand how to get more done in less time and with less pain, these books may help: Gawande (2007) FIXME Hock (2004) is an entertaining summary of some keys studies in psychology. Lyons (2018) describes the way tech companies chase one fad after another rather than addressing the root causes of poor performance. 23.8 Key Points FIXME References "],
["finale.html", "Chapter 24 Finale 24.1 Questions 24.2 Objectives 24.3 Introduction 24.4 Programs are data. 24.5 Computers don’t understand anything. 24.6 Programming is about creating and combining abstractions. 24.7 Every redundancy in software is an abstraction trying to be born. 24.8 Create models for computers and views for human beings. 24.9 Paranoia makes us productive. 24.10 Things that don’t change are easier to understand than things that do. 24.11 Better algorithms are better than better hardware. 24.12 Distributed is different. 24.13 Privacy, security, fairness, and responsibility can’t be added after the fact. 24.14 Summary 24.15 Exercises 24.16 Key Points", " Chapter 24 Finale 24.1 Questions What have we learned? 24.2 Objectives Explain ten rules for thinking like a programmer. 24.3 Introduction Ever since Wing (2006) introduced the term “computational thinking” in 2006, computer scientists and educators have debated what exactly it means Denning (2017). What isn’t debated is the fact that programmers tend to think about problems in ways inspired by programming. This paper describes ten of those ways which may help people in other domains see their own problems with fresh eyes. An early version of this paper was inspired by Jon Udell’s essay “Seven Ways to Think Like the Web” Udell (2011). 24.4 Programs are data. The key insight that all of modern computing is built on is that programs are just another kind of data. Source code is text, no different from a thesis or a poem; it can be searched with the same tools used to search other kinds of documents, and new text (such as documentation) can be generated from old. More importantly, once a program is loaded into memory its instructions are just bytes, no different in principle or practice from those used to represent the pixels in an image or the numbers in a vector field. Those blocks of instructions can be stored in data structures, passed to functions, or altered on the fly, just like other data. Almost all advanced programming techniques depend on this fact, from function pointers in C and callbacks in JavaScript to decorators in Python and lazy evaluation in R. 24.5 Computers don’t understand anything. Computers don’t understand: they only obey instructions that make them appear to. If a person looks at Figure 24.1, they see the word “data”: Figure 24.1: The Word ‘Data’ A machine doesn’t; it doesn’t even see four blobs of blue pixels on a gray background, because it doesn’t “see” anything. Equally, calling a variable “temperature” doesn’t mean the computer will store a temperature in it—it would do exactly the same thing if the variable was called “pressure” or “frankenstein” or “a7”. This may seem obvious once you have written a few programs, but thirty years after Pea (1986) first called it the “superbug”, believing that the computer will somehow understand intent remains a common error. 24.6 Programming is about creating and combining abstractions. Computers don’t have to understand instructions in order to execute them, but we do in order to create them (and fix them afterward). Since our working memory can only juggle a handful of things at once Miller (1956), we have to create abstractions so that our representations of problems will fit into hardware whose performance doubles over millions of years rather than every eighteen months. The key to making workable abstractions is to separate “what” from “how”, or in computing terms, to separate interface and implementation. An interface is what something knows how to do; its implementation is how it does those things. There can be dozens of different implementations of a single interface: if we do our work well, we shouldn’t have to care about the differences between them until something goes wrong or we need to improve its performance. 24.7 Every redundancy in software is an abstraction trying to be born. The history of programming is in part the history of people noticing patterns and then making it easier for programmers to use them. Does your program repeatedly search an array to find the largest and smallest values? Write a function called bounds. Does it repeatedly search arbitrary data structures to find values that meet certain criteria? Write a generator that returns values one by one and another function that filters those according to some criteria. The problem with eliminating redundancy is that it can make software denser, which in turn makes it harder for non-specialists to understand. Like mathematics and modern art, it can take years of training for someone to reach the point where they can see how beautiful something is. The other problem with patterns is that they can lead to expert blind spot Nathan and Petrosino (2003). Once experts have internalized patterns, they are often unable to remember that they ever saw the world any other way, or to see the world afresh through novice eyes. As a result, they are prone to say, “It’s obvious,” and then follow it with incomprehensible jargon. 24.8 Create models for computers and views for human beings. One consequence of the second and third rules is important enough to be a rule in its own right: we should create models for computers and views for human beings. A model is a precise, detailed representation of information that is easy for a computer to operate on; a view is a way of displaying information that human beings can easily understand and interact with. For example, an HTML page is represented in memory as a data structure containing nodes for elements like headings and paragraphs, which can in turn contain a mix of other nodes or text (Figure 24.2). Figure 24.2: Model-View Separation That model can be rendered in a browser, turned into speech, or displayed as text using angle brackets. None of these is the model: they’re all views that make the information in the model comprehensible in different ways. Turning a model into a view is hard, but turning a view back into a model is harder. For example, parsing the textual representation of HTML takes thousands of lines of code, but doing OCR or speech recognition to translate a rendered page or its spoken equivalent back into nodes and text can take millions. Ironically, the same programmers who insist on this separation in software they build for the rest of humanity have been remarkably resistant to the idea of adopting any kind of model-view separation in programming itself. 24.9 Paranoia makes us productive. “I want to count the cells in this photograph” is easy to say, but what does it actually mean? What constitutes a cell? When do you decide that a lumpy blob of pixels is two cells rather than one, or three instead of two? Every program embodies decisions like these; the sooner these decisions are made explicit and the earlier they’re checked, the more productive we will be. The precise order of steps doesn’t seem to matter: we can write tests, then write software to make those tests pass, or write the program first and then test it. What does matter is alternating development and testing in short interleaved bursts Fucci et al. (2017). Of course, we don’t stop worrying once we’ve typed our code in. We check that data is formatted properly to protect ourselves against “garbage in, garbage out”. We put checks in our code to make sure that parameters are sensible, data structures consistent, files aren’t empty, and so on. This is called defensive programming, and one of the signs of a mature programmer is a high density of assertions and other self-checks in her code. It is harder to do this in research software than in most commercial software because (almost by definition) we don’t know what the right answer is in research software, which makes it difficult to check that we’re getting it. 24.10 Things that don’t change are easier to understand than things that do. Programmers use the words mutable and immutable to refer to things that can be modified after they are created and things that cannot. Immutable things are easier to understand because you don’t have to re-trace their history in order to understand their state. However, immutable data can be less efficient than mutable data: for example, it’s very expensive to make a copy of an entire multi-megabyte image just because we want to change one pixel. Older programming languages like C and Fortran allowed most data to be mutable because computer time was expensive. This led to many hard-to-find bugs, so newer languages either make data immutable or automatically copy data when asked to make changes in order to give the appearance of immutability. One special case of this rule is automating workflows. As Whitehead (1958) said, “Civilization advances by extending the number of important operations which we can perform without thinking about them.” Every time we automate a task—i.e., make its steps immutable—we reduce the chances of getting it wrong the next time, and have more time to think about things that machines can’t do for us. 24.11 Better algorithms are better than better hardware. One of the greatest mathematical advances of the Twentieth Century was the idea of algorithmic complexity. The key insight is that the running time or memory requirements of an algorithm grow in a predictable way as a function of the size of the problem we are trying to solve Conery (2016). Some algorithms slow down gently as their inputs get larger, but others slow down so much that even if the whole universe was one large computer, it couldn’t solve any interesting problem. Faster chips are therefore very welcome, but the real key to speed is to focus on how we’re doing things. 24.12 Distributed is different. Distributed computing is intrinsically different from running a program on a single machine Waldo et al. (1994). On a single computer, we can usually pretend that nobody else is modifying the data while we’re trying to use it. Once our data is distributed, that simplification breaks down, and we have to worry about things like someone else adding records to the database between the time we ask how many there are and the time we start processing them. Similarly, we can pretend that a program running on a single computer either works or doesn’t. When that same program is accessing remote resources, we have to worry about whether a long delay means that something has failed, or whether it’s just being slow. Many attempts have been made to paper over these differences, but all have failed in the large. As a result, the future of programming is about how we deal with this—a statement that has been true since the 1980s 24.13 Privacy, security, fairness, and responsibility can’t be added after the fact. Our final rule may be the most important of all. As the last few years have shown, collecting and interpreting data is never a neutral activity: who we share data with, how we classify it, and most importantly, who gets to decide these things are all political decisions with ever-increasing impact, and we are past the point where we can pretend otherwise. Attempts to add privacy, security, and fairness to systems after they have been built and deployed have repeatedly failed. The other “future of programming” is therefore to take digital health as seriously as physical health, and to make those responsible for it as accountable as those responsible for other aspects of our wellbeing. 24.14 Summary Artisans have known for centuries that the tool shapes the hand. If computers are tools for thinking with, then it shouldn’t surprise us that writing software shapes the minds of those who do it. The ten rules listed above are just a few reflections of this; we hope that they, and the skills and tools taught in these lessons, will help you see the world in new ways. 24.15 Exercises FIXME: exercise for finale 24.16 Key Points Programs are data. Computers don’t understand anything. Programming is about creating and combining abstractions. Every redundancy in software is an abstraction trying to be born. Create models for computers and views for human beings. Paranoia makes us productive. Things that don’t change are easier to understand than things that do. Better algorithms are better than better hardware. Distributed is different. Privacy, security, fairness, and responsibility can’t be added after the fact. References "],
["references.html", "References", " References "],
["license.html", "A License", " A License This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],
["conduct.html", "B Code of Conduct B.1 Our Standards B.2 Our Responsibilities B.3 Scope B.4 Enforcement B.5 Attribution", " B Code of Conduct In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. B.1 Our Standards Examples of behavior that contributes to creating a positive environment include: using welcoming and inclusive language, being respectful of differing viewpoints and experiences, gracefully accepting constructive criticism, focusing on what is best for the community, and showing empathy towards other community members. Examples of unacceptable behavior by participants include: the use of sexualized language or imagery and unwelcome sexual attention or advances, trolling, insulting/derogatory comments, and personal or political attacks, public or private harassment, publishing others’ private information, such as a physical or electronic address, without explicit permission, and other conduct which could reasonably be considered inappropriate in a professional setting B.2 Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. B.3 Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. B.4 Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by emailing the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership. B.5 Attribution This Code of Conduct is adapted from the Contributor Covenant version 1.4. "],
["contributing.html", "C Contributing", " C Contributing Contributions of all kinds are welcome, from errata and minor improvements to entirely new sections and chapters: please email us or submit an issue or pull request to our GitHub repository. Everyone whose work is incorporated will be acknowledged; please note that all contributors are required to abide by our Code of Conduct. Please note that we use Simplified English rather than Traditional English, i.e., American rather than British spelling and grammar. We encourage translations; if you would like to take this on, please email us. If you wish to report errata or suggest improvements to wording, please include the chapter name in the first line of the body of your report (e.g., Testing Data Analysis). "],
["glossary.html", "D Glossary", " D Glossary Absolute error FIXME Accuracy FIXME Action (in Make) FIXME Active listening FIXME Actual output (of a test) FIXME Agile development FIXME Ally FIXME Analysis and estimation FIXME Annotated tag (in version control) FIXME Append mode FIXME Application Programming Interface (API) FIXME Assertion FIXME Authentic task A task which contains important elements of things that learners would do in real (non-classroom situations). To be authentic, a task should require learners to construct their own answers rather than choose between provided answers, and to work with the same tools and data they would use in real life. Auto-completion FIXME Automatic variable FIXME Backlog FIXME Bit rot FIXME Branch-per-feature workflow branch-per-feature Breakpoint FIXME Buffer FIXME Build tool FIXME Call stack FIXME Camel case FIXME Catch (an exception) FIXME Code browser FIXME Cognitive load FIXME Comma-separated values (CSV) FIXME Commit hash FIXME Commit message FIXME Commons FIXME Competent practitioner Someone who can do normal tasks with normal effort under normal circumstances. See also novice and expert. Computational competence FIXME Computational stylometry FIXME Computational thinking FIXME Conditional expression FIXME Configuration object FIXME Context manager FIXME Continuous integration FIXME Coverage FIXME Creative Commons - Attribution License (CC-BY) FIXME Data engineering FIXME Data package FIXME Declarative programming FIXME Default target FIXME Dependency graph FIXME Design patterns FIXME Destructuring FIXME Dictionary FIXME Digital Object Identifier (DOI) FIXME Docstring FIXME Error (result from a unit test) FIXME Escape sequence FIXME Exception FIXME Exit status FIXME Expected output (of a test) FIXME Expert Someone who can diagnose and handle unusual situations, knows when the usual rules do not apply, and tends to recognize solutions rather than reasoning to them. See also competent practitioner and novice. Exploratory programming FIXME Exponent FIXME External error FIXME Failure (result from a unit test) FIXME False beginner Someone who has studied a language before but is learning it again. False beginners start at the same point as true beginners (i.e., a pre-test will show the same proficiency) but can move much more quickly. Feature boxing FIXME Feature creep FIXME Filename stem FIXME Fixture FIXME Flag FIXME Forge FIXME Fork FIXME Format string FIXME Frequently Asked Questions (FAQ) FIXME Function (in Make) FIXME Function attribute FIXME GitHub Pages FIXME Globbing FIXME GNU Public License (GPL) FIXME Hot spot FIXME HTTP status code FIXME Impostor syndrome FIXME In-place operator FIXME Index FIXME Instrumenting profiler FIXME Integrated Development Environment (IDE) FIXME Internal error FIXME ISO date format FIXME Issue FIXME Iteration (in software development) FIXME Jenny (a repository) FIXME Join (of database tables) FIXME JSON FIXME Kebab case FIXME Label (in issue tracker) FIXME Linter FIXME List comprehension FIXME Logging framework FIXME Macro FIXME Magnitude FIXME Makefile FIXME Mantissa FIXME Mental model A simplified representation of the key elements and relationships of some problem domain that is good enough to support problem solving. MIT License FIXME Not Invented Here (NIH) FIXME Novice Someone who has not yet built a usable mental model of a domain. See also competent practitioner and expert. Object-oriented programming FIXME Open science FIXME Operational test FIXME Oppression FIXME ORCID FIXME Overlay configuration FIXME Package FIXME Pair programming FIXME Pattern rule FIXME Peer action FIXME Phony target FIXME Post-mortem FIXME Pothole case FIXME Precision FIXME Prerequisite (in Make) FIXME Privilege FIXME Procedural programming FIXME Product manager FIXME Profiler FIXME Project manager FIXME Provenance FIXME Pseudorandom number generator (PRNG) FIXME Public domain license (CC-0) FIXME Pull request FIXME Raise FIXME Raster image FIXME Rebase FIXME Redirection FIXME Refactor FIXME Relative error FIXME Remote login FIXME Representation State Transfer (REST) FIXME Reproducible example (reprex) FIXME Reproducible research FIXME Research software engineer (RSE) FIXME reStructured Text FIXME Rotating file FIXME Rule (in Make) FIXME Sampling profiler FIXME Scalable Vector Graphics (SVG) FIXME Seed (for pseudorandom number generator) FIXME Semantic versioning FIXME https://semver.org/ Set and override (pattern) FIXME Shebang FIXME Short circuit test FIXME Sign FIXME Silent error FIXME Silent failure FIXME Situational action FIXME Software development process FIXME SSH key FIXME SSH protocol FIXME Stand-up meeting FIXME Standard error FIXME Standard input FIXME Standard output FIXME Sturdy development FIXME Subsampling FIXME Success (result from a unit test) FIXME Sustainability FIXME Symbolic debugger FIXME Syntax highlighting FIXME Synthetic data FIXME Tab completion FIXME Tag (in version control) FIXME Target (in Make) FIXME Target (of oppression) FIXME Technical debt FIXME Test coverage FIXME Test-driven development FIXME Test framework FIXME Test runner FIXME Tidy data As defined in Wick2014, tabular data is tidy if: 1. Each variable is in one column. 2. Each different observation of that variable is in a different row. 3. There is one table for each kind of variable. 4. If there are multiple tables, each includes a key so that related data can be linked. Time boxing FIXME Timestamp (on a file) FIXME Tolerance FIXME Triage FIXME Tuning FIXME Tuple FIXME Typesetting language FIXME Unit test FIXME Update operator See in-place operator. Validation FIXME Variable (in Make) FIXME Vector image FIXME Verification FIXME Virtual environment FIXME What You See Is What You Get (WYSIWYG) FIXME Wildcard FIXME Working directory FIXME Working memory FIXME Wrapper FIXME YAML FIXME "],
["objectives.html", "E Learning Objectives E.1 Introduction E.2 The Unix Shell E.3 Automating Analyses E.4 Syndicating Data E.5 Configuring Software E.6 Logging E.7 Unit Testing E.8 Verification E.9 A Branching Workflow E.10 Managing Backlog E.11 Code Style and Review E.12 Development Process E.13 Continuous Integration E.14 Working Remotely E.15 Other Tools E.16 Documentation E.17 Refactoring E.18 Project Structure E.19 Including Everyone E.20 Packaging E.21 Publishing E.22 Teamwork E.23 Pacing E.24 Finale", " E Learning Objectives E.1 Introduction Explain the difference between open science, reproducible research, sustainability, and competence. Determine readiness for using this material. Explain what ‘done’ looks like for the computational component of a small or medium-sized research project. Determine whether a particular research project meets that standard. E.2 The Unix Shell FIXME E.3 Automating Analyses Explain what a build tool is and how build tools aid reproducible research. Describe and identify the three parts of a Make rule. Write a Makefile that re-runs a multi-stage data analysis. Explain and trace how Make chooses an order in which to execute rules. Explain what phony targets are and define a phony target. Explain what automatic variables are and correctly identify three commonly-used automatic variables. Rewrite Make rules to use automatic variables. Explain why and how to write a pattern rule in a Makefile. Rewrite Make rules to use patterns. Define variables in a Makefile explicitly and by using functions. Make a self-documenting Makefile. E.4 Syndicating Data \"Write Python programs to download data sets using simple REST APIs. \"Parse CSV data using the csv library. \"Test a program that parses CSV using multiline strings. \"Make a function more robust by explicitly handling errors. \"Write Python programs that share static data sets. E.5 Configuring Software Describe the four levels of configuration typically used by robust software. Explain what an overlay configuration is. Explain why nested configuration options are usually not a good idea. Add flat overlay configuration to a small application. E.6 Logging Explain the advantages of using a logging library rather than print statements in data science pipelines. Describe the intent of the five standard logging levels. Create and configure a simple logger. Define a custom format for log messages. Define a source name to use in consolidated logs. E.7 Unit Testing Explain what realistic technical and social goals for software testing are. Explain what a test runner is. Explain what a text fixture is. Write and run unit tests using Python’s pytest test runner. Check test coverage using Python’s coverage module. E.8 Verification Explain why floating point results aren’t random but can still be unpredictable. Explain why it is hard to test code that produces plots or other graphical output. Describe and implement heuristics for testing data analysis. Describe the role of inference in data analysis testing and use the tdda library to find and check constraints on tabular data. E.9 A Branching Workflow Explain what rebasing is and use it interactively to collapse a sequence of commits into a single commit. Describe a branch-per-feature workflow and explain why to use it. Describe what a repository tag is and create an annotated tag in a Git repository. E.10 Managing Backlog Explain what an issue tracking tool does and what it should be used for. Explain how to use labels on issues to manage work. Describe the information a well-written issue should contain. E.11 Code Style and Review Explain why consistent formatting of code is important. Describe standard Python formatting rules and identify cases where code does or doesn’t conform to them. Write functions whose parameters have default values. Explain which parameters should have default values and how to select good ones. Write functions that can handle variable numbers of arguments. Explain what problems can most easily be solved by creating functions with variable numbers of arguments. E.12 Development Process \"Explain what a software development process is. \"Define the key features of an agile development process. \"Describe the key roles in a planning-based software development process. \"Explain the true purpose of a schedule. E.13 Continuous Integration Explain how continuous integration works. Configure continuous integration for a small software project. E.14 Working Remotely Explain what SSH is and when it should be used. Use ssh to connect to a remote computer. Use scp to copy files to and from remote computers. Generate and install an SSH key pair. E.15 Other Tools List the criteria for choosing a good program editor. Explain what a symbolic debugger is and demonstrate its use. Explain what profiling is and describe two approaches to profiling. Explain what integrated development environments are and what advantages they have over standalone tools. Explain how to execute tasks at specified times. E.16 Documentation Explain what docstrings are and add correctly-formatted docstrings to short Python programs. Extract and format documentation from docstrings using pydoc. Explain why consistent formatting of code is important. E.17 Refactoring Describe at least four common refactorings and correctly apply each. E.18 Project Structure Describe and justify Noble’s Rules for organizing projects. Explain the purpose of README, LICENSE, CONDUCT, and CITATION files. E.19 Including Everyone Explain the purpose of a Code of Conduct and the essential features an effective one must have. Explain why adding licensing information to a repository is important. Explain differences in licensing and social expectations. Choose an appropriate license. Explain where and how to communicate licensing. Explain steps a project lead can take to be a good ally. E.20 Packaging Create and use virtual environments to manage library versions without conflict. Create and test a citable, shareable Pip package. E.21 Publishing Explain what to include in publications, and where to publish large, medium, and small datasets. Explain what DOIs and ORCIDs are. Get an ORCID. Describe the FAIR Principles and determine whether a dataset conforms to them. Obtain DOIs for datasets, reports, and software packages. E.22 Teamwork FIXME E.23 Pacing FIXME E.24 Finale Explain ten rules for thinking like a programmer. "],
["keypoints.html", "F Key Points F.1 Introduction F.2 The Unix Shell F.3 Automating Analyses F.4 Syndicating Data F.5 Configuring Software F.6 Logging F.7 Unit Testing F.8 Verification F.9 A Branching Workflow F.10 Managing Backlog F.11 Code Style and Review F.12 Development Process F.13 Continuous Integration F.14 Working Remotely F.15 Other Tools F.16 Documentation F.17 Refactoring F.18 Project Structure F.19 Including Everyone F.20 Packaging F.21 Publishing F.22 Teamwork F.23 Pacing F.24 Finale", " F Key Points F.1 Introduction Research is ‘open’ if everyone can read it. Research is ‘reproducible’ if people who have access can regenerate it. Research is ‘sustainable’ if it was built in reasonable time and without heroic effort. Computational competence is the digital equivalent of knowing how to use lab equipment properly. A project is ‘done’ when stakeholders can be reasonably sure the results are correct and the software can be understood, run, and extended by people other than the original authors without heroic effort. F.2 The Unix Shell FIXME F.3 Automating Analyses A build tool re-runs commands to bring files up to date with respect to the things they depend on. Make is a widely-used build tool that uses files’ timestamps to find out-of-date prerequisites. A Make rule has targets, prerequisites, and actions. A target can correspond to a file or be a phony target (used simply to trigger actions). When a target is out of date with respect to its prerequisites, Make executes the actions associated with its rule. Make executes as many rules as it needs to when updating files, but always respect prerequisite order. Make defines the automatic variables $@ (target), $^ (all prerequisites), and $&lt; (first prerequisite). Pattern rules can use % as a placeholder for parts of filenames. Makefiles can define variables using NAME=value. Makefiles can also use functions such as $(wildcard ...) and $(patsubst ...). Specially-formatted comments can be used to make Makefiles self-documenting. F.4 Syndicating Data \"FIXME \"Make data sets more useful by providing metadata. \"Write Python programs to download data sets using simple REST APIs. \"Parse CSV data using the csv library. \"Test a program that parses CSV using multiline strings. \"Make a function more robust by explicitly handling errors. \"Write Python programs that share static data sets. F.5 Configuring Software Use short command-line parameters to set commonly-changed options. Allow long command-line parameters to change all other options to facilitate scripting. Use the getopt library to parse command-line flags. Read a system-wide configuration file, then a user configuration file, then a job configuration file. Format configuration files using YAML. Dump all configuration values in the same format used for input on request. Include the software version number in the dumped configuration. F.6 Logging Use logging instead of print to report program activity. Separate messages into DEBUG, INFO, WARNING, ERROR, and CRITICAL levels. Use logging.basicConfig to define basic logging parameters. Always provide timestamps using YYYY-MM-DDTTHH:MM:SS format. Use standard input and standard output for normal input and output, and send log messages to a file. Use tail -f to monitor log files. F.7 Unit Testing Testing can only ever show that software has flaws, not that it is correct. Its real purpose is to convince people (including yourself) that software is correct enough, and to make tolerances on ‘enough’ explicit. A test runner finds and runs tests written in a prescribed fashion and reports their results. A unit test can pass (work as expected), fail (meaning the software under test is flawed), or produce an error (meaning the test itself is flawed). A fixture is the data or other input that a test is run on. Every unit test should be independent of every other to keep results comprehensible and reliable. Programmers should check that their software fails when and as it is supposed to in order to avoid silent errors. Write test doubles to replace unpredictable inputs such as random numbers or the current date or time with a predictable value. Use string I/O doubles when testing file input and output. Use a coverage tool to check how well tests have exercised code. F.8 Verification Programmers should use tolerances when comparing floating-point numbers (not just in tests). Test the data structures used in plotting rather than the plots themselves. Check that parametric or non-parametric statistics of data do not differ from saved values by more than a specified tolerance. Infer constraints on data and then check that subsequent data sets obey these constraints. F.9 A Branching Workflow Create a new branch for every feature, and only work on that feature in that branch. Always create branches from master, and always merge to master. Use rebasing to combine several related commits into a single commit before merging. F.10 Managing Backlog Create issues for bugs, enhancement requests, and discussions. Add people to issues to show who is responsible for working on what. Add labels to issues to identify their purpose. Use rules for issue state transitions to define a workflow for a project. F.11 Code Style and Review The brain thinks every difference is significant, so removing unnecessary differences in formatting reduces cognitive load. Python software should always conform to the formatting the rules in PEP 8. Use name=value to define a default value for a function parameter. Use *args to define a catch-all parameter for functions taking a variable number of unnamed arguments. Use **kwargs to define a catch-all parameter for functions taking a variable number of named arguments. Use destructuring to unpack data structures as needed. F.12 Development Process \"A software development process can be chaotic, agile, or sturdy. \"Agile development depends on feedback loops at several scales. \"Pair programming provides realtime feedback on code as it is written. \"The primary responsibility of a product manager is to translate users’ needs into features and priorities. \"The primary responsibility of a project manager is to create and maintain a schedule. F.13 Continuous Integration Continuous integration rebuilds and/or re-tests software every time something changes. Use continuous integration to check changes before they are inspected. Check style as well as correctness. F.14 Working Remotely ssh is a secure way to log in to a remote computer. scp is a secure way to copy files to and from a remote computer. SSH is a secure alternative to username/password authorization. SSH keys are generated in pairs: the public key can be shared with others, but the private keys stays on your machine only. F.15 Other Tools Choose an editor that is kind to your wrists, portable, syntax-aware, programmable, and can handle Unicode characters. A symbolic debugger is a program that allows you to control and inspect the execution of another program. Symbolic debuggers are a much more efficient way to find most bugs than print statements. A profiler records and reports the execution time of the statements in a program. Instrumenting profilers modify the source code to check the clock and record start and end times for statements. Sampling profilers interrupt a program’s execution to build a histogram of execution times. An integrated development environment (IDE) combines editing, execution, profiling, code checking, and many other tools in one. Use cron or at to execute tasks periodically or at some future time respectively. F.16 Documentation Documentation strings (docstrings) can be placed at the start of a file or at the start of a function. Docstrings can be formatted using a superset of Markdown. Tools like pydoc and Sphinx can extract and format docstrings to create documentation for software. F.17 Refactoring Reorganizing code in consistent ways makes errors less likely. Replace a value with a name to make code more readable and to forestall typing errors. Replace a repeated test with a flag to ensure consistency. Turn small pieces of large functions into functions in their own right, even if they are only used once. Combine functions if they are always used together on the same inputs. Use lookup tables to make decision rules easier to follow. Use comprehensions instead of loops. F.18 Project Structure Put source code for compilation in ./src/. Put runnable code in ./bin/. Put raw data in ./data/. Put results in ./results/. Put documentation and manuscripts in ./doc/. Use file and directory names that are easy to match and include dates for the level under ./data/ and ./results/. Create README, LICENSE, CONDUCT, and CITATION files in the root directory of the project. F.19 Including Everyone Create an explicit Code of Conduct for your project modelled on the Contributor Covenant. Be clear about how to report violations of the Code of Conduct and who will handle such reports. People who are not lawyers should not try to write licenses. Every project should include an explicit license to make clear who can do what with the material. People who incorporate GPL’d software into their own software must make their software also open under the GPL license; most other open licenses do not require this. The Creative Commons family of licenses allow people to mix and match requirements and restrictions on attribution, creation of derivative works, further sharing, and commercialization. Be proactive about welcoming and nurturing community members. F.20 Packaging Use virtualenv to create a separate virtual environment for each project. Use pip to create a distributable package containing your project’s software, documentation, and data. F.21 Publishing Include small datasets in repositories; store large ones on data sharing sites, and include metadata in the repository to locate them. An ORCID is a unique personal identifier that you can use to identify your work. A DOI is a unique identifier for a particular dataset, report, or software release. Data should be findable, accessible, interoperable, and reusable (FAIR). Use Zenodo to obtain DOIs. Publish your software as you would a paper. F.22 Teamwork FIXME F.23 Pacing FIXME F.24 Finale Programs are data. Computers don’t understand anything. Programming is about creating and combining abstractions. Every redundancy in software is an abstraction trying to be born. Create models for computers and views for human beings. Paranoia makes us productive. Things that don’t change are easier to understand than things that do. Better algorithms are better than better hardware. Distributed is different. Privacy, security, fairness, and responsibility can’t be added after the fact. "],
["rules.html", "G The Rules", " G The Rules Be kind: all else is details. A week of hard work can sometimes save you an hour of thought. While the hand makes the tool, the tool shapes the hand. Nothing in software development makes sense except in light of human psychology. If you need to write a parser, you’ve done something wrong. Always seed your random number generator, and always record the seed. Break any of these rules rather than doing something awkward. "]
]
