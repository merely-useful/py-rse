# Verification {#verify}

## Questions {#verify-questions}

```{r, child="questions/verify.md"}
```

## Objectives {#verify-objectives}

```{r, child="objectives/verify.md"}
```

## Introduction {#verify-intro}

The previous lesson explained how to test software in general;
this one focuses on testing data analysis.

## What is the difference between testing in software engineering and in data analysis? {#verify-diff}

Testing data analysis pipelines is often harder than testing mainstream software applications.
The reason is that data analysts often don't know what the right answer is,
which makes it hard to check correctness.
The key distinction is the difference between validation and verification.
[Validation](glossary.html#validation) asks, "Is specification correct?"
while [verification](glossary.html#verification) asks,
It's the difference between building the right thing and building something right;
the former question is often much harder for data scientists to answer.

Instead of unit testing,
a better analogy is often physical experiments.
When high school students are measuring acceleration due to gravity,
they should get a figure close to $$10 m/sec^2$$.
Undergraduates might get $$9.8 m/sec^2$$ depending on the equipment used,
but if either group gets $$9.806 m/sec^2$$ with a stopwatch, a marble, and an ramp,
they're either incredibly lucky or cheating.
Similarly,
when testing data analysis pipelines,
we often have to specify tolerances.
Does the answer have to be exactly the same as a hand-calculated value or a previously-saved value?
If not, how close is good enough?

We also need to distinguish between development and production.
During development,
our main concern is whether our answers are (close enough to) what we expect.
We do this by analyzing small datasets
and convincing ourselves that we're getting the right answer in some ad hoc way.

In production,
on the other hand,
our goal is to detect cases where behavior deviates significantly from what we previously decided what right.
We want this to be automated
so that our pipeline will ring an alarm bell to tell us something is wrong
even if we're busy working on something else.
We also have to decide on tolerances once again,
since the real data will never have exactly the same characteristics as the data we used during development.
We also need these checks because the pipeline's environment can change:
for example,
someone could upgrade a library that one of our libraries depends on,
which could lead to us getting slightly different answers than we expected.

## Why should I be cautious when using floating-point numbers? {#verify-float}

Every tutorial on testing numerical software has to include a discussion of the perils of floating point,
so we might as well get ours out of the way.
The explanation that follows is simplified to keep it manageable;
if you want to know more,
please take half an hour to read @Gold1991.

Finding a good representation for floating point numbers is hard:
we cannot represent an infinite number of real values with a finite set of bit patterns,
and unlike integers,
no matter what values we *do* represent,
there will be an infinite number of values between each of them that we can't.
These days,
floating point numbers are usually represented using [sign](glossary.html#sign),
[magnitude](glossary.html#magnitude) (or [mantissa](glossary.html#mantissa)),
and an [exponent](glossary.html#exponent).
In a 32-bit word,
the IEEE 754 standard calls for 1 bit of sign,
23 bits for the mantissa,
and 8 bits for the exponent.
To illustrate the problems with floating point,
we will use a much simpler 5-bit representation
with 3 bits for the magnitude and 2 for the exponent.
We won't worry about fractions or negative numbers,
since our simple representation will show off the main problems.

The table below shows the possible values (in decimal) that we can represent with 5 bits.
Real floating point representations don't have all the redundancy that you see in this table,
but it illustrates the point.
Using subscripts to show the bases of numbers,
$$110_2 \times 2^{11_2}$$
$$6 \times 2^3$$ or 48.

<table class="table table-striped">
  <tr>
    <th></th>
    <th colspan="4">Exponent</th>
  </tr>
  <tr>
    <th>Mantissa</th> <th>00</th> <th>01</th> <th>10</th> <th>11</th>
  </tr>
  <tr>
    <th>000</th> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
  </tr>
  <tr>
    <th>001</th> <td>1</td> <td>2</td> <td>4</td> <td>8</td>
  </tr>
  <tr>
    <th>010</th> <td>2</td> <td>4</td> <td>8</td> <td>16</td>
  </tr>
  <tr>
    <th>011</th> <td>3</td> <td>6</td> <td>12</td> <td>24</td>
  </tr>
  <tr>
    <th>100</th> <td>4</td> <td>8</td> <td>16</td> <td>32</td>
  </tr>
  <tr>
    <th>101</th> <td>5</td> <td>10</td> <td>20</td> <td>40</td>
  </tr>
  <tr>
    <th>110</th> <td>6</td> <td>12</td> <td>24</td> <td>48</td>
  </tr>
  <tr>
    <th>111</th> <td>7</td> <td>14</td> <td>28</td> <td>56</td>
  </tr>
</table>

Figure \@ref(fig:verify-spacing) is a clearer view of some of the values our scheme can represent:

```{r verify-spacing, echo=FALSE, fig.cap="Number Spacing"}
knitr::include_graphics("figures/verify/number-spacing.png")
```

A lot of values are missing from this diagram:
for example,
it includes 8 and 10 but not 9.
This is exactly like the problem writing out 1/3 in decimal:
we have to round that to 0.3333 or 0.3334.

But if this scheme has no representation for 9,
then 8+1 must be stored as either 8 or 10.
This raises an interesting question:
if 8+1 is 8, what is 8+1+1?
If we add from the left, 8+1 is 8, plus another 1 is 8 again.
If we add from the right, 1+1 is 2, and 2+8 is 10,
so changing the order of operations can make the difference between right and wrong.

In this case,
if we sort the values and then add from smallest to largest,
it gives us the best chance of getting the best answer.
In other situations,
like inverting a matrix,
the rules are more complicated.
Just as electrical engineers trust oscilloscope makers,
almost all data scientists should trust the authors of core libraries to get this right.

To make this more concrete,
consider the short Python program below.
This program loop runs over the integers from 1 to 9 inclusive
and puts the numbers 0.9, 0.09, 0.009, and so on in `vals`.
The sums should be 0.9, 0.99, 0.999, and so on, but are they?
To find out,
we can calculate the same values by subtracting .1 from 1,
then subtracting .01 from 1, and so on.
This should create exactly the same sequence of numbers, but it doesn't.

<!-- src="verify/fp_table.py" -->
```{python}
vals = []
for i in range(1, 10):
    number = 9.0 * 10.0 ** -i
    vals.append(number)
    total = sum(vals)
    expected = 1.0 - (10.0 ** -i)
    diff = total - expected
    print('{:2d} {:22.21f} {:22.21f}'.format(i, total, total-expected))
```

As the output shows,
the very first value contributing to our sum is already slightly off.
Even with 23 bits for a mantissa,
we cannot exactly represent 0.9 in base 2,
any more than we can exactly represent 1/3 in base 10.
Doubling the size of the mantissa would reduce the error,
but we can't ever eliminate it.

The good news is,
$$9 {\times} 10^{-1}$$ and $$1 - 0.1$$ are exactly the same:
the value might not be precisely right,
but at least they are consistent.
But some later values differ,
and sometimes accumulated error makes the result *more* accurate.

It's very important to note that *this has nothing to do with randomness*.
The same calculation will produce exactly the same results no matter how many times it is run,
because the process is completely deterministic, just hard to predict.
If you see someone run the same code on the same data with the same parameters many times and average the results,
you should ask if they know what they're doing.
(That said,
doing this *can* be defensible if there is parallelism,
which can change evaluation order,
or if you're changing platform,
e.g., moving computation to a GPU.)

## How can I express how close one number is to another? {#verify-error}

The absolute spacing in the diagram above between the values we can represent is uneven.
However,
the relative spacing between each set of values stays the same:
the first group is separated by 1,
then the separation becomes 2,
then 4,
and then 8.
This happens because we're multiplying the same fixed set of mantissas by ever-larger exponents,
and it leads to some useful definitions.
The [absolute error](glossary.html#absolute-error) in an approximation is the absolute value of
the difference between the approximation and the actual value.
The [relative error](glossary.html#relative-error) is the ratio of the absolute error to the value we're approximating.
For example,
it we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than in the second.
Relative error is almost always more important than absolute error when we are testing software
because it makes little sense to say that we're off by a hundredth
when the value in question is a billionth.

## How should I write tests that involved floating-point values? {#verify-numeric}

[Accuracy](glossary.html#accuracy) is how close your answer is to right,
and [precision](glossary.html#precision) is how close repeated measurements are to each other.
You can be precise without being accurate (systematic bias),
or accurate without being precise (near the right answer, but without many significant digits).
Accuracy is usually more important than precision for human decision making,
and a relative error of $$10^{-3}$$ (three decimal places) is more than good enough for most data science
because the decision a human being would make won't change if the number changes by 0.1%.

We now come to the crux of this lesson:
if the function you're testing uses floating point numbers,
what do you compare its result to?
If we compared the sum of the first few numbers in `vals` to what it's supposed to be,
the answer could be `False` even though we're doing nothing wrong.
If we compared it to a previously calculated result that we had stored somehow,
the match would be exact.

No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in context.
So what can you do to test your programs?
If you are comparing to a saved result,
and the result was saved at full precision,
you could use exact equality,
because there is no reason for the new number to differ.
However,
any change to your code,
however small,
could trigger a report of a difference.
Experience shows that these spurious warnings quickly lead developers to stop paying attention to their tests.

A much better approach is to write a test that checks whether numbers are the same within some [tolerance](glossary.html#tolerance),
which is best expressed as a relative error.
In Python,
you can do this with `pytest.approx`,
which works on lists, sets, arrays, and other collections,
and can be given either relative or absolute error bounds.
To show how it works,
here's an example with an unrealistically tight absolute bound:

<!-- src="verify/approx.py" -->
```python
from pytest import approx

for bound in (1e-15, 1e-16):
    vals = []
    for i in range(1, 10):
        number = 9.0 * 10.0 ** -i
        vals.append(number)
        total = sum(vals)
        expected = 1.0 - (10.0 ** -i)
        if total != approx(expected, abs=bound):
            print('{:22.21f} {:2d} {:22.21f} {:22.21f}'.format(bound, i, total, expected))
```
```text
9.999999999999999790978e-17  6 0.999999000000000082267 0.999998999999999971244
9.999999999999999790978e-17  8 0.999999990000000060775 0.999999989999999949752
```

This tells us that two tests pass with an absolute error of $$10^{-15}$$
but fail when the bound is $$10^{-16}$$,
both of which are unreasonably tight.
(Again, think of physical experiments:
an absolute error of $$10^{-15}$$ is one part in a trillion,
which only a handful of high-precision experiments have ever achieved.)

## How can I test plots and other graphical results? {#verify-plots}

Testing visualizations is hard:
any change to the dimension of the plot,
however small,
can change many pixels in a [raster image](glossary.html#raster-image),
and cosmetic changes such as moving the legend up a couple of pixels
will similarly generate false positives.

The simplest solution is therefore *not* to test the generated image,
but to test the data used to produce it.
Unless you suspect that the plotting library contains bugs,
feeding it the correct data should produce the correct plot.

If you *do* need to test the generated image,
the only practical approach is to compare it to a saved image that you have visually verified.
[pytest-mpl][pytest-mpl] does this by calculating the root mean square (RMS) difference between images,
which must be below a threshold for the comparison to pass.
It also allows you to turn off comparison of text,
because font differences can throw up spurious failures.
As with choosing tolerances for floating-point tests,
your rule for picking thresholds should be,
"If images are close enough that a human being would make the same decision about meaning,
the test should pass"

FIXME: example

Another approach is to save the plot in a [vector format](glossary.html#vector-image) like [SVG](glossary.html#svg)
that stores the coordinates of lines and other elements as text
in a structure similar to that of HTML.
You can then check that the right elements are there with the right properties,
although this is less rewarding than you might think:
again,
small changes to the library or to plotting parameters can make all of the tests fail
by moving elements by a pixel or two.
Vector-based tests therefore still need to have thresholds on floating-point values.

## How can I test the steps in a data analysis pipeline during development? {#verify-simple}

We can't tell you how to test your math,
since we don't know what math you're using,
but we *can* tell you where to get data to test it with.
The first method is [subsampling](glossary.html#subsampling):
choose random subsets of your data,
analyze it,
and see how close the output is to what you get with the full dataset.
If output doesn't converge as sample size grows,
something is probably unstable---which is not necessarily the same as wrong.
Instability is often a problem with the algorithm,
rather than with the implementation.

If you do this,
it's important that you select a random sample from your data
rather than (for example) the first N records or every N'th record.
If there is any ordering or grouping in your data,
those techniques can produce samples that are biased,
which may in turn invalidate some of your tests.

FIXME: add an exercise that subsamples the Zipf data.

The other approach is to test with [synthetic data](glossary.html#synthetic-data).
With just a few lines of code,
you can generate uniform data (i.e., data having the same values for all observations),
strongly bimodal data (which is handy for testing clustering algorithms),
or just sample a known distribution.
If you do this,
you should also try giving your pipeline data that *doesn't* fit your expected distribution
and make sure that something, somewhere, complains.
Doing this is the data science equivalent of testing the fire alarm every once in a while.

For example,
we can write a short program to generate data that conforms to Zips' Law and use it to test our analysis.
Real data will be integers (since words only occur or not),
and distributions will be fractional.
We will use 5% relative error as our threshold,
which we pick by experimentation:
1% excludes a valid correct value.
The test function is called `is_zipf`:

<!-- src="verify/test_zipf.py" -->
```python
from pytest import approx


RELATIVE_ERROR = 0.05

    
def is_zipf(hist):
    scaled = [h/hist[0] for h in hist]
    print('scaled', scaled)
    perfect = [1/(1 + i) for i in range(len(hist))]
    print('perfect', perfect)
    return scaled == approx(perfect, rel=RELATIVE_ERROR)
```

<!-- == noindent -->
Here are three tests that use this function
with names that suggest their purpose:

<!-- src="verify/test_zipf.py" -->
```python
def test_fit_correct():
    actual = [round(100 / (1 + i)) for i in range(10)]
    print('actual', actual)
    assert is_zipf(actual)


def test_fit_first_too_small():
    actual = [round(100 / (1 + i)) for i in range(10)]
    actual[0] /= 2
    assert not is_zipf(actual)


def test_fit_last_too_large():
    actual = [round(100 / (1 + i)) for i in range(10)]
    actual[-1] = actual[1]
    assert not is_zipf(actual)
```

## How can I check the steps in a data analysis pipeline in production? {#verify-operational}

An [operational test](glossary.html#operational-test) is one that is kept in place during production
to tell users if everything is still working as it should.
A common pattern for such tests is to have every tool append information to a log (Chapter \@ref(logging))
and then have another tool check that log file after the run is over.
Logging and then checking makes it easy to compare values between pipeline stages,
and ensures that there's a record of why a problem was reported.
Some common operational tests include:

-   Does this pipeline stage produce the same number of output records as input records?
-   Or fewer if the stage is aggregating?
-   If two or more tables are being [joined](glossary.html#join),
    is the number of output records equal to the product of the number of input records?
-   Is the standard deviation be smaller than the range of the data?
-   Are there any NaNs or NULLs where there aren't supposed to be any?

To illustrate these ideas,
here's a script that reads a document and prints one line per word:

<!-- src="verify/text_to_words.py" -->
```python
import sys

num_lines = num_words = 0
for line in sys.stdin:
    num_lines += 1
    words = [strip_punctuation(w) for w in line.strip().split()]
    num_words += len(words)
    for w in words:
        print(w)
with open('logfile.csv', 'a') as logger:
    logger.write('text_to_words.py,num_lines,{}\n'.format(num_lines))
    logger.write('text_to_words.py,num_words,{}\n'.format(num_words))
```

<!-- == noindent -->
Here's a complementary script that counts how often words appear in its input:

<!-- src="verify/word_count.py" -->
```python
import sys

num_words = 0
count = {}
for word in sys.stdin:
    num_words += 1
    count[word] = count.get(word, 0) + 1
for word in count:
    print('{} {}', word, count[word])
with open('logfile.csv', 'a') as logger:
    logger.write('word_count.py,num_words,{}\n'.format(num_words))
    logger.write('word_count.py,num_distinct,{}\n'.format(len(count)))
```

Both of these scripts write records to `logfile.csv`.
When we look at that file after a typical run,
we see records like this:

```text
text_to_words.py,num_lines,431
text_to_words.py,num_words,2554
word_count.py,num_words,2554
word_count.py,num_distinct,1167
```

We can then write a small program to check that everything went as planned:

```python
# read CSV file into the variable data
check(data['text_to_words.py']['num_lines'] <= data['word_count.py']['num_words'])
check(data['text_to_words.py']['num_words'] == data['word_count.py']['num_words'])
check(data['word_count.py']['num_words'] >= data['word_count.py']['num_distinct'])
```

## How can I infer and check properties of my data? {#verify-infer}

Writing tests for the properties of data can be tedious,
but some of the work can be automated.
In particular,
the [TDDA library][tdda-site] can infer test rules from data,
such as `age <= 100`, `Date` should be sorted ascending, or `StartDate <= EndDate`.
The library comes with a command-line tool called `tdda`,
so that the command:

```shell
$ tdda discover elements92.csv elements.tdda
```
<!-- used="verify/elements92.csv" -->

<!-- == noindent -->
infers rules from data,
while the command:

```shell
tdda verify elements92.csv elements.tdda
```

<!-- == noindent -->
verifies data against those rules.
The inferred rules are stored as JSON,
which is (sort of) readable with a bit of practice.
Reading the generated rules is a good way to get to know your data,
and modifying values
(e.g., changing the maximum allowed value for `Grade` from the observed 94.5 to the actual 100.0)
is an easy way to make constraints explicit:

```json
"fields": {
    "Name": {
        "type": "string",
        "min_length": 3,
        "max_length": 12,
        "max_nulls": 0,
        "no_duplicates": true
    },
    "Symbol": {
        "type": "string",
        "min_length": 1,
        "max_length": 2,
        "max_nulls": 0,
        "no_duplicates": true
    },
    "ChemicalSeries": {
        "type": "string",
        "min_length": 7,
        "max_length": 20,
        "max_nulls": 0,
        "allowed_values": [
            "Actinoid",
            "Alkali metal",
            "Alkaline earth metal",
            "Halogen",
            "Lanthanoid",
            "Metalloid",
            "Noble gas",
            "Nonmetal",
            "Poor metal",
            "Transition metal"
        ]
    },
    "AtomicWeight": {
        "type": "real",
        "min": 1.007947,
        "max": 238.028913,
        "sign": "positive",
        "max_nulls": 0
    },
    ...
}
```

We can apply these inferred rules to all elements
using the `-7` flag to get pure ASCII output
and the `-f` flag to show only fields with failures:

<!-- used="verify/elements118.csv" -->

```shell
$ tdda verify -7 -f elements118.csv elements92.tdda
```
```text
FIELDS:

Name: 1 failure  4 passes  type OK  min_length OK  max_length X  max_nulls OK  no_duplicates OK

Symbol: 1 failure  4 passes  type OK  min_length OK  max_length X  max_nulls OK  no_duplicates OK

AtomicWeight: 2 failures  3 passes  type OK  min OK  max X  sign OK  max_nulls X

...others...

SUMMARY:

Constraints passing: 57
Constraints failing: 15
```

Another way to use TDDA is to generate constraints for two datasets and then look at differences
in order to see how similar the datasets are to each other.
This is especially useful if the constraint file is put under version control.

## Summary {#verify-summary}

FIXME: create concept map for verification

## Exercises {#verify-exercises}

FIXME: create exercises for verification

## Key Points {#verify-keypoints}

```{r, child="keypoints/verify.md"}
```

```{r, child="etc/links.md"}
```
